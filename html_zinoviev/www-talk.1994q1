From hotsand!rhb  Sat Jan  1 19:30:25 1994 EST
Message-Id: <9401020030.AA22637@hotsand.dacsand>
Date: Sat, 1 Jan 94 19:30:25 EST
From: hotsand!rhb (Rich Brandwein)
Subject: Authentication in Mosaic


Some quick questions on the authentication mechanism, at least as
implemented in Mosaic 2.x.  I can't seem to find any specific 
documentation on this subject.

Does Mosaic 2.x ever stop sending the authentication fields
to a server,  i.e., is the only way to ensure that a session
is closed to close the window?

Secondly, how many different servers can Mosaic 2.x authenticate
to within the same window/process?  Is it greater than 1?

Rich 

---------------------
Rich Brandwein
AT&T Bell Labs
rich.brandwein@att.com



From robm@ncsa.uiuc.edu  Sat Jan  1 19:04:44 1994 -0600
Message-Id: <9401020104.AA10101@void.ncsa.uiuc.edu>
Date: Sat, 1 Jan 1994 19:04:44 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Semicolon's for all


I feel sorry for Ari, wherever he is, getting back to all 3000 lines of this
argument.

/*
 * Re: Semicolon's for all  by Charles Henrich (henrich@crh.cl.msu.edu)
 *    written on Dec 31,  5:15pm.
 *
 * This whole argument is getting rather
 * silly, Im amazed at the massive pushback at adding something that wont break
 * anything, even when CGI is still in its infancy.  What happens in a year
 * when folks want to add something and CGI is well entrenched?

But it WILL break something! First of all, I don't see the benefits as being
compelling enough to merit the changing of the specification. The benefits
that I've seen brought up have been:

1. It does not require the server to grope through the filesystem with stat
calls.

2. It's cleaner and We Would Really Like It Better This Way.

#1 may be valid, but it has been shown that the groping can be reduced to
1-2 stats. #2 I consider an opinion judgement and not compelling enough to
merit a change to the spec.

Now, as far as breaking something, let's consider current implementations.
The CERN server has extra path information available to its htbin scripts.
NCSA httpd 1.0 as we're all painfully aware has extra path information
available. Both of these are transparent, i.e. they're both
/cgi-bin/script/arg1/arg2.

Let's say we decide to adopt the ; as the start of the extra path
information, and let's also say that the server, if it doesn't find a ; and
doesn't find a script at /cgi-bin/script/arg1/arg2, should perform the
backward compatible groping to find path information. Under this system,
someone using transparent path information which happened to have a ; in it
would have his/her script break.

Just like the problem of HTTP/0.9 documents just happening to start with the
string ``HTTP/1.0'', a seemingly trivial backward compatible change turns
out to not be so trivial. Are you starting to understand why we're so
``inflexible''?

So, it's all or nothing. We either change to mandating that path information
start with a ; or we don't. I have not seen a compelling reason why we
should throw the current scheme overboard and adopt the new one. Please,
provide one.

 * This morning I absolutely needed the semicolon syntax to accomplish what I
 * want in any clean manner.  

Since I cannot seem to find any references, can you please explain in
graphic detail what exactly you are trying to do, why doing it under the
current setup is impossible or difficult, and why the semicolon syntax makes
it all better? Some concrete examples of why this addition to the
specification are necessary would help the argument greatly.

 * It took exactly 2 additional lines of code to the
 * server. 

How much code needs to be added to any given server is completely irrelevant
to the decision of whether or not to change a protocol, unless the amount of
code to add is so large that it becomes prohibitive. It does break
something, it will cause confusion for a very LARGE base of script authors
(who now have to address their scripts in a new way), and I really don't see
just what will be gained by doing so except headaches for me answering tech
support questions!

 * Im still amazed at the inflexibilty you folks are imposing on
 * something so new, that hasnt even begun to be used in all possible
 * situations it has been designed for.  

Again, all of the arguments I've seen have been based on extremely
subjective terms like ``elegant'' and ``clean'' which I am still not
convinced I can agree with. 

Perhaps it could be percieved that I am being inflexible to new ideas, but
consider my viewpoint. Changing the specification for what appears to be a
trivial reason, with a change which WILL BREAK existing scripts is something
I consider only for the most compelling of reasons. I have not been
convinced that your changes do anything except help John sleep at night and
make your AFS server run a little faster. I'm looking for practical
situations and examples as to why the current specification is not good
enough and we need to add things to it.

By the way, when new and compelling ideas are brought up, then CGI/1.1 will
be designed, specified, and implemented. As more and more situations are
dealt with by script authors, I'm sure that these ideas will occur to more
and more people. Please understand that changing the specification,
especially one which is NOT backward compatible, is not something I consider
lightly.

 * Its okay, I dont mind adding 2 lines
 * of code to every new release, its too bad that everyone who finds themselves
 * in my position will also need to do so.
 */

Again, can you please tell us what your ``position'' is so that we can
understand why your arguments are compelling?

Anyway, in a later message, John Franks pointed out something interesting,
and I just went back and verified it. The current speicifcation says nothing
about how you're supposed to get the path information, it just says that the
information comes at the end of the path. Therefore, a server author like
John could conceivably decide to impose the restriction that path
information must start with a ; when accessing a script under GN and still
be fully compliant to CGI/1.0. 

It would disappoint me to have GN act in a very different fashion than other
servers since the entire point of this exercise was to make a unified
gateway interface, however, as I've argued in the past, the URL after the
third slash is the server's business. Therefore, if he wants to do that, he
is within the specification to do so.

--Rob



From robm@ncsa.uiuc.edu  Sat Jan  1 19:16:42 1994 -0600
Message-Id: <9401020116.AA10200@void.ncsa.uiuc.edu>
Date: Sat, 1 Jan 1994 19:16:42 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Authentication in Mosaic

/*
 * Authentication in Mosaic  by rhb@hotsand.att.com (rhb@hotsand.att.com)
 *    written on Jan  1,  7:30pm.
 *
 * 
 * Some quick questions on the authentication mechanism, at least as
 * implemented in Mosaic 2.x.  I can't seem to find any specific 
 * documentation on this subject.
 * 
 * Does Mosaic 2.x ever stop sending the authentication fields
 * to a server,  i.e., is the only way to ensure that a session
 * is closed to close the window?

I don't believe there is any way to flush the list of server information,
the only way I've found is to exit Mosaic. I remember asking Marc about this
a month ago and there wasn't any way to do it. So, Mosaic will continue
automatically sending the same authentication information for a particular
server when it asks for it until you exit.

 * Secondly, how many different servers can Mosaic 2.x authenticate
 * to within the same window/process?  Is it greater than 1?
 */

The libwww code uses a linked list to store the information about the
servers and therefore it can authenticate to a large number of different
servers.

--Rob



From john@math.nwu.edu  Sat Jan  1 20:28:56 1994 -0600 (CST)
Message-Id: <9401020228.AA02668@hopf.math.nwu.edu>
Date: Sat, 1 Jan 1994 20:28:56 -0600 (CST)
From: john@math.nwu.edu (John Franks)
Subject: Re: Semicolon's for all

According to Rob McCool:
> Anyway, in a later message, John Franks pointed out something interesting,
> and I just went back and verified it. The current speicifcation says nothing
> about how you're supposed to get the path information, it just says that the
> information comes at the end of the path. Therefore, a server author like
> John could conceivably decide to impose the restriction that path
> information must start with a ; when accessing a script under GN and still
> be fully compliant to CGI/1.0. 
> 
> It would disappoint me to have GN act in a very different fashion than other
> servers since the entire point of this exercise was to make a unified
> gateway interface, however, as I've argued in the past, the URL after the
> third slash is the server's business. Therefore, if he wants to do that, he
> is within the specification to do so.
> 

My only interest in implementing CGI is to make scripts written for
other servers usable with gn.  It would be silly to do an
implementation which wouldn't achieve that. What I was referring to was
something like requiring that for gn in the "path/cmd/args" part of
the URL the "cmd" must end in a ';'.  Thus a URL would look like

http://host/foo1/foo2;/foo3

Any other semi-colons would have to be URL encoded.  The PATH_INFO
would be "/foo3" and SCRIPT_NAME would be "/foo1/foo2;".  Of course, the
filesystem name would be "/data_root/foo1/foo2". This
is CGI compliant and I think it would work with all the scripts which
work under NCSA httpd. This syntax shouldn't disappoint Rob, but it
disappoints me.  I am still trying to think of a better way.


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu




From rst@ai.mit.edu  Sun Jan  2 10:12:10 1994 EST
Message-Id: <9401021512.AA05772@volterra>
Date: Sun, 2 Jan 94 10:12:10 EST
From: rst@ai.mit.edu (Robert S. Thau)
Subject: Re: Authentication in Mosaic

If you control the server in question, then there is a trick which lets
you make Mosaic forget the information --- create a directory with a
.htaccess file which *always* denies access, by, e.g., requiring membership
in an empty group.  Attempts to retrieve a file in that directory will
result in authentication failures, to which Mosaic responds by letting you
enter a new name and password.

But this is a kludge, and for foreign servers, I still don't know anything
that works.

rst



From hotsand!rhb  Sun Jan  2 19:24:34 1994 EST
Message-Id: <9401030024.AA29725@hotsand.dacsand>
Date: Sun, 2 Jan 94 19:24:34 EST
From: hotsand!rhb (Rich Brandwein)
Subject: Re: Authentication

> ---From Robert S. Thau:
> If you control the server in question, then there is a trick which lets
> you make Mosaic forget the information --- create a directory with a
> .htaccess file which *always* denies access, by, e.g., requiring
> membership
> in an empty group.  Attempts to retrieve a file in that directory will
> result in authentication failures, to which Mosaic responds by letting
> you
> enter a new name and password.
> 
> But this is a kludge, and for foreign servers, I still don't know
> anything
> that works.

This doesn't seem to work because of the answer to my second question:

> 
> ----From Rob McCool:
>  * Secondly, how many different servers can Mosaic 2.x authenticate
>  * to within the same window/process?  Is it greater than 1?
>  */
> 
> The libwww code uses a linked list to store the information about the
> servers and therefore it can authenticate to a large number of
> different
> servers.
> 

When you go back to the authenticated directory, you will still
be authenticated.

There is no indication in Mosaic as to which places you
may be currently authenticated to and the authentication session
never "times out".  People also tend to open up multiple Mosaic sessions
and then it becomes a complete question as to which is authenticated.

This is particularly a problem in placing Mosaic on public
workstations, where there may be users using the same Mosaic
application.  The real current cludge is to force users to 
completely close the window when they are done. 

Leaving "sessions" open even on personal workstations (which can be
locked) is also troublesome for people creating applications.
It would be nice if Mosaic could be set to flush this info about 
particular sessions after some period of time.

It seems that I could (if I bothered to figure out where the cache is
stored) run a local process to do this for me on the client
as an interim kludge. 

Rich

----
Rich Brandwein
AT&T Bell Labs
rich.brandwein@att.com



From bellverc@vents.uji.es  Mon Jan  3 08:24:34 1994 +0000
Message-Id: <9401030725.AA08303@dxmint.cern.ch>
Date: Mon, 3 Jan 1994 08:24:34 +0000
From: bellverc@vents.uji.es (Carles Bellver)
Subject: unsubscribe

Please remove me from your distribution list www-talk





From courtaud@limeil.cea.fr  Mon Jan  3 09:15:45 1994
Message-Id: <9401030815.AA00278@limeil.cea.fr>
Date: 03 Jan 94 09:15:45+0100
From: courtaud@limeil.cea.fr (Didier.Courtaud)
Subject: HTML convertor

I am looking for a convertor from the SGML language produced by
the software "The Publisher" from ArborText, Inc. to HTML ?

Has someone already made such a convertor ?

------------------------------------------------------------------------------
                            Didier  COURTAUD

                     Graphical Applications Group Leader
                      Commissariat a l'Energie Atomique
                   Direction des Applications Militaires
                  Departement de Mathematiques Appliquees
              Service Architectures Informatiques et Methodes
                  94195 Villeneuve Saint Georges Cedex 
                               France

 Phone : (33 - 1) 45 95 67 07
 Fax   : (33 - 1) 45 95 95 55
 Email : courtaud@limeil.cea.fr
-----------------------------------------------------------------------------





From mkrause@maestro.MITRE.ORG  Mon Jan  3 09:53:59 1994 -0500
Message-Id: <9401030954.ZM27900@maestro>
Date: Mon, 3 Jan 1994 09:53:59 -0500
From: mkrause@maestro.MITRE.ORG (Mark Krause)
Subject: Re: CGI, semicolons, and so on...

On Dec 30,  9:09pm, Charles Henrich wrote:
>
> I still say we go with the execute bit, 1000 times more flexible and simple
> than any other method on the planet.

What if I want to run a server on a DOS PC or a Macintosh?  I think it is
important to make sure that GCI is not OS specific.  The continued growth
of the Web is going to depend upon how easy it is to get new servers up
and running.  Not everyone is going to have access to a UNIX system for
this.




-- 
Mark A. Krause			mkrause@mitre.org
The MITRE Corporation		Mail Stop W273
7525 Colshire Drive		(703)883-7642 (Voice)		
McLean, VA 22102		(703)883-6478 (Fax)



From junga@informatik.tu-muenchen.de  Mon Jan  3 18:56:07 1994 +0100
Message-Id: <1994Jan3.175607.17727@Informatik.TU-Muenchen.DE>
Date: Mon, 3 Jan 1994 18:56:07 +0100
From: junga@informatik.tu-muenchen.de (Achim Jung)
Subject: rtftohtml for UNIX


Hi!

Is there a version of rtftohtml for UNIX Systems available?

I tried to compile rtftohtml V1.1 and V2.1, but every version
needs a include file "rtf.h", which is not available on HP and SUN.

Can anyone help me?

Ciao, Achim
-------------------------------------------------------------------
Achim Jung        IRC: Flops        junga@informatik.tu-muenchen.de
WWW: http://www.informatik.tu-muenchen.de/personen/junga/junga.html




From vinay  Mon Jan  3 15:26:42 1994 PST
Message-Id: <9401032326.AA02982@eit.COM>
Date: Mon, 3 Jan 94 15:26:42 PST
From: vinay (Vinay Kumar)
Subject: Web literature pointers

Can anyone point me to any published literature (plus Internet Drafts, RFC's)
on:
	HTTP Protocol.
	WWW Project.
	HTML+ Draft.
	URN/URL Draft.

Thanks for your help !

--
  Vinay Kumar
vinay@eit.com




From WIGGINS@msu.edu  Mon Jan  3 18:59:37 1994 EST
Message-Id: <9401040011.AA08437@dxmint.cern.ch>
Date: Mon, 03 Jan 94 18:59:37 EST
From: WIGGINS@msu.edu (Rich Wiggins)
Subject: Re: CGI, semicolons, and so on...

>On Dec 30,  9:09pm, Charles Henrich wrote:
>>
>> I still say we go with the execute bit, 1000 times more flexible and simple
>> than any other method on the planet.
>
>What if I want to run a server on a DOS PC or a Macintosh?  I think it is
>important to make sure that GCI is not OS specific.  The continued growth
>of the Web is going to depend upon how easy it is to get new servers up
>and running.  Not everyone is going to have access to a UNIX system for
>this.

I think this is a valid point -- the issue of how hard it is for new,
relatively naive users to set servers up is a factor that should
be part of all these discussions.

And for that reason it seems to me that the specification of a script
and its parameters should be explicitly, visibly different than
pointing to a file to be delivered.  This discussion is a lot like
the question of whether a programming language should have implicit
typing or not.  A lot of the arguments raised seem to tout opaqueness
as a virtue in its own right; although it doesn't seem necessary to
make it possible for users to tell that a script is running instead
of a file being served, I don't understand why it's important to hide
that fact from them.

It also seems to me that requiring all script-type documents to be
isolated in a "this is for scripts" subdirectory is unnecessarily
confining to the server author.  Wouldn't it be better to be able
to store related flat HTML files and associated scripts in one
place?

It seems a suffix of .cgi may be a good compromise.  Then it doesn't
matter whether the separator is a / or a ;.

/Rich Wiggins, CWIS Coordinator, Michigan State U



From rivero@sol.unizar.es  Tue Jan  4 13:37:48 1994 GMT
Message-Id: <199401041337.AA16896@sol.unizar.es>
Date: Tue, 4 Jan 1994 13:37:48 GMT
From: rivero@sol.unizar.es (Alejandro Rivero)
Subject: Re: ismap functionality...

  
>    From: Tony Sanders <sanders@bsdi.com>
 
> 
>    Mainly because you forgot to allow for circles, bitmapped objects, arbitrary
>    polygons, external spatial indexers, etc.  The server can be very flexible
>    but the client cannot, any finite client side scheme you implement will
>    not be sufficient.  HTML+ defines polygon support with <FIGA> but it isn't
>    yet implemented anywhere.
 
Speaking of this, it would be interesting to know if are there currently some
project to implement FIGA. A lot of people is drawing ISMAP maps of
WWW zones, and if FIGA is to be released soon by someone, I feel all
the work on ismap drawings is wasted. Client support is more powerful,
as it would let, for example, click on HyperCard-stile "transparent" buttons.
And a drawing is not really need, as button coordinates would be enought
for the client to draw the "virtual maps"

			Alejandro



From rivero@sol.unizar.es  Tue Jan  4 13:46:34 1994 GMT
Message-Id: <199401041346.AA16927@sol.unizar.es>
Date: Tue, 4 Jan 1994 13:46:34 GMT
From: rivero@sol.unizar.es (Alejandro Rivero)
Subject: Re: Returned mail: User unknown MET


----- Begin Included Message -----

>From Mailer-Daemon@www0.cern.ch Tue Jan  4 13:44:05 1994
Received: from dxmint.cern.ch by sol.unizar.es with SMTP id AA16921
  (5.67a8/IDA-1.5 for rivero); Tue, 4 Jan 1994 13:43:52 GMT
Received: from www0.cern.ch by dxmint.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA01652; Tue, 4 Jan 1994 13:42:23 +0100
Received: by www0.cern.ch (5.0/SMI-4.0)
	id AB07489; Tue, 4 Jan 1994 13:37:04 --100
Date:    
 
  
>    From: Tony Sanders <sanders@bsdi.com>
 
> 
>    Mainly because you forgot to allow for circles, bitmapped objects, arbitrary
>    polygons, external spatial indexers, etc.  The server can be very flexible
>    but the client cannot, any finite client side scheme you implement will
>    not be sufficient.  HTML+ defines polygon support with <FIGA> but it isn't
>    yet implemented anywhere.
 
Speaking of this, it would be interesting to know if are there currently some
project to implement FIGA. A lot of people is drawing ISMAP maps of
WWW zones, and if FIGA is to be released soon by someone, I feel all
the work on ismap drawings is wasted. Client support is more powerful,
as it would let, for example, click on HyperCard-stile "transparent" buttons.
And a drawing is not really need, as button coordinates would be enought
for the client to draw the "virtual maps"

			Alejandro
 




From dsr@hplb.hpl.hp.com  Tue Jan  4 13:02:13 1994 GMT
Message-Id: <9401041302.AA26874@manuel.hpl.hp.com>
Date: Tue, 4 Jan 94 13:02:13 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: "internal-gopher-image"?!?

>> My vote:

>> <IMG SRC=/INLINE/icon01.gif>

> Here's another proposal:
>        Invent a new kind of reference.
>        Call it a `name' instead of a `location'.
>        MMM, how about URN instead of URL.

>        Others might retrieve them every time and just cache them
>        for a short time.

I am currently planning a caching scheme for URL's as our network
operators are concerned about the cost of all this traffic - we
pay $$ per megabyte received. The basic idea is to have a UDP
server for managing the cache while data transfer to/from the cache
is done via nfs. The cache server just serves file names.
Even a simple scheme looks like offering a good pay-off. The Expires:
header can be used to override admin defaults for purging the cache.

Dave



From mueller@sc.ZIB-Berlin.DE  Tue Jan  4 15:53:13 1994 +0100
Message-Id: <9401041453.AA01658@ave.ZIB-Berlin.DE>
Date: Tue, 4 Jan 94 15:53:13 +0100
From: mueller@sc.ZIB-Berlin.DE (Peter Mueller)
Subject: question about layout

Hello,

I've recognized the following problem, when specifying

<code><i>filename</i>.rec</code>

within a sentence. Under some circumstances the result looks like (in
Mosaic)

bla bla bla bla bla filename
.rec bla bla bla

although there is *no* space between the specification. Is this
word wrapping really necessary? I choose the specification above
to indicate, that the part 'filename' is to be replaced by an actual
value. So it *must* appear as (in the case of Mosaic):

filmname.rec
        ^^^^
            code-style
^^^^^^^^
        ---- italics (or emphasized, whatever)

Is there a possibility to keep words un-wrapped?

Hope you all have reached 1994 in a well manner,
Thanks for your help,

Peter Mueller



From sanders@BSDI.COM  Tue Jan  4 10:26:20 1994 -0600
Message-Id: <199401041626.KAA26253@austin.BSDI.COM>
Date: Tue, 04 Jan 1994 10:26:20 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: CGI, semicolons, and so on... 

> as a virtue in its own right; although it doesn't seem necessary to
> make it possible for users to tell that a script is running instead
> of a file being served, I don't understand why it's important to hide
> that fact from them.
The problem is that if you are already serving
a large tree of documents, e.g.:
	http://server/man/1/ls
	http://server/man/1/cat
and you wish to insert a script somewhere (to add searching or virtuality).
You are screwed if you must change all references (e.g., documents on
other servers that you don't own, many of which you probably don't even
know exist) to your documents because there is some special convention
for running a script that requires a change in the URL.

The current scheme will work on ANY platform.  The CGI spec doesn't say
anything about how the server should decide what is what.  Plexus uses
a config file to determine this, and doesn't require any stat()'s or
execute bits, any platform could do this, even MACs.

--sanders



From dsr@hplb.hpl.hp.com  Tue Jan  4 17:03:34 1994 GMT
Message-Id: <9401041703.AA27235@manuel.hpl.hp.com>
Date: Tue, 4 Jan 94 17:03:34 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Attaching docs, Virtual images

Joel Richardson writes:

> Can anyone help me with the following two problems?

> 1. In a forms application that we're developing, the bulk of what the
> user must enter usually exists in a text file already. The form can
> include a textarea, of course, but then the user either has to retype
> the data or cut and paste. Neither is particularly attractive. Is there
> a better way to do this currently? (Something like <INPUT TYPE="attach" ...>
> suggests itself, but far be it from me to suggest yet another feature ! :-)

People have also asked if it will be possible to paste hypertext and
other MIME formats into form fields. We definitely need to migrate forms
to use a multipart MIME based format for transferring data to the server.
I am currently writing up a proposal for this, along with some new ideas
for supporting dynamic forms, whereby the server can send updates together
with a status message to a form as a result of the user clicking a selection
or checkbox. Sending updates avoid the problems inherent in loading an
entirely new document, when you just want to change a few fields.

The <INPUT TYPE="attach" ...> idea seems a useful addition, which allows
users to enter a local file name. Browsers would have to work out the
appropriate content type to use when encoding the file as a part in a
multipart MIME transfer document. I will include this in the new draft.

Cheers,

Dave Raggett



From dolesa@smtp-gw.spawar.navy.mil  Tue Jan  4 12:15:13 1994 EDT
Message-Id: <9400047577.AA757714513@smtp-gw.spawar.navy.mil>
Date: Tue, 04 Jan 94 12:15:13 EDT
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: A/UX Compiling httpd 1.0 


When I tried to compile httpd_1.0 under A/UX 3.02 (Apple's Unix),
I get this:

dolomite.root # make
        gcc -c -g -DSUNOS4 http_config.c
Make: *** Error: Can't load 'gcc' (No such file or directory)
Make:            [line 57 in /httpd_1.0/src/Makefile]

Make: *** Error: Update of `http_config.o' terminated with exit code 1
Make:            [line 57 in /httpd_1.0/src/Makefile]


What next?  Gotta start somewhere...

                Andre' Doles
                Systems Administrator
                Space & Naval Warfare Systems Command HQ



From dsr@hplb.hpl.hp.com  Tue Jan  4 19:04:18 1994 GMT
Message-Id: <9401041904.AA27516@manuel.hpl.hp.com>
Date: Tue, 4 Jan 94 19:04:18 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Additions to the CGI archive

Martijn Koster writes:

> - Did the discussion on built-in IMG's result into anything
>   for HTML+ ? If not, can we advance the discussion?

We are all waiting impatiently for URN's as the long term
solution for this. Of course, browsers can use an internal
syntax for including icons in ftp directory listings etc.
provided this doesn't clash with legal URLs.

Maybe we ought to come up with a de facto URN mechanism, e.g.


        URN             ::=     DOMAINS [; SELECTOR]*
        DOMAINS         ::=     DOMAIN . DOMAINS

The DOMAIN is an opaque alphanumeric string which is used
to track down the domain servers a la DNS and identify the
individual URN.

In this model, URN's can stand for single objects or for
families of related objects, which can be selected from
according to the value of the SELECTORS, e.g. language,
most recent version, ...

The binding process converts such URNs into one or more
URLs plus info differentiating family members and giving
other useful info, e.g. ownership, date created, ...

> - Can a DT in a DL have multiple DD's? Mosaic copes nicely, 
>   but HTML+ talsk about <B>pairs</B>..

In the forthcoming revision to the HTML+ DTD I allow zero or
more DTs per DD which seems to be whats needed. I am also well
on the way to a working HTML+ browser. Including the capability
to support multiple hardware colormaps, so that you never run
out of colors.

Dave



From wei@sting.Berkeley.EDU  Tue Jan  4 16:03:04 1994 -0800
Message-Id: <9401050003.AA24155@sting.Berkeley.EDU>
Date: Tue, 4 Jan 94 16:03:04 -0800
From: wei@sting.Berkeley.EDU (Pei Y. Wei)
Subject: Re: ismap functionality...

> Speaking of this, it would be interesting to know if are there currently some
> project to implement FIGA.

ViolaWWW has FIGA, thou only of the rectangular kind at the moment.
Please contact me if you're interested in alpha testing it.  

-Pei



From dsr@hplb.hpl.hp.com  Wed Jan  5 10:18:41 1994 GMT
Message-Id: <9401051018.AA29531@manuel.hpl.hp.com>
Date: Wed, 5 Jan 94 10:18:41 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Which client is already supporting FIG?

Alejandro Rivero writes:

> Can anyone out there suggest a WWW client or HTML+ parser 
> implementing FIG directive?

You may wish to read: ftp://15.254.100.100/pub/draft-raggett-www-html-00.ps

I am currently rewriting the draft and will publish a revised version soon.
The spec for FIG has changed slightly following the WWW/TEI meeting last
November at Cork/Ireland. The IMAGE element described in the above document
was dropped. The new version looks like:

  <!-- figures which subsume the role of the earlier IMG element.

    Behaves identically to IMG for align = top, middle or bottom.
    Otherwise figure is inserted after next line break (soft or hard).
    For align=left, the image is left aligned and text is flowed
    on the right of the image, and similarly for align=right, with
    no text flow for align=center (the default). The caption is
    placed under the image.

    The <A> element is used for shaped buttons handled by browser,
    while the ISMAP mechanism sends pointer clicks/drags to server.
    The text contained by this element is used for text-only displays
    and authors should remember to provide effective descriptions,
    including label text for shaped buttons.
  -->
  <!ELEMENT FIG - - (CAPTION?,(%text;)*)>
  <!ATTLIST FIG
        id      ID      #IMPLIED
        align   (top|middle|bottom|left|center|right) center -- position --
        ismap   (ismap) #IMPLIED -- server can handle mouse clicks/drags --
        src     %URL;   #IMPLIED -- link to image data --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

  <!ELEMENT CAPTION - - (%text;)+ -- table or figure caption -->
  <!ATTLIST CAPTION
        id      ID      #IMPLIED
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

The FIG element is now formally allowed anywhere that IMG is permitted.
Shaped buttons are defined, as before, using the SHAPE attribute of the
<A> element to define shaped hypertext buttons, which can also be rendered
in the conventional way for text only displays. The spec includes code for
hit testing polygons specified by the shape attribute. I am working on an
HTML+ browser for X11/XLib which will be ready in early spring `94.

> I find it more powerful that the ismap atribute, as there are 
> no interaction with the server, and the figt and figa tags have
> other important result: no source .gif is needed (except to
> define size and proportion of the box, if the FIGD mechanism is not
> implemented). If geografical maps are going to proliferate, it will
> be a really needed tag.

The FIGT/FIGA elements were dropped following the WWW Wizards Workshop
last year. People felt that the FIGT overlay mechanism needed more thought
and should be deferred. The FIGA shaped buttons were replaced by the SHAPE
attribute on the <A> element as explained above for greater consistency.

Dave Raggett



From dsr@hplb.hpl.hp.com  Wed Jan  5 17:00:28 1994 GMT
Message-Id: <9401051700.AA00527@manuel.hpl.hp.com>
Date: Wed, 5 Jan 94 17:00:28 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Initializing HTTP headers from HTML documents

I would like to propose a scheme that HTTP servers can use to
initialize HTTP headers by reading information held at the start of
HTML/HTML+ documents. This is intended for fields like Expires: which
are best determined by the document author.

The generic META element takes the following attributes:

        NAME    the name of an HTTP header such as Expires
        VALUE   the value to be passed with the associated header


e.g.  <META NAME="Expires" VALUE="Tue, 04 Jan 1994 14:13:25 GMT">

This element is only permitted as part of the document's HEAD
along with TITLE, ISINDEX and LINK.

Any comments?

Dave Raggett




From hotsand!ellson  Wed Jan  5 12:59:01 1994 EST
Message-Id: <9401051759.AA07042@hotsand.dacsand>
Date: Wed, 5 Jan 94 12:59:01 EST
From: hotsand!ellson (John Ellson)
Subject: Re:  Initializing HTTP headers from HTML documents

> From: Dave_Raggett <dsr@hplb.hpl.hp.com>
> 
> I would like to propose a scheme that HTTP servers can use to
> initialize HTTP headers by reading information held at the start of
> HTML/HTML+ documents. This is intended for fields like Expires: which
> are best determined by the document author.
> 
> The generic META element takes the following attributes:
> 
>         NAME    the name of an HTTP header such as Expires
>         VALUE   the value to be passed with the associated header
> 
> 
> e.g.  <META NAME="Expires" VALUE="Tue, 04 Jan 1994 14:13:25 GMT">
> 
> This element is only permitted as part of the document's HEAD
> along with TITLE, ISINDEX and LINK.
> 
> Any comments?
> 
> Dave Raggett

Dave,

I agree that authors would like to expire their documents at some
point in the future, even if those documents are being served
from a cache site.  But I don't see how a timestamp is sufficiently
expressive of the reasons an author might have to expire a document.
The reasons might not even be known at the time that the document
is written.
 
Could VALUE be a boolean expression so that we could do something like:
 
   <META NAME="Expires" VALUE=EXISTSP(<http://original.host/original.file>)>
 
or:
 
   <META NAME="Expires" VALUE=GT(DATE, "Tue, 04 Jan 1994 14:13:25 GMT")>
 
There would need to be a way of preventing caches from cacheing the
result of EXISTSP. 
 
Also, clients need to do something reasonable if the boolean
expression cannot be evaluated.  Perhaps they could display the page
anyway but with a warning message saying that "the expiry status
of this document is unknown." 
 
 
John Ellson
AT&T Bell Labs



From jeff.grover@gtri.gatech.edu  Wed Jan  5 13:27:34 1994 -0500
Message-Id: <199401051827.AA18122@gtri.gatech.edu>
Date: Wed, 05 Jan 1994 13:27:34 -0500
From: jeff.grover@gtri.gatech.edu (Jeffrey L. Grover)
Subject: HTML authoring tool using Word for Windows


Seeing the traffic resulting from Howard Harawitz's request 
for an FTP site to host his "Windows HTML Edit Assistant"
promted me to do the following.

1.  Uploaded the GT_HTML tool to the ftp sites offered:

        ftp.law.cornell.edu/Incoming/gt_html.zip
        ftp.demon.co.uk:/pub/ibmpc/winsock/incoming/gt_html.zip

        (I hope Tom Bruce and Steve Kennedy see that I've done
        this, I also hope they don't mind me *barging* into
        their ftp sites like this {:-O)

2. Re-issue the following news release:

        GT_HTML.DOT, (ver 0.1a1) Release Notice

        The Georgia Tech Research Institute (GTRI) is pleased 
        to announce the initial alpha release of a set of 
        Microsoft Word for Windows macros to facilitate  
        HTML document authoring.  The macros are contained 
        in a document template (GT_HTML.DOT) which provides 
        a pseudo  WYSIWYG authoring environment.  
 

3. Confirm the following are still active:

        read about it at:   http://www.gatech.edu/word_html/release.htm 

        download it from:   ftp://ftp.gatech.edu/pub/www/gt_html.zip

+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
Jeffrey L. Grover         voice:  (404)894-8961
Ga Tech/GTRI/ITTL         fax:    (404)894-9081
347 Ferst Dr. N.W.             noman@gatech.edu
Atlanta, Ga 30332   jeff.grover@gtri.gatech.edu




From peveritt@pandora.ncts.navy.mil  Wed Jan  5 13:53:36 1994 +0600
Message-Id: <9401051953.AA01169@voltaire.ncts.navy.mil>
Date: Wed, 5 Jan 1994 13:53:36 +0600
From: peveritt@pandora.ncts.navy.mil (Paul Everitt)
Subject: WinMosaic can't attatch to server


Having an interesting situation, here are the symptoms:

1) remote WinMosaic running on PD or Trumpet can't attatch by HTTP to 
my httpd1.0 server running on Solaris2.3
2) same people in same scenario can attatch via WinMosaic to my gopher
server, running on the same machine as the HTTP server
2) WinMosaic locally works fine
3) WinMosaic over SLIP on the people in (1)'s machines works fine
4) MacMosaic on the people in (1)'s machines works fine
5) XMosaic on the people in (1)'s machines works fine

More symptoms: when the remote WinMosaic people go to my URL, they 
*do* go from "Connecting" to "Reading Response", and they do show
up on my access log, but the don't ever get any packets back.

Strange, eh?

--Paul



From john@math.nwu.edu  Wed Jan  5 13:56:56 1994 -0600 (CST)
Message-Id: <9401051956.AA07333@hopf.math.nwu.edu>
Date: Wed, 5 Jan 1994 13:56:56 -0600 (CST)
From: john@math.nwu.edu (John Franks)
Subject: Implementing CGI

I am still confused about (at least) one implementation issue for a
CGI compliant server, namely, when to put something on the command
line that invokes the script.

According to the spec this should be done only for ISINDEX items.
Looking at the source of imagemap, I see that it assumes the query is
in argv[1], but it is normally not an ISINDEX item.  The suggested way
to tell if something is an ISINDEX item is to see if there are +'s in
the QUERY, or to see that there are no ='s in the QUERY.  But what
happens if an ISINDEX query never contains an =, or if a form returns
a value containing '+'.

The spec also says that if a server decides a QUERY cannot be put on
the command line for security reasons it should be put in QUERY_STRING
untouched and the command line should be empty.  Presumably a good
CGI program expecting an ISINDEX should then go to QUERY_STRING if
there are no args, but I haven't seen examples of this.

Here is what I would like to see the spec mean (and I think this is
consistent with what it says, just not with current practice).  The
QUERY_STRING should be parsed and put on the command line *only* if
it contains a '+' (and it is deemed safe).  Presumably the only reason
to use the command line at all is so the server can do some parsing
for simple scripts.  This achieves that.  If an ISINDEX script sees
no args it should know that there is just one arg and it is in 
QUERY_STRING.  A form processing script for which a value containing
a '+' is returned would get stuff on the command line, but would ignore
it.


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu




From fielding@simplon.ICS.UCI.EDU  Thu Jan  6 00:59:09 1994 -0800
Message-Id: <9401060059.aa16159@paris.ics.uci.edu>
Date: Thu, 06 Jan 1994 00:59:09 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Initializing HTTP headers from HTML documents 

>> From: Dave_Raggett <dsr@hplb.hpl.hp.com>
>> 
>> I would like to propose a scheme that HTTP servers can use to
>> initialize HTTP headers by reading information held at the start of
>> HTML/HTML+ documents. This is intended for fields like Expires: which
>> are best determined by the document author.
>> 
>> The generic META element takes the following attributes:
>> 
>>         NAME    the name of an HTTP header such as Expires
>>         VALUE   the value to be passed with the associated header
>> 
>> 
>> e.g.  <META NAME="Expires" VALUE="Tue, 04 Jan 1994 14:13:25 GMT">
>> 
>> This element is only permitted as part of the document's HEAD
>> along with TITLE, ISINDEX and LINK.
>> 
>> Any comments?
>> 
>> Dave Raggett

I like this scheme since it makes it much easier on the server than
our previous discussions about OWNER and DATE elements.  It also allows
for site-specific additions to the headers without requiring special
changes to the server for each added header.

My only concern is that it allows the author to free-format information
which should normally appear in a fixed (specified) format.  However,
I think clients should be robust enough to just throw away bad headers.

I do wonder, however, what would happen if an author included
<META NAME="Content-Type" VALUE="text/bogus;"> as an odd joke, but I
can't think of any intentional spoofing that could adversely effect a client
other than just not being able to display the object.


> From: John Ellson <ellson@hotsand.att.com>
> 
> I agree that authors would like to expire their documents at some
> point in the future, even if those documents are being served
> from a cache site.  But I don't see how a timestamp is sufficiently
> expressive of the reasons an author might have to expire a document.
> The reasons might not even be known at the time that the document
> is written.

I don't understand why the reason is important.  The expires header should
indicate the date beyond which that information object may no longer be
"true" (or applicable or useful or whatever).  In any case, the only thing
the cache needs to know is when to get rid of it.  Since any boolean
expression would only make sense if evaluated on the server side 
(i.e. not by the cache manager), I don't see any reason why the cache
manager would want to see the boolean expression in the header.

If the author does not know the expires date, then it should either not
appear at all or be assigned separately by the server (using whatever
tables/logic that such a site would consider desirable).  This is, I believe,
how such things are handled in netnews (which is how the HTTP spec defines
the purpose of the Expires: header).

> Could VALUE be a boolean expression so that we could do something like:
>  
>    <META NAME="Expires" VALUE=EXISTSP(<http://original.host/original.file>)>
>  
> or:
>  
>    <META NAME="Expires" VALUE=GT(DATE, "Tue, 04 Jan 1994 14:13:25 GMT")>
>  
> There would need to be a way of preventing caches from cacheing the
> result of EXISTSP. 
>  
> Also, clients need to do something reasonable if the boolean
> expression cannot be evaluated.  Perhaps they could display the page
> anyway but with a warning message saying that "the expiry status
> of this document is unknown." 

The vast majority of information objects will have no expires header.
I think it would be more appropriate if the client simply displayed
the expiration date (if any) in a place for secondary feedback information,
such as the information line at the bottom of Mosaic for X windows.


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From fielding@simplon.ICS.UCI.EDU  Thu Jan  6 01:10:28 1994 -0800
Message-Id: <9401060110.aa16596@paris.ics.uci.edu>
Date: Thu, 06 Jan 1994 01:10:28 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Initializing HTTP headers from HTML documents 


>> From: Dave_Raggett <dsr@hplb.hpl.hp.com>
>> 
>> I would like to propose a scheme that HTTP servers can use to
>> initialize HTTP headers by reading information held at the start of
>> HTML/HTML+ documents. This is intended for fields like Expires: which
>> are best determined by the document author.
>> 
>> The generic META element takes the following attributes:
>> 
>>         NAME    the name of an HTTP header such as Expires
>>         VALUE   the value to be passed with the associated header
>> 
>> 
>> e.g.  <META NAME="Expires" VALUE="Tue, 04 Jan 1994 14:13:25 GMT">
>> 
>> This element is only permitted as part of the document's HEAD
>> along with TITLE, ISINDEX and LINK.
>> 
>> Any comments?
>> 
>> Dave Raggett

I like this scheme since it makes it much easier on the server than
our previous discussions about OWNER and DATE elements.  It also allows
for site-specific additions to the headers without requiring special
changes to the server for each added header.

My only concern is that it allows the author to free-format information
which should normally appear in a fixed (specified) format.  However,
I think clients should be robust enough to just throw away bad headers.

I do wonder, however, what would happen if an author included
<META NAME="Content-Type" VALUE="text/bogus;"> as an odd joke, but I
can't think of any intentional spoofing that could adversely effect a client
other than just not being able to display the object.


> From: John Ellson <ellson@hotsand.att.com>
> 
> I agree that authors would like to expire their documents at some
> point in the future, even if those documents are being served
> from a cache site.  But I don't see how a timestamp is sufficiently
> expressive of the reasons an author might have to expire a document.
> The reasons might not even be known at the time that the document
> is written.

I don't understand why the reason is important.  The expires header should
indicate the date beyond which that information object may no longer be
"true" (or applicable or useful or whatever).  In any case, the only thing
the cache needs to know is when to get rid of it.  Since any boolean
expression would only make sense if evaluated on the server side 
(i.e. not by the cache manager), I don't see any reason why the cache
manager would want to see the boolean expression in the header.

If the author does not know the expires date, then it should either not
appear at all or be assigned separately by the server (using whatever
tables/logic that such a site would consider desirable).  This is, I believe,
how such things are handled in netnews (which is how the HTTP spec defines
the purpose of the Expires: header).

> Could VALUE be a boolean expression so that we could do something like:
>  
>    <META NAME="Expires" VALUE=EXISTSP(<http://original.host/original.file>)>
>  
> or:
>  
>    <META NAME="Expires" VALUE=GT(DATE, "Tue, 04 Jan 1994 14:13:25 GMT")>
>  
> There would need to be a way of preventing caches from cacheing the
> result of EXISTSP. 
>  
> Also, clients need to do something reasonable if the boolean
> expression cannot be evaluated.  Perhaps they could display the page
> anyway but with a warning message saying that "the expiry status
> of this document is unknown." 

The vast majority of information objects will have no expires header.
I think it would be more appropriate if the client simply displayed
the expiration date (if any) in a place for secondary feedback information,
such as the information line at the bottom of Mosaic for X windows.


.....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From dsr@hplb.hpl.hp.com  Thu Jan  6 10:40:54 1994 GMT
Message-Id: <9401061040.AA03731@manuel.hpl.hp.com>
Date: Thu, 6 Jan 94 10:40:54 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re:  Initializing HTTP headers from HTML documents

John Ellson writes:

> I agree that authors would like to expire their documents at some
> point in the future, even if those documents are being served
> from a cache site.  But I don't see how a timestamp is sufficiently
> expressive of the reasons an author might have to expire a document.
> The reasons might not even be known at the time that the document
> is written.
 
> Could VALUE be a boolean expression so that we could do something like:
 
>   <META NAME="Expires" VALUE=EXISTSP(<http://original.host/original.file>)>
 
> or:
 
>   <META NAME="Expires" VALUE=GT(DATE, "Tue, 04 Jan 1994 14:13:25 GMT")>

It would be better to use a different attribute name than VALUE if the server
needs to evaluate some expression to compute the value string. I would
suggest EVAL. Note that the definition of Expires: is given in the HTTP
spec and is derived from the its use in News articles. So in this case
the expression must result in a date in the HTTP date format.

Since EVAL expressions would be evaluated by the server, it seems reasonable
to come up with a consensus approach like CGI to ensure a degree of
standardisation. Would you like to come up with a proposal?

Dave Raggett



From m.koster@nexor.co.uk  Wed Dec 22 15:56:18 1993 +0000
Message-Id: <9312221627.AA16476@dxmint.cern.ch>
Date: Wed, 22 Dec 1993 15:56:18 +0000
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: Additions to the CGI archive


> in the next release of NCSA httpd the directory indexing will be overhauled
> and with that a default icons directory of/icons/ will be included in the
> standard distribution.

But then they still need to be transferred. I would prefer to see a set
of magic URL's specified in the HTML+ spec for a number of images such as
folder, menu, file, bin file, text file etc. 

We can either adopt Mosaic's image names, or define a new set.
If people are unhappy with the messing with the http URL name space, 
why not use a new URL scheme internal:image/folder or something?

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From robm@ncsa.uiuc.edu  Wed Dec 22 09:29:18 1993 -0600
Message-Id: <9312221529.AA09312@void.ncsa.uiuc.edu>
Date: Wed, 22 Dec 1993 09:29:18 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Additions to the CGI archive

/*
 * Re: Additions to the CGI archive  by Guenther Fischer (guenther.fischer@hrz.tu-chemnitz.de)
 *    written on Dec 21,  2:31pm.
 *
 * YES - I`ve tried to contribute this with no response. The third try:
 * 
 * Inline images could be very nice. There should be a small set of
 * standardized images to give them to clients as well as to servers.
 * 
 * Clients could know and include them without tranfers. Servers should
 * have them at the server database - the name should be something like
 * IMG SRC=/inline/internal-gopher-binary
 */

On the server side at least, in the next release of NCSA httpd the directory
indexing will be overhauled, and with that a default icons directory of
/icons/ will be included in the standard distribution. I am planning to use
the icons from Mosaic for some of them.

--Rob




From guenther.fischer@hrz.tu-chemnitz.de  Thu Jan  6 13:58:16 1994 +0100 (MET)
Message-Id: <9401061258.AA01519@flash1.hrz.tu-chemnitz.de>
Date: Thu, 6 Jan 1994 13:58:16 +0100 (MET)
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: Re: Additions to the CGI archive

> Would the inclusion of images in the client significantly increase the size
> of the executable?  There have already been several comments in my office

The images could be in a library path .../lib/Mosaic/icons and loaded as
needed.
On the server side too - for clients who don't have this feature. 
This should only a small set of icons like lines, bitmaps for
directories, files, some pointers, ...

> over the large size of the PC Mosaic.  It seems wiser to include stock images
> in server distribution -- perhaps a set of images that the various server
> developers might informally agree on.  Then with the arrival of URN's, each
> image would be transferred once and cached by the client for the extent of
> the session regardless of type or number of servers visited.
> (assuming I understand URN's properly.)

When will URNs come up? I think we have to wait for it some times ...

	~Guenter

-- 
Name:      Guenther Fischer
Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361
mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From nikos@cbl.leeds.ac.uk  Thu Jan  6 13:06:46 1994 GMT
Message-Id: <26782.9401061306@cblelca.cbl.leeds.ac.uk>
Date: Thu, 6 Jan 94 13:06:46 GMT
From: nikos@cbl.leeds.ac.uk (nikos@cbl.leeds.ac.uk)
Subject: HTML icon set was: Additions to the CGI archive

>> in the next release of NCSA httpd the directory indexing will be overhauled
>> and with that a default icons directory of/icons/ will be included in the
>> standard distribution.
>
>But then they still need to be transferred. I would prefer to see a set
>of magic URL's specified in the HTML+ spec for a number of images such as
>folder, menu, file, bin file, text file etc. 
>

Apart from images such as those above which are used in documents
that are generated on the fly, there are a lot of other images
which are used as navigation icons in authored or converted documents,
e.g. next page, previous page, up group, index, contents, information/help,
home page, get code, mail author, audio link, movie link, etc.

In some cases these icons are well designed (e.g. GNN) but for those 
of us lacking artistic flair it would be better to have a set of 
standard icons that could also become part of the HTML+ spec. 

This will reduce the amount of traffic and give a more consistent look
and feel to HTML documents. Of course nobody will be obliged to use them.

So if anyone is designing an HTML icon set (is anyone doing that?)
I think it would be a good idea to consider this larger set.

>-- Martijn

Nikos.
__
Nikos Drakos			
Computer Based Learning Unit   	nikos@cbl.leeds.ac.uk
University of Leeds		http://cbl.leeds.ac.uk/nikos/personal.html




From guenther.fischer@hrz.tu-chemnitz.de  Thu Jan  6 14:41:01 1994 +0100 (MET)
Message-Id: <9401061341.AA01648@flash1.hrz.tu-chemnitz.de>
Date: Thu, 6 Jan 1994 14:41:01 +0100 (MET)
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: Re: HTML icon set was: Additions to the CGI archive

> Apart from images such as those above which are used in documents
> that are generated on the fly, there are a lot of other images
> which are used as navigation icons in authored or converted documents,
> e.g. next page, previous page, up group, index, contents, information/help,
> home page, get code, mail author, audio link, movie link, etc.
> 
> In some cases these icons are well designed (e.g. GNN) but for those 
> of us lacking artistic flair it would be better to have a set of 
> standard icons that could also become part of the HTML+ spec. 
> 
> This will reduce the amount of traffic and give a more consistent look
> and feel to HTML documents. Of course nobody will be obliged to use them.
> 
> So if anyone is designing an HTML icon set (is anyone doing that?)
> I think it would be a good idea to consider this larger set.
> 

Yeah - thats what I want to say - you have the better english words :-).

What says the author of the HTML+ draft and the WWW gurus. A signal from
there could help to initiate first steps.

	~Guenther

-- 
Name:      Guenther Fischer
Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361
mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From dkulp@gdb.org  Thu Jan  6 07:34:25 1994 EST
Message-Id: <9401061234.AA10395@dev.gdb.org>
Date: Thu, 6 Jan 94 07:34:25 EST
From: dkulp@gdb.org (David Kulp)
Subject: Re: Additions to the CGI archive

> From: Martijn Koster <m.koster@nexor.co.uk>
>
> > in the next release of NCSA httpd the directory indexing will be overhauled
> > and with that a default icons directory of/icons/ will be included in the
> > standard distribution.
> 
> But then they still need to be transferred. I would prefer to see a set
> of magic URL's specified in the HTML+ spec for a number of images such as
> folder, menu, file, bin file, text file etc. 
> 
> We can either adopt Mosaic's image names, or define a new set.
> If people are unhappy with the messing with the http URL name space, 
> why not use a new URL scheme internal:image/folder or something?

Would the inclusion of images in the client significantly increase the size
of the executable?  There have already been several comments in my office
over the large size of the PC Mosaic.  It seems wiser to include stock images
in server distribution -- perhaps a set of images that the various server
developers might informally agree on.  Then with the arrival of URN's, each
image would be transferred once and cached by the client for the extent of
the session regardless of type or number of servers visited.
(assuming I understand URN's properly.)

-david kulp
dkulp@gdb.org



From hoesel@chem.rug.nl  Thu Jan  6 18:02:12 1994 +0100 (MET)
Message-Id: <9401061702.AA00941@Xtreme>
Date: Thu, 6 Jan 1994 18:02:12 +0100 (MET)
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Re: HTML icon set was: Additions to the CGI archive

> 
> >> in the next release of NCSA httpd the directory indexing will be overhauled
> >> and with that a default icons directory of/icons/ will be included in the
> >> standard distribution.
> >
> >But then they still need to be transferred. I would prefer to see a set
> >of magic URL's specified in the HTML+ spec for a number of images such as
> >folder, menu, file, bin file, text file etc. 
> >
> 
> Apart from images such as those above which are used in documents
> that are generated on the fly, there are a lot of other images
> which are used as navigation icons in authored or converted documents,
> e.g. next page, previous page, up group, index, contents, information/help,
> home page, get code, mail author, audio link, movie link, etc.
> 
> In some cases these icons are well designed (e.g. GNN) but for those 
> of us lacking artistic flair it would be better to have a set of 
> standard icons that could also become part of the HTML+ spec. 
> 
> This will reduce the amount of traffic and give a more consistent look
> and feel to HTML documents. Of course nobody will be obliged to use them.
> 
> So if anyone is designing an HTML icon set (is anyone doing that?)
> I think it would be a good idea to consider this larger set.
> 
 why not forget about the icons completly and make a much more flexible
way for storing images like icons on a local disk. perhaps a kind of
URL cache implemented in the browser. but then the permanent type of
cache: simply a list of URL of wich the contents can be found on
local disk.
as an example
say I want to retrieve the EXPO ticket_office.html it contains an URL for
the inlined bitmap. let the browser look up this URL in its local table; if
it finds it there, then use the local version; if it doesn't find it there
simply retrieve it anyway. People will need to store there most popular
images by hand into the local disksystem (a sysadmin would do that for you)
or some smarter interface fo storing them.

nothing realy special is needed, and you don't depend on somebody's choice
of what would be an often needed icon. I formyself almost never see the
file icon, but I could include GNN's icons.

-frans







From m.koster@nexor.co.uk  Thu Jan  6 19:15:07 1994 +0000
Message-Id: <9401061915.AA22544@dxmint.cern.ch>
Date: Thu, 06 Jan 1994 19:15:07 +0000
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Redirection: "Location" or "Uri" ?


Hi all,

I was playing around with redirection today, and got conflicting info
on how it is done: both draft-ietf-iiir-http-00.txt and 
http://info.cern.ch/hypertext/WWW/Technical.html talk about

	Uri: <url> String CrLf

but Mosaic for X and httpd seem to use

	Location: <url>

Why are these contradictory? Which one is right?

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From m.koster@nexor.co.uk  Thu Jan  6 20:33:05 1994 +0000
Message-Id: <9401062033.AA05047@dxmint.cern.ch>
Date: Thu, 06 Jan 1994 20:33:05 +0000
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: HTML icon set was: Additions to the CGI archive


frans van hoesel wrote:
> why not forget about the icons completly...
> permanent type of cache: simply a list of URL of wich the contents 
> can be found on local disk.

This doesn't solve the problem, you could end up with lots of

	http://some.machine/gifs/file.gif
	http://some.oher.machine/icons/file.gif
	...

on your disk. 

Dave Ragget wrote:
> We are all waiting impatiently for URN's as the long term
> solution for this. 

URN's are a good idea, but in order for a browser to map a URN to an
internal image you still need to define a set of URNs for "standard" icons. 
So we might as well do that with URL's, which are here now, and use
URN equivalents when they arrive. Or am I missing something?

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From dolesa@smtp-gw.spawar.navy.mil  Thu Jan  6 13:21:02 1994 EDT
Message-Id: <9400067578.AA757891262@smtp-gw.spawar.navy.mil>
Date: Thu, 06 Jan 94 13:21:02 EDT
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: Compiling XMosaic 2.1 under A/UX


     Many thanks to all who helped me get httpd_1.0 compiled and running 
     under A/UX 3.02.  Not bad, it only took a total of 3 days!  Last night 
     I successfully compiled it with gcc (because Apple's standard cc 
     compiler isn't Ansi C compatible).  Again, MANY thanks to all the 
     replies in helping me accomplish my goal.
     
     Now, I would like to compile xMosaic 2.1 under A/UX.  Does anyone have 
     the patched Makefile for A/UX?  What else is necessary?
     
     Thanks in advance.
     
                                Andre' Doles
                                Space & Naval Warfare Systems Command HQ



From hotsand!ellson  Thu Jan  6 15:57:41 1994 EST
Message-Id: <9401062057.AA13697@hotsand.dacsand>
Date: Thu, 6 Jan 94 15:57:41 EST
From: hotsand!ellson (John Ellson)
Subject: Re:  Initializing HTTP headers from HTML documents

Dave Ragget writes

> It would be better to use a different attribute name than VALUE if the server
> needs to evaluate some expression to compute the value string. I would
> suggest EVAL. Note that the definition of Expires: is given in the HTTP
> spec and is derived from the its use in News articles. So in this case
> the expression must result in a date in the HTTP date format.
> 
> Since EVAL expressions would be evaluated by the server, it seems reasonable
> to come up with a consensus approach like CGI to ensure a degree of
> standardisation. Would you like to come up with a proposal?

I could try.  But let me make sure that we agree on what problem needs
solving.

I thought you were addressing a cache coherency problem:

	before serving a document a cache server should check if 
	the original document still exists and if it is unchanged.

	if it no longer exists the no document should be served
	and the cache copy should be purged.

	if it has changed then the cache should be refreshed and the
	new version served.

	if the original site cannot be contacted then the document
	should be served with a warning that the validity of the
	document could not be verified.

	if the document still exists unchanged then the cache copy can
	be served.

It seems that the Expires mechanism is a little more hand-offish than
this.  Perhaps Expires provides sufficient coherency control?

If Expires is an optional attribute provided by the author then what
prevents stale copies staying indefinitely in cache servers?

If it turns out that we agree that something is required to make cache
servers operate correctly, then should the additional mechanism be
user visible, or should it be built into the client-server protocols?


John Ellson



From hoesel@chem.rug.nl  Thu Jan  6 22:26:11 1994 +0100 (MET)
Message-Id: <9401062126.AA02465@Xtreme>
Date: Thu, 6 Jan 1994 22:26:11 +0100 (MET)
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Re: HTML icon set was: Additions to the CGI archive 

From hoesel Thu Jan  6 22:24:44 1994
Subject: Re: HTML icon set was: Additions to the CGI archive
To: m.koster@nexor.co.uk (Martijn Koster)
Date: Thu, 6 Jan 1994 22:24:44 +0100 (MET)
In-Reply-To: <9401062057.AA02406@Xtreme> from "Martijn Koster" at Jan 6, 94 08:33:05 pm
X-Mailer: ELM [version 2.4 PL5]
Content-Type: text
Content-Length: 1596      

> 
> 
> frans van hoesel wrote:
> > why not forget about the icons completly...
> > permanent type of cache: simply a list of URL of wich the contents 
> > can be found on local disk.
> 
> This doesn't solve the problem, you could end up with lots of
> 
> 	http://some.machine/gifs/file.gif
> 	http://some.oher.machine/icons/file.gif
> 	...
you could!, but you shouldn't; if a particular document writer chooses
to use some icon, then he should request it from the right place on the net
(preferable the sire that created the icon). This situation is comparable
with copying documents for local retrieval. you don't want that unless
you retrieved them very often. 

the description is a bit vague, but iamge that you happen to like expo
you might want to retrieve the ticketoffice html page every time you 
visit the expo, but if you would choose to copy it localy, then that's
your choise. nobody copies the whole of expo to their own site and
just the same with images (or icons): you shouldn't copy them, but
use them by refering to the URL.

indeed URN would be a good idea and solve the problem completely. there is
no need to define standard URN. if the URN is the same as you store locally;
then there is no need to resolve the URN or search the standard set; just
use the local copy on your disk.

this way you can still store whatever icons/text/images you want.
the problem is in another thread, about how to expire a local document.
But for icons that might not be a big problem. If the icon appear bogus at
some point; simply clear your local copies; and they will reload.

- frans








From ZAPANTIS@uvphys.phys.UVic.CA  Thu Jan  6 13:44:34 1994 -0800 (PST)
Message-Id: <940106134434.25000070@uvphys.phys.UVic.CA>
Date: Thu, 6 Jan 1994 13:44:34 -0800 (PST)
From: ZAPANTIS@uvphys.phys.UVic.CA (Nik Zapantis, UVic Physics, Victoria BC)
Subject: HomePage URL without a file name.How?

I've noticed that the URL for the home page of some sites, is just the site
name;
eg: the URL for the CERN home page is just "http://info.cern.ch"

I have started running an HTTP server on my VMS system, and I would like to
have my home page defined as above, but I have not been able to find the
appropriate info in the Docs I have.
Is there a default filename that WWW and/or Mosaic look for, when the URL is
just the node name? I have tried naming my home page HOME.HTML, WELCOME.HTML,
DEFAULT.HTML, WWW_HOME.HTML but without any results.
I have no problem reading my home page if the URL contains the filename with
the node name.

thanks,
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
| Nik Zapantis                  | zapantis@uvphys     (BITNET)  |
| Dept. of Physics & Astronomy  | 45393::zapantis(HEPnet/SPAN)  |
| University of Victoria        | zapantis@uvphys.phys.UVic.CA  |
| Victoria, BC                  | Phone: (604)721-7729          |
| V8W 3P6                       | FAX:   (604)721-7715          |
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 



From dolesa@smtp-gw.spawar.navy.mil  Thu Jan  6 17:43:12 1994 EDT
Message-Id: <9400067579.AA757906992@smtp-gw.spawar.navy.mil>
Date: Thu, 06 Jan 94 17:43:12 EDT
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: Re: HomePage URL without a file name.How?


     
It took me a while to figure that one out too.  Index.html is the default
name.  If you don't have a document named that, it will create a directory
index for you, thus bypassing your intended home page.  This is not 
documented very clearly.  Hope this helps.

                        Andre' Doles
                        Space & Naval Warfare Systems Command HQ
                        D.C.
______________________________ Reply Separator _________________________________
Subject: HomePage URL without a file name.How?
Author:  ,"Nik Zapantis, UVic Physics, Victoria BC (604)721-7729" 
<ZAPANTIS@uvphys.phys.UVic.CA> at SMTP-GW
Date:    1/6/94 4:54 PM


I've noticed that the URL for the home page of some sites, is just the site 
name;
eg: the URL for the CERN home page is just "http://info.cern.ch"
     
I have started running an HTTP server on my VMS system, and I would like to 
have my home page defined as above, but I have not been able to find the 
appropriate info in the Docs I have.
Is there a default filename that WWW and/or Mosaic look for, when the URL is 
just the node name? I have tried naming my home page HOME.HTML, WELCOME.HTML, 
DEFAULT.HTML, WWW_HOME.HTML but without any results.
I have no problem reading my home page if the URL contains the filename with 
the node name.
     
thanks,
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
| Nik Zapantis                  | zapantis@uvphys     (BITNET)  | 
| Dept. of Physics & Astronomy  | 45393::zapantis(HEPnet/SPAN)  | 
| University of Victoria        | zapantis@uvphys.phys.UVic.CA  | 
| Victoria, BC                  | Phone: (604)721-7729          | 
| V8W 3P6                       | FAX:   (604)721-7715          | 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
     
     



From dolesa@smtp-gw.spawar.navy.mil  Thu Jan  6 18:09:27 1994 EDT
Message-Id: <9400067579.AA757908567@smtp-gw.spawar.navy.mil>
Date: Thu, 06 Jan 94 18:09:27 EDT
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: reply to HomePage URL without a file name. How?


     
It took me a while to figure that one out too.  Index.html is the default 
name.  If you don't have a document named that, it will create a directory 
index for you, thus bypassing your intended home page.  This is not 
documented very clearly.  Hope this helps.
     
                        Andre' Doles
                        Space & Naval Warfare Systems Command HQ 
                        D.C.
______________________________ Reply Separator _________________________________
Subject: HomePage URL without a file name.How?
Author:  ,"Nik Zapantis, UVic Physics, Victoria BC (604)721-7729" 
<ZAPANTIS@uvphys.phys.UVic.CA> at SMTP-GW
Date:    1/6/94 4:54 PM
     
     
I've noticed that the URL for the home page of some sites, is just the site 
name;
eg: the URL for the CERN home page is just "http://info.cern.ch"
     
I have started running an HTTP server on my VMS system, and I would like to 
have my home page defined as above, but I have not been able to find the 
appropriate info in the Docs I have.
Is there a default filename that WWW and/or Mosaic look for, when the URL is 
just the node name? I have tried naming my home page HOME.HTML, WELCOME.HTML, 
DEFAULT.HTML, WWW_HOME.HTML but without any results.
I have no problem reading my home page if the URL contains the filename with 
the node name.
     
thanks,
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
| Nik Zapantis                  | zapantis@uvphys     (BITNET)  | 
| Dept. of Physics & Astronomy  | 45393::zapantis(HEPnet/SPAN)  | 
| University of Victoria        | zapantis@uvphys.phys.UVic.CA  | 
| Victoria, BC                  | Phone: (604)721-7729          | 
| V8W 3P6                       | FAX:   (604)721-7715          | 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
     
     
     



From dolesa@smtp-gw.spawar.navy.mil  Thu Jan  6 18:52:45 1994 EDT
Message-Id: <9400067579.AA757911165@smtp-gw.spawar.navy.mil>
Date: Thu, 06 Jan 94 18:52:45 EDT
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: Inline Images under A/UX won't display.


     Problem:  I used to run Mac http server and moved my html's and gif's 
     over to NCSA http server running under A/UX.  Well, now, the image 
     transfers, but refuses to show up on the screen.  I ftp'd a gif from a 
     live server that I know the gif works on, and that gif will not show 
     up when referenced from my server.  When I 
     http://dolomite.spawar.navy.mil/navy.gif, lview 3.1 reports back: 
     Invalid or unsupported Targa file.  The file is clearly named navy.gif 
     and opens locally with lview 3.1.  What's happening?
     
     Images will not display inline from Xmosaic, MacMosaic, or WinMosaic.
     
     I know the pictures are not corrupt gif's because they open locally 
     under Mac & PC as gif's.  I'm clueless.
     
                                      Andre'



From dale@ora.com  Thu Jan  6 16:44:38 1994 -0800
Message-Id: <9401061644.ZM3712@rock.west.ora.com>
Date: Thu, 6 Jan 1994 16:44:38 -0800
From: dale@ora.com (Dale Dougherty)
Subject: Re: HTML icon set was: Additions to the CGI archive

On Jan 6,  1:06pm, nikos@cbl.leeds.ac.uk wrote:
} Subject: HTML icon set was: Additions to the CGI archive
>>> in the next release of NCSA httpd the directory indexing will be overhauled
>>> and with that a default icons directory of/icons/ will be included in the
>>> standard distribution.
>>
>>But then they still need to be transferred. I would prefer to see a set
>>of magic URL's specified in the HTML+ spec for a number of images such as
>>folder, menu, file, bin file, text file etc. 
>>
>
>Apart from images such as those above which are used in documents
>that are generated on the fly, there are a lot of other images
>which are used as navigation icons in authored or converted documents,
>e.g. next page, previous page, up group, index, contents, information/help,
>home page, get code, mail author, audio link, movie link, etc.
>
>In some cases these icons are well designed (e.g. GNN) but for those 
>of us lacking artistic flair it would be better to have a set of 
>standard icons that could also become part of the HTML+ spec. 
>
>This will reduce the amount of traffic and give a more consistent look
>and feel to HTML documents. Of course nobody will be obliged to use them.
>
>So if anyone is designing an HTML icon set (is anyone doing that?)
>I think it would be a good idea to consider this larger set.
>
>>-- Martijn
>
>Nikos.
>__
>Nikos Drakos			
>Computer Based Learning Unit   	nikos@cbl.leeds.ac.uk
>University of Leeds		http://cbl.leeds.ac.uk/nikos/personal.html
>
}-- End of excerpt from nikos@cbl.leeds.ac.uk

If there's sufficient interest in a common icon library,
I'd be willing to involve our graphic designers who developed
the icons for GNN.  A common library of icons would help users 
have a consistent navigational interface. 

What our designers would need (besides more time) is a list
of the icons and their desired functions.  I recommend that
we start with a small list of functions (next, previous, up). 

The only problem I see in proposing this solution is how
do we get an agreement on such a library?  Perhaps the best
way to do it is to make available a library of icons, identify
the library so that other libraries can be used, and if folks don't
like the existing library, new ones can be developed.  (This
does seem, after all, like the Web way.)

I'd appreciate any comments on what basic icons are required.  
As I said earlier, I'd also like to get some sense whether 
creating this icon library is worthwhile in the first place
before committing to develop it.

Dale

-- 
Dale Dougherty (dale@ora.com) 
Publisher, Global Network Navigator, O'Reilly & Associates, Inc.
103A Morris Street, Sebastopol, California 95472 
(707) 829-3762 (home office); 1-800-998-9938



From hotsand!rhb  Thu Jan  6 20:52:12 1994 EST
Message-Id: <9401070152.AA20069@hotsand.dacsand>
Date: Thu, 6 Jan 94 20:52:12 EST
From: hotsand!rhb (Rich Brandwein)
Subject: More CGI Comments


After playing with CGI-based httpd servers for awhile and writing scripts
to them, I have the following observations/questions:

1) If you let users export information via their UserDir (i.e., ~/public_html
by default), how can you gracefully allow them to create anything that requires
a shell execution without giving everyone write access to the cgi-bin
directory or creating cgi aliases for all users in srm.conf?

2) To get at any of the authentication information (e.g., the $REMOTE_USER variable)
it seems that my pages that want to use any of this info need to all become shell scripts
(which means that they'll need to be in cgi-bin type directories).  Once I authenticate
someone, it seems that I generally want to know the user on every page served in many
apps (in fact, it would certainly be nice to log this info - I can't differentiate
authenticated users from the log file if they're coming from the same server...).

3) Because of (1), (2) and my general preferences of arranging files, I find it would be
much easier to identify executables on the server side by being able to use a server
defined suffix (notwithstanding the previous arguments against this) for these files
(e.g., .cgi).  

Rich

----
Rich Brandwein
AT&T Bell Labs
rich.brandwein@att.com



From Paul.Wain@brunel.ac.uk  Fri Jan  7 09:14:59 1994 +0000 (GMT)
Message-Id: <14193.9401070914@thor.brunel.ac.uk>
Date: Fri, 7 Jan 1994 09:14:59 +0000 (GMT)
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Re: HomePage URL without a file name.How?

@ eg: the URL for the CERN home page is just "http://info.cern.ch"

@ I have started running an HTTP server on my VMS system, and I would like to
@ have my home page defined as above, but I have not been able to find the
@ appropriate info in the Docs I have.
@ Is there a default filename that WWW and/or Mosaic look for, when the URL is
@ just the node name? I have tried naming my home page HOME.HTML, WELCOME.HTML,
@ DEFAULT.HTML, WWW_HOME.HTML but without any results.
@ I have no problem reading my home page if the URL contains the filename with
@ the node name.

Here is a bit more of a detailed reply than the others that people have
submitted so far. Sorry to those of you who this is repeating
information for.

The home URL here at Brunel is:
	
	http://http1.brunel.ac.uk:8080

I am assuming that the directives are pretty similar in the version of
the daemon that you are using. I shall walk through this as for NCSA's
httpd 1.0.

The above was set up in the httpd configuration files as:

1) Set your DocumentRoot to the correct directory. Im assuming that you
already have this. In my case this was in srm.conf as: 
	
	DocumentRoot /vol/PACK/brunel-html-data

2) ***KEY BIT*** Now modify the DirectoyIndex value. The srm.conf file
with the NCSA httpd_1.0 gives this as ``Name of the file to use as a
pre-written HTML directory index'' or the default file to be loaded
rather than giving the directory listing. I modified mine to be:

	DirectoryIndex home.html

3) All I then did was in the directory ``/vol/PACK/brunel-html-data''
make sure that a file ``home.html'' exist. Now when someone requests
just the URL of Brunel given above, they get the brunel home page.

Paul

p.s. By the same way, the URL in my .signature actually gets the files
from:
		http://http2.brunel.ac.uk:8080/~ccsrpsw/home.html

.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|                 http://http2.brunel.ac.uk:8080/paul                     |
`-------------------------------------------------------------------------'



From courtaud@limeil.cea.fr  Mon Jan  7 08:56:45 1994
Message-Id: <9401070756.AA01680@limeil.cea.fr>
Date: 07 Jan 94 08:56:45+0100
From: courtaud@limeil.cea.fr (Didier.Courtaud)
Subject: HTML +

Does someone can tell me where I can get the actual "official" specs
on the Internet about HTML+ ?

Thanks

------------------------------------------------------------------------------
                            Didier  COURTAUD


 Phone : (33 - 1) 45 95 67 07
 Fax   : (33 - 1) 45 95 95 55
 Email : courtaud@limeil.cea.fr
-----------------------------------------------------------------------------





From J.Larmouth@iti.salford.ac.uk  Mon Jan  7 09:46:00 1994
Message-Id: <9401070957.AA13545@dxmint.cern.ch>
Date: 7 Jan 94 9:46
From: J.Larmouth@iti.salford.ac.uk (J.Larmouth@iti.salford.ac.uk)
Subject: Useful icons or standard icons?

=========================================================================
E-mail from: Prof J Larmouth              J.Larmouth @ ITI.SALFORD.AC.UK
             Director                       Telephone: +44 61 745 5657
             IT Institute                         Fax: +44 61 745 8169
             University of Salford              Telex: 668680 (Sulib)
             Salford M5 4WT
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

To:     www-talk @ info.cern.ch

Subject:      Useful icons or standard icons?

I am not at all sure that "heavyweight" standardisation of icons is right
for WWW at the present time.  On the other hand, I have a lot of sympathy
with the idea of making a commonly used set of icons readily available
for HTML authors.

The problem with "standardisation" is that

        a)      we don't really know what set of icons we want,  and
        adding new ones or changing the graphics for a "standard" set
        will be difficult;

        b)      we certainly don't yet have the best graphics for the
        icons we need - real artists need to get involved!;

        c)      different styles and sizes of graphic for icons with the
        same meaning might be appropriate for different document "styles"
        (artistic material might want more "flowery" icons than
        scientific material,  and some subject areas might want
        icons of a common style related to the subject;  small and larger
        versions of most icons are probably needed).

The following HTML page works in Windows Mosaic (TRY IT!),  and
illustrates what I think would be a more appropriate approach.

I think that all we need is a "well-known" site (or sites) to host a
"Useful ICONs" page,  based on the prototype page given below.  (Obvious
sites are those that provide WWW software).  Note that the icons should
actually be copied to local files and should be "moderated",  and that
some assurance of continuous and continued access to the icons should be
given.  They should also all be massaged to a consistent size.

Any offers to host such a page?

Note that HTML authors can either reference the URLs of the icons in
someone's "Useful ICONs" page,  or can copy them to local files.

**** None of this addresses the question of minimising network traffic,
but I really feel this is pretty irrelevant for ICON access. ****

Here follows the draft prototype HTML page (no doubt a real host site
would improve it!):

<title>Useful ICONs for HTML authors</title>
<p>
<h1>Useful ICONs</h1>
<p>
<h2>Commitment and disclaimer</h2>
<p>
The following sets of icons are intended for use in HTML documents.  The
icons have been provided by a number of sources,  but are stored locally
and can be referenced by
"http://xxx.yyy.zzz/icons/<i>icon-name</i>.<i>image-type</i>".
<p>
The HTTP server containing this document and these icons is normally
available on a
twenty-four hour basis,  and every attempt will be made to ensure
continuous access.
<p>
Nonetheless,  no guarantees of continued availability can be given.
<p>
<h2>Use by authors</h2>
<p>
These icons can either be referenced in an HTML document (in which case
the latest versions will always be obtained) or can be copied to local
files (providing protection against permanent or temporary withdrawal of
this system from the Net).   The former approach is recommended.
<p>
<h2>Contribution of new or improved icons</h2>
<p>
The owners of this WWW page will add new useful icons,   new versions of
the current icons in different image formats (as image standards
change),  and alternative or improved graphics for current icons.
<p>
URLs for new or improved icons should be sent to ........ for possible
inclusion as a "useful icon".   The decision of the authors of this page
on the inclusion of new icons or the replacement of icon graphics is
final.
<p>
<h2>Icons currently available</h2>
<p>
The following "well-known" icons are currently available:
<p>
<a href="http://www.ncsa.uiuc.edu/demoweb/sound.xbm"><img
src="http://www.ncsa.uiuc.edu/demoweb/sound.xbm">
</a><pre>   </pre>This icon is called <i>sound</i>,  and is available in
<i>xbm</i>
and <i>gif</i> and <i>bmp</i> formats.   It is intended for use in an
anchor that points to a sound recording.
<p>
<a href="http://www.hcc.hawaii.edu/dinos/next.gif"><img
src="http://www.hcc.hawaii.edu/dinos/next.gif">
</a><pre>   </pre>This icon is called <i>next</i>,  and is available in
<i>xbm</i>
and
<i>gif</i> and <i>bmp</i> formats.   It is intended for use in an anchor
that points to the next item in a set of related pieces of information.
<p>
<a href="http://www.hcc.hawaii.edu/dinos/prev.gif"><img
src="http://www.hcc.hawaii.edu/dinos/prev.gif">
</a><pre>   </pre>This icon is called <i>prev</i>,  and is available in
<i>xbm</i>
and
<i>gif</i> and <i>bmp</i> formats.   It is intended for use in an anchor
that points to the previous item in a set of related pieces of
information.
<p>
<a href="http://www.hcc.hawaii.edu/dinos/table.gif"><img
src="http://www.hcc.hawaii.edu/dinos/table.gif">
</a><pre>   </pre>This icon is called <i>table</i>,  and is available in
<i>xbm</i>
and
<i>gif</i> and <i>bmp</i> formats.   It is intended for use in an anchor
that points to the table of contents for a set of related pieces of
information.
<p>
<a href="http://www.hcc.hawaii.edu/dinos/home.gif"><img
src="http://www.hcc.hawaii.edu/dinos/home.gif">
</a><pre>   </pre>This icon is called <i>home</i>,  and is available in
<i>xbm</i>
and
<i>gif</i> and <i>bmp</i> formats.   It is intended for use in an anchor
that points to the "top" of a set of related pieces of information,  or
to a site's home page.
<p>
Etc Etc.
<p>




From m.koster@nexor.co.uk  Fri Jan  7 10:49:41 1994 +0000
Message-Id: <9401071058.AA19185@dxmint.cern.ch>
Date: Fri, 07 Jan 1994 10:49:41 +0000
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: Useful icons or standard icons?


> The problem with "standardisation" is that
> 
>         a)      we don't really know what set of icons we want,  and
>         adding new ones or changing the graphics for a "standard" set
>         will be difficult;

Sure.

>         b)      we certainly don't yet have the best graphics for the
>         icons we need - real artists need to get involved!;

No, but if you have standard names for icons, you can use your favourite
physical icon for it.
 
>         c)      different styles and sizes of graphic for icons with the
>         same meaning might be appropriate for different document "styles"
>         (artistic material might want more "flowery" icons than
>         scientific material,  and some subject areas might want
>         icons of a common style related to the subject; 

Well, of course you can never cover all cases with a general mechanism.
I actaully think different platforms may want different icons: a Mac
client wants hypercard icons, a Windows one program-manager icons etc.

> I think that all we need is a "well-known" site (or sites) to host a
> "Useful ICONs" page,  based on the prototype page given below.  (Obvious
> sites are those that provide WWW software).  Note that the icons should
> actually be copied to local files and should be "moderated",  and that
> some assurance of continuous and continued access to the icons should be
> given.  They should also all be massaged to a consistent size.

This is what Frans van Hoesal proposed too.

So we have an access protocol, (http), a "well-know-site", and a name.
That looks like a URL to me. You want people to start using these,
so you give assurance they will be available. That sounds pretty much
like the standard URLs I propose.

> Note that HTML authors can either reference the URLs of the icons in
> someone's "Useful ICONs" page,  or can copy them to local files.
> 
> **** None of this addresses the question of minimising network traffic,
> but I really feel this is pretty irrelevant for ICON access. ****

Well, I don't think it is irrelevant. I happen to have slow'ish internet
access, and I often have to wait for icons. But if we have this set of
well-known URL's, we can actually have clients intervene and use local 
copies.

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From m.koster@nexor.co.uk  Fri Jan  7 09:33:26 1994 +0000
Message-Id: <9401071054.AA15608@dxmint.cern.ch>
Date: Fri, 07 Jan 1994 09:33:26 +0000
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: HTML icon set was: Additions to the CGI archive

> [Dale offering to have the GNN designers design a set of icons]

That sounds promising :-)

> I recommend that we start with a small list of functions
> (next, previous, up)

plainfile, binaryfile, folder, search, help ?

> how do we get an agreement on such a library?

Well, that's why I want to resolve it with standard URL's. Your client
can then map the standard URL's to whatever icon library you want,
(So you don't need agreement :-) or use internal ones (so it's fast).
And of course, as Frans van Hoesel suggested, clients should make sure
they cache these things.

Pitty I'm not a client writer, then I could stop chatting and implement 
something :-).

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From koblas@netcom.com  Fri Jan  7 04:14:21 1994 -0800
Message-Id: <199401071214.EAA11635@mail.netcom.com>
Date: Fri, 7 Jan 1994 04:14:21 -0800
From: koblas@netcom.com (David Koblas)
Subject: HTML and end tags


As best I understand HTML has some optional end tags for markup.
(i.e. it is optional to have a </P> after a <P>).

Since presently I'm writing a HTML syntax checker (actually full
base parser), and I'm trying to track down all areas of "fuzzyness"
in an HTML document.  The <P> .. </P> is simply to ignore/understand,
but things get a little problimatic when faced with:
        <A NAME="Some Nice Name">
Since this is an anchor tag it really should have an </A> associated
with it, but they don't.  So at the very least this brings up the
question of what is the meaning of the following:

        <PRE>
        <A HREF=reference>this is some text
        <A NAME=here>that is all</A>
        an anchor.</A>
        </PRE>

Or some equally similar contrived example.  Now from the working HTML+
spec, I see that NAME= is now ID=, but the same question still lurks.
Is it required to have an ending mark, thus one correctly needs to
specify <A ID=tag></A> to insert a simple reference?

Thanks,
--koblas@netcom.com



From fielding@simplon.ICS.UCI.EDU  Fri Jan  7 05:12:14 1994 -0800
Message-Id: <9401070512.aa06857@paris.ics.uci.edu>
Date: Fri, 07 Jan 1994 05:12:14 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Inline Images under A/UX won't display. 


Andre' <dolesa@smtp-gw.spawar.navy.mil> writes:
>
>      Images will not display inline from Xmosaic, MacMosaic, or WinMosaic.

That sounds like the same problem I am experiencing with 2-color GIFs
and Mosaic for X 2.1.  They were fine in 2.0, but now they display as
blank on monochrome screens and sometimes blank/sometimes just faint grey
(depending on the image) on color screens.  I'd bet this has something
to do with the GIF89 enhancement.

I notified NCSA (but I can imagine they are pretty busy right now) and
I have a test page setup on <http://www.ics.uci.edu/Test/> if anyone
wants to try their hand at debugging.  Please let me know the results.


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From bert@let.rug.nl  Fri Jan  7 16:01:28 1994 +0100 (MET)
Message-Id: <9401071501.AA18562@freya.let.rug.nl>
Date: Fri, 7 Jan 1994 16:01:28 +0100 (MET)
From: bert@let.rug.nl (Bert Bos)
Subject: Re: HTML and end tags

 |As best I understand HTML has some optional end tags for markup.
 |(i.e. it is optional to have a </P> after a <P>).
 |
 |Since presently I'm writing a HTML syntax checker (actually full
 |base parser), and I'm trying to track down all areas of "fuzzyness"
 |in an HTML document.  The <P> .. </P> is simply to ignore/understand,
 |but things get a little problimatic when faced with:
 |        <A NAME="Some Nice Name">
 |Since this is an anchor tag it really should have an </A> associated
 |with it, but they don't.  So at the very least this brings up the

According to the draft DTD for HTML+, the </a> end tag *is*
obligatory, unlike the </p>.

Since HTML+ will be defined by the DTD (unlike HTML, where a DTD was
added as an afterthought), there can be no "fuzzyness" in the syntax.
(Semantics is another matter.)

Btw., an HTML+ checker is nice, but why don't you use "sgmls"? It is
an SGML parser, so by implication it is also an HTML+ parser. In fact,
I use it routinely to check (and even generate!) the pages for our
soon to be announced WWW server.


Bert
-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|



From sjweiler@bsu-cs.bsu.edu  Fri Jan  7 08:09:02 1994 EST
Message-Id: <9401071309.AA09618@bsu-cs.bsu.edu>
Date: Fri, 7 Jan 94 8:09:02 EST
From: sjweiler@bsu-cs.bsu.edu (Steven Jul Weiler)
Subject: WWW Client/Server for dial-up line

	i THINK i SAW SOMEWHERE ON THE NET; THAT THERE WAS A WWW CLIENT THAT
YOU COULD USE ON DIAL-UP LINES AND I THINK IT RAN in Windows. Any suggestions.
My college is moving toward 14.4K modems on it's dial-up lines.



From guenther.fischer@hrz.tu-chemnitz.de  Thu Dec 23 18:20:55 1993 +0100 (MET)
Message-Id: <9312231720.AA07648@flash1.hrz.tu-chemnitz.de>
Date: Thu, 23 Dec 1993 18:20:55 +0100 (MET)
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: Re: Additions to the CGI archive

> 
> > in the next release of NCSA httpd the directory indexing will be overhauled
> > and with that a default icons directory of/icons/ will be included in the
> > standard distribution.
> 
> But then they still need to be transferred. I would prefer to see a set
> of magic URL's specified in the HTML+ spec for a number of images such as
> folder, menu, file, bin file, text file etc. 
> 
> We can either adopt Mosaic's image names, or define a new set.
> If people are unhappy with the messing with the http URL name space, 
> why not use a new URL scheme internal:image/folder or something?
> 

If we get an well defined icon set this should be a feature for clients
and servers. I put the iconlib to the server for clients who don't have
it. Clients who have the "standard" library don't need transfers.

What do you mean with internal:image/folder?

Means internal: a ressource the client must have. This is a very hard
decission. I think the way to define it as feature is better.

As a document writer I'll be sure the client see what I mean ...

	~Guenther

PS: Happy Christmas and a Happy new year  www-talkers!

-- 
Name:      Guenther Fischer
Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361
mail:      fischer@hrz.tu-chemnitz.de



From john@math.nwu.edu  Fri Jan  7 09:26:02 1994 -0600 (CST)
Message-Id: <9401071526.AA09270@hopf.math.nwu.edu>
Date: Fri, 7 Jan 1994 09:26:02 -0600 (CST)
From: john@math.nwu.edu (John Franks)
Subject: Re: More CGI Comments

According to rhb@hotsand.att.com:
> 
> 3) Because of (1), (2) and my general preferences of arranging files, I find it would be
> much easier to identify executables on the server side by being able to use a server
> defined suffix (notwithstanding the previous arguments against this) for these files
> (e.g., .cgi).  
> 
> Rich
> 
> ----
> Rich Brandwein
> AT&T Bell Labs
> rich.brandwein@att.com
> 

Despite, having been vigorously involved in the "previous arguments 
against this", I nevertheless am implementing exactly this in the 
gn server.  I finally decided that this is the least of several evils.
The winning argument for me was that this is the simplest scheme 
for server maintainers to understand which is also consitent with the
current CGI standard.


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu




From neuss@igd.fhg.de  Fri Jan  7 16:20:58 1994 +0100
Message-Id: <9401071520.AA00925@wildturkey.igd.fhg.de>
Date: Fri, 7 Jan 94 16:20:58 +0100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: Useful icons or standard icons?

Hiyall,

> The problem with "standardisation" is that
> 

>         a)      we don't really know what set of icons we want,  and
>         adding new ones or changing the graphics for a "standard" set
>         will be difficult;
Yip.. plus setting something like a standard will make it more difficult
to change things later on. But what if we just used an aditional attribute
describing the role of a URL as e.g. "folder icon", an have the browsers
decide whether of not they use builtin icons? 


Later,
Chris



From fielding@simplon.ICS.UCI.EDU  Fri Jan  7 06:29:37 1994 -0800
Message-Id: <9401070629.aa10138@paris.ics.uci.edu>
Date: Fri, 07 Jan 1994 06:29:37 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Caching and Expires (was: Initializing HTTP headers from HTML)

John Ellson writes:
> 
> I thought you were addressing a cache coherency problem:
> 
> 	before serving a document a cache server should check if 
> 	the original document still exists and if it is unchanged.
> 
> 	if it no longer exists the no document should be served
> 	and the cache copy should be purged.
> 
> 	if it has changed then the cache should be refreshed and the
> 	new version served.
> 
> 	if the original site cannot be contacted then the document
> 	should be served with a warning that the validity of the
> 	document could not be verified.
> 
> 	if the document still exists unchanged then the cache copy can
> 	be served.
> 
> It seems that the Expires mechanism is a little more hand-offish than
> this.  Perhaps Expires provides sufficient coherency control?

I really don't think the Expires header should indicate anything other
than a hint to the reader (and possible cache manager) that the document
should not be considered "current" after a certain date.  As such, it
doesn't provide any coherency control, as would be expected by an optional
header.

**************************** A PROPOSAL *****************************

The cache coherency problem outlined (very well) above is a separate issue
because it requires a special request to the original server to determine
the status of the actual document.  Although some people have suggested that
the HEAD request is sufficient for this purpose, I find it entirely too
inefficient for a caching mechanism (because of the server overhead from
connecting twice and finding the file twice).

The solution is to implement a conditional GET request -- one that includes
a date to be checked against the Last-modified date of the information object.
Someone else (I didn't save the message) suggested a solution along these
lines in which the normal GET request was followed by a Last-modified:
header similar to the current Content-type, Authorization, etc.

Formally, if the server receives the request:
----------------------------------------------
GET /ICShome.html HTTP/1.0
Last-modified: Thu, 06 Jan 1994 15:57:41 GMT

----------------------------------------------
Then the server would respond:

(a) If the object /ICShome.html is inaccessible (for whatever reason), 
    then the server should return a 4XX message just like it does now.

(b) If /ICShome.html no longer exists, the server should return a
    404 Not Found response (i.e. same as now).

(c) If /ICShome.html is accessible but its last modification date is
    earler (less than) then the date passed (Thu, 06 Jan 1994 15:57:41 GMT),
    the server should return a 304 Use Local Copy message (with no body).

(d) If /ICShome.html is accessible and its last modification date is
    later than or equal to the date passed (Thu, 06 Jan 1994 15:57:41 GMT),
    the server should return a 200 OK message (i.e. same as now) with body.

In this way, cache managers would just send a GET request with the
Last-modified date equal to the date it originally requested the local
copy of the object it has in its cache.

Note that implementing this protocol would have no effect whatsoever
on existing servers and clients.  Old clients (and any without caches)
would just continue making requests without Last-modified headers.
Old servers (at least the NCSA httpd 1.0 that I use) will already accept
a message of the above format and just ignore the Last-modified header.

How's that for a proposal?

*********************************************************************

> If Expires is an optional attribute provided by the author then what
> prevents stale copies staying indefinitely in cache servers?

Nothing -- if the cache manager wants to keep stale copies, there
should be nothing to stop them (nor do I think it is possible to stop them).

> If it turns out that we agree that something is required to make cache
> servers operate correctly, then should the additional mechanism be
> user visible, or should it be built into the client-server protocols?

Because the Web is uncrypted, everything is user visible one way or another.
I would establish a client-server protocol and leave it to the clients
as to what information should be passed on to the user.  I think the expires
date is useful regardless of whether a cache is present or not.


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From dsr@hplb.hpl.hp.com  Fri Jan  7 16:23:05 1994 GMT
Message-Id: <9401071623.AA08021@manuel.hpl.hp.com>
Date: Fri, 7 Jan 94 16:23:05 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: HTML icon set was: Additions to the CGI archive

>> So if anyone is designing an HTML icon set (is anyone doing that?)
>> I think it would be a good idea to consider this larger set.
>> 

> What says the author of the HTML+ draft and the WWW gurus. A signal from
> there could help to initiate first steps.

Lets get some graphics artists to work on a really cool set of icons
(as good as Mosaic's world icon), which we can all share.

Dave Raggett



From dsr@hplb.hpl.hp.com  Fri Jan  7 17:00:03 1994 GMT
Message-Id: <9401071700.AA08048@manuel.hpl.hp.com>
Date: Fri, 7 Jan 94 17:00:03 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: HTML icon set was: Additions to the CGI archive

frans van hoesel writes:

> indeed URN would be a good idea and solve the problem completely. there is
> no need to define standard URN. if the URN is the same as you store locally;
> then there is no need to resolve the URN or search the standard set; just
> use the local copy on your disk.

If the icons are used by many people then there will be lots of different
URLs for each icon. Perhaps we ought to resurrect Tim's URN attribute
for links, e.g.

<img src="http://foo.edu/WWW/icons/folder.xpm" urn="folder.icons.www">

The URN attribute here specifies a common name you can use to check
if the one in your local cache is really the same object, regardless
of where you obtained that one from. When you retrieve an icon over
the net, its URN is given by an HTTP header (which header Tim?).

I have already suggested that we go ahead and start defining URNs
based on the same domain syntax as host names, see my message of the
4th January.

Dave



From dsr@hplb.hpl.hp.com  Fri Jan  7 16:48:46 1994 GMT
Message-Id: <9401071648.AA08035@manuel.hpl.hp.com>
Date: Fri, 7 Jan 94 16:48:46 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re:  Initializing HTTP headers from HTML documents

In response to John Ellson:

>> Since EVAL expressions would be evaluated by the server, it seems reasonable
>> to come up with a consensus approach like CGI to ensure a degree of
>> standardisation. Would you like to come up with a proposal?

> I could try.  But let me make sure that we agree on what problem needs
> solving.

> I thought you were addressing a cache coherency problem:

The EVAL idea is to allow headers to be included in responses which
can be dynamically evaluated by servers before sending the response.
I can envisage cron jobs which walk through the cache and use the HTTP HEAD
command to query the server for an update on the document status. The EVAL
mechanism could be very effective when triggered in this way, without the
need to transfer the document itself, especialy, when the document is
generated automatically from other information.

>        before serving a document a cache server should check if 
>        the original document still exists and if it is unchanged.

I see this as too inefficient for most people. The Expires: header can
be used when a client requests a document without the expense of
contacting the server. The following points can be met more effectively
by the cron job (cache minder) run at a suitable interval.

>        if it no longer exists the no document should be served
>        and the cache copy should be purged.

>        if it has changed then the cache should be refreshed and the
>        new version served.

>        if the original site cannot be contacted then the document
>        should be served with a warning that the validity of the
>        document could not be verified.

The cache minder can handle all these effectively, using the HTTP error
codes and the Last-Modified: header.

> If Expires is an optional attribute provided by the author then what
> prevents stale copies staying indefinitely in cache servers?

The cache minder will handle this eventuality.

> If it turns out that we agree that something is required to make cache
> servers operate correctly, then should the additional mechanism be
> user visible, or should it be built into the client-server protocols?

I am considering an NFS-based cache with a simple UDP protocol to get the
file name for URLs which are present in the cache. In this case, the clients
will need to be configured to know their address/port for this protocol.

Dave Raggett



From wade@cs.utk.edu  Fri Jan  7 12:38:53 1994 -0500
Message-Id: <9401071738.AA01077@honk.cs.utk.edu>
Date: Fri, 07 Jan 1994 12:38:53 -0500
From: wade@cs.utk.edu (Reed Wade)
Subject: comments on html+ discussion document (dated 2 Nov 93)


Hi,


I've been poking thru the html+ document and implementing bits
for the WWW client we're working on. I only have a few comments.
I haven't been following this discussion as closely as I'd like
recently so I apologize if some of these items have already been
dealt with.


sec 5.9 Images

Why have 2 elements that do the same thing? I'd suggest dropping
IMG from html+.

The example shows text flowing around the right side of the image.
This looks nice, but it isn't clear that it should be rendered
that way if an image is a 'character like element'. Is this simply
a matter of the renderer doing what would look best in a given 
situation? (That seems reasonable.)

There is mention here of shaped buttons which can be overlayed on
the image but no specifics are given. Is the intention here to support
the FIG style shapes?


sec 8.1 Active Areas

Is there any way to specify whether the server wants a region or
a point? I can see situations where you would want either, or only
one or the other. Is the server required to accept both forms in
all cases?


sec 8.2 Hypertext Buttons on Images

Should CAPTION's prepend a "Figure %d: " to their text, as in tables?

Should CAPTION's have an option allowing placement above the figure
instead of below?


sec 9 Tables

What if I want lines around some cells but not others?

Should CAPTION's have an option allowing placement above the figure
instead of below?


sec 10 Forms

This MH element feels like a pasted on bit of ugliness to make up
for functionality that really ought to be in the mailto url (smiley
face goes here). I'm probably overreacting to this, it may not be 
significant.

In the list of types, is IMAGE supposed to be IMAGEMAP?


The SCRIBBLE type might do well to allow other image types than
JOT. 

I phoned Slate at the number given and they said they didn't
have any information, I would have to call the Arizona Software 
Association (they couldn't tell me the number). Dave Raggett gave
me pointers to some useful info (including another organization
to contact for the spec.), I'm waiting for a call back from them
(Software Publishers Assoc).

In any case, bitmap info would be a handy option, maybe in pbm
format.


For the SELECT type (with SEVERAL indicated) how do you encode 
the multiple options that may be selected?


sec 10.2 Sending a Form via Email

An example would be useful here. If this is specified more clearly
it should be possible for the form to be interpreted by machine
on receipt. I understand that the thought now is to produce a MIME
multipart document.


thanks,
Reed Wade
wade@cs.utk.edu




From dsr@hplb.hpl.hp.com  Fri Jan  7 18:05:23 1994 GMT
Message-Id: <9401071805.AA08315@manuel.hpl.hp.com>
Date: Fri, 7 Jan 94 18:05:23 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re:  HTML and end tags

> So at the very least this brings up the question of what is
> the meaning of the following:

>        <PRE>
>        <A HREF=reference>this is some text
>        <A NAME=here>that is all</A>
>        an anchor.</A>
>        </PRE>

In my parser, the second <A> start tag automatically terminates the
first since anchors can't be nested, so the above is equivalent to:

        <PRE>
        <A HREF=reference>this is some text
        </A><A NAME=here>that is all</A>
        an anchor.</A>
        </PRE>

The DTD *REQUIRES* the closing tag, but my topdown parser knows what elements
are permitted in any context and hence can easily insert missing tags.

It doesn't handle <A HREF="..."><H1>some text</H1></A> very well, though,
as it sees an empty anchor, then a little later an unexpected </A> tag
which it ignores, but you can't win all the time ...

Dave Raggett



From dsr@hplb.hpl.hp.com  Fri Jan  7 18:29:44 1994 GMT
Message-Id: <9401071829.AA08406@manuel.hpl.hp.com>
Date: Fri, 7 Jan 94 18:29:44 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: HTML icon set was: Additions to the CGI archive 

Roy T. Fielding writes:

>> I have already suggested that we go ahead and start defining URNs
>> based on the same domain syntax as host names, see my message of the
>> 4th January.

> I don't recall seeing a message like that.  Could you send me (or www-talk)
> another copy?  I've noticed that the listserver has been acting strange
> lately -- one of Rob McCool's messages spent two weeks in Switzerland
> before I received it here.  Come to think of it, Rob's message arrived
> on the 4th...hmmmmmmmmm   Was that the day they changed to www0.cern.ch ?

well here it is again:

Maybe we ought to come up with a de facto URN mechanism, e.g.


        URN             ::=     DOMAINS [; SELECTOR]*
        DOMAINS         ::=     DOMAIN . DOMAINS

The DOMAIN is an opaque alphanumeric string which is used
to track down the domain servers a la DNS and identify the
individual URN. (it doesn't have to be meaningless though).

In this model, URN's can stand for single objects or for
families of related objects, which can be selected from
according to the value of the SELECTORS, e.g. language,
most recent version, ...

The binding process converts such URNs into one or more
URLs plus info differentiating family members and giving
other useful info, e.g. ownership, date created, ...

While people are developing suitable binding protocols and s/w
we can start using URNs by introducing a URN attribute as
previously described.

Dave



From ianf@random.se  Fri Jan  7 08:15:51 1994 +0100
Message-Id: <a952cab0@random.se>
Date: Fri, 07 Jan 94 08:15:51 +0100
From: ianf@random.se (Ian Feldman)
Subject: administrivia: listserver runs AMOK

Forgive me for mailing directly to the list, but there
seems to be 2 www-talk lists at CERN, the original one
 from which I managed to unsubscribe, and one other,
mailing from the "dxcern.cern.ch" address, which seems to
have run amok: there is no way to unsubscribe from it and
all requests addressed to listserver at either this
or the original address are returned with "no such user"
(listserv/ listserver) error message and the original
message's To: address substituted by 

To: listserver@www0.cern.ch
               ^^^^^^^^^^^^

Could someone please fix that, once and for all, please.

Thanks!

__Ian



From dsr@hplb.hpl.hp.com  Fri Jan  7 18:55:47 1994 GMT
Message-Id: <9401071855.AA08430@manuel.hpl.hp.com>
Date: Fri, 7 Jan 94 18:55:47 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: comments on html+ discussion document (dated 2 Nov 93)

Reed Wade asks about html+ details based on the draft spec (dated 2 Nov 93):

> sec 5.9 Images

> Why have 2 elements that do the same thing? I'd suggest dropping
> IMG from html+.

Following discussions at the WWW/TEI meeting in mid November I
dropped the IMAGE element, but kept IMG. FIG has been revised
by adding ALIGN=TOP, MIDDLE or BOTTOM, for which it is expected
to behave as a character like element in exactly the same way
as IMG now does. For ALIGN=LEFT, CENTER, BOTTOM, and by default
the image is placed after the next line break (soft or hard)
and aligned to the left, center or right. Text is flowed only
for left and right alignment of the image. Note that text which
follows the FIG element in the HTML+ document may be placed
on the line *before* the image to fill out that line.

> The example shows text flowing around the right side of the image.
> This looks nice, but it isn't clear that it should be rendered
> that way if an image is a 'character like element'. Is this simply
> a matter of the renderer doing what would look best in a given 
> situation? (That seems reasonable.)

Hope the above makes sense of this.

> There is mention here of shaped buttons which can be overlayed on
> the image but no specifics are given. Is the intention here to support
> the FIG style shapes?

Shaped buttons are only possible with FIG.

> Is there any way to specify whether the server wants a region or
> a point? I can see situations where you would want either, or only
> one or the other. Is the server required to accept both forms in
> all cases?

I haven't considered this, and wonder if we should just stick to
points for the moment?

> Should CAPTION's prepend a "Figure %d: " to their text, as in tables?

This seems like a stylistic issue for browsers. I don't think we
should force this for figures or tables.

> Should CAPTION's have an option allowing placement above the figure
> instead of below?

Maybe. I left one out thinking that perhaps this be left up to browsers.
However, it would be an easy feature to add and one which wouldn't hurt
any browsers that didn't recognise the attribute.

> sec 9 Tables
> What if I want lines around some cells but not others?

This might be making things rather too complicated for browser writers.
Lets wait until we get experience with the simple approach first.

> This MH element feels like a pasted on bit of ugliness to make up
> for functionality that really ought to be in the mailto url (smiley
> face goes here). I'm probably overreacting to this, it may not be 
> significant.

Yup, its been chucked out in favour of a new element STATE which can
be used by servers to hold state information in a manner that is
opaque to clients. The changes to forms need more space to detail 
than I can give right now.

> In the list of types, is IMAGE supposed to be IMAGEMAP?

No.

> The SCRIBBLE type might do well to allow other image types than JOT.

> In any case, bitmap info would be a handy option, maybe in pbm format.

OK, the content type will be specified in the MIME multipart/related message.

> For the SELECT type (with SEVERAL indicated) how do you encode 
> the multiple options that may be selected?

The options are specified by the OPTION element. Look at the SELECTED
attribute for the OPTION element. This can be used to specify which
options are currently selected.

> sec 10.2 Sending a Form via Email

This capability has been dropped for now, under pressure from browser
writers. You can always do this from the server.

Regards,

Dave Raggett

p.s. I enclose a copy of the current DTD which is still subject to revision.

----------------------------------------------------------------------------

<!SGML  "ISO 8879:1986"
--
  Document Type Definition for the HyperText Markup Language Plus 
  for use with the World Wide Web application (HTML+ DTD).

     The HTML+ DTD which is structured as an HTML core plus a
     number of additional modules which can be included by an
     entity definition in a document's <!DOCTYPE> element.
     You can include specific features in your document
     using the DOCTYPE declaration at the start, e.g.

     <!DOCTYPE htmlplus [
        <!ENTITY % HTML.tables "INCLUDE">
        <!ENTITY % HTML.forms "INCLUDE">
     ]>

     This spec also allows for authors to extend the DTD and
     to define how any new elements are rendered in terms of
     existing ones. This should be used with caution.

     Dave Raggett 5th January 1994
--
CHARSET
        BASESET "ISO 646:1983//CHARSET
                 International Reference Version (IRV)//ESC 2/5 4/0"
        DESCSET 0   9   UNUSED
                9   2   9
                11  2   UNUSED
                13  1   13
                14 18   UNUSED
                32 95   32
               127  1   UNUSED
        BASESET "ISO Registration Number 100//CHARSET
                 ECMA-94 Right Part of Latin Alphabet Nr. 1//ESC 2/13 4/1"
        DESCSET 128  32  UNUSED
                160  95  32
                255   1  UNUSED

CAPACITY        SGMLREF
                TOTALCAP        150000
                GRPCAP          150000

SCOPE   DOCUMENT
SYNTAX
        SHUNCHAR CONTROLS  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
                           19 20 21 22 23 24 25 26 27 28 29 30 31 127 255
        BASESET "ISO 646:1983//CHARSET
                 International Reference Version (IRV)//ESC 2/5 4/0"
        DESCSET 0 128 0
        FUNCTION RE         13
                 RS         10
                 SPACE      32
                 TAB SEPCHAR 9
        NAMING   LCNMSTRT ""
                 UCNMSTRT ""
                 LCNMCHAR ".-"
                 UCNMCHAR ".-"
                 NAMECASE GENERAL YES
                          ENTITY  NO
        DELIM    GENERAL  SGMLREF
                 SHORTREF SGMLREF
        NAMES    SGMLREF
        QUANTITY SGMLREF
                 NAMELEN  32
                 TAGLVL   100
                 LITLEN   1024
                 GRPGTCNT 150
                 GRPCNT   64

FEATURES
  MINIMIZE
    DATATAG  NO
    OMITTAG  YES
    RANK     NO
    SHORTTAG NO
  LINK
    SIMPLE   NO
    IMPLICIT NO
    EXPLICIT NO
  OTHER
    CONCUR  NO
    SUBDOC  NO
    FORMAL  YES
  APPINFO NONE
>

<!DOCTYPE HTMLPLUS [

<!-- DTD for HTML+ 
Markup minimisation should be avoided, otherwise the default <!SGML>
declaration is fine. Browsers should be forgiving of markup errors,
while authoring tools *should* enforce compliance with the DTD.

Common Attributes:

id      This attribute allows authors to name elements such as headers 
        and paragraphs as potential destinations for links. Note that 
        links don't specify points, but rather extended objects.

charset This allows authors to switch to a different char set for
        quotations or list etc. This is particularly useful for oriental
        languages which need two byte character codes, e.g. see RFC 1468
        "Japanese Character Encoding for Internet Messages"
-->
        
<!-- ENTITY DECLARATIONS
 <!ENTITY % foo "X | Y | Z"> is a macro definition for parameters and in 
 subsequent statements, the string "%foo;" is expanded to "X | Y | Z"

 Various classes of SGML text types:

  CDATA  text which doesn't include markup or entity references
  RCDATA text with entity references but no markup
  PCDATA text occurring in a context in which markup and entity references 
     may occur.
-->

<!-- Core HTML+ DTD omits following features -->
<!ENTITY % HTML.math "IGNORE">
<!ENTITY % HTML.tables "IGNORE">
<!ENTITY % HTML.figures "IGNORE">
<!ENTITY % HTML.emph "IGNORE">
<!ENTITY % HTML.forms "IGNORE">

<!ENTITY % cextra "" -- for character-like elements -->
<!ENTITY % pextra "" -- for paragraph-like elements -->

<!-- %cextra; and %pextra are designed to allow document specific
     extensions to the HTML+ DTD, e.g.

      <!DOCTYPE htmlplus [
        <!ENTITY % cextra "|PROPNAME">
        <!ELEMENT PROPNAME - - CDATA>
      ]>

     Use the RENDER element to specify how the browser should
     display new elements in terms of existing ones, e.g.

      <RENDER tag="PROPNAME" style="I">
-->

<!ENTITY % URL "CDATA" -- a URL or URN designating a hypertext node -->

<!-- Browsers should render the following types of emphasis
     distinctly when the obvious rendering is impractical

     I = italic, B = bold, U = underline, S = strikethru,
     TT = teletype font, SUP = superscript, SUB = subscript
     REV = reverse video for highlighting hit areas in the result of a query
     Q = inline quote (render according to local conventions)
-->

<!ENTITY % emph1 "I|B|U|TT|CITE|EM|STRONG|KBD|VAR|DFN|CODE|SAMP">
<!ENTITY % emph2 "S|Q|PERSON|ACRONYM|ABBREV|CMD|ARG|REMOVED|ADDED|REV">
<!ENTITY % emph3 "SUP|SUB|CHANGED">

<!ENTITY % emph  "%emph1;">
<![ %HTML.emph [ <!ENTITY % emph  "%emph1;|%emph2;|%emph3;"> ]]>

<!ENTITY % misc1 "">
<![ %HTML.emph [ <!ENTITY % misc1 "|RENDER|FOOTNOTE|MARGIN"> ]]>

<!ENTITY % misc2 "">
<![ %HTML.forms [ <!ENTITY % misc2 "|INPUT|TEXTAREA|SELECT"> ]]>

<!ENTITY % misc "BR %misc1 %misc2; %cextra;">

<!ENTITY % text "#PCDATA|A|IMG|%emph;|%misc;">
<![ %HTML.figures [ <!ENTITY % text "#PCDATA|A|IMG|FIG|%emph;|%misc;"> ]]>

<!ENTITY % paras "P|PRE %pextra;">
<![ %HTML.emph [ <!ENTITY % paras "P|PRE|LIT %pextra;"> ]]>

<!ENTITY % lists "UL|OL|DL">

<!ENTITY % block1 "ADDRESS|HR">
<![ %HTML.emph [ <!ENTITY % block1 "NOTE|QUOTE|ABSTRACT|ADDRESS|HR"> ]]>

<!ENTITY % block2 "">
<![ %HTML.tables [ <!ENTITY % block2 "|TABLE"> ]]>

<!ENTITY % block3 "">
<![ %HTML.forms [ <!ENTITY % block3 "|FORM"> ]]>

<!ENTITY % block4 "">
<![ %HTML.math [ <!ENTITY % block4 "|MATH"> ]]>

<!ENTITY % block "%block1; %block2; %block3; %block4;">

<!ENTITY % setup1 "">
<![ %HTML.emph [<!ENTITY % setup1 "& RENDER*"> ]]>

<!ENTITY % setup "(TITLE? & ISINDEX? & BASE? & META* & LINK* %setup1;)">

<!ENTITY % main "%block;|%lists;|%paras;">

<!-- these entities are used to simplify element definitions -->

<!ENTITY % heading "H1|H2|H3|H4|H5|H6"> 
<!ENTITY % table "P|%heading;|%lists;">
<!ENTITY % math "BOX|ARRAY|ROOT|%text;">

<!-- Browsers should as a minimum support the following types
     of INPUT fields, in addition to TEXTAREA and SELECT:

     text, checkbox, radio, submit, and reset

   password, int, float, date, url can be mapped to text fields
   while image, scribble and audio fields can be ignored
-->

<![ %HTML.forms [
 <!ENTITY % fields "text|password|checkbox|radio|submit|reset|int|
                          float|date|url|range|image|scribble|audio">
]]>


<!-- Core DTD includes basic Latin-1 entities -->
<!ENTITY % ISOlat1 PUBLIC "ISO 8879-1986//ENTITIES Added Latin 1//EN">
%ISOlat1;

<!-- additional entities normally found in Latin-1 char sets-->
<!ENTITY % ISOnum PUBLIC "ISO 8879-1986//ENTITIES Numeric and Special Graphic//EN">
%ISOnum;

<!-- diacritical marks normally found in Latin-1 char sets-->
<!ENTITY % ISOdia PUBLIC "ISO 8879-1986//ENTITIES Diacritical Marks//EN">
%ISOdia;

<!-- misc. from ISO Publishing entities -->
<!ENTITY ndash  SDATA "[ndash ]"--=en dash-->
<!ENTITY mdash  SDATA "[ndash ]"--=em dash-->
<!ENTITY ensp   SDATA "[ensp  ]"--=en space (1/2-em)-->
<!ENTITY emsp   SDATA "[emsp  ]"--=em space-->
<!ENTITY hellip SDATA "[hellip]"--=ellipsis (horizontal)-->
<!ENTITY vellip SDATA "[vellip]"--=ellipsis (vertical)-->

<!-- maths symbols when needed -->
<![ %HTML.math [
    <!ENTITY % ISOtech PUBLIC "ISO 8879-1986//ENTITIES General Technical//EN">
    %ISOtech;

    <!ENTITY % ISOgrk3 PUBLIC "ISO 8879-1986//ENTITIES Greek Symbols//EN">
    %ISOgrk3;

    <!ENTITY % ISOamso PUBLIC "ISO 8879-1986//ENTITIES Added Math Symbols: Ordinary//EN">
    %ISOamso;

    <!ENTITY % ISOamsr PUBLIC "ISO 8879-1986//ENTITIES Added Math Symbols: Relations//EN">
    %ISOamsr;

    <!ENTITY % ISOamsc PUBLIC "ISO 8879-1986//ENTITIES Added Math Symbols: Delimiters//EN">
    %ISOamsc;

    <!-- misc. from ISO Binary and Large operators -->

    <!ENTITY thinsp SDATA "[thinsp]"--=thin space (1/6 em)-->
    <!ENTITY coprod SDATA "[coprod]"--/coprod L: coproduct operator-->
    <!ENTITY prod   SDATA "[prod  ]"--/prod L: product operator-->
    <!ENTITY sum    SDATA "[sum   ]"--/sum L: summation operator-->
]]>

<!-- Basic types of elements:
  <!ELEMENT tagname - - CONTENT> elements needing end tags
  <!ELEMENT tagname - O CONTENT> elements with optional end tags
  <!ELEMENT tagname - O EMPTY> elements without content or end tags

The content definition is:
       -  an entity definition as defined above
       -  a tagname
       -  (brackets enclosing the above)
These may be combined with the operators:
  A*      A occurs zero or more times
  A+      A occurs one or more times
  A|B     implies either A or B
  A?      A occurs zero or one times
  A,B     implies first A then B
  A&B     either or both A and B (in either order A B or B A)
-->

<!ELEMENT HTMLPLUS O O (HEAD, BODY)>
<!ATTLIST HTMLPLUS
        version CDATA #IMPLIED -- the HTML+ version number --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!ELEMENT HEAD O O (%setup;) -- delimits document wide properties -->
<!ELEMENT BODY O O ((%main;)*, DIV6*, DIV5*, DIV4*, DIV3*, DIV2*, DIV1*)>

<!-- the following causes each header to imply a container of everything
     up to (but not including) the next peer or higher level header -->

<!ELEMENT DIV1 O O (H1, (%main;)*, DIV6*, DIV5*, DIV4*, DIV3*, DIV2*)>
<!ELEMENT DIV2 O O (H2, (%main;)*, DIV6*, DIV5*, DIV4*, DIV3*)>
<!ELEMENT DIV3 O O (H3, (%main;)*, DIV6*, DIV5*, DIV4*)>
<!ELEMENT DIV4 O O (H4, (%main;)*, DIV6*, DIV5*)>
<!ELEMENT DIV5 O O (H5, (%main;)*, DIV6*)>
<!ELEMENT DIV6 O O (H6, (%main;)*)>

<!ATTLIST (DIV6|DIV5|DIV4|DIV3|DIV2|DIV1)
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!-- Document headers -->
<!ELEMENT (%heading;) - - (#PCDATA | %emph;)+>
<!ATTLIST (%heading;)
        id      ID      #IMPLIED -- defines link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!-- character emphasis -->
<!ELEMENT (%emph1;) - - (%text;)*>
<!ATTLIST (%emph1;)
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!-- Paragraphs which act as containers for the following text -->
<!ELEMENT P O O (%text;)+>
<!ATTLIST P
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese --
        align   (left|indent|center|right|justify) left>

<!ELEMENT HR - O EMPTY -- Horizontal Rule -->
<!ELEMENT BR - O EMPTY -- forced line break -->

<!ELEMENT PRE - - (%text;)+ -- preformatted fixed pitch text -->
<!ATTLIST PRE
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!ELEMENT ADDRESS - - (P)+ -- info on author -->
<!ATTLIST ADDRESS
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!-- Lists which can be nested -->
<!ELEMENT OL - - (LI)+ -- ordered list -->
<!ATTLIST OL
        id      ID      #IMPLIED
        charset CDATA   #IMPLIED    -- eg "ISO-2022-JP" for japanese --
        compact (compact) #IMPLIED  -- reduced interitem spacing -->

<!ELEMENT UL - - (LI)+ -- unordered list -->
<!ATTLIST UL
        id      ID      #IMPLIED    -- link destination --
        charset CDATA   #IMPLIED    -- eg "ISO-2022-JP" for japanese --
        compact (compact) #IMPLIED  -- reduced interitem spacing --
        plain   (plain) #IMPLIED    -- suppress bullets --
        wrap (vert|horiz|none) none -- multicolumn list wrap style -->

<!-- List items for UL and OL lists
 The icon or label overides the default rendering -->
<!ELEMENT LI - O (DL|UL|OL|P|HR)+>
<!ATTLIST LI
        id      ID      #IMPLIED
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese --
        icon    %URL;   #IMPLIED -- icon for use in place of bullet --
        label   CDATA   #IMPLIED -- when you can't show the icon -->

<!-- Definition Lists (terms + definitions) -->
<!ELEMENT DL - - (DT*,DD)+>
<!ATTLIST DL
        id      ID      #IMPLIED
        charset CDATA   #IMPLIED    -- eg "ISO-2022-JP" for japanese --
        compact (compact) #IMPLIED  -- reduced interitem spacing -->

<!ELEMENT DT - O (%text;)+          -- term text -- >
<!ELEMENT DD - O (P|UL|OL|DL|HR)+   -- definition text -- >
<!ATTLIST (DT|DD)
        id      ID      #IMPLIED
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!-- Hypertext Links from points within document nodes

 The HREF attribute specifies the link destination as a URL or URN.
 In figures, the SHAPE attribute defines the extent of the link as
 a polygonal region. The PLAY attribute specifies an accompanyment,
 e.g. a talking head or just a simple sound sequence that is to be
 played upon following the link.

 The PRINT attribute determines how the browser should deal with
 links when printing the document out. This makes it possible for
 users to print out a document and related subdocuments with a
 single menu action.

 The TITLE attribute may be used for links in which the destination
 node doesn't define a title itself, e.g. non-html documents.
-->

<!ELEMENT A - - (#PCDATA | IMG | %emph;)*>
<!ATTLIST A
        id      ID      #IMPLIED -- as target of link --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese --
        shape   CDATA   #IMPLIED -- list of points for shaped buttons --
        href    %URL;   #IMPLIED -- destination node --
        rel     CDATA   #IMPLIED -- forward relationship type --
        rev     CDATA   #IMPLIED -- reverse relationship type --
        print   CDATA   #IMPLIED -- reference/footnote/section --
        title   CDATA   #IMPLIED -- when otherwise unavailable -->

<!-- Other kinds of relationships between documents

 There are a set of standard RELationship types which alter the
 browser's navigation menu, e.g. Next, Previous, UseIndex, ...

 The AFTER attribute specifies a delay after which if the user
 has done nothing, the browser should follow the link. This makes
 it possible to run storyboards

 The IDREF attribute makes it possible to specify annotation links
 separately from the document text flow. With HTTP, servers can
 use the WWW-Link: header to "insert" such annotations into a
 document.
-->

<!ELEMENT LINK - O EMPTY>
<!ATTLIST LINK
        id      ID      #IMPLIED -- to allow meta info on links --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese --
        from    IDREF   #IMPLIED -- starting point --
        href    %URL;   #IMPLIED -- destination node --
        rel     CDATA   #IMPLIED -- forward relationship type --
        rev     CDATA   #IMPLIED -- reverse relationship type --
        print   CDATA   #IMPLIED -- reference/footnote/section --
        title   CDATA   #IMPLIED -- when otherwise unavailable -->

<!-- Document title -->
<!ELEMENT TITLE - - (#PCDATA | %emph;)+>
<!ATTLIST TITLE
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!-- Original document URL for resolving relative URLs  -->
<!ELEMENT BASE - O EMPTY>
<!ATTLIST BASE HREF %URL; #IMPLIED>

<!-- Signifies the document's URL accepts queries,
     and may be implied by HTTP header info -->
<!ELEMENT ISINDEX - O EMPTY>
<!ATTLIST ISINDEX href %URL; #IMPLIED -- defaults to document's URL -->

<!--
 Servers should read the document head to generate HTTP headers
 corresponding to META elements, e.g. if the document contains:

        <meta name="Expires" value="Tue, 04 Dec 1993 21:29:02 GMT">

 The server should include the HTTP date format header field:

        Expires: Tue, 04 Dec 1993 21:29:02 GMT

 Other likely names are Date (creation date), Last-Modified,
 Owner (a name) Contact (an email address).
-->

<!ELEMENT META - O EMPTY>
<!ATTLIST META
        id      ID      #IMPLIED -- to allow meta info  --
        name    CDATA   #IMPLIED -- HTTP header e.g. "Expires" --
        value   CDATA   #IMPLIED -- associated value -->

<![ %HTML.emph [

<!-- additional character emphasis -->
<!ELEMENT (%emph2;) - - (%text;)*>
<!ATTLIST (%emph2;)
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!ELEMENT (SUP|SUB) - - (%text;)* -- superscripts and subscripts -->
<!ATTLIST (SUP|SUB)
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese --
        pre     (pre)   #IMPLIED -- for use as prefixes for math -->

<!ELEMENT (FOOTNOTE|MARGIN) - - (%text;)* -(FOOTNOTE|MARGIN)>
<!ATTLIST (FOOTNOTE|MARGIN)
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!-- RENDER only appears in the document head -->
<!ELEMENT RENDER -O EMPTY -- how to render unknown elements -->
<!ATTLIST RENDER
        id      ID      #IMPLIED -- to allow meta info  --
        tag     CDATA   #IMPLIED -- tag name --
        equiv   CDATA   #IMPLIED -- HTML+ equivalent tag name --
        style   NAMES   #IMPLIED -- space separated list of styles -->

<!ELEMENT LIT - - (TAB|%text;)+ -- literal variable pitch text -->
<!ATTLIST LIT
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!ELEMENT TAB - O EMPTY -- tabs for imported text -->
<!ATTLIST TAB
        id      ID      #IMPLIED -- to allow meta info  --
        at      NUMBER  #IMPLIED -- in em units or width of an "m" --
        align   (left|center|right|decimal) left -- tab alignment -->

<!ELEMENT QUOTE - - (P*) -- block quote -->
<!ATTLIST QUOTE
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!ELEMENT ABSTRACT - - (P*) -- document summary -->
<!ATTLIST ABSTRACT
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!-- often rendered with an icon in left margin,
     the role is shown before the first paragraph -->
<!ELEMENT NOTE - - (P*) -- admonishment -->
<!ATTLIST NOTE
        id      ID      #IMPLIED -- link destination --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese --
        role (Simple|Tip|Note|Warning|Error) Simple >

<!-- change bars can bridge markup boundaries -->
<!ELEMENT CHANGED - O EMPTY>
<!ATTLIST CHANGED -- one of id or idref is always required --
        id      ID      #IMPLIED -- signals start of changes --
        idref   IDREF   #IMPLIED -- signals end of changes -->
]]>


<!-- defined in core because SGML doesn't define logical conjunctions
     such that it could be defined iff (HTML.forms or HTML.tables)  -->
<!ELEMENT CAPTION - - (%text;)+ -- table or figure caption -->
<!ATTLIST CAPTION
        id      ID      #IMPLIED
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<![ %HTML.tables [

<!-- a pre-pass is needed to count columns and determine
     min/max widths before sizing to match window size -->

<!ELEMENT TABLE - - (CAPTION?, (TR)*) -- mixed headers and data -->
<!ATTLIST TABLE
        id      ID      #IMPLIED
        border (border) #IMPLIED -- draw borders --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!ELEMENT TR O O (TH|TD)* -- acts like row separator -->
<!ATTLIST TR id ID #IMPLIED>

<!ELEMENT TH - O (%table;)* -- a header cell -->
<!ATTLIST TH
        id      ID      #IMPLIED
        colspan NUMBER    1      -- columns spanned --
        rowspan NUMBER    1      -- rows spanned --
        align (left|center|right|numeric) center
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!ELEMENT TD - O (%table;)* -- a data cell -->
<!ATTLIST TD
        id      ID      #IMPLIED
        colspan NUMBER    1      -- columns spanned --
        rowspan NUMBER    1      -- rows spanned --
        align (left|center|right|numeric) center
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->
]]>

<![ %HTML.forms [

<!--
 The form contents are sent to the server upon pressing a submit
 button or a field with the update attribute. The HTTP reply status
 code signifies whether the data is a new document or whether it just
 specifies updates to field values. For the latter the ID values
 are used to specify which field is involved.

 Fields can be disabled (greyed out) or marked as being in error.
 The MESSAGE element may be used by the server to set error messages.
 Servers can store state information in forms with the STATE element.
-->

<!ELEMENT FORM - - (STATE?, (%main;)*, MESSAGE?) -(FORM) -- forms can't be nested -->
<!ATTLIST FORM
        id      ID      #IMPLIED
        action  %URL;   #IMPLIED -- defaults for URL for current doc --
        method  CDATA   #IMPLIED -- GET, PUT, POST, DELETE etc. --
        enctype CDATA   #IMPLIED -- encoding type for form transfers --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!--
 This element is opaque to the client and *NOT* rendered,
 It is used by servers to store state info so that the
 server itself remains stateless. The state data is sent
 the server along with the form contents. Similarly servers
 can send back state data with form updates.
-->
<!ELEMENT STATE - - (#CDATA)>
<!ATTLIST STATE
        id      ID      #IMPLIED -- to allow meta info  -->

<!-- input fields in forms - I want to throw out maxlength as it is over
     the default NAMELEN limit of 8 chararcters, but leave it in for now -->
<!ELEMENT INPUT - O EMPTY>
<!ATTLIST INPUT
        id      ID      #IMPLIED -- to allow meta info  --
        name    CDATA   #IMPLIED -- attribute name (may not be unique) --
        type    (%fields) text   -- a wide variety of field types --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese --
        size    CDATA   #IMPLIED -- visible size of text fields --
        min     NUMBER  #IMPLIED -- for range controls --
        max     NUMBER  #IMPLIED -- for range controls or text fields --
        maxlength NUMBER #IMPLIED -- max length of text fields (equiv to max)--
        value   CDATA   #IMPLIED -- attribute value (altered by user) --
        checked (checked) #IMPLIED -- for check boxes and radio buttons --
        disabled (disabled) #IMPLIED -- if grayed out --
        error   (error) #IMPLIED -- if in error --
        src      %URL;  #IMPLIED -- for IMAGE, SCRIBBLE & AUDIO fields --
        alt     CDATA   #IMPLIED -- alternative text for VT100's etc --
        align (top|middle|bottom) top -- for IMAGE fields only --
        update (update) #IMPLIED -- for dynamic updates by server -->

<!-- multiline text input fields, we probably will want
     to generalise this to accept arbitrary clipboard data
     e.g. hypertext and images, in addition to plain text -->
<!ELEMENT TEXTAREA - - RCDATA -- multi-line text fields -->
<!ATTLIST TEXTAREA
        id      ID      #IMPLIED -- to allow meta info  --
        name    CDATA   #IMPLIED -- attribute name (may not be unique) --
        cols    NUMBER  #IMPLIED -- visible width in characters --
        rows    NUMBER  #IMPLIED -- visible height in characters --
        disabled (disabled) #IMPLIED -- if grayed out --
        error   (error) #IMPLIED -- if in error --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese --
        update (update) #IMPLIED -- for dynamic updates by server -->

<!ELEMENT SELECT - - (OPTION+) -- combo style selection lists -->
<!ATTLIST SELECT
        id      ID      #IMPLIED -- to allow meta info  --
        name    CDATA   #IMPLIED -- attribute name (may not be unique) --
        multiple (multiple) #IMPLIED -- permits multiple selections --
        error   (error) #IMPLIED -- if in error --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese --
        update (update) #IMPLIED -- for dynamic updates by server -->

<!ELEMENT OPTION - - RCDATA>
<!ATTLIST OPTION
        id      ID      #IMPLIED -- to allow meta info  --
        selected (selected) #IMPLIED -- if initially selected --
        disabled (disabled) #IMPLIED -- if grayed out --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->

<!-- used by server to set warning/error messages -->
<!ELEMENT MESSAGE - - RCDATA -- place for error/warning/info -->
<!ATTLIST MESSAGE
        id      ID      #IMPLIED -- to allow meta info  --
        status  (info|warning|error) info
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->
]]>

<![ %HTML.figures [

<!-- figures which subsume the role of the earlier IMG element.

  Behaves identically to IMG for align = top, middle or bottom.
  Otherwise figure is inserted after next line break (soft or hard).
  For align=left, the image is left aligned and text is flowed
  on the right of the image, and similarly for align=right, with
  no text flow for align=center (the default). The caption is
  placed under the image.

  The <A> element is used for shaped buttons handled by browser,
  while the ISMAP mechanism sends pointer clicks/drags to server.
  The text contained by this element is used for text-only displays
  and authors should remember to provide effective descriptions,
  including label text for shaped buttons.
-->
<!ELEMENT FIG - - (CAPTION?,(%text;)*)>
<!ATTLIST FIG
        id      ID      #IMPLIED
        align   (top|middle|bottom|left|center|right) center -- position --
        ismap   (ismap) #IMPLIED -- server can handle mouse clicks/drags --
        src     %URL;   #IMPLIED -- link to image data --
        charset CDATA   #IMPLIED -- eg "ISO-2022-JP" for japanese -->
]]>

<!-- img is left in for at least the short term -->
<!ELEMENT IMG - O EMPTY>
<!ATTLIST IMG
        src     %URL;   #REQUIRED -- where to get image data --
        align   (top|middle|bottom) top  -- top, middle or bottom --
        seethru CDATA   #IMPLIED  -- for chromakey --
        alt     CDATA   #IMPLIED -- description for text-only displays --
        ismap   (ismap) #IMPLIED  -- send mouse clicks/drags to server -->

<![ %HTML.math [

<!-- Proposal for representing formulae

  Delimiters should stretch to match the size of the delimited
  object. <SUB> and <SUP> are used for subscripts and superscripts

                                     i j
  X<SUP PRE>i</SUP><SUP>j</SUP>  is   X
-->

<!ELEMENT MATH - - (%math;)*>
<!ATTLIST MATH id ID #IMPLIED>

<!-- Invisible brackets which may also be
     used for numerators and denominators:

                                   1 + X
     <BOX>1 + X<OVER>Y</BOX>  is  _______
                                     Y

                                 _____
     <BOX><OVER>X + Y</BOX>  is  X + Y
-->
<!ELEMENT BOX - - ((%math;)*, (OVER, (%math;)*)?)>

<!-- Horizontal line between numerator and denominator
     The symbol attribute allows authors to supply an
     entity name for an arrow symbol etc.
 -->
<!ELEMENT OVER - O EMPTY>
<!ATTLIST OVER symbol ENTITY #IMPLIED>

<!-- Roots - default to square root -->
<!ELEMENT ROOT - - (%math;)*>
<!ATTLIST ROOT root CDATA #IMPLIED>

<!-- LaTeX like arrays. The align attribute specifies
     a single letter for each column, which also determines
     how the column should be aligned, e.g. align=ccc"

        "l"     left
        "c"     center
        "r"     right
-->
<!ELEMENT ARRAY - - (ITEM)+>
<!ATTLIST ARRAY align CDATA #REQUIRED>

<!ELEMENT ITEM - O (%math;)*>

]]>

<!-- The END -->
]>



From fielding@simplon.ICS.UCI.EDU  Fri Jan  7 12:43:58 1994 -0800
Message-Id: <9401071244.aa01874@paris.ics.uci.edu>
Date: Fri, 07 Jan 1994 12:43:58 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Inline Images under A/UX won't display. 

I wrote:

> That sounds like the same problem I am experiencing with 2-color GIFs
> and Mosaic for X 2.1.  They were fine in 2.0, but now they display as
> blank on monochrome screens and sometimes blank/sometimes just faint grey
> (depending on the image) on color screens.  I'd bet this has something
> to do with the GIF89 enhancement.
> 
> I notified NCSA (but I can imagine they are pretty busy right now) and
> I have a test page setup on <http://www.ics.uci.edu/Test/> if anyone
> wants to try their hand at debugging.  Please let me know the results.

Well, after getting several messages from others indictating that
they could see my images just fine, I decided to investigate further.

I pulled down the Mosaic-sun binary from NCSA and tried that -- no change.
I then changed my window manager from Motif 1.2 to Twm and tried it
again -- this time it worked!  The problem seems to be somewhere in
the Motif window manager (I SHOULD HAVE GUESSED THAT FROM THE START).

Does anyone know if this is fixed in one of the 1.2.X bug-fix releases that
OSF wants us to pay $1500/year to get?


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From altis@ibeam.jf.intel.com  Fri Jan  7 13:16:57 1994 -0800
Message-Id: <m0pIOTg-00041hC@ibeam.intel.com>
Date: Fri, 7 Jan 1994 13:16:57 -0800
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: minimal HTML

There are a number of HTML guides on the Web in addition to the HTML
specification at CERN telling users how to create HTML, but whenever I look
at the source of most HTML on the Web, it typically has the form of:

<TITLE>blah</TITLE>
<H1>blah</H1>
....rest of the file

That's legal, or at least works with existing clients. The question is
whether it is good style and/or will work with future clients. Also, there
is the issue of teaching new HTML authors good practices and following
those same practices for CGI gateways to archie, oracle... I would like to
propose that the HTML below is probably the minimal HTML file (assuming
mailto: is still okay ;-). HTML Guide writers, people writing translators
from latex, rtf, etc. to HTML, etc. might want to use this. Comments,
additions, tomatoes?

<HTML>
<HEAD>
<TITLE>blah</TITLE>
<LINK REV="MADE" HREF="mailto:username@host.domain">
</HEAD>

<BODY>
<H1>blah</H1>
..
..
..
</BODY>
</HTML>

ka








From Guido.van.Rossum@cwi.nl  Fri Jan  7 18:00:20 1994 +0100
Message-Id: <9401071700.AA07217=guido@voorn.cwi.nl>
Date: Fri, 07 Jan 1994 18:00:20 +0100
From: Guido.van.Rossum@cwi.nl (Guido.van.Rossum@cwi.nl)
Subject: Re: HTML icon set was: Additions to the CGI archive 

> >> So if anyone is designing an HTML icon set (is anyone doing that?)
> >> I think it would be a good idea to consider this larger set.
> 
> > What says the author of the HTML+ draft and the WWW gurus. A signal from
> > there could help to initiate first steps.

> Lets get some graphics artists to work on a really cool set of icons
> (as good as Mosaic's world icon), which we can all share.

Hmm...  Shouldn't we first agree on an abstract set of icons with
(fuzzy) semantics attached to them before letting the artists loose?
I can imagine that different clients (assuming there is life after
Mosaic :-) may use different icon styles, corresponding to the style
of the rest of their user interface (e.g. monochrome, grayscale 3D
look, "MTV look", ...) with the same semantics (e.g. file, folder,
program, left arrow, explanation, definition, image, video, sound,
animation, game, ...).  I have a feeling that the development of this
latter set is more important -- or have I missed total disagreement
here?

--Guido van Rossum, CWI, Amsterdam <Guido.van.Rossum@cwi.nl>
URL:  <http://www.cwi.nl/cwi/people/Guido.van.Rossum.html>




From vinay@eit.COM  Fri Jan  7 14:26:44 1994 PST
Message-Id: <9401072226.AA07474@eit.COM>
Date: Fri, 7 Jan 94 14:26:44 PST
From: vinay@eit.COM (Vinay Kumar)
Subject: Re: minimal HTML

A newbee suggesting with great trepidation:

How about replacing <HTML> with <HTML 1.0> indicating version number of
the HTML spec (MIF files certainly do use a similar tag). Maybe it's already 
included in the spec !
--
  Vinay Kumar
vinay@eit.com

--------------------------------------
> From: altis@ibeam.jf.intel.com (Kevin Altis) Fri Jan  7 13:44:44 1994
> 
> <HTML>
> <HEAD>
> <TITLE>blah</TITLE>
> <LINK REV="MADE" HREF="mailto:username@host.domain">
> </HEAD>
> 
> <BODY>
> <H1>blah</H1>
> ..
> ..
> ..
> </BODY>
> </HTML>
> 
> ka
> 
> 
> 
> 
> 
> 



From fielding@simplon.ICS.UCI.EDU  Fri Jan  7 15:04:54 1994 -0800
Message-Id: <9401071504.aa09921@paris.ics.uci.edu>
Date: Fri, 07 Jan 1994 15:04:54 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: minimal HTML 

> <HTML>
> <HEAD>
> <TITLE>blah</TITLE>
> <LINK REV="MADE" HREF="mailto:username@host.domain">
> </HEAD>
> 
> <BODY>
> <H1>blah</H1>
> ..
> ..
> ..
> </BODY>
> </HTML>

I would not consider the LINK to be part of a "minimal" document.
There are many times when including an e-mail address is not desirable.
Otherwise, I agree with your suggestion -- that is certainly the
way I have set up my own documents.


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From bert@let.rug.nl  Sat Jan  8 00:15:03 1994 +0100 (MET)
Message-Id: <9401072315.AA07873@freya.let.rug.nl>
Date: Sat, 8 Jan 1994 00:15:03 +0100 (MET)
From: bert@let.rug.nl (Bert Bos)
Subject: Re: minimal HTML

 |How about replacing <HTML> with <HTML 1.0> indicating version number of
 |the HTML spec (MIF files certainly do use a similar tag). Maybe it's already 
 |included in the spec !

Yes, it is in the daft for HTML+


Bert
-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|



From bert@let.rug.nl  Sat Jan  8 00:02:59 1994 +0100
Message-Id: <9401072303.AA07705@freya.let.rug.nl>
Date: Sat, 8 Jan 1994 00:02:59 +0100
From: bert@let.rug.nl (Bert Bos)
Subject: 

From bert Sat Jan  8 00:01:25 1994 remote from tyr
Subject: Re: minimal HTML
To: altis@ibeam.jf.intel.com (Kevin Altis)
Date: Sat, 8 Jan 1994 00:01:25 +0100 (MET)
In-Reply-To: <m0pIOTg-00041hC@ibeam.intel.com> from "Kevin Altis" at Jan 7, 94 01:16:57 pm
X-Mailer: ELM [version 2.4 PL13]
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
Content-Length: 2448      
Sender: bert

 |propose that the HTML below is probably the minimal HTML file (assuming
 |mailto: is still okay ;-). HTML Guide writers, people writing translators
 |from latex, rtf, etc. to HTML, etc. might want to use this. Comments,
 |additions, tomatoes?
 |
 |<HTML>
 |<HEAD>
 |<TITLE>blah</TITLE>
 |<LINK REV="MADE" HREF="mailto:username@host.domain">
 |</HEAD>
 |
 |<BODY>
 |<H1>blah</H1>
 |..
 |..
 |..
 |</BODY>
 |</HTML>

I'm not sure... On the one hand it might simplify writing parsers for
HTML if no tags are omitted. On the other hand these tags are not
really meant for people to use. They are hooks for an SGML parser or
(in the jargon of grammar writers) "nonterminals" that do not show up
in the final surface structure.

When I write HTML, I write not HTML, but something that I find easier
to use. The real HTML is generated by an SGML parser. Whether or not I
want <!DOCTYPE>, <BODY>, or <HEAD> to be generated is simply a matter
of instructing the parser. (My "authoring system" is a thrown together
from Makefiles, Emacs modes, etc, but a special SGML or HTML editor
would have at least the same capabilities.)

A comparable situation would have been the result of a set of new tags
that I once suggested. Consider that an HTML doc be split into
<SECT1>s, <SECT2>s, etc., corresponding to <H1>, <H2>, etc. This would
allow an anchor to be attached to a whole section of the text, instead
of to a header. The <SECTn> tag would be implied by the occurrence of
<Hn>, in the same manner as <BODY>. (The suggestion was turned down,
because people wanted to be able to put <Hn> tags inside lists and
other places where they don't belong.)

The inclusion of the <LINK... tag is a different matter. it is
certainly something that I would urge people to add. Somebody must be
responsible for the contents of a document and that somebody should
provide his address. A Reply command like in E-mail or Usenet is one
of the first things I'm going to add when I write my browser. (Lynx
has it!)


Bert
-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|




From jtilton@jupiter.willamette.edu  Fri Jan  7 15:57:13 1994 -0800 (PST)
Message-Id: <Pine.3.88.9401071504.A9903-0100000@jupiter>
Date: Fri, 7 Jan 1994 15:57:13 -0800 (PST)
From: jtilton@jupiter.willamette.edu (James)
Subject: re: minimal HTML

On Sat, 8 Jan 1994, Bert Bos wrote:

> I'm not sure... On the one hand it might simplify writing parsers for
> HTML if no tags are omitted. On the other hand these tags are not
> really meant for people to use. They are hooks for an SGML parser or
> (in the jargon of grammar writers) "nonterminals" that do not show up
> in the final surface structure.

Not having written a parser for HTML, I'm shooting a bit in the dark 
here, but it seems to me that promoting more "legitimate" HTML (w/ <HTML> 
tags, et al) is a smart move.  After all, that's what's in the 
specification -- and while laxness is often allowed by the browsers, how 
can we be sure the browsers will always be lax in the same ways?  It 
seems that it would be better to be anal :), and have legal HTML floating 
around instead of quasi-legal HTML.

This would probably be a good job for some sort of prettifying filter for 
HTML, that would take someone's "lax" code and add the appropriate extra 
tags.

						-et

/ (James) Eric Tilton, Student AND Student Liaison, WITS               \
\ Class of '95 - CS/Hist  -- Internet - jtilton@willamette.edu         /
<a href="http://www.willamette.edu/~jtilton/">ObHyPlan!</a>, chock fulla
<a href="http://www.willamette.edu/~jtilton/whatsnew.html">Fun Stuff!</a>




From dolesa@smtp-gw.spawar.navy.mil  Fri Jan  7 19:45:30 1994 EDT
Message-Id: <9400077580.AA758000730@smtp-gw.spawar.navy.mil>
Date: Fri, 07 Jan 94 19:45:30 EDT
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: Re[2]: Inline Images under A/UX won't display. 


     
I've narrowed it down.  For some reason the http server just doesn't like
my mac or pc gifs.  What are people using to save gif's as on either Mac
or PC platforms that conforms to http's requirements of a gif?  I can open
my gif's under lview 3.1, photoshop, photostyler... you name it.  Please
someone point me to a graphics package that will save a gif that http can
read.  I just downloaded the newest version of giffer, graphic converter,
and Cybergif.  Maybe one of those will work.  Help appreciated. 

                        Andre'

______________________________ Reply Separator _________________________________
Subject: Re: Inline Images under A/UX won't display. 
Author:  "Roy T. Fielding" <fielding@simplon.ICS.UCI.EDU> at SMTP-GW
Date:    1/7/94 4:04 PM


I wrote:
     
> That sounds like the same problem I am experiencing with 2-color GIFs 
> and Mosaic for X 2.1.  They were fine in 2.0, but now they display as
> blank on monochrome screens and sometimes blank/sometimes just faint grey 
> (depending on the image) on color screens.  I'd bet this has something
> to do with the GIF89 enhancement.
> 
> I notified NCSA (but I can imagine they are pretty busy right now) and 
> I have a test page setup on <http://www.ics.uci.edu/Test/> if anyone
> wants to try their hand at debugging.  Please let me know the results.
     
Well, after getting several messages from others indictating that 
they could see my images just fine, I decided to investigate further.
     
I pulled down the Mosaic-sun binary from NCSA and tried that -- no change. 
I then changed my window manager from Motif 1.2 to Twm and tried it
again -- this time it worked!  The problem seems to be somewhere in 
the Motif window manager (I SHOULD HAVE GUESSED THAT FROM THE START).
     
Does anyone know if this is fixed in one of the 1.2.X bug-fix releases that 
OSF wants us to pay $1500/year to get?
     
     
...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
     



From bert@let.rug.nl  Sat Jan  8 14:43:53 1994 +0100 (MET)
Message-Id: <9401081344.AA17306@freya.let.rug.nl>
Date: Sat, 8 Jan 1994 14:43:53 +0100 (MET)
From: bert@let.rug.nl (Bert Bos)
Subject: Re: HTML icon set was: Additions to the CGI archive

Guido van Rossum writes:

 |> >> So if anyone is designing an HTML icon set (is anyone doing that?)
 |> >> I think it would be a good idea to consider this larger set.
 |> 
 |> > What says the author of the HTML+ draft and the WWW gurus. A signal from
 |> > there could help to initiate first steps.
 |
 |> Lets get some graphics artists to work on a really cool set of icons
 |> (as good as Mosaic's world icon), which we can all share.
 |
 |Hmm...  Shouldn't we first agree on an abstract set of icons with
 |(fuzzy) semantics attached to them before letting the artists loose?
 |I can imagine that different clients (assuming there is life after
 |Mosaic :-) may use different icon styles, corresponding to the style
 |of the rest of their user interface (e.g. monochrome, grayscale 3D
 |look, "MTV look", ...) with the same semantics (e.g. file, folder,

I agree, to the point that it would be nice if HTML doc authors could
rely on the availability of some standard set of symbols. But why
should they be images?

My suggestion: create a set of entities (&file;, &folder;, etc.) like
the set that already exists for accented characters (&eacute;, &euml;,
etc.). If we agree on this, maybe Dave Raggett could add such a list
to the HTML+ draft?

Browser developers are then free to display those entities in whatever
way they like. Either as an "ASCII graphic", a bitmap, or a character
from a special symbols font.

Btw. ISO is standardizing the names and approximate look of a small
set of "desktop" icons. I don't have the drafts here at home, but we
should definitely use their names.


Bert
-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|



From hotsand!rhb  Sat Jan  8 14:04:24 1994 EST
Message-Id: <9401081904.AA23102@hotsand.dacsand>
Date: Sat, 8 Jan 94 14:04:24 EST
From: hotsand!rhb (Rich Brandwein)
Subject: More CGI Comments


After playing with CGI-based httpd servers for awhile and writing scripts
to them, I have the following observations/questions:

1) If you let users export information via their UserDir (i.e., ~/public_html
by default), how can you gracefully allow them to create anything that requires
a shell execution without giving everyone write access to the cgi-bin
directory or creating cgi aliases for all users in srm.conf?

2) To get at any of the authentication information (e.g., the $REMOTE_USER variable)
it seems that my pages that want to use any of this info need to all become shell scripts
(which means that they'll need to be in cgi-bin type directories).  Once I authenticate
someone, it seems that I generally want to know the user on every page served in many
apps (in fact, it would certainly be nice to log this info - I can't differentiate
authenticated users from the log file if they're coming from the same server...).

3) Because of (1), (2) and my general preferences of arranging files, I find it would be
much easier to identify executables on the server side by being able to use a server
defined suffix (notwithstanding the previous arguments against this) for these files
(e.g., .cgi).  

By the way, I agree with the message about one of the listservs running amok.
It took quite some time to post this, and this is an excerpt of one of the
messages I've received back:

*********************************
   ----- Transcript of session follows -----
/W3/hypertext/WWW/Administration/Mailing/www-talk: line 391: add... User unknown
sh: /usr/local/bin/deliver: not found
"|/usr/local/bin/deliver -b /userd/tbl/hypertext/WWW/Archive/www-talk.archive"... unknown mailer error 1
sh: /usr/local/bin/deliver: not found
"|/usr/local/bin/deliver -b /userd/tbl/Mailboxes/WWW_Talk_Unread.mbox/mbox"... unknown mailer error 1
550 "|/usr/lib/sendmail -odi -oi -f www-talk-request@info.cern.ch www-talk-members"... User unknown

   ----- Unsent message follows -----
*****************************


Rich

----
Rich Brandwein
AT&T Bell Labs
rich.brandwein@att.com




From hotsand!rhb  Sat Jan  8 15:26:23 1994 EST
Message-Id: <9401082026.AA23597@hotsand.dacsand>
Date: Sat, 8 Jan 94 15:26:23 EST
From: hotsand!rhb (Rich Brandwein)
Subject: More CGI Comments

Subject: More CGI Comments
content-length: 2073


After playing with CGI-based httpd servers for awhile and writing scripts
to them, I have the following observations/questions:

1) If you let users export information via their UserDir (i.e., ~/public_html
by default), how can you gracefully allow them to create anything that requires
a shell execution without giving everyone write access to the cgi-bin
directory or creating cgi aliases for all users in srm.conf?

2) To get at any of the authentication information (e.g., the $REMOTE_USER variable)
it seems that my pages that want to use any of this info need to all become shell scripts
(which means that they'll need to be in cgi-bin type directories).  Once I authenticate
someone, it seems that I generally want to know the user on every page served in many
apps (in fact, it would certainly be nice to log this info - I can't differentiate
authenticated users from the log file if they're coming from the same server...).

3) Because of (1), (2) and my general preferences of arranging files, I find it would be
much easier to identify executables on the server side by being able to use a server
defined suffix (notwithstanding the previous arguments against this) for these files
(e.g., .cgi).  

By the way, I totally agree with the message about one of the listservs running amok.
It took quite some time to post this, and this is an excerpt of one of the
messages I've received back:

*********************************
   ----- Transcript of session follows -----
/W3/hypertext/WWW/Administration/Mailing/www-talk: line 391: add... User unknown
sh: /usr/local/bin/deliver: not found
"|/usr/local/bin/deliver -b /userd/tbl/hypertext/WWW/Archive/www-talk.archive"... unknown mailer error 1
sh: /usr/local/bin/deliver: not found
"|/usr/local/bin/deliver -b /userd/tbl/Mailboxes/WWW_Talk_Unread.mbox/mbox"... unknown mailer error 1
550 "|/usr/lib/sendmail -odi -oi -f www-talk-request@info.cern.ch www-talk-members"... User unknown

   ----- Unsent message follows -----
*****************************


Rich

----
Rich Brandwein
AT&T Bell Labs
rich.brandwein@att.com



From fielding@simplon.ICS.UCI.EDU  Sat Jan  8 13:49:22 1994 -0800
Message-Id: <9401081349.aa07138@paris.ics.uci.edu>
Date: Sat, 08 Jan 1994 13:49:22 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Patch to fix transparent GIF89 bug in XMosaic 2.1

Hello all,

Well, the third round of testing finally identified (and fixed)
the problem I was having with some of my GIFs going blank.

The problem was that the global variables used to determine whether a
GIF is transparent are only initialized once.  Therefore, after the
first transparent GIF is read, Mosaic for X 2.1 treats all later 2-color
GIFs as if they too are transparent.

To test your system to see if it is affected by the Gif89 bug, look
at <http://www.ics.uci.edu/Test/Gif89-test.html>.  I have no idea 
whether or not the same bug affects the other Mosaic platforms.

What follows is the patch to fix the initialization bug in
Mosaic-2.1/src/gifread.c which causes 2-color inlined GIFs to appear
faded/blank after a transparent GIF89 has been read.

PLEASE NOTE that this is NOT AN OFFICIAL PATCH, but it works
and it is so small and harmless that it justifies a general post.
Be sure to be in the Mosaic-2.1/src before doing the patch.

....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)

-------------------------- cut here ----------------------------------
*** save/gifread.c	Sat Jan  8 11:36:09 1994
--- gifread.c	Sat Jan  8 11:43:37 1994
***************
*** 91,96 ****
--- 91,101 ----
  	verbose = FALSE;
  	showComment = FALSE;
  
+ 	Gif89.transparent = -1;
+ 	Gif89.delayTime = -1;
+ 	Gif89.inputFlag = -1;
+ 	Gif89.disposal = 0;
+ 
  	if (! ReadOK(fd,buf,6))
  	{
  #if 0




From rst@ai.mit.edu  Sat Jan  8 17:03:19 1994 EST
Message-Id: <9401082203.AA08999@volterra>
Date: Sat, 8 Jan 94 17:03:19 EST
From: rst@ai.mit.edu (Robert S. Thau)
Subject: More CGI Comments --- scripts, suffixes and such...

   From: rhb@hotsand.att.com

   ... [reformatted to fit on 80-character windows]

   3) Because of (1), (2) and my general preferences of arranging files, I
   find it would be much easier to identify executables on the server side
   by being able to use a server defined suffix (notwithstanding the
   previous arguments against this) for these files (e.g., .cgi).

   ...

   Rich Brandwein
   AT&T Bell Labs
   rich.brandwein@att.com

A note of agreement --- I implemented a suffix-based scheme myself a while
ago for some of the same reasons, and I've been running with it for a few
weeks now.  If you feel comfortable playing with unsupported software, you
could take a look at my hacks, which I've made available as a source patch
to NCSA httpd 1.0.  See

  http://www.ai.mit.edu/xperimental/run-scripts.html

for documentation and a pointer to the patch.

Briefly, the patch adds a new option, 'RunScripts' to the Allow directive
of access.conf and .htaccess files; this option designates a directory as
containing both files and scripts.  Scripts in these directories are
identified by a suffix (either '.doit' or '.nph', the latter having the
same effect as the 'nph-' prefix).

These suffixes are added to the incoming URL by the server during the
search for a script, and are therefore *not* present in the URLs which
invoke the scripts.  This means that any file in a 'RunScripts' directory
can be replaced with a script without changing documents which have links
to it.  This is a feature, IMATSHO (that is, In My And Tony Sanders' Humble
Opinions :-), although it does get a little awkward when the file being
replaced already had a type-suffix.

The main problem with any such scheme is security. For all sorts of
reasons, it's a bad idea to let The Outside World (including potential
attackers) read the code of your scripts.  In order to prevent this, at
least around here, it's *not* enough to prevent the server from tossing
files which appear to be scripts over the wall --- script code appears in
emacs backup and auto-save files as well, and so, to be thorough, I wound
up preventing the export of those.  The trouble is that it's hard to tell
when to stop --- scripts may eventually get patched, for instance; do we
ban retrieval of '*.orig'?

What might be safer is to have each directory contain either scripts or
files, but not both --- files could not be retrieved at all from a scripts
directory.  This would also get rid of the suffixes, which some people find
objectionable (even though most Web servers already use dot-suffixes to
determine a file's MIME type).  However, this does represent a real loss in
flexibility for the server maintainer as compared to the suffix hack.

Any comments?

rst



From fielding@simplon.ICS.UCI.EDU  Sat Jan  8 14:26:35 1994 -0800
Message-Id: <9401081426.aa08569@paris.ics.uci.edu>
Date: Sat, 08 Jan 1994 14:26:35 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: More CGI Comments 


Rich Brandwein writes:

> After playing with CGI-based httpd servers for awhile and writing scripts
> to them, I have the following observations/questions:
> 
> 1) If you let users export information via their UserDir
> (i.e., ~/public_html by default), how can you gracefully allow them to
> create anything that requires a shell execution without giving everyone
> write access to the cgi-bin directory or creating cgi aliases for all
> users in srm.conf?
>...

Just prior to reading this I was looking at a local notice about login
security.  Thus, my first thought was what would happen if some user
created a script which deletes (recursively) all of the files in the
invokers home directory.  Since the script would be executed under the
server's user ID (I think), would the script then delete all of the
server's subdirectories?

I'm not sure what would happen (I'm damn sure I don't want to test it),
but I think this question should be considered before allowing other
users to add scripts at will.


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From rst@ai.mit.edu  Sat Jan  8 18:15:43 1994 EST
Message-Id: <9401082315.AA09021@volterra>
Date: Sat, 8 Jan 94 18:15:43 EST
From: rst@ai.mit.edu (Robert S. Thau)
Subject: More CGI Comments 

   Date: Sat, 08 Jan 1994 14:26:35 -0800
   From: "Roy T. Fielding" <fielding@simplon.ics.uci.edu>

   Just prior to reading this I was looking at a local notice about login
   security.  Thus, my first thought was what would happen if some user
   created a script which deletes (recursively) all of the files in the
   invokers home directory.  Since the script would be executed under the
   server's user ID (I think), would the script then delete all of the
   server's subdirectories?

Depending on the local setup, it may or may not.  For instance, you could
have all the server's files owned by, say, 'webmaster', and run the server
itself under the uid 'nobody'.  The server's directories could then be
read-only to the server itself, and to any scripts which it happens to run.
However, this doesn't preclude other forms of mischief --- evasion of local
accounting rules, and so forth.

   I'm not sure what would happen (I'm damn sure I don't want to test it),
   but I think this question should be considered before allowing other
   users to add scripts at will.

It's a matter of local policy, really --- specifically, how much trust you
have in your users.  It's not an issue here, for instance, because people
here generally have write permission on the server's directories anyway.
If they want to destroy the server, nothing as messy as a trick script is
required.

Of course, we can afford to be that trusting because we don't have a large
population of potentially hostile users running directly on our machines.
People who aren't so blessed probably *shouldn't* allow users to add
scripts at will --- or should at least restrict the privilege to users who
aren't likely to abuse it.  Any server which provides such a facility
should likewise provide the adminstrator with the tools to control it, via
access config files or the like.

(Incidentally, I don't think that any server should default to the sort of
wide-open configuration that I have running here --- it makes it too easy
for naive sysadmins to get into trouble.  However, for those of us who can
use the option, it's very nice to have it).

   ....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
		      (fielding@ics.uci.edu)

rst



From mueller@sc.ZIB-Berlin.DE  Sun Jan  9 00:30:56 1994 +0100
Message-Id: <9401082330.AA11610@ave.ZIB-Berlin.DE>
Date: Sun, 9 Jan 94 00:30:56 +0100
From: mueller@sc.ZIB-Berlin.DE (Peter Mueller)
Subject: question to mosaic word wrapping

hello all,

i'm still wondering about mosaic's wrapping mechanism. i've specified
the following sequence in one of my html-documents:

<code><i>name</i>.ext</code>

i would like to express, that 'name' is user defined, whereas '.ext'
is some kind of standard extension.

under some circumstances, mosaic splitts this 'atomic' element in the
following form:

... name
.ext ...

where 'name' and '.ext' are correctly emphasized. can someone please
say, if this wrapping is correct? and why? do </i> imply a word separation?

best wishes and thanks for any help,

peter



From jtilton@jupiter.willamette.edu  Sat Jan  8 14:55:26 1994 -0800 (PST)
Message-Id: <Pine.3.88.9401081416.A4979-0100000@jupiter>
Date: Sat, 8 Jan 1994 14:55:26 -0800 (PST)
From: jtilton@jupiter.willamette.edu (James)
Subject: Re: More CGI Comments 

On Sat, 8 Jan 1994, Roy T. Fielding wrote:

> > 1) If you let users export information via their UserDir
> > (i.e., ~/public_html by default), how can you gracefully allow them to
> > create anything that requires a shell execution without giving everyone
> > write access to the cgi-bin directory or creating cgi aliases for all
> > users in srm.conf?
> >...

What we've done here at Willamette is to create a pair of local groups, 
one called "webmgr" and one called "webdev".  "webmgr" is for the few 
people trusted to work on the main trunk of our web (althought I'm pretty 
much the only one who uses it :) ), but "webdev" is for a group of 
students who want to collaboratively work on WWW projects.  "webdev" has 
been given an additional directory for developing their own scripts.  
Granted, this is a "human" solution rather then a technical solution, as
it still relies on giving access to those who are trusted -- it doesn't 
address giving every user access to this capability.

> Just prior to reading this I was looking at a local notice about login
> security.  Thus, my first thought was what would happen if some user
> created a script which deletes (recursively) all of the files in the
> invokers home directory.  Since the script would be executed under the
> server's user ID (I think), would the script then delete all of the
> server's subdirectories?
> 

On the NCSA httpd, at least, I beleive that if root is running the 
server, there's an option for the server to change it's UID.  Ours 
changes to "nobody", which means that the server really can't do much 
except read world-readable files.

However, this approach may not make sense for future development.  
Specifically, what about the eventual use of PUT and POST protocols for 
things like dynamic document generation?  It might be nice, for instance, 
to be able to edit a document that I've grabbed over the web, and use PUT 
to create a new revision for the changes I've made (shades of Xanadu!).  
Granted, this isn't immediately looming, but it wouldn't be implementable 
if the server can change the documents it has control over...

						-et

/ (James) Eric Tilton, Student AND Student Liaison, WITS               \
\ Class of '95 - CS/Hist  -- Internet - jtilton@willamette.edu         /
<a href="http://www.willamette.edu/~jtilton/">ObHyPlan!</a>, chock fulla
<a href="http://www.willamette.edu/~jtilton/whatsnew.html">Fun Stuff!</a>




From jtilton@jupiter.willamette.edu  Sat Jan  8 16:12:25 1994 -0800 (PST)
Message-Id: <Pine.3.88.9401081523.A6844-0100000@jupiter>
Date: Sat, 8 Jan 1994 16:12:25 -0800 (PST)
From: jtilton@jupiter.willamette.edu (James)
Subject: HTML spec. questions

Following my comments on comp.infosystems.www about device-indpendent 
HTML and the like, a number of people have suggested that what's needed 
is a tool for the checking of HTML code for inconsistencies and bad 
practices.  To that end, I'm starting to work on "lint for the web" sort 
of program.

To that end, I've been reading through the specification for HTML at 
CERN, and have some questions:

  * the comment is made in
    http://info.cern.ch/hypertext/WWW/MarkUp/Text.html that "neither spaces
    nor tabs should be used to make SGML source layout more attractive to
    read".  This is understandable in the case of tabs, since their
    behaviour is undefined.  But why shouldn't somebody use spaces to make
    their HTML source more readable?  I thought the specification called
    for spaces to be collapsed into a single space?  Or do we not get to
    make that assumption?  I'd like to be able to format my HTML like:

<ul>
<li> this is my unordered list.  I realize that this first entry is awfully
     long, and I'd like to have spaces to indent it in the source in order
     to make it readable to me, as an author.
  <ul>
  <li> and if I nest lists, I'd like to be able to indent, so I don't
       lost track of things.
  </ul>
</ul>

    Is this sort of thing acceptable?  Shouldn't it be?

  * It's not explicitly declared in the specification whether a <HR>
    implies a paragraph break.  I'm assuming it does, but I'd like
    confirmation :).

  * On that note, I'm also under the impression that if an element implies
    a paragraph break (such as the ADDRESS element), then a <P> should
    neither be place immediately before OR after it.  Is this correct?

  * Does PRE imply a paragraph break?

I'm not sure yet what the scope of this program will be.  At the minimum, 
it will do things like point out incorrect usages of <P>, and other 
things which are pointed out as not recommended by the specification.  
That is, things that will be parsed successfully, but aren't really 
device independent.  I'm not sure whether or not it should also check to 
see whether the HTML is just plain illegal -- is this neccessary or even 
desired functionality?  (And do I want to go to the extra effort?  :) )

Any comments appreciated!

						-et

/ (James) Eric Tilton, Student AND Student Liaison, WITS               \
\ Class of '95 - CS/Hist  -- Internet - jtilton@willamette.edu         /
<a href="http://www.willamette.edu/~jtilton/">ObHyPlan!</a>, chock fulla
<a href="http://www.willamette.edu/~jtilton/whatsnew.html">Fun Stuff!</a>




From hotsand!rhb  Sat Jan  8 20:27:36 1994 EST
Message-Id: <9401090127.AA28418@hotsand.dacsand>
Date: Sat, 8 Jan 94 20:27:36 EST
From: hotsand!rhb (Rich Brandwein)
Subject: Re: More CGI Comments


Robert S. Thau writes:
> 
> The main problem with any such scheme is security. For all sorts of
> reasons, it's a bad idea to let The Outside World (including potential
> attackers) read the code of your scripts.  In order to prevent this, at
> least around here, it's *not* enough to prevent the server from tossing
> files which appear to be scripts over the wall --- script code appears in
> emacs backup and auto-save files as well, and so, to be thorough, I wound
> up preventing the export of those.  The trouble is that it's hard to tell
> when to stop --- scripts may eventually get patched, for instance; do we
> ban retrieval of '*.orig'?
> 
> What might be safer is to have each directory contain either scripts or
> files, but not both --- files could not be retrieved at all from a scripts
> directory.  This would also get rid of the suffixes, which some people find
> objectionable (even though most Web servers already use dot-suffixes to
> determine a file's MIME type).  However, this does represent a real loss in
> flexibility for the server maintainer as compared to the suffix hack.

All my scripts have a #! notation at the top.  I would think looking
for any files of this type would indicate scripts (though this may
be unmanageable/inefficient).  In any case, it's not clear to me that
looking through old versions of scripts that may exist in a directory
is particularly dangerous (especially if, as you say, they are are typically
saved with a common suffix). 

By default, many systems don't allow read access to "world" in the umask.  If 
you run the server as "nogroup" you would thus need to "make public" each
file after you've written them.  Even if this isn't the case, you can do 
things like set emacs to backup files to a mask that prevents group/world
read.  BTW, some previous random tests have found that
about 80% of all Plexus based servers out there have their scripts wide open and readable
by just knowing a common place to look for them (this may have changed, though).
In general, I actually prefer this since it makes it easier to share scripts
(though I agree it is dangerous).

Even if we assume that we segregate scripts into seperate 
directories for users, we can't let all the users use the same bin
directory for scripts (one possible solution is assuming
by default in the httpd server a public_html/cgi-bin directory for add-on users...?).

> 
> Any comments?
> 
> rst
> 

Rich



From HARMO@valt.Helsinki.FI  Mon Jan  9 17:22:43 1994 EET DST
Message-Id: <MAILQUEUE-101.940109172243.416@valt.Helsinki.FI>
Date: 9 Jan 94 17:22:43 EET DST
From: HARMO@valt.Helsinki.FI (Timo Harmo - SocSci U of Helsinki)
Subject: WWW as a seminar-environment

I am writing a paper on the use of WWW (or in general : hypertext
and networks) as an environment for seminars in higher education.
What I have in mind is a hypertextual set of course-materials
and a forum for discussion between teachers/students/...
that could be used to support normal university seminars and
courses (I am not dreaming of a "virtual university").

Could you tell me if there are grave errors in the following claims
I am about to make in the paper:

 * Currently some WWW-clients make it easy for the students to
comment any part of the materials presented on the Web by sending
private e-mail messages. Soon some servers will make it
possible also to make these comments and any discussion
also in public ("Post"-method).
   (will clients also be able to take full advantage of this?
    How will it be indicated to a user that there exist comments
    to the document they are reading?
    Will many clients/servers be able to handle posting?)

 * WWW will allow for closed seminars by use of its authentication
  and authorizations schemes.

 * WWW will not be a "full-blown seminar system" for some
   time, so other systems (e-mail, news or mailing-lists) will
   be needed to supplement it.
     -eg. I have not seen any discussion on easy-to-use mechanisms
     (like newsrc) for checking out if there are new contributions.

  * Running a seminar on the Web will require non-minimal
    computer skills for some time to come.
     (I am thinking of teachers who use just wordprocessors and
      e-mail using PC's or Macintoshes -
     or how do you do things like having received a comment in e-mail,
     check out the  text it was a comment on, and maybe add the
     comment to the web via a link in the original document - or could
     e-mail comments replaced by "private comments" that
     are stored in the web but that only the author of the
     commented text is authorized to see).

By the way, when will this seminar (www-talk) move to the Web?

 -Timo




From HARMO@valt.Helsinki.FI  Sun Jan  9 16:44:24 1994 EET DST
Message-Id: <MAILQUEUE-101.940109164423.256@valt.Helsinki.FI>
Date: Sun, 9 Jan 1994 16:44:24 EET DST
From: HARMO@valt.Helsinki.FI (TIMO HARMO)
Subject: WWW as a seminar-environment

I am writing a paper on the use of WWW (or in general : hypertext 
and networks) as an environment for higher education.
What I have in mind is a hypertextual set of course-materials
and a forum for discussion between teachers/students/...
that could be used to support normal university seminars and
courses (I won't be dreaming of a "virtual university").

Could you tell me if there are grave errors in the following claims
I am about to make in the paper:

 * Currently some WWW-clients make it easy for the students to 
comment any part of the materials presented on the Web by sending 
private e-mail messages. Soon some servers will make it
possible also to make these comments and any discussion
also in public ("Post"-method).
   (will clients also be able to take full advantage of this?
    How will it be indicated to a user that there exist comments
    to the document they are reading?
    Will many clients/servers be able to handle posting?)

 * WWW will allow for "closed seminars" by use of its authentication 
  and authorizations schemes.

 * WWW will not be a "full-blown seminar system" for some
   time, so other systems (e-mail, news or mailing-lists) will
   be needed to supplement it.
     -eg. I have not seen any discussion on easy-to-use mechanisms
     (like newsrc) for checking out if there are new contributions.

  * Running a seminar on the Web will require non-minimal
    computer skills for some time to come.
     (I am thinking of teachers who use just wordprocessors and
      e-mail using PC's or Macintoshes -
     or how do you do things like having received a comment in e-mail, 
     check out the  text it was a comment on, and maybe add the 
     comment to the web via a link in the original document - or could
     e-mail comments replaced by "private comments" that
     are stored in the web but that only the author of the
     commented text is authorized to see).
     
By the way, when will this seminar (www-talk) move to the Web?

 -Timo   




From rst@ai.mit.edu  Sun Jan  9 13:45:33 1994 EST
Message-Id: <9401091845.AA09251@volterra>
Date: Sun, 9 Jan 94 13:45:33 EST
From: rst@ai.mit.edu (Robert S. Thau)
Subject: More CGI Comments

   From: rhb@hotsand.att.com
   Date: Sat, 8 Jan 94 20:27:36 EST

   All my scripts have a #! notation at the top.  I would think looking
   for any files of this type would indicate scripts (though this may
   be unmanageable/inefficient).

Good idea.  There are two complications.  First off, to make this work
properly, you need to check for the binary executable magic numbers, in
addition to '#!', so the server can run C programs as 'scripts'.  (Such
programs exist --- imagemap, for one).

The other, more serious, complication is that if some binary data file
happens to begin with a magic number, the server would refuse to serve it
up (instead trying, and failing, to run it).  If I'm not mistaken, most
common binary data formats (Sun .au, AIFF, JPEG (JFIF), MPEG, tar,
compressed data...) define their own series of magic numbers which are
unlikely to conflict, but relying on this would be dicey.

I don't see that efficiency is necessarily an issue --- if the thing in
question turns out to be a script, then the open and read for the magic
number check is minor next to the cost of actually running the script (this
assumes, of course, that open() is cheaper than exec()).  Conversely, if it
turns out to be an ordinary file, then the server would have to open and
read it anyway.

   In any case, it's not clear to me that
   looking through old versions of scripts that may exist in a directory
   is particularly dangerous (especially if, as you say, they are are typically
   saved with a common suffix). 

It's every bit as dangerous (or not) as letting people browse the current
versions, unless the last edit closed *all* the security holes which the
old versions may have had.  Maybe it did, but in my experience, this is
not the way to bet.

   Even if we assume that we segregate scripts into seperate directories
   for users, we can't let all the users use the same bin directory for
   scripts (one possible solution is assuming by default in the httpd
   server a public_html/cgi-bin directory for add-on users...?).

   Rich

At sites where ordinary users can and do put up scripts, something better
than a single cgi-bin is obviously necessary.  In conversation on this list
over the past couple of weeks, people have discussed all sorts of
alternatives...

  *) Let users create (and designate) their own bin directories,
     with a  ~/cgi-bin default, .htaccess file, or some similar mechanism.

  *) Let them mix files and scripts in certain directories ---
     the server tells the files from the scripts by

     - magic number tests (as you suggest, including '#!' as a magic number)

     - naming conventions (as in the server running here)

     - -x bits set on the individual files (praised by some for simplicity,
       opposed by others because of the traps it lays for the bumble-fingered)

     - explicit designation as scripts in some sort of external meta-database,
       such as the GN server's .cache files.

To lay my own cards on the table, I think it's important for access control
that scripts have some sort of mark on them which they cannot easily lose.
If that can be achieved without segregating scripts from ordinary files by
directory, so much the better.

IMHO, a suffix naming convention is all right from this perspective, while
I have my doubts about -x bits (because, in my experience, accidental
chmods are easier to make than accidental renames, and harder to detect).

If the binary-data-format problem can be resolved, then the magic number
idea is better yet.  It helps a lot with the "'*.orig'-and-then-what-else?"
problem of trying to anticipate where outdated versions of script code
might appear.

However, I could live with any of these alternatives.  What would be nice
at this point would be to develop a consensus around one of them, and get
it into the servers, so that users could actually have the capability
(where, of course, the server administrator feels it proper to grant it).

rst



From hotsand!rhb  Sun Jan  9 18:53:13 1994 EST
Message-Id: <9401092353.AA07290@hotsand.dacsand>
Date: Sun, 9 Jan 94 18:53:13 EST
From: hotsand!rhb (Rich Brandwein)
Subject: Passing info between pages


I'm looking for guidance on a way of passing information
between pages (specifically forms).  

There are many application in which may want to pass "state"
information between pages. For instance (based on an app 
someone's written here) I may want to select a database
from an initial form and be able to bring up a generic
page that allows you to search on that particular database.
How do I pass information to this "generic" page?  

The only viable methods in a form (at least that I can 
see) is to pass the information as a value to a form field
or pass the information via the *name* of a form field
(i.e., you're limited to the values passed in the query).
The latter method allows you to pass the information tranparently
to the user.  This seems a bit awkward - is there another 
method?  

For the non-form case, my application can add variables
to the query subset of the path and I can pass this transparently
via the POST method.  In the form case, the query subset is
under control of the form (e.g., I can't stick query terms into
the form action) as far as I can tell.

Rich



From hotsand!rhb  Sun Jan  9 20:04:00 1994 EST
Message-Id: <9401100104.AA07753@hotsand.dacsand>
Date: Sun, 9 Jan 94 20:04:00 EST
From: hotsand!rhb (Rich Brandwein)
Subject: Re: More CGI Comments


>    All my scripts have a #! notation at the top.  I would think looking
>    for any files of this type would indicate scripts (though this may
>    be unmanageable/inefficient).
> 
> Good idea.  There are two complications.  First off, to make this work
> properly, you need to check for the binary executable magic numbers, in
> addition to '#!', so the server can run C programs as 'scripts'.  (Such
> programs exist --- imagemap, for one).
> 
> The other, more serious, complication is that if some binary data file
> happens to begin with a magic number, the server would refuse to serve it
> up (instead trying, and failing, to run it).  If I'm not mistaken, most
> common binary data formats (Sun .au, AIFF, JPEG (JFIF), MPEG, tar,
> compressed data...) define their own series of magic numbers which are
> unlikely to conflict, but relying on this would be dicey.

This in fact points up the need to have a "smart" server.  It is important
that the server be able to identify the files it's serving not just by their
suffix (i.e., in the mime type file) but via any available attribute
(such as magic numbers).  The information that's available differs
among platforms and application (server) needs.  I'm in agreement that
we need a flexible method that allows the server to use it's "smarts"
and the users' needs (e.g., the scripts I am interested in providing
are all perl and tcl, so the #! would work for my application...).

> However, I could live with any of these alternatives.  What would be nice
> at this point would be to develop a consensus around one of them, and get
> it into the servers, so that users could actually have the capability
> (where, of course, the server administrator feels it proper to grant it).

Based on some limited discussion here, I propose the following: 

Add to the srm.conf file:
***************************

# UserExec: Whether local user executables is activated
# 

UserExec True
***************************

Add a .srm_conf file in the users public_html directory
(or whatever it's set to in the server's srm.conf file)
***************************

# ScriptAlias: This controls which directories contain server scripts.
# Format: ScriptAlias fakename realname

ScriptAlias /cgi-bin /home/rhb/myscripts


#and possibly a line that includes info on how I identify 
#my executable files

ExecAlias magic, suffix=exe
***************************

Now a more general question that turns the previous question inside
out.  The original concern was in adding executables to directories
that were not indicated as having executables.  Well it seems that
I can certainly have "regular" (non script) html files in executable
directories.  What do I gain from having a directory "not executable"?
Why aren't all directories cgi-bin directories?  I can restrict
GET in the directories in which I really have scripts if I'm concerned
about reading scripts...

One point brought to my attention is that all directories are already
"executable" in the NCSA server environment via the <inc var> directive
(i.e., this takes care of my original problem of getting to the
user variable in authentication), but it doesn't take care of things
like forms.

Rich

> 
> rst

By the way - I'm continuing to receive all kinds of spurious messages
from the listserv server (e.g., the error messages for unknown users
are returned to me...).



From kevinh@eit.COM  Sun Jan  9 18:15:12 1994 -0800
Message-Id: <199401100215.SAA08606@kmac.eit.com>
Date: Sun, 9 Jan 1994 18:15:12 -0800
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re: minimal HTML

Kevin Altis (altis@ibeam.jf.intel.com) writes:

 |propose that the HTML below is probably the minimal HTML file (assuming
 |mailto: is still okay ;-). HTML Guide writers, people writing translators
 |from latex, rtf, etc. to HTML, etc. might want to use this. Comments,
 |additions, tomatoes?

	I agree that the <HTML>, <HEAD>, and <BODY> tags should be a
part of standard practice, but for new and old manual HTML writers,
it can get pretty tedious, unless you have great command over macros.
	I'd really like to see a program that can roam around
a Web directory tree and add standard tags like these automatically
(and/or substitute user-defined strings), like an HTML "lint" that can
clean up for you. I've played with an older HTML checker before, but it
didn't alert me to the lack of those tags, and its configuration was a bit
arcane.
	Until HTML editors/convertors are widely used, this would be the
best way of assuring standard "by the book" HTML. Who'd like to
volunteer to write this needed webmaster's tool? I'm a bit busy at the
moment! :)

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://UNDER.CONSTRUCTION/)
Hypermedia Industrial Designer * Duty now for the future!



From kevinh@eit.COM  Sun Jan  9 18:39:01 1994 -0800
Message-Id: <199401100239.SAA08708@kmac.eit.com>
Date: Sun, 9 Jan 1994 18:39:01 -0800
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re:  HTML spec. questions

(James) Eric Tilton writes:

> Following my comments on comp.infosystems.www about device-indpendent
> HTML and the like, a number of people have suggested that what's needed
> is a tool for the checking of HTML code for inconsistencies and bad
> practices.  To that end, I'm starting to work on "lint for the web" sort
> of program.

	I think you could put in a user-defined flag, to see whether
the program would collapse empty lines and remove empty spaces before
lines.
	I'd like to know what's correct about paragraph breaks myself;
"<pre>a line</pre>" puts a break at the end in Mosaic for X, but not
MacMosaic; the same for ending header tags and list tags.
	I'd like the program to traverse a directory tree and automagically
fix the files, say replacing them and moving the original to
"original.html.bak" or "original.html~" or some other user-defined suffix.
Perhaps a "noclobber" option would be in order...
	...and it would be great to be able to substitute strings, so when
host names in URLs change, I could fix all my HTML documents in one
swoop! Well, I don't want to sound pushy, but I've been wanting to do that
for some time. Good luck and thanks for taking on the job!

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://UNDER.CONSTRUCTION/)
Hypermedia Industrial Designer * Duty now for the future!



From henrich@crh.cl.msu.edu  Sun Jan  9 23:03:39 1994 -0500 (EST)
Message-Id: <9401100403.AA09295@crh.cl.msu.edu>
Date: Sun, 9 Jan 1994 23:03:39 -0500 (EST)
From: henrich@crh.cl.msu.edu (Charles Henrich)
Subject: Re: minimal HTML

> Kevin Hughes * kevinh@eit.com
> Enterprise Integration Technologies Webmaster (http://UNDER.CONSTRUCTION/)
> Hypermedia Industrial Designer * Duty now for the future!

Whats this, EIT snapping up all the available web wizards are they?

-Crh

    Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu

                     http://rs560.msu.edu/~henrich/



From jtilton@jupiter.willamette.edu  Sun Jan  9 21:00:00 1994 -0800 (PST)
Message-Id: <Pine.3.88.9401092012.A5280-0100000@jupiter>
Date: Sun, 9 Jan 1994 21:00:00 -0800 (PST)
From: jtilton@jupiter.willamette.edu (James)
Subject: Re: HTML spec. questions

On Sun, 9 Jan 1994, Kevin 'Kev' Hughes wrote:

> (James) Eric Tilton writes:
> 
> > Following my comments on comp.infosystems.www about device-indpendent
> > HTML and the like, a number of people have suggested that what's needed
> > is a tool for the checking of HTML code for inconsistencies and bad
> > practices.  To that end, I'm starting to work on "lint for the web" sort
> > of program.
> 
> 	I think you could put in a user-defined flag, to see whether
> the program would collapse empty lines and remove empty spaces before
> lines.
> 	I'd like to know what's correct about paragraph breaks myself;
> "<pre>a line</pre>" puts a break at the end in Mosaic for X, but not
> MacMosaic; the same for ending header tags and list tags.

Also, I've noticed that <HR> doesn't always put white space before or 
after itself.  I'd like to suggest that all tags that imply a paragraph 
break also imply white space before AND after themselves.  I'd also like 
to suggest that <HR> should officially imply a paragraph break, since it 
effectively "breaks two sections of text that might otherwise flow 
together".

Out of curiosity, how up to date is the HTML spec at CERN?  Is that the 
canonical reference?  If not, does such a thing exist?  It seems like 
such a thing is *really* important to have, especially with all these 
clients.

> 	I'd like the program to traverse a directory tree and automagically
> fix the files, say replacing them and moving the original to
> "original.html.bak" or "original.html~" or some other user-defined suffix.
> Perhaps a "noclobber" option would be in order...
> 	...and it would be great to be able to substitute strings, so when
> host names in URLs change, I could fix all my HTML documents in one
> swoop! Well, I don't want to sound pushy, but I've been wanting to do that
> for some time. Good luck and thanks for taking on the job!

Hmmmm.  String substitution would be a useful feature...  As for directory
traversal, I'll bear it in mind, but I won't commit to it yet.  (If worse
comes to worse, you could use "find" to apply the checker to files in a
tree *ducking to avoid tomatoes* :) I think my first goal will be to write
some kind of reusable module/object that'll take in code and output
"correct" code (or warnings).  Then this can be incorporated into other
things like web spiders or browsers, perhaps.  I'll try and work up a
prototype in this next week or so, and see how it goes. 

						-et

/ (James) Eric Tilton, Student AND Student Liaison, WITS               \
\ Class of '95 - CS/Hist  -- Internet - jtilton@willamette.edu         /
<a href="http://www.willamette.edu/~jtilton/">ObHyPlan!</a>, chock fulla
<a href="http://www.willamette.edu/~jtilton/whatsnew.html">Fun Stuff!</a>




From earhart+@CMU.EDU  Mon Jan 10 01:06:17 1994 -0500 (EST)
Message-Id: <IhAD1Ni00WC7EZ_GJj@andrew.cmu.edu>
Date: Mon, 10 Jan 1994 01:06:17 -0500 (EST)
From: earhart+@CMU.EDU (Rob Earhart)
Subject: Re: Passing info between pages

rhb@hotsand.att.com writes:
> I'm looking for guidance on a way of passing information
> between pages (specifically forms).  
> 
> There are many application in which may want to pass "state"
> information between pages. For instance (based on an app 
> someone's written here) I may want to select a database
> from an initial form and be able to bring up a generic
> page that allows you to search on that particular database.
> How do I pass information to this "generic" page?  

  I'm not sure why you couldn't use an "almost" generic page: your
server hands the user the database-selection page, user selects a
database, and your server replies with a form page for the lookup in
which the <form action> field has been frobbed as needed so that when
the user completes the form, what they hand back to you embeds the name
of the database being selected.  Does this page-rewriting really seem
awkward?

  Otherwise, I don't see how you could do it; WWW being a stateless
protocol, they can hand you a query resulting from the completion of
the form in that "generic" page at any time, with nothing to determine
which database they're expecting to perform a lookup on...

  )Rob



From henrich@crh.cl.msu.edu  Mon Jan 10 01:15:27 1994 -0500 (EST)
Message-Id: <9401100615.AA09626@crh.cl.msu.edu>
Date: Mon, 10 Jan 1994 01:15:27 -0500 (EST)
From: henrich@crh.cl.msu.edu (Charles Henrich)
Subject: Whitespace

I'd like to suggest that *no* tags imply any whitespace.  If you want
whitespace, explicitly place a <p> tag in the text.  By forcing everything
explicit, we can be more flexible.

-Crh

    Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu

                     http://rs560.msu.edu/~henrich/



From jtilton@jupiter.willamette.edu  Sun Jan  9 21:07:41 1994 -0800 (PST)
Message-Id: <Pine.3.88.9401092109.A5280-0100000@jupiter>
Date: Sun, 9 Jan 1994 21:07:41 -0800 (PST)
From: jtilton@jupiter.willamette.edu (James)
Subject: Rough Draft -- "Good HTML Composition" document

Ok, in preparation to get down and dirty with an HTML "lint" (hey, can 
anyone think of a catchy name for this program?  "html-lint" just doesn't 
cut it with the home crowd at Peoria :), I've prepared a document of 
common errors and things to avoid in HTML composition.  It doesn't really 
say anything new or exciting that isn't in the specification or style 
guide, but it does (hopefully) place them all in the same document for 
Handy Reference (tm).  (Note, please, that this is still a *rough 
draft*)  Please check out

  http://www.willamette.edu/html-composition/strict-html.html

and comment on it.  Specifically, is there anything else that should be 
in there?  Is there anything that shouldn't?  Are any of my assumptions 
wrong?

I'm also thinking about hanging some subdocuments off of it that give 
examples of the mistakes, etc.  Any comments?  (And if anyone wants to 
contribute such a subdocument...)

Cheers,

							-et

/ (James) Eric Tilton, Student AND Student Liaison, WITS               \
\ Class of '95 - CS/Hist  -- Internet - jtilton@willamette.edu         /
<a href="http://www.willamette.edu/~jtilton/">ObHyPlan!</a>, chock fulla
<a href="http://www.willamette.edu/~jtilton/whatsnew.html">Fun Stuff!</a>




From jtilton@jupiter.willamette.edu  Sun Jan  9 22:30:48 1994 -0800 (PST)
Message-Id: <Pine.3.88.9401092243.A5280-0100000@jupiter>
Date: Sun, 9 Jan 1994 22:30:48 -0800 (PST)
From: jtilton@jupiter.willamette.edu (James)
Subject: Re: Whitespace

Some tags already do, however, such as headers.  If we change this, then 
a lot of HTML gets broken.  Besides, I think that it does make sense for 
certain tags to imply whitespace.  I'm under the impression that the <p> 
exists for those times when it isn't otherwise evident by the semantic 
encoding of the other tags whether there should be whitespace or not.  
I'm of the belief that the authors of documents don't have any business 
having control over where white space is in a document, because it's 
contrary to the principle of giving control over formatting to the 
browser.  And when people do try to use <p> to force whitespace -- to 
take layout considerations into their own hands -- it leads to documents 
that look good on the author's browser, but may not on other's browsers.  
For example, the NCSA What's New document has <p>'s after every <li> 
item, and it looks kinda silly when viewed on Lynx.

						-et

/ (James) Eric Tilton, Student AND Student Liaison, WITS               \
\ Class of '95 - CS/Hist  -- Internet - jtilton@willamette.edu         /
<a href="http://www.willamette.edu/~jtilton/">ObHyPlan!</a>, chock fulla
<a href="http://www.willamette.edu/~jtilton/whatsnew.html">Fun Stuff!</a>


On Mon, 10 Jan 1994, Charles Henrich wrote:

> I'd like to suggest that *no* tags imply any whitespace.  If you want
> whitespace, explicitly place a <p> tag in the text.  By forcing everything
> explicit, we can be more flexible.
> 
> -Crh
> 
>     Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu
> 
>                      http://rs560.msu.edu/~henrich/
> 



From john@math.nwu.edu  Wed Dec 29 21:15:58 1993 -0600 (CST)
Message-Id: <9312300315.AA00800@hopf.math.nwu.edu>
Date: Wed, 29 Dec 1993 21:15:58 -0600 (CST)
From: john@math.nwu.edu (John Franks)
Subject: CGI and typing files by suffix

According to Robert S. Thau:
> 
> n) There should not be any indication within the URL, selector string,
>    etc., as to whether or not a retrieval will cause a script to be
>    invoked.
> 
> The reason I want it is that having put something up as a file, or
> collection of files, I may want to turn it into a script, without having to
> track down all the references to it and change them as well --- which I
> would necessarily have to do if the script/file distinction were explicit
> in the URL (selector, whatever).  

I know you have alreadly expressed a distaste for the redirection
mechanism in HTTP/1.0, but this situation is exactly what it is
designed to deal with.  If you find this mechanism cumbersome in the
server you use then perhaps that is the part of the server you need to
rewrite.

> 
> Now, in order to satisfy the goal above, you need some way of
> distinguishing the scripts from the ordinary files, other than selector
> syntax.  In other words, you need a mechanism for typing the files.
> 
> Like it or not, most of the existing servers already have such a mechanism
> --- to tell what type a file is (in order to report the proper MIME type),
> they discriminate on the basis of the name.  If you put a GIF file up under
> the name 'foo.au', or even just plain 'foo', then (with the stock NCSA
> server, and Plexus and CERN as well, I believe), the wrong MIME type will
> be reported back to the client, and things fall apart.

If typing files by suffix were adequate there would be no point in
using the MIME type at all, the client could just look at the
suffix. In HTTP/0.9 file types were determined by suffix and there was
no MIME type.  One of the reasons for HTTP/1.0 was to get away from
this limitation.  It is reasonable for the default MIME type for a
file named foo.gif to be image/gif, but it is not reasonable to
require that an image/gif file must have a name with suffix .gif.

It is a bad idea to do so, but surely it should be *possible* with any
HTTP/1.0 server to put up a GIF file with the name foo.au and have it
work perfectly.  BTW, the reason it is a bad idea is precisely the
same reason it is a bad idea to have PATH_INFO data look like part of
the path -- it is misleading and obscure.


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu




From dsr@hplb.hpl.hp.com  Mon Jan 10 10:08:08 1994 GMT
Message-Id: <9401101008.AA15175@manuel.hpl.hp.com>
Date: Mon, 10 Jan 94 10:08:08 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re:  Initializing HTTP headers from HTML documents

John Ellson writes:

> I agree with your suggestion that one way to maintain
> cache coherency is by a background polling activity prior to the
> document being re-requested from the cache.  This improves cache
> response at the expense of some background polling overhead.  I think
> this suggestion can be combined with Roy's proposal (see below).
...

Thanks for working thru the arguments nicely. I agree that EVAL would
only be useful for the primary server, and hence better done by making
using a script to generate the document instead.

>        If the document has an unexpired "Expires" statement in the
>        header then a cache server can immediately serve a document 
>        without checking back with the original server.

>        A document that has expired should not be served, instead a
>        new copy should be obtained from the original server.

>        A document without an "Expires" statement should be checked
>        against the source to see if the original document still
>        exists and is unchanged.  Cache implementers can do this
>        either as a periodic polling activity or at the time the
>        document is re-requested.  In either case the poll can be
>        done using Roy's suggested protocol.

This seems like a workable approach. All we need to do now is to
persuade people to modify their servers to implement Roy's check on
a "Last-Modified:" header in the client's request for HEAD and GET.
This can be done incrementally, as Roy points out:

> Note that implementing this protocol would have no effect whatsoever
> on existing servers and clients.  Old clients (and any without caches)
> would just continue making requests without Last-modified headers.
> Old servers (at least the NCSA httpd 1.0 that I use) will already accept
> a message of the above format and just ignore the Last-modified header.

Many thanks,

Dave Raggett



From neuss@igd.fhg.de  Mon Jan 10 12:04:12 1994 +0100
Message-Id: <9401101104.AA02628@fourroses>
Date: Mon, 10 Jan 94 12:04:12 +0100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: Re: Passing info between pages

Dear fellow Webbers,

rhb@hotsand.att.com writes:
> I'm looking for guidance on a way of passing information
> between pages (specifically forms).  

> 

> There are many application in which may want to pass "state"
> information between pages. For instance (based on an app 

> someone's written here) I may want to select a database
> from an initial form and be able to bring up a generic
> page that allows you to search on that particular database.
> How do I pass information to this "generic" page?  


This is actually a bit of a problem.. we started using forms as an oracle
front end and of course have run into the same problems.

What we do in order to pass information is the following:
1. every forms document is represented by a virtual HTML document which
   contains $variables. These will be replaced with information from
   the database.
2. virtual documents are called up by the query mechanism.
   Example:
     <FORM ACTION=
     "http://maotai/htbin/query/next=$trans_id/WWW/HTML/v-dieReise.html">
   Since the documents are accessed via queries, one can pass additional
   information in the URL. In our case this is a transaction id (which
   represents the database connection) and the document which is to be
   retrieved as result of the form action.
   A simple program can then strip "id" and access the database to make
   a query (any kind of state information can be identified via "id" and
   represented in the database). Database values can then be entered into
   the forms or other HTML documents by use of $variables.
   Example:
     <INPUT NAME="name" VALUE="$name"> Username	 ...
     This is the name: <B> $name </B>
     

If anybody is interested in details, feel free to send me email.

Hope the above helps,
Chris
--
/*
 *  Christian Neuss  %  neuss@igd.fhg.de  %  ..in the humdrum
 */



From dsr@hplb.hpl.hp.com  Mon Jan 10 11:40:58 1994 GMT
Message-Id: <9401101140.AA15662@manuel.hpl.hp.com>
Date: Mon, 10 Jan 94 11:40:58 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Whitespace

> I'd like to suggest that *no* tags imply any whitespace.  If you want
> whitespace, explicitly place a <p> tag in the text.  By forcing everything
> explicit, we can be more flexible.

Surely, block elements should and do imply vertical whitespace.

Dave



From m.koster@nexor.co.uk  Fri Jan  7 17:09:42 1994 +0000
Message-Id: <9401101156.AA17015@dxmint.cern.ch>
Date: Fri, 07 Jan 1994 17:09:42 +0000
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: Additions to the CGI archive

> What do you mean with internal:image/folder?
> Means internal: a ressource the client must have. This is a very hard
> decission. I think the way to define it as feature is better.

YEs, it's not what I'd prefer. I think some "magic URL"s are fine...


-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From henrich@crh.cl.msu.edu  Mon Jan 10 10:32:12 1994 -0500 (EST)
Message-Id: <9401101532.AA10666@crh.cl.msu.edu>
Date: Mon, 10 Jan 1994 10:32:12 -0500 (EST)
From: henrich@crh.cl.msu.edu (Charles Henrich)
Subject: Re: Whitespace

> > I'd like to suggest that *no* tags imply any whitespace.  If you want
> > whitespace, explicitly place a <p> tag in the text.  By forcing everything
> > explicit, we can be more flexible.
>
> Surely, block elements should and do imply vertical whitespace.

The problem is given that, there is no way you can mix various size fonts on a
single line of text.   I.e.

Welcome to <h1>MSU</h1>.

or whatever.  Perhaps we should define a <fontsize [1-6]> tag, instead of the
<h> tags.

-Crh

    Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu

                     http://rs560.msu.edu/~henrich/



From hotsand!rhb  Mon Jan 10 10:30:12 1994 EST
Message-Id: <9401101530.AA16978@hotsand.dacsand>
Date: Mon, 10 Jan 94 10:30:12 EST
From: hotsand!rhb (Rich Brandwein)
Subject: Re: Passing info between pages

> 
Dave Ragget writes: 
> The latest ideas for forms include a new STATE element that
> allows servers to store information in forms in a manner
> that is opaque to browsers. 

This is *exactly* the element I'm looking for.  Any status on 
where it stands? 

Christian Neuss writes: 
> This is actually a bit of a problem.. we started using forms as an oracle
> front end and of course have run into the same problems.
> 
> What we do in order to pass information is the following:
> 1. every forms document is represented by a virtual HTML document which
>    contains $variables. These will be replaced with information from
>    the database.
> 2. virtual documents are called up by the query mechanism.
>    Example:
>      <FORM ACTION=
>      "http://maotai/htbin/query/next=$trans_id/WWW/HTML/v-dieReise.html">
>    Since the documents are accessed via queries, one can pass additional
>    information in the URL. In our case this is a transaction id (which
>    represents the database connection) and the document which is to be
>    retrieved as result of the form action.
>    A simple program can then strip "id" and access the database to make
>    a query (any kind of state information can be identified via "id" and
>    represented in the database). Database values can then be entered into
>    the forms or other HTML documents by use of $variables.
>    Example:
>      <INPUT NAME="name" VALUE="$name"> Username  ...
>      This is the name: <B> $name </B>
>      

This is another neat trick to getting state information into the process
and looks really valuable for some other apps.  It does now put
the emphasis on a back-end app to untangle the query.

For the bulk of many form based apps (coming from a number of similiar
questions by people trying to generate form based apps here) the STATE
element is a simple and effective method.  Really all that I'm looking
for is a form widget that isn't visible on the page.  That is,you could
also handle this as adding an attribute to a form field, for example:

	<INPUT TYPE="text" STATUS="hidden" VALUE="info I want to pass">
or
	<INPUT TYPE="hidden" VALUE="info I want to pass">

Of course, "INPUT" loses it's proper meaning, so a STATE element sounds right...

Rich



From koblas@netcom.com  Mon Jan 10 08:47:34 1994 -0800
Message-Id: <199401101647.IAA27401@mail.netcom.com>
Date: Mon, 10 Jan 1994 08:47:34 -0800
From: koblas@netcom.com (David Koblas)
Subject: Re: <fontsize>

> Perhaps we should define a <fontsize [1-6]> tag, instead of the <h> tags.

It would be nice, since just today I wanted to do:
        <UL>
        <LI><h1><A HREF=...>Some Text</A></h1>
        <LI> ...
The results were not what I was expecting, it does appear funny at times
that there is no way to vary the point size.  If there was then <H?> tags
would just become <P><em size=?>...<em size=?><P>.

Additionally, though I'm sure this really is beyond HTML+ (is it?), it 
would occasionally be nice to specify the COLOR of text and HR lines. 
This is partly motivated out of recently noticing a lot of documents 
with images for hrules just to make the documents a little more snazzy.
        <em color=red>
        <hr color=red>

--koblas@netcom.com



From atotic@ncsa.uiuc.edu  Mon Jan 10 11:10:49 1994 -0600 (CST)
Message-Id: <9401101710.AA22999@void.ncsa.uiuc.edu>
Date: Mon, 10 Jan 1994 11:10:49 -0600 (CST)
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: <fontsize>

> > Perhaps we should define a <fontsize [1-6]> tag, instead of the <h> tags.
> 
> It would be nice, since just today I wanted to do:
>         <UL>
>         <LI><h1><A HREF=...>Some Text</A></h1>
>         <LI> ...
> The results were not what I was expecting, it does appear funny at times
> that there is no way to vary the point size.  If there was then <H?> tags
> would just become <P><em size=?>...<em size=?><P>.
> 
How about <em h1>? This is really what the document writers are doing now.

Aleks



From germuska@casbah.acns.nwu.edu  Mon Jan 10 11:15:07 1994 -0600 (CST)
Message-Id: <9401101715.AA03485@casbah.acns.nwu.edu>
Date: Mon, 10 Jan 1994 11:15:07 -0600 (CST)
From: germuska@casbah.acns.nwu.edu (Joe Germuska)
Subject: Re: <fontsize>

David Koblas wrote:
> It would be nice, since just today I wanted to do:
>         <UL>
>         <LI><h1><A HREF=...>Some Text</A></h1>
>         <LI> ...

Well, using headers in lists is expressly prohibited based on what a header
is "supposed" to mean -- however, it might be nice if nested lists could be
rendered with different font sizes.  Maybe a tag to represent "outline"?
Or should the Header tag be adapted to serve that purpose in a nested list?

	Joe

-- 
joe germuska * j-germuska@nwu.edu * www * res hall net * instruct tech
      academic computing & network services * northwestern univ
A monk told Joshu: "I have just entered the monastery.  Please
teach me."  Joshu asked: "Have you eaten your rice porridge?"  The
monk replied: "I have eaten."  Joshu said: "Then you had better wash
your bowl."  At that moment, the monk was enlightened!



From sanders@BSDI.COM  Mon Jan 10 11:16:21 1994 -0600
Message-Id: <199401101716.LAA03138@austin.BSDI.COM>
Date: Mon, 10 Jan 1994 11:16:21 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: CGI and typing files by suffix 

> > The `;' scheme simply has too many drawbacks (namely you can't front-end
> > existing directory hierarchies with scripts).
> 
> It's a matter of taste of what's cleaner, and thats not what im pushing for,
> its the useless stat calls that are driving me nuts here.  Let me explain what
> happens on an AFS filesystem when you stat something thats not existant.
This is an implemenation issue, E.g., Plexus uses a config file instead
of doing stat()'s.  The current protocol does *NOT* imply that you must
use stat() calls to decide what is what.

> > Using the execute bit, where available, is probably the best plan.
> I concur, and agree, and wish it were so. :)

I disagree with myself here.  I've decided that I like have this in a
config file like Plexus does.  It's more secure and it's tons faster.

I don't understand why you are concuring on this point.  Using the
execute bit implies doing all the stat()'s which you are so against.

--sanders



From relihanl@ul.ie  Mon Jan 10 17:02:55 1994 +0000 (WET)
Message-Id: <Pine.3.05.9401101754.B1507-c100000@itdsrv1.ul.ie>
Date: Mon, 10 Jan 1994 17:02:55 +0000 (WET)
From: relihanl@ul.ie (Liam Relihan)
Subject: Re: <fontsize>

On Mon, 10 Jan 1994, David Koblas wrote:

> > Perhaps we should define a <fontsize [1-6]> tag, instead of the <h> tags.
> 
> It would be nice, since just today I wanted to do:
>         <UL>
>         <LI><h1><A HREF=...>Some Text</A></h1>
>         <LI> ...
> The results were not what I was expecting, it does appear funny at times
> that there is no way to vary the point size.  If there was then <H?> tags
> would just become <P><em size=?>...<em size=?><P>.

This might be nice but I reckon that the whole point of something like
HTML is to define a *document type*. This means logical components that
can be arranged in an aggregation hierarchy. Logical components are a good
thing because they reflect something in the real world and can be
interpreted at a later stage according to the capabilities of the
local rendition software. By encoding things like fonts into documents,
you are immediately creating problems for browsers that can't handle fonts,
etc.

> Additionally, though I'm sure this really is beyond HTML+ (is it?), it 
> would occasionally be nice to specify the COLOR of text and HR lines. 
> This is partly motivated out of recently noticing a lot of documents 
> with images for hrules just to make the documents a little more snazzy.
>         <em color=red>
>         <hr color=red>

Any number of extra codes can be added to HTML/HTML+. However, it is up to
browsers to use those codes and as already hinted at, some browsers have
limited display capabilities.

I believe that certain markups such as <B> and <I> are already destined
for the hatchet and the use of <EM> and <STRONG> will be encouraged
instead. This is good since browsers can treat the more logical <EM> and
<STRONG> according to their capabilities at a local level.

The moral of the story is:
"You can map logical markup into font sizes, colours, etc. easily
- but- 
it is very difficult to map font sizes, colours, etc into logical markup"

Liam


--
     Liam Relihan        | Voice: +353-61-333644 ext. 5015   |     _ | __ \
 CSIS, Schumann Building | Fax:   +353-61-330876             |    |  | |   |
 University Of Limerick  | E-mail: relihanl@ul.ie            |    |  | __ /
        Ireland                                             ____|____|_|  _\




From joe@MIT.EDU  Mon Jan 10 12:23:30 1994 -0500
Message-Id: <9401101723.AA11992@theodore-sturgeon.MIT.EDU>
Date: Mon, 10 Jan 94 12:23:30 -0500
From: joe@MIT.EDU (joe@MIT.EDU)
Subject: Online WWW Conference Sessions


Would it be possible to some person or persons to arrange sessions of
the upcoming WWW conference to occur over a MOO or MUD for the benefit
of those of us who can't attend in person?





From dsr@hplb.hpl.hp.com  Mon Jan 10 10:49:23 1994 GMT
Message-Id: <9401101049.AA15553@manuel.hpl.hp.com>
Date: Mon, 10 Jan 94 10:49:23 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: HTML icon set was: Additions to the CGI archive

> My suggestion: create a set of entities (&file;, &folder;, etc.) like
> the set that already exists for accented characters (&eacute;, &euml;,
> etc.). If we agree on this, maybe Dave Raggett could add such a list
> to the HTML+ draft?

> Browser developers are then free to display those entities in whatever
> way they like. Either as an "ASCII graphic", a bitmap, or a character
> from a special symbols font.

> Btw. ISO is standardizing the names and approximate look of a small
> set of "desktop" icons. I don't have the drafts here at home, but we
> should definitely use their names.

Sounds good to me. Bert, can you mail me some info on the ISO "icons".
It would be a shame to invent our own entity names when official ISO
names exist already.

Dave



From dsr@hplb.hpl.hp.com  Mon Jan 10 10:41:25 1994 GMT
Message-Id: <9401101041.AA15191@manuel.hpl.hp.com>
Date: Mon, 10 Jan 94 10:41:25 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: minimal HTML

Its useful to distinguish several cases:

    a)  handwritten HTML intended for Web browsers only

    b)  HTML documents generated by authoring tools

    c)  document format conversion: to/from other formats

For (a) the minimal style is just about OK as web browsers
can easily infer omitted tags:

    <TITLE>blah</TITLE>
    <H1>blah</H1>
    ...

For documents generated by well designed authoring tools,
I would expect the full set of tags to be used.

When you want to use conventional SGML parsers, then you
SHOULD use the full set of tags, including the <!DOCTYPE>
declaration at the start.

Bert writes:

> A comparable situation would have been the result of a set of new tags
> that I once suggested. Consider that an HTML doc be split into
> <SECT1>s, <SECT2>s, etc., corresponding to <H1>, <H2>, etc. This would
> allow an anchor to be attached to a whole section of the text, instead
> of to a header. The <SECTn> tag would be implied by the occurrence of
> <Hn>, in the same manner as <BODY>. (The suggestion was turned down,
> because people wanted to be able to put <Hn> tags inside lists and
> other places where they don't belong.)

In the current HTML+ DTD I use similar techniques to allow browsers to
infer missing <P> elements. Rather than SECTn I use DIVn, but the idea
is exactly the same.

One problem with this approach to allowing the context to imply elements
is that standard SGML parsers are crippled in this regard, and can only
infer an omitted start tag when that tag is the *only* thing permitted
at this point. This explains why you should use the full set of tags
when you intend to process documents with other SGML tools.

Dave Raggett



From dsr@hplb.hpl.hp.com  Mon Jan 10 11:26:01 1994 GMT
Message-Id: <9401101126.AA15579@manuel.hpl.hp.com>
Date: Mon, 10 Jan 94 11:26:01 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Passing info between pages

> I'm looking for guidance on a way of passing information
> between pages (specifically forms).  

> There are many application in which may want to pass "state"
> information between pages. For instance (based on an app 
> someone's written here) I may want to select a database
> from an initial form and be able to bring up a generic
> page that allows you to search on that particular database.
> How do I pass information to this "generic" page?  

What kind of information do you want to pass? Is it possible
to express this as selection fields on the initial form?

If you use a script at the server to generate the form for
a particular database, you can then tailor the form according
to info entered in the initial form.

The latest ideas for forms include a new STATE element that
allows servers to store information in forms in a manner
that is opaque to browsers. Other changes on the way will
allow forms to be dynamically updated when a user makes
a selection without the need to resend the entire form.

These ideas will be discussed in the May WWW conference.

Dave Raggett




From germuska@casbah.acns.nwu.edu  Mon Jan 10 12:29:27 1994 -0600 (CST)
Message-Id: <9401101829.AA15779@casbah.acns.nwu.edu>
Date: Mon, 10 Jan 1994 12:29:27 -0600 (CST)
From: germuska@casbah.acns.nwu.edu (Joe Germuska)
Subject: About Clients and ISMAP

As I continue work on mapping, I think more and more that it would work
better if clients retrieved the config file from the server and cached it
rather than repeatedly asking the server what it should do:

1) If the client had the config file, it could preview the destination URL
of each "hotspot", like many clients do with text-links now.

2) If the client had the config file, it could better cache and retrieve
URL's since it would know if adjacent pixels are mapped to the same URL.

3) If the client had the config file, it could better handle relative URL's
-- this may be only a quirk of the MacMosaic implementation, but a relative
URL tries to cycle back through the image map!  It would be much nicer if
the client labeled the current document by the URL listed in the config
file than by the URL the client created based on the ISMAP click. (i.e.
http://www.acns.nwu.edu/jazz/styles/world-music.html
not
http://www.acns.nwu.edu/cgi-bin/imagemap/jzstyle?457,214

(Similar "translation" would be useful for directory index files -- the
client should use the URL of the document the server provides, which may be
equivalent to, but not identical to the URL the client requested.)

Comments?

	Joe
-- 
joe germuska * j-germuska@nwu.edu * www * res hall net * instruct tech
      academic computing & network services * northwestern univ
A monk told Joshu: "I have just entered the monastery.  Please
teach me."  Joshu asked: "Have you eaten your rice porridge?"  The
monk replied: "I have eaten."  Joshu said: "Then you had better wash
your bowl."  At that moment, the monk was enlightened!






From PHIL@mash.colorado.edu  Mon Jan 10 12:47:04 1994 -0700 (MST)
Message-Id: <01H7INS4JTKI00009J@VAXF.COLORADO.EDU>
Date: Mon, 10 Jan 1994 12:47:04 -0700 (MST)
From: PHIL@mash.colorado.edu (PHIL@mash.colorado.edu)
Subject: Re: Rough Draft -- "Good HTML Composition" document

James (Eric) Tilton asks:

> Ok, in preparation to get down and dirty with an HTML "lint" (hey, can 
> anyone think of a catchy name for this program?  "html-lint" just doesn't 
> cut it with the home crowd at Peoria :),...

How about "hint"?

 --
 Phil Helms                                    Internet: PHIL@MASH.Colorado.EDU
 Community College Computer Services
 Denver, Colorado                                                Time: GMT-0700




From atotic@ncsa.uiuc.edu  Mon Jan 10 14:23:02 1994 -0600 (CST)
Message-Id: <9401102023.AA25996@void.ncsa.uiuc.edu>
Date: Mon, 10 Jan 1994 14:23:02 -0600 (CST)
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: Whitespace

>The problem is given that, there is no way you can mix various size fonts on a
>single line of text.   I.e.
> 
> Welcome to <h1>MSU</h1>.
> 
> or whatever.  Perhaps we should define a <fontsize [1-6]> tag, instead of the
> <h> tags.

How about <em h1>? The semantic meaning is: "This is a header where it should
not appear".

Aleks




From redback!jimmc@eskimo.com  Mon Jan 10 12:22:32 1994 PST
Message-Id: <9401102022.AA00994@redback.>
Date: Mon, 10 Jan 94 12:22:32 PST
From: redback!jimmc@eskimo.com (Jim McBeath)
Subject: PATHs in HTML

Dave Raggett and I have been having an email discussion of PATHs in HTML.
What follows is the results of that discussion, which I present as
a proposal for an enhancement to HTML (and WWW clients).

A PATH is a way of defining a linearization of a set of nodes.
The user of a client can perform operations on the nodes in a path, such
as traversing (browsing) them in order, searching, or printing.

The requirements for paths:
1. A path must be able to include nodes for which the user creating the path
   has only read access; therefore, it must be possible to define a path
   which includes nodes in which no path information resides, including
   non-HTML nodes.
2. A single node may be included in multiple paths; therefore paths must
   be given some kind of name to distinguish them.
3. It must be possible to incrementally define paths; therefore a path
   must be able to include another path.
4. It must be possible for a user with no special privileges to be able to
   create his own PATHs which reference any documents (for example, it can't
   require modifying HTTP server files).
5. The mechanism for creating PATHs must be reasonably simple to understand.

This proposed implementation requires the use of REL=Subdocument (already
proposed in the current version of the HTML+ spec at
 ftp://15.254.100.100/pub/draft-raggett-www-html-00.ps )
plus one additional enhancemnt:

	Add "Path" to the set of legal REL values.

Given these two REL values for anchors, a path is defined as follows:
1. A path is defined by anchors in an HTML node.  All anchors which
   have REL=Subdocument or REL=Path are included in the path definition.
2. A single HTML node contains a single path definition.
3. An anchor with REL=Subdocument causes the referenced document to be
   included in the path.
4. An anchor with REL=Path causes the referenced document to be interpreted
   as a path, and all members of that path are included in the top level path.
   Nesting is allowed: the included path can also include other paths.
5. A path node containing anchors with REL=Path can be treated either as a
   single path containing subpaths, or as a collection of independent named
   paths.
5. When interpreting a path node as a collection of independent paths,
   the anchor string is used as the user-visible name of each path.

An example of an HTML node which defines a single path:
        <H1>my path</H1>
        This is a path of things I've found useful.
        <DL>
          <DT> <A HREF="node1" REL="Subdocument">label1</A>
          <DD> a summary
 
          <DT> <A HREF="path2" REL="Path">label2</A>
          <DD> a summary
 
          <DT> <A HREF="node3" REL="Subdocument">label3</A>
          <DD> a summary
        </DL>
This path includes node1, followed by all of the nodes defined in path2,
followed by node3.

An example of an HTML node which points to independent paths:
       <H1>useful paths</H1>
       These are some paths I've found useful.
       <DL>
         <DT> <A HREF="alphapath.html" REL="Path">Alphabetical</A>
         <DD> nodes of interest sorted alphabetically
 
         <DT> <A HREF="geopath.html" REL="Path">Geographical</A>
         <DD> sorted by location
 
         <DT> <A HREF="chronopath.html" REL="Path>Chronological</A>
         <DD> sorted by time
       </DL>
This node points to three separate paths.

Each path operation could come in two versions:
1. Treat the current node as a single path.
2. Treat the current node as a collection of pointers to separate (named) paths.

Client programs could add the following commands that deal with paths:
1. Traverse current path
2. Traverse named path
3. Find in current path
4. Find in named path
5. Print current path
6. Print named path

"Traverse current path" would collect the list of nodes in the path
definition from the current node, then switch to viewing the first node
in that path.  This would be appropriate, for example, for a path node
which was also a nicely formatted Table of Contents page.

"Traverse named path" would collect the list of anchor strings from all
anchors with REL=Path in the current node, allow the user to
select one of those, then retrieve that (path defining) node and switch
to viewing the first node defined by it.  Note that in this approach the user
never actually views the HTML node which defines the path selected.

"PathNext" and "PathPrevious" buttons would facilitate stepping forwards
and backwards through the nodes of the currently active path.

Conceptually, at the moment the user selected the Traverse command (or any
other path command), the client program would build the entire list of
nodes for that path (although in practice this could easily be done
incrementally), so that the PathNext and PathPrevious commands are
unambiguous, even when a node is a member of multiple paths, or is itself
a path definition included as REL=Subdocument.

The user could switch to another path at any time; but note that all
paths are defined by some particular node: there is no way to select
a new path from within a node except for a path that is defined in
or referenced by that node (except for paths that the client already
knows about).  This restriction is similar to the fact that you can only
jump to other nodes that are pointed to from the current node (except for
nodes that the client already knows about, e.g. in window history).

"Find in current path" would collect the path definition from the current
node, then search through each node in the path in sequence, stopping when
it found the string asked for by the user.

"Find in named path" would let the user select one of the paths referenced
by the current node (just like in "Traverse named path"), then search
through the nodes defined by that path.

"Print current path" and "Print named path" would have the same options
as the current command to print a single node (or save it to a file),
but would print (or save) all of the nodes in the selected path.

Given this capability, I could define a path to print out my entire
Users Manual, and (just as important) to allow me to search through
the contents of the entire manual with a single command.
I could then define other, specialized paths that went through different
pieces of the manual in another order, for people who wanted to learn
about a particular subject.

I would also define a set of paths in my personal home page, so that I
could search through multiple documents more easily:
    HTML - would include pointers to a bunch of the HTML documents
	(including HTTP, HTML primer, HTML+ CGP, etc).
    Indexes - would include pointers to the indexes I most frequently
	search through when looking for something that I know I've seen
	before, but can't remember which index it was in.
    Personal - all of my personal pages, so that I can split up my
	huge home page into separate pages and still be able to easily
	search through them all.

This scheme is easy to understand, easy to define, and easy to implement,
entirely in the client.  Clients which did not understand REL=Subdocument
and REL=Path would just ignore those attributes; the user could still use
the path node to get to the nodes in the path by selecting an anchor.

It allows people to create new paths that build on paths that others
have defined without requiring modification of all the nodes in the path.
It would give document providers a way to define a set of nodes to be
printed into a manual, and would allow users to define their own custom
manuals, either for printing or for searching.

-Jim McBeath
jimmc@eskimo.com



From henrich@rs560.cl.msu.edu  Thu Dec 30 10:40:08 1993 -0500 (EST)
Message-Id: <9312301540.AA22066@rs560.cl.msu.edu>
Date: Thu, 30 Dec 1993 10:40:08 -0500 (EST)
From: henrich@rs560.cl.msu.edu (Charles Henrich)
Subject: Re: CGI and typing files by suffix

> I fail to see how:
>     http://server/path/cmd;args
> Is really any clearer or better than:
>     http://server/path/cmd/args
>
> The `;' scheme simply has too many drawbacks (namely you can't front-end
> existing directory hierarchies with scripts).

It's a matter of taste of what's cleaner, and thats not what im pushing for,
its the useless stat calls that are driving me nuts here.  Let me explain what
happens on an AFS filesystem when you stat something thats not existant.

The call goes into the unix kernel, is re-directed to the AFS cache-manager who
check's the local disk cache, obviously the directory isnt there since it isnt
real, so the cache-manager sends a request over the network to the AFS server
for an update of that directory, since it doesnt exist the server returns "it
aint here".  Talk about a waste of a few billion clock cycles!  It places undue
strain on bot the client machine, and the server!  And when a server is using
this sort of embedded information all over the place for 80% of the accesses
its rediculus!  Can you imagine what this would do to the network/client/server
if the box was anywhere near as active as NCSA's (2.4 million hits per month!)

> Using the execute bit, where available, is probably the best plan.

I concur, and agree, and wish it were so. :)

-Crh

    Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu

                     http://rs560.msu.edu/~henrich/




From davis@DRI.cornell.edu  Mon Jan 10 09:46:31 1994 -0500
Message-Id: <199401101446.AA27842@willow.tc.cornell.edu>
Date: Mon, 10 Jan 1994 09:46:31 -0500
From: davis@DRI.cornell.edu (Jim Davis)
Subject: Re: CGI and typing files by suffix


> From www-talk-request@www0.cern.ch Mon Jan 10 09:36:07 1994
> From: Charles Henrich <henrich@rs560.cl.msu.edu>
> Subject: Re: CGI and typing files by suffix
> Date: Thu, 30 Dec 1993 10:40:08 -0500 (EST)
> 
> its the useless stat calls that are driving me nuts here.  
> Let me explain what happens on an AFS filesystem when you stat 
> something thats not existant.... Talk about a waste of a few billion 
> clock cycles!  

We will be better served in the long run if we do not base our
designs on the current implementation of AFS or for that matter Unix.
Clock cycles get cheaper every day, but every kludgy feature you
add is paid for in human labor, which grows ever more expensive.



From jtilton@jupiter.willamette.edu  Mon Jan 10 13:27:29 1994 -0800 (PST)
Message-Id: <Pine.3.88.9401101308.A2598-0100000@jupiter>
Date: Mon, 10 Jan 1994 13:27:29 -0800 (PST)
From: jtilton@jupiter.willamette.edu (James)
Subject: Re: Rough Draft -- "Good HTML Composition" document

On Mon, 10 Jan 1994, Kevin Altis wrote:

> Good stuff. The only thing I'm not sure about is the comments <!--...-->.
> Do all clients support this now? At one point, the ability to do comments
> went away and I haven't used them since, but would like to.

Good question -- X Mosaic and Lynx seem to, at least, but I'm not sure
about the other browsers (Mac Mosaic is the only other browser I have
access to).  One meme that was floating around was that, to date, comments
start with the "<!--", but can end with ">".  From what I understand of
SGML, the comment is technically really the bits between "--" and "--",
anywhere within a "<!.........>" (don't you love my command of the jargon?
:).  Can anyone comment on whether there is there any plan to make (at
least) the terminator for a comment "-->", so that we can comment out HTML
code? 

							-et



From germuska@casbah.acns.nwu.edu  Mon Jan 10 12:29:27 1994 -0600 (CST)
Message-Id: <9401101829.AA15779@casbah.acns.nwu.edu>
Date: Mon, 10 Jan 1994 12:29:27 -0600 (CST)
From: germuska@casbah.acns.nwu.edu (Joe Germuska)
Subject: About Clients and ISMAP

As I continue work on mapping, I think more and more that it would work
better if clients retrieved the config file from the server and cached it
rather than repeatedly asking the server what it should do:

1) If the client had the config file, it could preview the destination URL
of each "hotspot", like many clients do with text-links now.

2) If the client had the config file, it could better cache and retrieve
URL's since it would know if adjacent pixels are mapped to the same URL.

3) If the client had the config file, it could better handle relative URL's
-- this may be only a quirk of the MacMosaic implementation, but a relative
URL tries to cycle back through the image map!  It would be much nicer if
the client labeled the current document by the URL listed in the config
file than by the URL the client created based on the ISMAP click. (i.e.
http://www.acns.nwu.edu/jazz/styles/world-music.html
not
http://www.acns.nwu.edu/cgi-bin/imagemap/jzstyle?457,214

(Similar "translation" would be useful for directory index files -- the
client should use the URL of the document the server provides, which may be
equivalent to, but not identical to the URL the client requested.)

Comments?

	Joe
-- 
joe germuska * j-germuska@nwu.edu * www * res hall net * instruct tech
      academic computing & network services * northwestern univ
A monk told Joshu: "I have just entered the monastery.  Please
teach me."  Joshu asked: "Have you eaten your rice porridge?"  The
monk replied: "I have eaten."  Joshu said: "Then you had better wash
your bowl."  At that moment, the monk was enlightened!






From ldonahue@chicagokent.Kentlaw.EDU  Mon Jan 10 09:51:37 1994 CST
Message-Id: <9401101551.AA06258@chicagokent.Kentlaw.EDU>
Date: Mon, 10 Jan 94 9:51:37 CST
From: ldonahue@chicagokent.Kentlaw.EDU (Larry Donahue)
Subject: Easy question - ftp archive site for HTML converters?

I've been monitoring this list waiting for someone to ask, or point out,
an ftp archive site where one can find scripts to convert various data
formats to HTML.

In particular, I'm interested in moving nroff/troff, WP 5.1, and possibly
Folio Views v2.0 to HTML.  Any ideas if the WP and Folio conversion to HTML
is possible?

Thanks in advance.  Larry.

-- 
Laurence S. Donahue
Student/Staff of Chicago-Kent College of Law
ldonahue@chicagokent.Kentlaw.EDU
(312)906-5313

Portable, adj.:
	Survives system reboot.



From kevinh  Mon Jan 10 19:00:54 1994 PST
Message-Id: <9401110300.AA09573@eit.COM>
Date: Mon, 10 Jan 94 19:00:54 PST
From: kevinh (Kevin 'Kev' Hughes)
Subject: I'll be making photo rounds...


	Just so everyone knows, I'd like to start taking photos -
*good* photos - of everyone with the digital camera, so the Web site
will be up to date. The last time I tried, a lot of people told me they
had a "bad hair day".
	SO... I'll be wandering around Wednesday afternoon, approx.
2 pm, or let me know a time when I can come by and take a picture.
Be prepared to have a good hair day, or suffer the wrath of candid camera. :)

	-- Kev



From henrich@crh.cl.msu.edu  Mon Jan 10 23:37:21 1994 -0500 (EST)
Message-Id: <9401110437.AA12487@crh.cl.msu.edu>
Date: Mon, 10 Jan 1994 23:37:21 -0500 (EST)
From: henrich@crh.cl.msu.edu (Charles Henrich)
Subject: Re: Whitespace

> How about <em h1>? The semantic meaning is: "This is a header where it should
> not appear".

Why not just <f1> ?

-Crh

    Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu

                     http://rs560.msu.edu/~henrich/



From atotic@ncsa.uiuc.edu  Mon Jan 10 23:07:25 1994 -0600 (CST)
Message-Id: <9401110507.AA05335@void.ncsa.uiuc.edu>
Date: Mon, 10 Jan 1994 23:07:25 -0600 (CST)
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: Whitespace

>>How about <em h1>? The semantic meaning is: "This is a header where it should
>>not appear".
> 
> Why not just <f1> ?

Because <f1> is similar to <b>, <i>, whose use should be discouraged. <em>
tag is the one that should be used for rendering hints.

Aleks




From atotic@ncsa.uiuc.edu  Mon Jan 10 23:07:25 1994 -0600 (CST)
Message-Id: <9401110507.AA05335@void.ncsa.uiuc.edu>
Date: Mon, 10 Jan 1994 23:07:25 -0600 (CST)
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: Whitespace

>>How about <em h1>? The semantic meaning is: "This is a header where it should
>>not appear".
> 
> Why not just <f1> ?

Because <f1> is similar to <b>, <i>, whose use should be discouraged. <em>
tag is the one that should be used for rendering hints.

Aleks




From henrich@crh.cl.msu.edu  Tue Jan 11 00:10:57 1994 -0500 (EST)
Message-Id: <9401110510.AA12668@crh.cl.msu.edu>
Date: Tue, 11 Jan 1994 00:10:57 -0500 (EST)
From: henrich@crh.cl.msu.edu (Charles Henrich)
Subject: Re: Whitespace

> > Why not just <f1> ?
>
> Because <f1> is similar to <b>, <i>, whose use should be discouraged. <em>
> tag is the one that should be used for rendering hints.

Ah Thats where I disagree.  Overloading the <em> tag excessivly is a bad idea,
which has at some point been agreed to on the list, when the initial HTML+ spec
used <em bold> instead of <b>.   Explicit tags are not a bad thing IMHO.

-Crh

    Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu

                     http://rs560.msu.edu/~henrich/



From kevinh@eit.COM  Mon Jan 10 22:50:13 1994 -0800
Message-Id: <199401110650.WAA19886@kmac.eit.com>
Date: Mon, 10 Jan 1994 22:50:13 -0800
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re: minimal HTML

> Good question -- are you now an ex-HCC person?  By the way, I tried
> to connect to them recently -- server down -- no longer your server?
> 
> /rich

	I'm no longer at HCC, but am now working in Palo Alto for EIT.
From time to time I may be checking HCC's web server as I migrate some
services over the EIT's web, but I'm no longer involved in long or
short-term maintenance of Honolulu Community College's site. Ken
Hensarling (ken@pulua.hcc.hawaii.edu), HCC's Director of Academic
Computing, is the new webmaster, but he really has a lot to do!
	Hawaii has been having flaky/slow connections lately. As
just about everything goes through the University of Hawaii system, it's
prone to ethernet card failures and the like. It comes with the
territory!
	I'm now the webmaster of EIT's site - I'm not quite done
putting all the finishing touches and services on it, but I'll
announce it when I'm ready, any day now!

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://UNDER.CONSTRUCTION/)
Hypermedia Industrial Designer * Duty now for the future!



From WIGGINS@msu.edu  Tue Jan 11 10:11:49 1994 EST
Message-Id: <9401111518.AA04093@dxmint.cern.ch>
Date: Tue, 11 Jan 94 10:11:49 EST
From: WIGGINS@msu.edu (Rich Wiggins)
Subject: Re: CGI and typing files by suffix

>We will be better served in the long run if we do not base our
>designs on the current implementation of AFS or for that matter Unix.
>Clock cycles get cheaper every day, but every kludgy feature you
>add is paid for in human labor, which grows ever more expensive.

Not all of us accept the arguments of some of you that the elegance
of hiding the fact that a URL is a script is a Good Thing.  I
continue to believe that the idea of having script authors identify
scripts versus files by what directory the files reside in, or
the execute bit, or something in the server config file is far
from the best solution.  I believe the .CGI suffix (or even better
.EXEC or .SCRIPT) -- ie explicit naming of scripts -- is the way
to go.

This notion that it's important to be able to replace files with
scripts without changing file names is only one argument.  I think
if you want the masses to be able to write HTML and share HTML
documents from server to server it's better if scripts are
flagged explicitly, not by some server-dependent "out-of-band"
notation.



/Rich Wiggins, CWIS Coordinator, Michigan State U



From dsr@hplb.hpl.hp.com  Tue Jan 11 16:32:34 1994 GMT
Message-Id: <9401111632.AA19420@manuel.hpl.hp.com>
Date: Tue, 11 Jan 94 16:32:34 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Passing info between pages

> Dave Ragget writes: 
>> The latest ideas for forms include a new STATE element that
>> allows servers to store information in forms in a manner
>> that is opaque to browsers. 

> This is *exactly* the element I'm looking for.  Any status on 
> where it stands? 

It's in the new DTD for HTML+ I recently mailed out. You can get
another copy from ftp://15.254.100.100/pub/htmlplus.dtd.txt

The idea originated in a phone call with Tim Berners-Lee and
I have yet to write it up formally. I hope to make the next
draft of the HTML+ spec available via the Web rather than as
a large postscipt file and expect to complete this write up
by mid-February.

Dave



From swb1@cornell.edu  Tue Jan 11 11:44:39 1994 -0500
Message-Id: <199401111641.AA24346@postoffice2.mail.cornell.edu>
Date: Tue, 11 Jan 1994 11:44:39 -0500
From: swb1@cornell.edu (Scott W Brim)
Subject: Re: Whitespace

Even if this would be overloading <em *>, doing so would be
significantly better than using <h*>s -- using <h*>s to express font
size would be a real departure from their intended use.





From RASHTY@vms.huji.ac.il  Tue Jan  1 19:26:00 1994 +0200
Message-Id: <11010094192638@HUJIVMS>
Date: Tue,  11 Jan 94 19:26 +0200
From: RASHTY@vms.huji.ac.il (Dudu Rashty +972-2-584848)
Subject: EARN Guide to Network Resources Tools Ver 2.0

Hi

EARN Guide to Network Resources Tools Ver 2.0 is available from
http://vms.huji.ac.il/WWW_HELP/earn.html

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
|                                                      **** Thanks !! **** |
| Dudu Rashty                Rashty@Vms.Huji.Ac.Il         __o     o__     |
| Computation Center         Bitnet:Rashty@HUJIVMS       _ \<,_   _.>/ _   |
| The Hebrew University      Phone:  +972-2-584397      (_)/ (_) (_) \(_)  |
| Jerusalem 91904, ISRAEL.   Fax:    +972-2-527349       D    u    d    u  |
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From dsr@hplb.hpl.hp.com  Tue Jan 11 17:45:48 1994 GMT
Message-Id: <9401111745.AA19762@manuel.hpl.hp.com>
Date: Tue, 11 Jan 94 17:45:48 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Minutes of WWW/TEI meeting for last Noveber

Somewhat late, I know, but you can now read the minutes of the
WWW/TEI meeting held last November in Cork, Ireland. This
proved a very valuable occasion, bringing together browser writers
with members of the SGML community.

        ftp://15.254.100.100/pub/cork_tei_www.html

My thanks to Lou Burnard for his permission to publish his
notes on the meeting.

Dave Raggett

p.s. I would be very grateful if someone would volounteer to move
the report to a more permanent home than our anonymous ftp server.



From FisherM@is3.indy.tce.com  Tue Jan 11 12:33:00 1994 PST
Message-Id: <2D330DE4@MSMAIL.INDY.TCE.COM>
Date: Tue, 11 Jan 94 12:33:00 PST
From: FisherM@is3.indy.tce.com (Fisher Mark)
Subject: RE: Whitespace


HTML is (fortunately or unfortunately) a document type of SGML, which 
defines document content, not document formatting.  IMHO, it is the 
responsibility of browser writers to supply adequate options for handling 
whitespace.  If HTML is extended to include formatting information, it 
becomes YAWPF (Yet Another Word Processing Format :), unsuitable for users 
who do not possess (or could not use) GUI browsers.
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From rst@ai.mit.edu  Tue Jan 11 13:57:46 1994 EST
Message-Id: <9401111857.AA09875@volterra>
Date: Tue, 11 Jan 94 13:57:46 EST
From: rst@ai.mit.edu (Robert S. Thau)
Subject: CGI and scripts [counter?] proposal...

So long as we're exchanging proposals for how servers might more flexibly
find CGI scripts, here's what I've done to my server.  More or less the
same thing is available in HTML as

  http://www.ai.mit.edu/xperimental/run-scripts.html

That file also contains a pointer to the code which implements all this, as
a patch to NCSA httpd 1.0.  I'm also forwarding some comments on Rich
Brandwein's proposal (which seems to me to be basically in the same spirit)
under separate cover.

The RunScripts option

I've implemented a new option, RunScripts, to the Allow directive of
access.conf and .htaccess files. This option allows the server maintainer
to designate certain directories as containing both scripts and ordinary
files, and to have the server be able to retrieve both from those (and only
those) directories.

What it does

The RunScripts option allows scripts to be run from any directory where the
server might find ordinary files, whether those directories are
subdirectories of SERVER_ROOT, a user's public_html area, an Aliased
directory, or whatever, so long as RunScripts is enabled for the directory
in question. It may be enabled (or disabled) for a particular directory and
its subdirectories via Options directives in access.conf in the usual way,
and overrides via .htaccess files may likewise be permitted or disallowed
as usual.

(In fact, you will have to edit these files to tell the daemon which
directories you want RunScripts to apply to, unless they already say
Options All).

Having scripts and ordinary files in the same directory raises the question
of how to tell which is which. That's resolved by a naming convention, as
follows:

If a file named .../some_dir/foo.doit or .../some_dir/foo.nph is placed in
a directory .../some_dir which has RunScripts enabled, the server will
attempt to run it as a CGI script to handle HTTP GETs and POSTs to URLs of
the form:

   *) .../some_dir/foo/bar/zot 
   *) .../some_dir/foo?some+query+args 
   *) HTTP POSTs to .../some_dir/foo 
   *) HTTP GETs to .../some_dir/foo unless an actual file named 
      .../some_dir/foo is sitting in the file system alongside 
      .../some_dir/foo.doit. 

      If such a file does exist, it will be retrieved for parameterless
      GETs, and the script will not be invoked. This allows 'coversheets'
      for ISINDEX documents to be put in a separate file, which is an
      occasional minor convenience.

Giving a script the .nph extension instead of .doit has exactly the same
effect as making its name start with nph-, namely, that it has complete
control of the connection back to the client, and is therefore required to
generate all relevant HTTP and MIME headers. The only functional difference
between supplying nph as a prefix vs. as an extension is that in the latter
case, implementation details of the script aren't wired into the URL.

Note also that the .doit and .nph extensions are *not* visible to clients.
This is so that pre-existing files and directories can be turned into
scripts, and vice versa, without changing all references to them as well.

One last special case --- .doit and .nph files cannot be retrieved
directly. That is, an attempt to retrieve .../some_dir/foo.doit will fail
even if the file does exist, and an attempt to retrieve .../some_dir/foo
would invoke it as a script. Attempts to retrieve Emacs backup or auto-save
files (i.e., those ending in '~' or '#') will likewise fail, and all of
these files are left out of automatically-generated directory lists. This
is an attempt on my part to make sure people don't export code without
knowing it, which can be a potential security risk.

Compatibility with existing stuff

The RunScripts patch has no effect on the behavior of ScriptAlias at all.
This means that a ScriptAlias to some directory and an Alias to the same
directory have different effects, even if that directory has RunScripts
enabled. Different naming conventions are in effect (when looking for
scripts in ScriptAliased space, the server does not use the .doit or .nph
suffixes), attempts to retrieve ordinary files from ScriptAliased space
will fail, and of course, the RunScripts option is not required to run
scripts from a URL covered by a ScriptAlias.

There are two known incompatibilities, both minor (at least as I see them): 

*) Options All means something different with the patch installed, since
   All includes RunScripts.

   The only way to do this back-compatibly is to have multiple levels of
   all-inclusive keywords --- as on certain Unices, in which stty all shows
   every tty mode setting you ever heard of, while stty everything shows
   you the rest of them.

*) The prohibitions on retrieval of certain files apply even in directories
   where RunScripts is off. The intention here is that if some malign
   influence arranges for a .htaccess file to appear in some directory
   which says Options Indexes, they don't then get to cruise the scripts in
   that directory for something useful while waiting for the site
   maintainer to remove the thing.

If people would like to play around with this, and don't mind experimenting
with unsupported software, then the code is available --- see the URL above.

rst



From rst@ai.mit.edu  Tue Jan 11 13:57:52 1994 EST
Message-Id: <9401111857.AA09878@volterra>
Date: Tue, 11 Jan 94 13:57:52 EST
From: rst@ai.mit.edu (Robert S. Thau)
Subject: More CGI Comments

   From: rhb@hotsand.att.com
   Date: Sun, 9 Jan 94 20:04:00 EST

[ Back from the NSF site visit... ]  Here are a few gut-level responses to
Rich's proposal.  I've posted what amounts to a proposal of my own, in the
form of the documentation for the changes I've made to the server, which
I'll forward along under separate cover.  Here's some comparison between
mine and Rich's...

   Based on some limited discussion here, I propose the following: 

   Add to the srm.conf file:
   ***************************

   # UserExec: Whether local user executables is activated
   # 

   UserExec True
   ***************************

   Add a .srm_conf file in the users public_html directory
   (or whatever it's set to in the server's srm.conf file)
   ***************************

   # ScriptAlias: This controls which directories contain server scripts.
   # Format: ScriptAlias fakename realname

   ScriptAlias /cgi-bin /home/rhb/myscripts

The net effect of this, if I understand it right, would be to designate
certain directories as all-script and others as all-file.  It may be
simpler to just add an Allow attribute to the .htaccess files, so that a
.htaccess file with "Allow Scripts" in it would enable execution of scripts
from the directory containing it, and any subdirectories.  This also would
ease the load on the server a bit --- it's looking for .htaccess files
anyway, while it is not yet looking for .srm_confs as well.  Then again,
I'm biased ;-).

At any rate, that's why I added the attribute to the .htaccess files,
rather than adding code for an entirely new config file.

   #and possibly a line that includes info on how I identify 
   #my executable files

   ExecAlias magic, suffix=exe
   ***************************

   One point brought to my attention is that all directories are already
   "executable" in the NCSA server environment via the <inc var> directive
   (i.e., this takes care of my original problem of getting to the
   user variable in authentication), but it doesn't take care of things
   like forms.

   Now a more general question that turns the previous question inside
   out.  The original concern was in adding executables to directories
   that were not indicated as having executables.  Well it seems that
   I can certainly have "regular" (non script) html files in executable
   directories.  What do I gain from having a directory "not executable"?
   Why aren't all directories cgi-bin directories?  I can restrict
   GET in the directories in which I really have scripts if I'm concerned
   about reading scripts...

In fact, that's basically the situation that I've got here --- all
directories are, at least potentially, cgi-bin directories (modulo the
trivia which I discuss under "compatibility").

Briefly, what you might gain from having a directory "not-executable" is
that if the directory is controlled by people you don't trust, they can't
screw you over as much.  Universities with a large population of
potentially hostile undergraduates are the classic example.  This is why
the NCSA server allows the adminstrator to control use of server scripts,
for instance, which (as you point out) raise many of the same security
issues.

   This in fact points up the need to have a "smart" server.  It is important
   that the server be able to identify the files it's serving not just by their
   suffix (i.e., in the mime type file) but via any available attribute
   (such as magic numbers).  The information that's available differs
   among platforms and application (server) needs.  I'm in agreement that
   we need a flexible method that allows the server to use it's "smarts"
   and the users' needs (e.g., the scripts I am interested in providing
   are all perl and tcl, so the #! would work for my application...).

It's certainly true that in the best of all possible worlds,
metainformation such as script-ness and MIME type would be represented
explicitly.  Gopher servers do this, for instance --- and explicitly
maintaining all the metainformation, and keeping it in synch with the files
themselves, turns out to be something of a chore.  The trick is to come up
with a scheme which is as simple and maintainable as the hack of
overloading this information onto file names (which is admittedly ugly, but
time-honored --- going back to 1963 at least).

   Rich

rst



From waterbug@epims1.gsfc.nasa.gov  Tue Jan 11 14:11:26 1994 +0500
Message-Id: <9401111911.AA07303@epims1>
Date: Tue, 11 Jan 1994 14:11:26 +0500
From: waterbug@epims1.gsfc.nasa.gov (Steve Waterbury)
Subject: Re: EARN Guide to Network Resources Tools Ver 2.0


Dudu Rashty writes:

> EARN Guide to Network Resources Tools Ver 2.0 is available from
> http://vms.huji.ac.il/WWW_HELP/earn.html

Can anyone get to this?  I get a "Requested document ... could not 
be accessed ..."

Steve.

=====================================================================
Stephen C. Waterbury		Phone:	301-286-7557
NASA Parts Project Office	FAX:	301-286-1695
Code 310.A			email:	waterbug@epims1.gsfc.nasa.gov
NASA/GSFC			"Sometimes you're the windshield;
Greenbelt, MD 20771			sometimes you're the bug."
=====================================================================



From masinter@parc.xerox.com  Tue Jan 11 11:20:57 1994 PST
Message-Id: <94Jan11.112112pst.2732@golden.parc.xerox.com>
Date: Tue, 11 Jan 1994 11:20:57 PST
From: masinter@parc.xerox.com (Larry Masinter)
Subject: RE: Whitespace

In reviewing the HTML design, try to keep in mind folks with an audio
interface, who expect the document to be read to them. There are lots
of ways to render a heading or a title in spoken language, and even a
way to render 'emphasis', and even varying degrees of emphasis.
However, it's very hard to render 'bold'.




From mcharity@hq.lcs.mit.edu  Tue Jan 11 18:57:55 1994 EST
Message-Id: <9401112357.AA25534@hq.lcs.mit.edu>
Date: Tue, 11 Jan 94 18:57:55 EST
From: mcharity@hq.lcs.mit.edu (Mitchell N Charity)
Subject: A note on naming www services

When creating a new www "service", it can be useful to reflect the
service name in the "path", in addition to reflecting it, as is
customary, in the DNS "host"name.

Suppose you are announcing a new www service, which is running on a
machine mydesk.cs.nowhere.edu .  People used to announce the service as
http://mydesk.cs.nowhere.edu/ .

This has the recognized problem that you may someday wish to use a
different machine.  So the convention has developed of creating a DNS
alias or pointer.  Thus you might announce http://www.cs.nowhere.edu/ .

But a related problem is NOT addressed by this solution:
  It is not posible to transparently move two services,
  begun on different servers, to a single server.

For service identity is being uniquely determined by the DNS name,
as a server receiving a connection does not know the name(s) which
the client resolved to find the server's IP address.

Adding service identity to the "path" addresses this problem.
One might announce  http://www.cs.nowhere.edu/www-cs/ .
This allows you to later, transparently, let the server providing
http://www.nowhere.edu/nowhere/ to also provide your service.
And lets you run take over the experimental
http://mumble-www.cs.nowhere.edu/mumble/ .

Mitchell



From luotonen@ptsun00.cern.ch  Wed Jan 12 10:15:39 1994 +0100
Message-Id: <9401120915.AA06381@ptsun03.cern.ch>
Date: Wed, 12 Jan 94 10:15:39 +0100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: Authentication in Mosaic


> Some quick questions on the authentication mechanism, at least as
> implemented in Mosaic 2.x.  I can't seem to find any specific 
> documentation on this subject.

Mosaic uses libwww authentication code, documented in

	http://info.cern.ch/hypertext/WWW/AccessAuthorization/Overview.html

What you need to read is the page:

	http://info.cern.ch/hypertext/WWW/AccessAuthorization/Browser.html


> Does Mosaic 2.x ever stop sending the authentication fields
> to a server,  i.e., is the only way to ensure that a session
> is closed to close the window?

For that server to directories that are protected -- no, it won't
stop.  AA info is only lost when exiting Mosaic, otherwise it's
cached globally (so exiting one window won't lose it -- this is
how it was designed to work, to minimize the amount of wasted time
in typing in usernames and passwords).

Important note: username and password for a given server are NEVER
sent to any other server, so you don't need to worry about your
authentication info going to vicious servers and their maintainers.


> Secondly, how many different servers can Mosaic 2.x authenticate
> to within the same window/process?  Is it greater than 1?

Unlimited number of servers/process.  Windows have nothing to do
with authentication.

-- Cheers, Ari --




From dsr@hplb.hpl.hp.com  Wed Jan 12 10:52:52 1994 GMT
Message-Id: <9401121052.AA22176@manuel.hpl.hp.com>
Date: Wed, 12 Jan 94 10:52:52 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: RE: Whitespace

Larry Masinter writes:

> In reviewing the HTML design, try to keep in mind folks with an audio
> interface, who expect the document to be read to them. There are lots
> of ways to render a heading or a title in spoken language, and even a
> way to render 'emphasis', and even varying degrees of emphasis.
> However, it's very hard to render 'bold'.

In using emphasis authors would like to be sure that a given set of
emphasis tags will in fact be rendered differently on *ALL* browsers.
The precise way these are differentiated will clearly depend on the
characteristics of each browser, on dumb terminals email conventions
could be used while on others color and font attributes can be used.

Now, while we could use neutral names such as <HP0> <HP1> ... <HP4>
most of us would prefer more meaningful names which convey the common
interpretation. This explains why <B>, <I> etc are well liked.

The problem with "logical" emphasis tags is there is no easy way of
pulling together an effective small set. In my attempts to do so for
HTML+, it rapidly became apparent that this is a bottomless pit.
The set in the DTD provides just a few extras to those in HTML, perhaps
leaning too far towards the needs of computer manuals. As for <EM> and
<STRONG> these are ok for some purposes, but comprise too small a set.

Dave Raggett



From relihanl@ul.ie  Wed Jan 12 11:30:25 1994 +0000 (WET)
Message-Id: <Pine.3.05.9401121125.B29214-a100000@itdsrv1.ul.ie>
Date: Wed, 12 Jan 1994 11:30:25 +0000 (WET)
From: relihanl@ul.ie (Liam Relihan)
Subject: Minutes of WWW/TEI meeting for last Noveber

Dave was looking for a more permanent place to put "Minutes of 
WWW/TEI meeting for last Noveber".

You can now find it on the University of Limerick server:

<A HREF="http://itdsrv1.ul.ie/Research/WWW/cork_tei_www.html">Minutes are
here</A>


Liam

--
     Liam Relihan        | Voice: +353-61-333644 ext. 5015   |     _ | __ \
 CSIS, Schumann Building | Fax:   +353-61-330876             |    |  | |   |
 University Of Limerick  | E-mail: relihanl@ul.ie            |    |  | __ /
        Ireland                                             ____|____|_|  _\





From luotonen@ptsun00.cern.ch  Wed Jan 12 12:44:27 1994 +0100
Message-Id: <9401121144.AA06457@ptsun03.cern.ch>
Date: Wed, 12 Jan 94 12:44:27 +0100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: Semicolon's for all



Rob McCool feeling sorry for me:
> I feel sorry for Ari, wherever he is, getting back to all 3000 lines of this
> argument.

I'm back, I've just been quiet watching you handle it very well :-)

As for the whole discussion about semicolons et al, I'm against it
simply because it makes scripts non-seemless.  I may have a huge
amount of some information in some format, lets take a simple example
of httpd log files.  I have what seem to be files:

	/httpd/statistics/1993/Dec/ByHost.html
	/httpd/statistics/1993/Dec/ByDomain.html
	/httpd/statistics/1993/Dec/ByDocument.html

I may first implement this as a script that takes the year (1993),
month (Dec) and target format (ByWhatEver.html) as its arguments,
and generates it from a single file stored somewhere is some format.

Two months later I'll notice, oh shit, this wastes *so*much* CPU
time -- I'll just create these files after the end of each month.

Two years later I naturally run out of disk space, and also know
that only the few latest months are usually referenced.  So I'll
remove all the By*.html files execpt this year's ones, and
reintroduce my statistics script to generate older ones from a
single compressed log file.

This would be a mess with semicolons going back and forth.

I don't care if someone starts using these ;'s in his or her
URLs, especially as it is CGI/1.0 compliant as Rob mentioned,
for my server the rule file can certainly be used to strip
them away anyway, if necessary.  But always requiring ; to
terminate the script name is bad.

The example above is what I regard as the most important aspect
of scripts, and it would kill me to see it destroyed.

-- Cheers, Ari --




From "Eberhard  Wed Jan 12 13:15:24 1994 EST
Message-Id: <009786D5.8D9642A0.20128@grzap1.rz.go.dlr.de>
Date: Wed, 12 Jan 1994 13:15:24 EST
From: "Eberhard (DLR)
Subject: 

help



From luotonen@ptsun00.cern.ch  Wed Jan 12 16:12:51 1994 +0100
Message-Id: <9401121512.AA06711@ptsun03.cern.ch>
Date: Wed, 12 Jan 94 16:12:51 +0100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: CGI, semicolons, and so on...



Mark A. Krause wrote:
> On Dec 30,  9:09pm, Charles Henrich wrote:
> >
> > I still say we go with the execute bit, 1000 times more flexible and simple
> > than any other method on the planet.
> 
> What if I want to run a server on a DOS PC or a Macintosh?  I think it is
> important to make sure that GCI is not OS specific.  The continued growth
> of the Web is going to depend upon how easy it is to get new servers up
> and running.  Not everyone is going to have access to a UNIX system for
> this.

There is constant confusion about what the CGI specifies.  Let's
make it clear:

	CGI specifies the interface between HTTP server and
	a script: the command line args, environment variables,
	standard input and standard output.

CGI has NOTHING WHATSOEVER to do with how a server decides if
a file is executable or not.

One server writer could use x-bits to resolve executability.
Another could use a fixed cgi-bin directory and a fixed prefix in
URL pathname.  Another can have many of them, with generic mapping
scheme.  Yet another can just list them in its config file.
It's all up to server implementation.

	CGI guarantees that *scripts* written for one CGI server
	indeed work on another CGI compliant server.

A script can be detected executable in numerous ways -- never mind
how it's done.  In any case, once the server has somehow found out that
a file is an executable CGI script, it WILL call it in a manner described
in CGI/1.0 spec.

-- Cheers, Ari --




From ctrbdo%iapa.uucp@constellation.ecn.uoknor.edu  Wed Jan 12 08:46:34 1994 CST
Message-Id: <9401121446.AA00982@maple.iapa>
Date: Wed, 12 Jan 94 08:46:34 CST
From: ctrbdo%iapa.uucp@constellation.ecn.uoknor.edu (bryan oakley)
Subject: <B> vs. <STRONG>, <I> vs. emphasis


Dave Raggett wrote:

>> Larry Masinter writes:

> In reviewing the HTML design, try to keep in mind folks with an audio
> interface, who expect the document to be read to them. There are lots
> of ways to render a heading or a title in spoken language, and even a
> way to render 'emphasis', and even varying degrees of emphasis.
> However, it's very hard to render 'bold'.

Good point! (In my office is a blind programmer, believe it or not)

>> In using emphasis authors would like to be sure that a given set of
>> emphasis tags will in fact be rendered differently on *ALL* browsers.
>> The precise way these are differentiated will clearly depend on the
>> characteristics of each browser, on dumb terminals email conventions
>> could be used while on others color and font attributes can be used.

Very true.

>> Now, while we could use neutral names such as <HP0> <HP1> ... <HP4>
>> most of us would prefer more meaningful names which convey the common
>> interpretation. This explains why <B>, <I> etc are well liked.

>> The problem with "logical" emphasis tags is there is no easy way of
                                               ^^^^^^^^^^^^^^^^^^^^^^^
>> pulling together an effective small set. In my attempts to do so for
   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
maybe there is.... 

>> HTML+, it rapidly became apparent that this is a bottomless pit.
>> The set in the DTD provides just a few extras to those in HTML, perhaps
>> leaning too far towards the needs of computer manuals. As for <EM> and
>> <STRONG> these are ok for some purposes, but comprise too small a set.

>> Dave Raggett

All very good points, and I have hit this wall several times (ie: I
have many different 'logical' types of text), and ask myself such
questions as 'do I use <CODE> or <KBD> or <SAMP> to tag a directory
name', and so on.  I am *eagerly* awaiting support for <RENDER ...> so
that I can preface my documents with <RENDER DIR KBD> (or whatever the
syntax; I don't have my HTML+ spec handy).

Does it not make sense to remove all of the physical styles (bold,
italic, underline, etc.) in favor of the logical (emphasis, strong,
etc.) since each author can use <RENDER> to make a HTML+ compliant
document using any tags that seem appropriate?  In some respects I
think the generic names would be fine for the following proposal,
though I use <EM> and <STRONG> whenever possible.

Now, I know some folks will say "but I want this word ITALIC,
dagnabbit!".  I firmly believe that the 'spirit' of HTML is that we
are free from having to make such decisions.

My suggestion is perhaps to have a <FACE> tag (or maybe <HINT> or
<STYLE>, but I'll use <FACE> for this discussion) which more-or-less
says "If at all possible, render <TAG> as 'style' (eg: <FACE EM
'italic'> would mean: if at all possible, render '<EM>' in an italic
style.  The first argument (?) to FACE would be a valid tag such as
EM, STRONG, KBD, etc.  The second argument would be a fixed set
of styles such as 'italic', 'bold', 'bold-italic', 'reverse', which
could be combined into 'faces' (ala emacs).  Unlike RENDER, which is
effectively a preprocessor statement, <FACE> would speak directly to
the browser, expressing the wishes of the author.  One could then do
something like (forgive syntactical errors):

<RENDER FILENAME KBD>    <!-- render <FILENAME> like <KBD>      -->
<RENDER DIRNAME  KBD>    <!-- ditto for <DIRNAME>               -->
<FACE KBD 'fixed-bold'>  <!-- suggest that <KBD> be rendered as -->
                         <!-- fixed-width characters, bolded    -->

<H2>Typographic conventions</H2
The following typographic conventions are used in this document:<BR>
filenames are shown in <FILENAME>this font</FILENAME> (typically
bolded fixed width characters)<BR>
directory names are shown in <DIRNAME>this font</DIRNAME>... <BR>

... etc. and so on ad infinitum ad nauseum...

With this, authors are free to choose the emphasis styles that best
meet their needs for a given document, and HTML+ is not riddled with
100's of emphasis styles.  Those that want to use <I> and <B> are
happy without having <I> and <B> officially part of the spec, and the
browsers can attempt to render the way the author intended.  

Comments?

---------------------------------------------------------------------
Instrument Approach Procedures Automation             DOT/FAA/AMI-230
---------------------------------------------------------------------
Bryan D. Oakley              ctrbdo%iapa@constellation.ecn.uoknor.edu
KENROB and Associates, Inc.              voice: (405) 954-7176 (work)
5909 NW Expwy Suite 209                         (405) 366-6248 (home)
Oklahoma City, Ok.  73132            



From WIGGINS@msu.edu  Wed Jan 12 10:51:27 1994 EST
Message-Id: <9401121556.AA18133@dxmint.cern.ch>
Date: Wed, 12 Jan 94 10:51:27 EST
From: WIGGINS@msu.edu (Rich Wiggins)
Subject: Re: CGI, semicolons, and so on...

>There is constant confusion about what the CGI specifies.  Let's
>make it clear:
>
>	CGI specifies the interface between HTTP server and
>	a script: the command line args, environment variables,
>	standard input and standard output.
>
>CGI has NOTHING WHATSOEVER to do with how a server decides if
>a file is executable or not.
>
>One server writer could use x-bits to resolve executability.
>Another could use a fixed cgi-bin directory and a fixed prefix in
>URL pathname.  Another can have many of them, with generic mapping
>scheme.  Yet another can just list them in its config file.
>It's all up to server implementation.

This compartmentalization of protocol specs has its advantages,
but if you step back for a second, what you've just said is that
we're deliberately designing things so as to minimize portability
of HTTP/CGI/HTML works among servers.  Someone porting a corpus
that includes scripts is going to have to harvest all the
out-of-band signals and re-specify the information in the foreign
server's config files, exec bits, suffix, or whatever.

Yes, I realize we're not going to have portability of scripts
across platforms, but we seem to be explicitly defining away
compatibility across servers, even running on the same OS.

/Rich Wiggins, CWIS Coordinator, Michigan State U



From mcclanah@dlgeo.cr.usgs.gov  Wed Jan 12 10:50:08 1994 -0600
Message-Id: <9401121650.AA03872@dlgeo.cr.usgs.gov>
Date: Wed, 12 Jan 1994 10:50:08 -0600
From: mcclanah@dlgeo.cr.usgs.gov (mcclanah@dlgeo.cr.usgs.gov)
Subject: WYSIWYG HTML editor for the X Window System - comp.multimedia #13003

Found this on the comp.multimedia newsgroup.  Everyone provide your input - maybe it will
be what everyone has been wanting. (or at least the beginnings of something that can be 
polished up by someone else if they don't quite make it)

----------------------------------------------------------------

In article <2gueru$m94@crcnis1.unl.edu>, reich@unlinfo.unl.edu (stephen reichenbach) writes:
I am assigning the students in my Multimedia Computing Seminar to
write a WYSIWYG HTML editor for the X Window System (to be done as
a group project).  We plan to make the program available.

I am writing this note to request from interested users suggestions
about the design, functionality, etc. of this project.

Please post your suggestions so that others in the community will
have an opportunity to expand or comment on others' suggestions.

Thank you.

Steve Reichenbach (reich@cse.unl.edu)

-- 
Pat McClanahan		Internet:mcclanah@dlgeo.cr.usgs.gov
EROS Data Center 		 mcclanah@edcserver1.cr.usgs.gov
Sioux Falls, SD
605-361-4607





From robm@ncsa.uiuc.edu  Wed Jan 12 08:55:54 1994 -0600
Message-Id: <9401121455.AA18110@void.ncsa.uiuc.edu>
Date: Wed, 12 Jan 1994 08:55:54 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: HomePage URL without a file name.How?

/*
 * Re: HomePage URL without a file name.How?  by dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
 *    written on Jan  6,  5:43pm.
 *
 * 
 *      
 * It took me a while to figure that one out too.  Index.html is the default
 * name.  If you don't have a document named that, it will create a directory
 * index for you, thus bypassing your intended home page.  This is not 
 * documented very clearly.  Hope this helps.
 *      
 */

Actually, I created the document
http://hoohoo.ncsa.uiuc.edu/docs/setup/admin/AccessingFiles.html because I
was recieving nine million questions exactly like that... I'm not sure how
much more clearly I can document it.

--Rob



From bert@let.rug.nl  Wed Jan 12 18:00:28 1994 +0100 (MET)
Message-Id: <9401121700.AA13451@freya.let.rug.nl>
Date: Wed, 12 Jan 1994 18:00:28 +0100 (MET)
From: bert@let.rug.nl (Bert Bos)
Subject: Re: PATHs in HTML

I would definitely vote "yes" to the PATHs-in-HTML proposal.

Jim McBeath writes:

 |Dave Raggett and I have been having an email discussion of PATHs in HTML.
 |What follows is the results of that discussion, which I present as
 |a proposal for an enhancement to HTML (and WWW clients).

Maybe the word "PATH" is too vague and too overloaded already. Why not
call it a "TOUR"?

 |Client programs could add the following commands that deal with paths:
 |1. Traverse current path
 |2. Traverse named path
 |3. Find in current path
 |4. Find in named path
 |5. Print current path
 |6. Print named path

If I understand correctly, these commands automatically become
available to the user iff the browser detects a "Subdocument" or
"Path". It would be interesting to see how interface designers handle
this: greyed menu items? a pop-up tool bar?

 |This scheme is easy to understand, easy to define, and easy to implement,
 |entirely in the client.  Clients which did not understand REL=Subdocument
 |and REL=Path would just ignore those attributes; the user could still use
 |the path node to get to the nodes in the path by selecting an anchor.

!!!

The most heavily used button in my Mosaic browser is the "Back"
button, which proves to me that this is something I've been waiting
for.


Bert
-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|



From putz@parc.xerox.com  Wed Jan 12 10:01:48 1994 PST
Message-Id: <94Jan12.100151pst.2445@spoggles.parc.xerox.com>
Date: Wed, 12 Jan 1994 10:01:48 PST
From: putz@parc.xerox.com (Steve Putz)
Subject: Re: PATHs in HTML


Jim McBeath wrote:
> "PathNext" and "PathPrevious" buttons would facilitate stepping forwards
> and backwards through the nodes of the currently active path.

Two suggestions:

*** Next and Previous Buttons ***

I would like to see WWW browsers (especially Mosaic) implement "Next"
and "Previous" as they are in the original CERN line mode browser.
That way, any HTML document with a list of links in it can be used to
traverse a "PATH".

*** Display Selected Documents in a *Fixed* Second Viewer ***

A related feature I would like to see in Mosaic is this (inspired by a
feature in the original Viola browser):

Currently middle button selections (in X Mosaic) always spawn a new
document view window.  I would much rather it only create a new window
the first time, and then reuse that viewer for subsequent middle button
selections.  (The "Clone" button can always be used to open additional
viewers).


These changes are even simpler than the proposed PATH features
(although some of those would be nice also).

Comments?

Steve Putz Xerox Palo Alto Research Center



From rmz@ifi.uio.no  Wed Jan 12 19:05:52 1994 +0100
Message-Id: <19940112180552.14642.solva.ifi.uio.no@ifi.uio.no>
Date: Wed, 12 Jan 1994 19:05:52 +0100
From: rmz@ifi.uio.no (=?iso-8859-1?Q?Bj=F8rn_Remseth?= )
Subject: Re: PATHs in HTML



> I would definitely vote "yes" to the PATHs-in-HTML proposal.

Me too.  

We have an application, a CAI system, where a document is gradually
revealed by pushing a "reveal" button in the dokument.  At the end a
complete document is shown.  When you then press "back", you get to
the document at the previous "reveal" level, which is clearly wrong,
the "back" button should bring you the previous document, not the
previous "revelation" of the current document.

                                                    (Rmz)

Bj\o rn Remseth   !Institutt for Informatikk    !Net:  rmz@ifi.uio.no
Phone:+47 22855802!Universitetet i Oslo, Norway !ICBM: N595625E104337



From sanders@BSDI.COM  Wed Jan 12 12:31:37 1994 -0600
Message-Id: <199401121831.MAA07495@austin.BSDI.COM>
Date: Wed, 12 Jan 1994 12:31:37 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: CGI, semicolons, and so on... 


Ari Luotonen writes:
> Mark A. Krause wrote:
...
> > What if I want to run a server on a DOS PC or a Macintosh?  I think it is
> > important to make sure that GCI is not OS specific.  The continued growth
> > of the Web is going to depend upon how easy it is to get new servers up
> > and running.  Not everyone is going to have access to a UNIX system for
> > this.
>
> There is constant confusion about what the CGI specifies.  Let's
> make it clear:
> 
> 	CGI specifies the interface between HTTP server and
> 	a script: the command line args, environment variables,
> 	standard input and standard output.
Just to be clear, the CGI *is* OS specific because it requires things like
command line arguments, environment variables, and stdin/stdout.  I don't
think you can define an interface at this level without being somewhat
OS specific.

> CGI has NOTHING WHATSOEVER to do with how a server decides if
> a file is executable or not.

You are correct that CGI *does not* specify how to determine if a file is
executable but let me clarify that CGI does (potentially) impact the URL.
If scripts are to be portable then the URLs themselves must be portable.
That means that CGI must also define how the server decodes the URL (to
some extent anyway).

Let's consider all the known schemes for determining "scriptability":
	;
	cgi-bin
	.cgi (or other magic extension)
	xbit
	config file
The portability constraint requires that any scheme adopted must work on
all servers.  So, if a scheme requires special processing by the server
(like ";" might, depending on how it's used) then it must be covered by
the CGI spec.  e.g., http://server/foo;/arg1/arg2 is probably ok but
http://server/foo;arg1/arg2 is not.

There isn't anything wrong with adding special decoding in the server for
some format (e.g., forms), but it will require coverage in the CGI spec
and all CGI servers must support it (or else it's not really useful).

--sanders



From bert@let.rug.nl  Wed Jan 12 19:56:13 1994 +0100 (MET)
Message-Id: <9401121856.AA15285@freya.let.rug.nl>
Date: Wed, 12 Jan 1994 19:56:13 +0100 (MET)
From: bert@let.rug.nl (Bert Bos)
Subject: Re: PATHs in HTML

Bill Perry asks:

 |  Now what exactly is the format of something pointed to by
 |REL=Subdocument?  Just a list of URLs like:
 |http://cs.indiana.edu/finger
 |http://cs.indiana.edu/LCD/cover.html
 |...
 |
 |   Or just a bunch of <LINK > attributes?  Would the REL attributes still
 |all be Subdocument?  Would there be a NAME attribute?
 |
 |<LINK REL=Subdocument NAME="My weird path around IU">

No, the beauty of the scheme is that there is nothing in the target
document that makes it into a node of a path. The document that
contains the rel=Subdocument itself *is* the path. As an example, here
is a valid path. Note that it is a perfectly normal HTML document.

  <!doctype html>
  <title>My path around Mosaic</title>
  <h1>My path around Mosaic</h1>
  You should read the following in this order:
  <ol>
  <li><a rel=subdocument href=
  "http://www.ncsa.uiuc.edu/demoweb/demo.html">Mosaic demo</a>
  <li><a rel=subdocument href= 
  "http://www.ncsa.uiuc.edu/demoweb/html-primer.html">About HTML</a>
  <li><a rel=subdocument href=
  "http://info.cern.ch/hypertext/WWW/Addressing/Addressing.html">About URLs</a>
  </ol>
  OK?

This would be a document on my server, but it builds a path out of
nodes that are read-only to me. A browser that displays the above text
would note the occurrence of three Subdocuments and offer to the user
to display all three of them one after the other (the "Traverse
current path" button).

When the user decides to "traverse the path", the first subdocument
("Mosaic demo") is retrieved and displayed normally. The user can do
anything he normally does, but in addition he can now also press the
spacebar and be transported to the second document ("About HTML"). The
next press of the spacebar would retrieve the third node ("About
URLs") and the fourth would be an error.

Another way to view this is to imagine that the browser "queues" all
links that have rel=Subdocument. When the user doesn't select a link
explicitly, the next node from the queue is popped and displayed
instead.

NB1: the "spacebar" is just an example, it could be any other key or
mouse click. 

NB2: the "queue" model is inaccurate, in the following way: when
you're already traversing a path, occurences of Subdocument are not
queued, but occurrences of Path are.

NB3: The above is just my interpretation. Jim McBeath or Dave Raggett
can probably do better.


Bert
-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|



From bert@let.rug.nl  Wed Jan 12 20:15:23 1994 +0100
Message-Id: <9401121915.AA15620@freya.let.rug.nl>
Date: Wed, 12 Jan 1994 20:15:23 +0100
From: bert@let.rug.nl (Bert Bos)
Subject: 

>From bert@let.rug.nl Wed Jan 12 20:14 MET 1994 remote from tyr
Received: from freya.let.rug.nl by tyr.let.rug.nl with SMTP
	(1.37.109.8/16.2) id AA21206; Wed, 12 Jan 1994 20:14:45 +0100
Return-Path: <bert@let.rug.nl>
Received: from tyr.let.rug.nl by freya.let.rug.nl with SMTP
	(1.37.109.8/16.2) id AA15614; Wed, 12 Jan 1994 20:15:03 +0100
Message-Id: <9401121915.AA15614@freya.let.rug.nl>
Received: by tyr.let.rug.nl
	(1.37.109.8/16.2) id AA21203; Wed, 12 Jan 1994 20:14:42 +0100
From: Bert Bos <bert@let.rug.nl>
Subject: Re: PATHs in HTML
To: bert@let.rug.nl (Bert Bos)
Date: Wed, 12 Jan 1994 20:14:42 +0100 (MET)
In-Reply-To: <9401121856.AA15285@freya.let.rug.nl> from "Bert Bos" at Jan 12, 94 07:56:13 pm
X-Mailer: ELM [version 2.4 PL13]
Mime-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
Content-Length: 676       
Sender: bert

If you wonder which message my earlier mail:

 |Bill Perry asks:
 |
 | |  Now what exactly is the format of something pointed to by

refers to; I accidentally sent it to the list instead of to Bill Perry
personnally. Sorry.

-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|




From ctrbdo%iapa.uucp@constellation.ecn.uoknor.edu  Wed Jan 12 10:40:30 1994 CST
Message-Id: <9401121640.AA01030@maple.iapa>
Date: Wed, 12 Jan 94 10:40:30 CST
From: ctrbdo%iapa.uucp@constellation.ecn.uoknor.edu (bryan oakley)
Subject: need printable docs on WWW

I am looking for some postscript files that I once had and hope some
kind soul can help me acquire them.  I have been 'volunteered' to demo
what we've done with HTML to a group who is tasked to develop a
monolithic system for distributing and maintaining documents (for a
major US government 8^|).  They seemed interested in what the WWW is all
about, but skeptical at it's newness (and cheapness).

When I first investigated what WWW was all about I had found some nice
postscript documents which I would like to give to the demo-ees.
Problem is, I don't have the files anymore, I don't know where I got
them in the first place, and I don't have an internet connection so I
can't easily get them (yeah, I know, but Mosaic works great for LANs
too). 

I am hoping that if you know of, or possess, some of the documents I
am looking for we can arrange for you to email them to me.  Sadly, I
need them Real Soon (tm) (Jan 13th).  The documents I am looking for
are described below.  In all cases I have probably-old hardcopy from
the original postscript document (as opposed to a printed HTML file),
and am looking to obtain the most recent version of the original
postscript source.   

1) 'NCSA Mosaic Technical Summary'.  This is a nice paper written by
Marc Andreessen on what the NCSA Mosaic product is all about.  The
version I have is dated February 20, 1993.

2) 'Getting Started with NCSA Mosaic'.  Again written by Marc
Andreessen, this details what is needed to use Mosaic.  The copy I
have is dated May 8, 1993.

3) A document written by the folks at CERN, with 'World-Wide Web
server software, Generated from the Hypertext' on the front sheet, 
and dated June 2, 1993.  This is a tome of some 7 chapters, including
an HTML style guide and info on a shell script HTTPD server.
This is the one I really want, as it gives a nice overview of the
software available, at least at the time of the writing.  Hopefully
its been updated? 

4) Another CERN document.  My copy begins with "Chapter 1 WWW Daemon
user guide" and looks to be a document written with LaTeX.  Chapter 2
is "Hypertext daemon", and chapters 3 and 4 are "The W.A.I S. - WWW
gateway" and "Gateway to VMS Help".  

5) Yet another CERN document titled "World-Wide Web: An Information
Infrastructure for High-Energy Physics".  In the footnote it states
the document is a preprint of a paper presented at "Software
Engineering, Articicial (sic) Inteligence and Expert Systems for High
Energy and Nuclear Physics" in January, 1992.

If you know where I can get my hands on any of these documents, or any
other documents which effectively describe the WWW or WWW specific
programs (such as Mosaic, various httpd implementations and the like),
I would appreciate being given the document, or at least an ftp
address (I can find a host to ftp from in an emergency).  URLs won't
do me much good.

Thanks for your help.

---------------------------------------------------------------------
Instrument Approach Procedures Automation             DOT/FAA/AMI-230
---------------------------------------------------------------------
Bryan D. Oakley                   ctrbdo%iapa@mailhost.ecn.uoknor.edu
KENROB and Associates, Inc.              voice: (405) 954-7176 (work)
5909 NW Expwy Suite 209                         (405) 366-6248 (home)
Oklahoma City, Ok.  73132            



From montulli@stat1.cc.ukans.edu  Wed Jan 12 14:02:19 1994 CST
Message-Id: <9401122002.AA17011@stat1.cc.ukans.edu>
Date: Wed, 12 Jan 94 14:02:19 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: RE: Whitespace

> 
> Larry Masinter writes:
> 
> > In reviewing the HTML design, try to keep in mind folks with an audio
> > interface, who expect the document to be read to them. There are lots
> > of ways to render a heading or a title in spoken language, and even a
> > way to render 'emphasis', and even varying degrees of emphasis.
> > However, it's very hard to render 'bold'.
> 
> In using emphasis authors would like to be sure that a given set of
> emphasis tags will in fact be rendered differently on *ALL* browsers.
> The precise way these are differentiated will clearly depend on the
> characteristics of each browser, on dumb terminals email conventions
> could be used while on others color and font attributes can be used.
> 
> Now, while we could use neutral names such as <HP0> <HP1> ... <HP4>
> most of us would prefer more meaningful names which convey the common
> interpretation. This explains why <B>, <I> etc are well liked.
> 
> The problem with "logical" emphasis tags is there is no easy way of
> pulling together an effective small set. In my attempts to do so for
> HTML+, it rapidly became apparent that this is a bottomless pit.
> The set in the DTD provides just a few extras to those in HTML, perhaps
> leaning too far towards the needs of computer manuals. As for <EM> and
> <STRONG> these are ok for some purposes, but comprise too small a set.
> 
> Dave Raggett
> 
What about my suggestion at the WWW/TEI meeting to have 4-6 different
kinds of emphasis.  For instance <em1>,<em2>... or <em a>, <em b>....
The different kinds wouldn't neccessarily be any kind of precedence
order but would simply specify DIFFERENT kinds of emphasis for each
of the tags.  That way you could specify different words to be
emphasised and be sure that each emphasis style will be unique.

The problem is that there is only a limited number of ways to
show emphasis, especially on non graphical terminals, so if you
define 20+ kinds of logical emphasis they all get mapped to a
smaller set of physical styles.  In many cases you could specify
two different logical emphasis and they will get rendered
EXACTLY the same.  As a document writer it would be nice to be
able to know that text marked with different emphasis tags
will indeed be shown in a different styles.  Specifying a 
smaller set of somewhat logical emphasis tags that will have
unique physical renderings will solve this problem.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From jern@spaceaix.jhuapl.edu  Wed Jan 12 15:07:45 1994 -0500 (EST)
Message-Id: <9401122007.AA18664@sdrmis.jhuapl.edu>
Date: Wed, 12 Jan 1994 15:07:45 -0500 (EST)
From: jern@spaceaix.jhuapl.edu (jern@spaceaix.jhuapl.edu)
Subject: Re: WYSIWYG HTML editor for the X Window System - comp.multimedia #13003

> I am assigning the students in my Multimedia Computing Seminar to
> write a WYSIWYG HTML editor for the X Window System (to be done as
> a group project).  We plan to make the program available.
> 
> Steve Reichenbach (reich@cse.unl.edu)
> 
WYSIWYG or WYSINNWOS (What You See Is Not Necessarily What Others See)??
If viewers are configurable, wrt font sizes and faces...  This
is the whole problem with the <whitespace> discussion (mixing metaphors).
X-HTML GUI editor?  Bring it on!

bob jernigan



From relihanl@ul.ie  Wed Jan 12 21:59:21 1994 +0000 (WET)
Message-Id: <Pine.3.05.9401122121.A19693-a100000@itdsrv1.ul.ie>
Date: Wed, 12 Jan 1994 21:59:21 +0000 (WET)
From: relihanl@ul.ie (Liam Relihan)
Subject: A page for WWW Research topics

Hi...

For a few months now, I have been doing research into various aspects of
W3. Unfortunately, I found it difficult to locate important research
documents among the user manuals, etc.

I am now setting up a page which will contain links to things like RFCs,
papers, abstracts, etc.

I reckon it is a good thing if we can find all useful research stuff at a
single location.

The page is at http://itdsrv1.ul.ie/Research/WWW/www-research.html

If there are papers, etc. that I hape omitted, please mail me.


Thanks

Liam



--
     Liam Relihan        | Voice: +353-61-333644 ext. 5015   |     _ | __ \
 CSIS, Schumann Building | Fax:   +353-61-330876             |    |  | |   |
 University Of Limerick  | E-mail: relihanl@ul.ie            |    |  | __ /
        Ireland                                             ____|____|_|  _\




From kaehms@sedbsvr.se.ssd.lmsc.lockheed.com  Wed Jan 12 15:08:09 1994 PST
Message-Id: <9401122308.AA22393@eagle.is.lmsc.lockheed.com>
Date: Wed, 12 Jan 94 15:08:09 PST
From: kaehms@sedbsvr.se.ssd.lmsc.lockheed.com (Bob Kaehms)
Subject: status of clients?

Hi...

Someone posted a survey of www clients a while back in a nice chart format  
that listed their capabilities.  Could someone point me to the latest rev
of such a beast?  Thanks.

Also, how's the NCSA Mac/PC groups doing on the next rev?

-bob kaehms




From ctrbdo%iapa.uucp@constellation.ecn.uoknor.edu  Wed Jan 12 16:46:44 1994 CST
Message-Id: <9401122246.AA01302@maple.iapa>
Date: Wed, 12 Jan 94 16:46:44 CST
From: ctrbdo%iapa.uucp@constellation.ecn.uoknor.edu (bryan oakley)
Subject: printable docs...

In regards to my earlier request for postscript documentation on the
Web:

Many thanks to all who responded.  I've now got the docs I was looking
for.  I appreciate the speedy replies.

---------------------------------------------------------------------
Instrument Approach Procedures Automation             DOT/FAA/AMI-230
---------------------------------------------------------------------
Bryan D. Oakley              ctrbdo%iapa@constellation.ecn.uoknor.edu
KENROB and Associates, Inc.              voice: (405) 954-7176 (work)
5909 NW Expwy Suite 209                         (405) 366-6248 (home)
Oklahoma City, Ok.  73132            



From janssen@parc.xerox.com  Wed Jan 12 17:12:52 1994 PST
Message-Id: <0hB_0IQB0KGW43cWon@holmes.parc.xerox.com>
Date: Wed, 12 Jan 1994 17:12:52 PST
From: janssen@parc.xerox.com (Bill Janssen)
Subject: Re: Whitespace

Excerpts from ext.WorldWideWeb: 12-Jan-94 RE: Whitespace
Dave_Raggett@hplb.hpl.hp (1329)

> As for <EM> and <STRONG> these are ok for some purposes, but comprise
> too small a set.

No, they comprise too *large* a set -- there's no semantic difference
between them.

As I've noted in other circumstances, most of English literature has
gotten along with a *very* small set of procedural markup -- basically
italics and left and right margins.  Italics are used for basically two
purposes (though Fowler's _Modern English Usage_ cites 8 usages, these
can be grouped into just two major categories):  emphasis and
alternate-usage (such as ship names, foreign words, book titles,
newspaper names, variable or type names in programming languages). 
Sometimes capitalization, bold face, or underlining is used to convey
alternate-usage.  (The only real use for margins is to indicate
quotations or excerpted material.)  But what realling needs to be
communicated is not <B>, but rather <ALTERNATE-USAGE>, not <I>, but
<EMPHASIS>.  There are no agreed-upon conventions as to when bold face
is used in a document -- so you can't tell what it means if someone
sends you a document that merely says <B>.  That's the crucial
difference between descriptive and procedural markup.

Chapters, paragraphs, and sections are typically indicated in English
literature with with a mix of presentational and procedural markup, and
it makes some sense to include descriptive markup to handle these.

I think the real driving force behind the welter of markup now in HTML+
is the desire to make posters, or pictures, or whatever you might call
them -- non-textual art -- combined with the misapprehension that HTML+
is the only document formatting language we can use.

Bill



From montulli@stat1.cc.ukans.edu  Wed Jan 12 20:03:28 1994 CST
Message-Id: <9401130203.AA38308@stat1.cc.ukans.edu>
Date: Wed, 12 Jan 94 20:03:28 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Whitespace

> 
> Excerpts from ext.WorldWideWeb: 12-Jan-94 RE: Whitespace Lou
> Montulli@stat1.cc.uk (2945)
> 
> > That way you could specify different words to be
> > emphasised and be sure that each emphasis style will be unique.
> 
> But what do you *mean* by each of the emphasis styles?
> 
> Bill
> 
Many people don't care what they mean, they just want to make
different sections of text look different than other sections
to help the reader distinquish between different topics or
meanings.

This technique combined with semantic emphasis will insure that
semantic meaning will not be lost and will also ensure that a proper
representation will be made in terms of physical styles.

For instance you may specify a tag <em style=1 meaning=citation>
and another <em style=2 meaning=quotation>.  Browsers that use
physical style will be sure to use unique physical representations
of the emphasis while browsers that have the ability to show
the true meaning behind the emphasis (I don't know of any that exist
right now) will use the "meaning".  That way you can be covered
no matter what the representation.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From janssen@parc.xerox.com  Wed Jan 12 18:33:02 1994 PST
Message-Id: <YhB==S4B0KGW03caQU@holmes.parc.xerox.com>
Date: Wed, 12 Jan 1994 18:33:02 PST
From: janssen@parc.xerox.com (Bill Janssen)
Subject: Re: Whitespace

Excerpts from ext.WorldWideWeb: 12-Jan-94 Re: Whitespace Lou
Montulli@stat1.cc.uk (1652)

> Many people don't care what they mean, they just want to make
> different sections of text look different than other sections
> to help the reader distinquish between different topics or
> meanings.

They'd better not count on that, though.  HTML was originally designed
to be universal, which means that browsers for it have been written in
Emacs -- single-font, no type faces, Emacs.  I can't see how to write
such a browser for HTML any more.  There is no effective way to indicate
alternate-usage (except for some very restricted set of alternate
usages:  title, acronym, abbrev, cmd, arg, var -- and even these are
``recommended'' optional roles), so there's no way to know what <em>
really means any more.  I guess you'd map every <em>foo</em> to either
"*foo*" or "foo", depending on the role (if any).

The mere fact that the _Physical Styles_ section is still in the HTML+
document is quite distasteful (as is _Inlined Graphics or Icons_, which
should be obsoleted by ``figure'' and/or ``embed''.  _Horizontal Rule_ 
is also a wart that should be removed.  I'm not sure yet about _Floating
Panels_, though I don't see why they can't be replaced by ``figure''
and/or ``embed''.

Bill



From WIGGINS@msu.edu  Thu Jan 13 00:20:20 1994 EST
Message-Id: <9401130527.AB07952@dxmint.cern.ch>
Date: Thu, 13 Jan 94 00:20:20 EST
From: WIGGINS@msu.edu (Rich Wiggins)
Subject: Re: Whitespace

>I think the real driving force behind the welter of markup now in HTML+
>is the desire to make posters, or pictures, or whatever you might call
>them -- non-textual art -- combined with the misapprehension that HTML+
>is the only document formatting language we can use.

I think people want to be able to transmit the same sort of documents
over networks that they read on paper.  Documents rendered by Mosaic
are such a leap forward over the old flat ASCII titles with images
as separate titles that our collective appetite has been whetted;
it's not posters people want to deliver, it's glossy magazine
articles with sound and video attachments.   It's anything that
you can do on a CD ROM.

So don't look at a poster on the wall as what folks would like to
transmit.  Look at the last issue of Byte or Wired, and add videos.
I understand the standard arguments about markup being to denote
semantics not precise presentation, but there's a revolution of
rising expectations here.

/rich



From bobs@sco.com  Wed Jan 12 22:30:17 1994 PST
Message-Id: <9401122230.aa28262@srv425b.sco.com>
Date: Wed, 12 Jan 94 22:30:17 PST
From: bobs@sco.com (Bob Stayton)
Subject: Re: Whitespace

> From: Rich Wiggins <WIGGINS@msu.edu>
> 
> I think people want to be able to transmit the same sort of
> documents over networks that they read on paper.  Documents
> rendered by Mosaic are such a leap forward over the old
> flat ASCII titles with images as separate titles that our
> collective appetite has been whetted; it's not posters
> people want to deliver, it's glossy magazine articles with
> sound and video attachments.   It's anything that you can
> do on a CD ROM.

I think trying to get HTML to deliver fully-formatted
slick magazines is like trying to make your car fly.
You could probably do it, but why bother when there
are perfectly good airplanes out there?  And you
probably wouldn't have a very good car when you were done.

As stated before, HTML documents are intended to be viewed
on a variety of platforms in viewers of varying
capabilities.  Delivering fully formatted documents (let
alone multimedia) in a platform-independent format is a
very tough problem.  But Adobe has taken a crack at it with
Acrobat.  Instead of trying to get HTML to do things
it wasn't designed for, why not deliver a PDF Content-type
and spawn an Acrobat reader?  Let's leverage the work
that has already been done.

bobs
Bob Stayton                                 425 Encinal Street
Technical Publications                      Santa Cruz, CA  95060
The Santa Cruz Operation, Inc.              (408) 425-7222
                                            bobs@sco.com



From marca@eit.COM  Wed Jan 12 22:51:08 1994 PST
Message-Id: <9401130651.AA10778@eit.COM>
Date: Wed, 12 Jan 94 22:51:08 PST
From: marca@eit.COM (Marc Andreessen)
Subject: Re: Whitespace

Bob Stayton writes:
> > I think people want to be able to transmit the same sort of
> > documents over networks that they read on paper.  Documents
> > rendered by Mosaic are such a leap forward over the old
> > flat ASCII titles with images as separate titles that our
> > collective appetite has been whetted; it's not posters
> > people want to deliver, it's glossy magazine articles with
> > sound and video attachments.   It's anything that you can
> > do on a CD ROM.

Amen.

> I think trying to get HTML to deliver fully-formatted
> slick magazines is like trying to make your car fly.
> You could probably do it, but why bother when there
> are perfectly good airplanes out there?  And you
> probably wouldn't have a very good car when you were done.
> 
> As stated before, HTML documents are intended to be viewed
> on a variety of platforms in viewers of varying
> capabilities.  Delivering fully formatted documents (let
> alone multimedia) in a platform-independent format is a
> very tough problem.  But Adobe has taken a crack at it with
> Acrobat.  Instead of trying to get HTML to do things
> it wasn't designed for, why not deliver a PDF Content-type
> and spawn an Acrobat reader?  Let's leverage the work
> that has already been done.

Acrobat isn't free and/or ubiquitous, and Acrobat doesn't provide
network-wide hypermedia.

Marc



From kevinh@eit.COM  Wed Jan 12 23:26:30 1994 -0800
Message-Id: <199401130726.XAA00868@kmac.eit.com>
Date: Wed, 12 Jan 1994 23:26:30 -0800
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re: Whitespace

> From: Bob Stayton <bobs@sco.com>
> 
> I think trying to get HTML to deliver fully-formatted
> slick magazines is like trying to make your car fly.
> You could probably do it, but why bother when there
> are perfectly good airplanes out there?  And you
> probably wouldn't have a very good car when you were done.
> 
> ...Instead of trying to get HTML to do things
> it wasn't designed for, why not deliver a PDF Content-type
> and spawn an Acrobat reader?
> ...

	Hm, can I quote you on that in a year? :)

	-- Kevin



From Paul.Wain@brunel.ac.uk  Thu Jan 13 09:10:23 1994 +0000 (GMT)
Message-Id: <11747.9401130910@thor.brunel.ac.uk>
Date: Thu, 13 Jan 1994 09:10:23 +0000 (GMT)
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Re: Whitespace

Marc wrote:

@ > I think people want to be able to transmit the same sort of
@ > documents over networks that they read on paper.  Documents
@ > rendered by Mosaic are such a leap forward over the old
@ > flat ASCII titles with images as separate titles that our
@ > collective appetite has been whetted; it's not posters
@ > people want to deliver, it's glossy magazine articles with
@ > sound and video attachments.   It's anything that you can
@ > do on a CD ROM.
@ 
@ Amen.

I feel inclined to agree with Marc here. 

Judging by the (high quality) of work that some people here at Brunel,
and notably the Computer Science department's students, HTML (along
with Mosaic and Lynx (You never can thank NCSA and the Lynx developers
too much)) is being used more and more as a multi-media authoring
language.

Being on a number of other mailing lists, the pressure to incorporate
items like HTML mark up into say an X.500 directory is rapidly
increasing because people have seen what can be done with it. Taking the
X.500 side (since thats the one I know), people want to be able to
include URL's etc into their entries so they can point to glossy, full
colour pages of their work and themselves etc.

I think that to now turn around and say, "No HTML should only be used to
design text based documents" is, well to say the least, it will
certainly slam the breaks on the rapid advance of the Web. 

Yes I say the web. Okay, the web has OTHER aspects than just displaying
HTML (I like to use web clients as an FTP interface) but to a MAJORITY o
novice users out there it is really only a multimedia viewing
environment.

I think that is one thing that we have to never lose track of. The
novice user.

Its all very well saying lets use strange embeded commands to hilight
text. For example I think that <em type=bold> was one being pushed
around. (Cant recall). But for a majority of novice users, <B> or <bold>
is all they want. Why should they have to remember to implicitly define
all the text styles that they want.

On this topic, while I remember, why is there need for more than just a
bold, italic, normal and underline set? These can be rendered effectivly
on MOST text terminals and all graphics terminals. Why do we need more?

I agree that font size changes in a document would be nice, other than
headers, but again other than say an <sX> or <size X> (x=a number) tag
any implimentation would probably be too complicated for a lot of users.

And bear in mind that many professional text producers will tell you
never to use more than 3 fonts (including sizes but excluding italic,
bold and underline) in a text document before it becomes just too
complicated on the eye.

Anyway enough of my ramblings, I was just getting worried that we are
trying to make things too complicated for many people who are trying to
use the web. I think that, like say some of the DOS based programers
that I know, (sorry if I have lumped some of you in with that) that
develop windows based software we are in danger of making things TOO
complicated for a majority of users.

Looking at that I seem to have prevented two counter arguments. So let
me quickly try and paraphrase all that again:

1 - What is wrong with wanting pictures, movies and sound in HTML?
2 - Simple tags are needed, perhaps with fuller EQUIVILANTS
3 - Lets not limit the scope of HTML just because some people ONLY have
    text based machines.
4 - Be warned, too many tag types get complicated.

and most importantly of all

5 - Bear in mind the novice users who write HTML.

@ > I think trying to get HTML to deliver fully-formatted
@ > slick magazines is like trying to make your car fly.
@ > You could probably do it, but why bother when there
@ > are perfectly good airplanes out there?  And you
@ > probably wouldn't have a very good car when you were done.
@ > 
@ > As stated before, HTML documents are intended to be viewed
@ > on a variety of platforms in viewers of varying
@ > capabilities.  Delivering fully formatted documents (let
@ > alone multimedia) in a platform-independent format is a
@ > very tough problem.  But Adobe has taken a crack at it with
@ > Acrobat.  Instead of trying to get HTML to do things
@ > it wasn't designed for, why not deliver a PDF Content-type
@ > and spawn an Acrobat reader?  Let's leverage the work
@ > that has already been done.
@ 
@ Acrobat isn't free and/or ubiquitous, and Acrobat doesn't provide
@ network-wide hypermedia.

Going back to this topic (and sorry to hop around, I havent woken up yet :)

I think that slick multi-media fully formated documents is what HTML
_IS_ (ooh, bold and underlined) and that is the path we should be
taking. There are one or two notable missing bits (like being able to do
ruled tables but that is coming (isnt it? I dont have a HTML+ spec to
hand :)

And as Marc says, using commercial based applications is BAD. Imagine
forcing everyone who wants to read HTML to have to buy Framemaker or
Interleaf? No one who didnt have it would buy it just for that.

I do think that the Web relies on a lot of goodwill but it works! Why
hinder it?

Anyways, enough of my babbling. Let me just summarise with my view on
the matter:

HTML is, I think, for slick hypermedia presentations, but it should be
as easy for a novice to create them as an experienced HTML designer.

The usual "Comments?" :) 

Paul.

.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From phillips@cs.ubc.ca  Mon Jan 13 01:55:00 1994 -0800
Message-Id: <7165*phillips@cs.ubc.ca>
Date: 13 Jan 94  1:55 -0800
From: phillips@cs.ubc.ca (George Phillips)
Subject: Slow HTTP Responses

Sometimes a server can take a long time to return a document because
it is doing something slow like an Archie search.  It would be nice
if the server could give the client some kind of status information.
I have two suggestions on how to do that:

1) Add a new response header called "Status:".  Whenever the browser
sees "Status:", it will report the string to the user.  For example:

client says:
	GET /archie?lynx HTTP/1.0

server responds:
	HTTP/1.0 200 Document Follows
	MIME-Version: 1.0
	Status: 0 % of search done
	  ... and after a bit
	Status: 20 % of search done
	 ... and a bit later
	Status: 100 % done!

	<TITLE>etc.etc.etc.

This is simple and simple to implement.  Unfortunately, the server
can report a HTTP level-error if something goes wrong -- it has
already said that things are OK.

2) Make a new Content-Type: for HTTP responses (http/response?).
And a new reponse code 205 that triggers the browser to report
the status message.  So you get server reponses like:

	HTTP/1.0 205 Starting search...
	MIME-Version: 1.0
	Content-Type: http/response

	HTTP/1.0 205 Search 10 % complete
	MIME-Version: 1.0
	Content-Type: http/response
	etc.

A little trickier to implement, would probably enable libwww to
handle .http files, makes the reponses a bit verbose.  Don't
take the "http/reponse" Content-Type: name too seriously - the
real solution will do better.

Any preferences or better ideas?  I think the "http/reponse"
Content-Type: could have many collateral benefits as it frees
HTTP request/reponse from TCP/IP transports (mail servers, anyone?).
Both ideas are at least compatible with current practice
while not being too hard to do.  I thought about a multipart/alternative
solution but quickly got scared away.



From jtilton@jupiter.willamette.edu  Thu Jan 13 02:02:15 1994 -0800 (PST)
Message-Id: <Pine.3.88.9401130135.A28935-0100000@jupiter>
Date: Thu, 13 Jan 1994 02:02:15 -0800 (PST)
From: jtilton@jupiter.willamette.edu (James)
Subject: Re: Whitespace

(p.s. my mailings to this list keep showing up twice... anyone know 
what's going on?  tx)

On Thu, 13 Jan 1994, Paul S. Wain wrote:

> I think that to now turn around and say, "No HTML should only be used to
> design text based documents" is, well to say the least, it will
> certainly slam the breaks on the rapid advance of the Web. 

I don't think that the Web should be used only to present text-based 
documents -- certainly not!  The Web is a joy to view over Mosaic, and 
perhaps the first thing that really makes the Internet into an exciting 
and accessible resource for people.  As a matter of fact, I don't think 
anyone is really arguing for that.

On the other hand, I do think we need to be cautious about trying to turn 
HTML into a page layout description.  One of the nice features, IMHO, of 
HTML is that the display of the document can be matched easily with the 
capabilities of the display device.  Going whole hog into things like 
specifying font sizes seems like a mistake, from that vantage point.  Is 
the next step font families?  We can provide multimedia authoring -- and, 
in fact, have -- without giving the author ultimate control over the 
document.

In fact, not providing ultimate control is well in keeping with the 
argument about making HTML accessible for novices.  I'd be willing to 
wager that a lot of documents that people have put together look a heck 
of a lot nicer when Mosaic renders them, then if people had been 
responsible for their own type-setting, etc.  Remember the early days of 
the Macintosh?

> On this topic, while I remember, why is there need for more than just a
> bold, italic, normal and underline set? These can be rendered effectivly
> on MOST text terminals and all graphics terminals. Why do we need more?

I can see points to this argument -- I think I'm a little less opposed to 
the use of these tags now then I was a few days ago.  Another strong 
argument for this is Dave Raggett's point about logical types being a 
bottomless pit.  On the other hand, I find the concept of logical types 
to be really fascinating...

> 3 - Lets not limit the scope of HTML just because some people ONLY have
>     text based machines.

I don't think we should limit the scope, but I also don't think we should 
forget them, either.  A well composed document -- even if it takes 
advantage of graphics and sounds -- would still be useable on a vt100 
screen, I should hope.  So, at the level of the text itself, it makes 
sense to not have tags that don't translate well to a vt100 screen.  
Graphics and sounds are understandably a whole 'nuther ball game that the 
Lynx users (for instance) or going to miss out on.

Plus, before we get to chauvinistic about our sophisticated graphical 
browsers, let's bear in mind that most of those novice users probably 
have no access to them.  At Willamette, for instance, we've got a grand 
total of one x-term, and a whole heck of a lot of Macs and PC's.  Most of 
our Internet users are using it through a Unix environment (since the 
main use of the Internet around here is e-mail, which our Sun handles), 
and their primary use of the Web is probably going to be through Lynx.  
Heck, nobody ever uses our Macintosh gopher clients, either!

> HTML is, I think, for slick hypermedia presentations, but it should be
> as easy for a novice to create them as an experienced HTML designer.

This is probably an issue that will best be solved with the eventual 
development of WYSIWYG (aren't acronyms wonderful?) authoring tools.

Anyway, I'll shut up now, and sit back down again, now that I've struck 
my blow for the poor defenseless Lynx user :).

Cheers,

							-et

/ (James) Eric Tilton, Student AND Student Liaison, WITS               \
\ Class of '95 - CS/Hist  -- Internet - jtilton@willamette.edu         /
<a href="http://www.willamette.edu/~jtilton/">ObHyPlan!</a>, chock fulla
<a href="http://www.willamette.edu/~jtilton/whatsnew.html">Fun Stuff!</a>




From Paul.Wain@brunel.ac.uk  Thu Jan 13 09:24:52 1994 +0000 (GMT)
Message-Id: <11833.9401130924@thor.brunel.ac.uk>
Date: Thu, 13 Jan 1994 09:24:52 +0000 (GMT)
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Re: Whitespace

Marc wrote:

@ > I think people want to be able to transmit the same sort of
@ > documents over networks that they read on paper.  Documents
@ > rendered by Mosaic are such a leap forward over the old
@ > flat ASCII titles with images as separate titles that our
@ > collective appetite has been whetted; it's not posters
@ > people want to deliver, it's glossy magazine articles with
@ > sound and video attachments.   It's anything that you can
@ > do on a CD ROM.
@ 
@ Amen.

I feel inclined to agree with Marc here. 

Judging by the (high quality) of work that some people here at Brunel,
and notably the Computer Science department's students, HTML (along
with Mosaic and Lynx (You never can thank NCSA and the Lynx developers
too much)) is being used more and more as a multi-media authoring
language.

Being on a number of other mailing lists, the pressure to incorporate
items like HTML mark up into say an X.500 directory is rapidly
increasing because people have seen what can be done with it. Taking the
X.500 side (since thats the one I know), people want to be able to
include URL's etc into their entries so they can point to glossy, full
colour pages of their work and themselves etc.

I think that to now turn around and say, "No HTML should only be used to
design text based documents" is, well to say the least, it will
certainly slam the breaks on the rapid advance of the Web. 

Yes I say the web. Okay, the web has OTHER aspects than just displaying
HTML (I like to use web clients as an FTP interface) but to a MAJORITY o
novice users out there it is really only a multimedia viewing
environment.

I think that is one thing that we have to never lose track of. The
novice user.

Its all very well saying lets use strange embeded commands to hilight
text. For example I think that <em type=bold> was one being pushed
around. (Cant recall). But for a majority of novice users, <B> or <bold>
is all they want. Why should they have to remember to implicitly define
all the text styles that they want.

On this topic, while I remember, why is there need for more than just a
bold, italic, normal and underline set? These can be rendered effectivly
on MOST text terminals and all graphics terminals. Why do we need more?

I agree that font size changes in a document would be nice, other than
headers, but again other than say an <sX> or <size X> (x=a number) tag
any implimentation would probably be too complicated for a lot of users.

And bear in mind that many professional text producers will tell you
never to use more than 3 fonts (including sizes but excluding italic,
bold and underline) in a text document before it becomes just too
complicated on the eye.

Anyway enough of my ramblings, I was just getting worried that we are
trying to make things too complicated for many people who are trying to
use the web. I think that, like say some of the DOS based programers
that I know, (sorry if I have lumped some of you in with that) that
develop windows based software we are in danger of making things TOO
complicated for a majority of users.

Looking at that I seem to have prevented two counter arguments. So let
me quickly try and paraphrase all that again:

1 - What is wrong with wanting pictures, movies and sound in HTML?
2 - Simple tags are needed, perhaps with fuller EQUIVILANTS
3 - Lets not limit the scope of HTML just because some people ONLY have
    text based machines.
4 - Be warned, too many tag types get complicated.

and most importantly of all

5 - Bear in mind the novice users who write HTML.

@ > I think trying to get HTML to deliver fully-formatted
@ > slick magazines is like trying to make your car fly.
@ > You could probably do it, but why bother when there
@ > are perfectly good airplanes out there?  And you
@ > probably wouldn't have a very good car when you were done.
@ > 
@ > As stated before, HTML documents are intended to be viewed
@ > on a variety of platforms in viewers of varying
@ > capabilities.  Delivering fully formatted documents (let
@ > alone multimedia) in a platform-independent format is a
@ > very tough problem.  But Adobe has taken a crack at it with
@ > Acrobat.  Instead of trying to get HTML to do things
@ > it wasn't designed for, why not deliver a PDF Content-type
@ > and spawn an Acrobat reader?  Let's leverage the work
@ > that has already been done.
@ 
@ Acrobat isn't free and/or ubiquitous, and Acrobat doesn't provide
@ network-wide hypermedia.

Going back to this topic (and sorry to hop around, I havent woken up yet :)

I think that slick multi-media fully formated documents is what HTML
_IS_ (ooh, bold and underlined) and that is the path we should be
taking. There are one or two notable missing bits (like being able to do
ruled tables but that is coming (isnt it? I dont have a HTML+ spec to
hand :)

And as Marc says, using commercial based applications is BAD. Imagine
forcing everyone who wants to read HTML to have to buy Framemaker or
Interleaf? No one who didnt have it would buy it just for that.

I do think that the Web relies on a lot of goodwill but it works! Why
hinder it?

Anyways, enough of my babbling. Let me just summarise with my view on
the matter:

HTML is, I think, for slick hypermedia presentations, but it should be
as easy for a novice to create them as an experienced HTML designer.

The usual "Comments?" :) 

Paul.

..-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From Paul.Wain@brunel.ac.uk  Thu Jan 13 10:13:52 1994 +0000 (GMT)
Message-Id: <12447.9401131013@thor.brunel.ac.uk>
Date: Thu, 13 Jan 1994 10:13:52 +0000 (GMT)
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Re: Whitespace

James wrote:
@ (p.s. my mailings to this list keep showing up twice... anyone know 
@ what's going on?  tx)

Opps, my mailing twice was a mistake. I got an error message or 3 so
resent. I appologise to those whose mailboxes I clogged up :)

@ On Thu, 13 Jan 1994, Paul S. Wain wrote:
@
@ > I think that to now turn around and say, "No HTML should only be used to
@ > design text based documents" is, well to say the least, it will
@ > certainly slam the breaks on the rapid advance of the Web. 
@ 
@ I don't think that the Web should be used only to present text-based 
@ documents -- certainly not!  The Web is a joy to view over Mosaic, and 
@ perhaps the first thing that really makes the Internet into an exciting 
@ and accessible resource for people.  As a matter of fact, I don't think 
@ anyone is really arguing for that.
@ 
@ On the other hand, I do think we need to be cautious about trying to turn 
@ HTML into a page layout description.  One of the nice features, IMHO, of 
@ HTML is that the display of the document can be matched easily with the 
@ capabilities of the display device.  Going whole hog into things like 
@ specifying font sizes seems like a mistake, from that vantage point.  Is 
@ the next step font families?  We can provide multimedia authoring -- and, 
@ in fact, have -- without giving the author ultimate control over the 
@ document.

Exactly :) The summary of font usage above is really what I was trying
to say :) But lets try and keep what we have already simple!

@ > 3 - Lets not limit the scope of HTML just because some people ONLY have
@ >     text based machines.
@ 
@ I don't think we should limit the scope, but I also don't think we should 
@ forget them, either.  A well composed document -- even if it takes 
@ advantage of graphics and sounds -- would still be useable on a vt100 
@ screen, I should hope.  So, at the level of the text itself, it makes 
@ sense to not have tags that don't translate well to a vt100 screen.  
@ Graphics and sounds are understandably a whole 'nuther ball game that the 
@ 
@ > HTML is, I think, for slick hypermedia presentations, but it should be
@ > as easy for a novice to create them as an experienced HTML designer.
@ 
@ Anyway, I'll shut up now, and sit back down again, now that I've struck 
@ my blow for the poor defenseless Lynx user :).

Re-reading that final point of mine, I dont think that I stressed my
point on text based usage clearly enough :) I to use Lynx a lot, mainly
from a PC-based telnet session or via modem. Being able to see the
document in these circumstances is a great help.

I think that I should maybe add a bit more to the above summary and sau
that:

Whilst it should be kept in mind that some people only have text based
viewers, please dont let those hinder the graphics based ones from using
the potential of HTML to the full!

P.

.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From dsr@hplb.hpl.hp.com  Thu Jan 13 12:21:05 1994 GMT
Message-Id: <9401131221.AA25411@manuel.hpl.hp.com>
Date: Thu, 13 Jan 94 12:21:05 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: RE: Whitespace

> What about my suggestion at the WWW/TEI meeting to have 4-6 different
> kinds of emphasis.  For instance <em1>,<em2>... or <em a>, <em b>....
> The different kinds wouldn't neccessarily be any kind of precedence
> order but would simply specify DIFFERENT kinds of emphasis for each
> of the tags.  That way you could specify different words to be
> emphasised and be sure that each emphasis style will be unique.

There seems to be several choices:

    a)  require browsers to differentiate each of the physical
        styles (B, I, U, TT and S)

    b)  introduce <em1>, <em2>, ...<em4>

    c)  Use Bryan Oakley's FACE idea and require browsers
        to differentiate a small set of style names

Right now, the simplest seems to use (a) and encourage browsers
to comply with differentiating the small set of existing physical styles.

Dave Raggett



From henrich@crh.cl.msu.edu  Thu Jan 13 09:44:22 1994 -0500 (EST)
Message-Id: <9401131444.AA19494@crh.cl.msu.edu>
Date: Thu, 13 Jan 1994 09:44:22 -0500 (EST)
From: henrich@crh.cl.msu.edu (Charles Henrich)
Subject: Re: Whitespace

> There seems to be several choices:
>
>     a)  require browsers to differentiate each of the physical
>         styles (B, I, U, TT and S)
>
>     b)  introduce <em1>, <em2>, ...<em4>
>
>     c)  Use Bryan Oakley's FACE idea and require browsers
>         to differentiate a small set of style names
>
> Right now, the simplest seems to use (a) and encourage browsers
> to comply with differentiating the small set of existing physical styles.

I agree completely.

-Crh

    Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu

                     http://rs560.msu.edu/~henrich/



From henrich@crh.cl.msu.edu  Thu Jan 13 09:56:58 1994 -0500 (EST)
Message-Id: <9401131456.AA19546@crh.cl.msu.edu>
Date: Thu, 13 Jan 1994 09:56:58 -0500 (EST)
From: henrich@crh.cl.msu.edu (Charles Henrich)
Subject: Whitespace and <size> tags

> So don't look at a poster on the wall as what folks would like to
> transmit.  Look at the last issue of Byte or Wired, and add videos.
> I understand the standard arguments about markup being to denote
> semantics not precise presentation, but there's a revolution of
> rising expectations here.

Its funny how an issue about implied whitespace can spiral out of control :)
The whole suggestion to add a <text size> tag was just to allow getting the
same effect as <h?> tags witout the whitespace.  For those who are screaming
"but that doesnt belong!  Next we'll have font families" your wrong.  First let
me say that we *already* have multiple font sizes, in use on virtually every
page out there.  It just happens that the use of the font sizes is very
hindered by the implied whitespace.  And the reason font families would not
succeed is because most folks do not have 250 different font sets, nor the disk
space to house them.  When most folks go through all the effort of putting up a
web, they want people to be able to read what they've done!  By using obscure
fonts their information wouldnt be available to the rest of the world.  Course
we could always ship browsers with a standard set of fonts... (running for
cover! :)   [Im kidding!  Multifonts are not good IMHO!]

-Crh

    Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu

                     http://rs560.msu.edu/~henrich/



From kims@ncsa.uiuc.edu  Thu Jan 13 09:13:51 1994 -0600
Message-Id: <9401131513.AA08537@void.ncsa.uiuc.edu>
Date: Thu, 13 Jan 1994 09:13:51 -0600
From: kims@ncsa.uiuc.edu (Kim Stephenson)
Subject: Re: status of clients?

The next revision 1.0.3 (a maintaince upgrade) of NCSA Mosaic for the Mac
is coming along nicely and will be released fairly soon.

Kim Stephenson

>Hi...
>
>Someone posted a survey of www clients a while back in a nice chart format
>that listed their capabilities.  Could someone point me to the latest rev
>of such a beast?  Thanks.
>
>Also, how's the NCSA Mac/PC groups doing on the next rev?
>
>-bob kaehms





From kchang@ncsa.uiuc.edu  Thu Jan 13 09:47:31 1994 CST
Message-Id: <9401131547.AA07941@landrew.ncsa.uiuc.edu>
Date: Thu, 13 Jan 94 09:47:31 CST
From: kchang@ncsa.uiuc.edu (Kenneth Chang)
Subject: Re: Whitespace

The fundamental paradox of this issue is
* Document authors want to specify the appearance of their documents.
* Users want to configure their browsers.

This results in strange contradictions, such as in MacMosaic, where for
each heading tag, you can specify the font and font size (control by user),
but _not_ the font style (because the developers thought this should be
controlled by the document author through <B> and <I>).

At some point, WWW developers need to come to some sort of consensus about
which of these two is more important.

If you want glossy, good graphic design, you _can't_ allow the user to
arbitrarily change page size, font, font size, font style, etc. This would
require a full-blown PDF, MultiMaster fonts (or the equivalent) and a
pretty good WYSIWIG authoring environment.

If you want a system where a document can be rendered reasonably across not
only a range of different platforms, but different _configurations_ of a
given browser, then document authors have to accept limited layout tools
and trust the browser to do the presentation.

What people are doing now is designing documents that look good in one
browser and fairly crappy elsewhere, and that's not a good solution...

--ken chang
  NCSA Publications

Paul Wain writes:
>Marc wrote:
>
>@ > I think people want to be able to transmit the same sort of
>@ > documents over networks that they read on paper.  Documents
>@ > rendered by Mosaic are such a leap forward over the old
>@ > flat ASCII titles with images as separate titles that our
>@ > collective appetite has been whetted; it's not posters
>@ > people want to deliver, it's glossy magazine articles with
>@ > sound and video attachments.   It's anything that you can
>@ > do on a CD ROM.
>@
>@ Amen.
>
>I feel inclined to agree with Marc here.
>
>Judging by the (high quality) of work that some people here at Brunel,
>and notably the Computer Science department's students, HTML (along
>with Mosaic and Lynx (You never can thank NCSA and the Lynx developers
>too much)) is being used more and more as a multi-media authoring
>language.





From terry@ora.com  Thu Jan 13 07:48:04 1994 PST
Message-Id: <199401131548.AA03689@rock.west.ora.com>
Date: Thu, 13 Jan 1994 07:48:04 PST
From: terry@ora.com (Terry Allen)
Subject: Paths, HTML+, SGML

 
| Date: Mon, 10 Jan 94 12:22:32 PST
| From: redback!jimmc@eskimo.com (Jim McBeath)
| To: www-talk@www0.cern.ch
| Subject: PATHs in HTML

I think this is a fine idea; I have a few questions about how to 
handle the large virtual document that results from following
a path.

| [various omitted passim]
| The requirements for paths:
| 1. A path must be able to include nodes for which the user creating the path
|    has only read access; therefore, it must be possible to define a path
|    which includes nodes in which no path information resides, including
|    non-HTML nodes.

Note that means SGML and non-SGML formats may be combined in the same
path.

| This proposed implementation requires the use of REL=Subdocument (already
| proposed in the current version of the HTML+ spec at
|  ftp://15.254.100.100/pub/draft-raggett-www-html-00.ps )
| plus one additional enhancemnt:
| 	Add "Path" to the set of legal REL values.

This implies that one can parse the virtual doc by using the SGML
SUBDOC feature.  To do this in general, in your SGML declaration set 
SUBDOC to YES n, where n is the "maximum number of subdocuments that
will be open at any point in the document" (purists please forgive
me for citing Martin Bryan, *SGML, an Author's Guide,* p. 199,
instead of the SGML Handbook).  Then set up the nodes to be 
included as entities with the SUBDOC keyword:
 
<!DOCTYPE html SYSTEM "html.dtd" [
<!entity firstnode system "node1.html" SUBDOC>
<!entity secondnode system "node2.html" SUBDOC>
]>
<html><head><TITLE>Title of Initial Document</title></head><body>
<P>Our sample document includes the file node1.html by means
of the following entity reference.
&firstnode;
<P>And after the next included file we can quit.
&secondnode;
</body></html>

Presumably, browsers can do more or less this same thing
in their own ways.  Note that subdocuments can have their
own DOCTYPE lines (document type declarations).  And you
could retrieve all the nodes, making local copies, and 
parse the collection together.

However, in the example given, subdocuments are included within 
a list:

| An example of an HTML node which defines a single path:
|         <H1>my path</H1>
|         This is a path of things I've found useful.
|         <DL>
|           <DT> <A HREF="node1" REL="Subdocument">label1</A>
|           <DD> a summary
|  
|           <DT> <A HREF="path2" REL="Path">label2</A>
|           <DD> a summary
|  
|           <DT> <A HREF="node3" REL="Subdocument">label3</A>
|           <DD> a summary
|         </DL>
| This path includes node1, followed by all of the nodes defined in path2,
| followed by node3.

Now the nodes may include anything, from GIFs to (shudder) RTF
to full HTML instances (complete with HTML, HEAD, and BODY tags),
to HTML fragments, such as:

<!DOCTYPE p system "html.dtd">
<P>Just a para to stick in somewhere.

I can see inserting nodes such as this in a DL, but how are 
full documents to be displayed?  One is indicating some 
semantic relationship among the parts here, but what is it?
Is the TITLE of a subdoc'd node to be suppressed, for example?

I'm also unclear about how a REL=path is to be displayed, as 
opposed to a subdocument.  Are paths to be indicated as
anchors?  And how does that square with the following?

| Given this capability, I could define a path to print out my entire
| Users Manual, and (just as important) to allow me to search through
| the contents of the entire manual with a single command.
| I could then define other, specialized paths that went through different
| pieces of the manual in another order, for people who wanted to learn
| about a particular subject.

I want to be able to do this; I just want to understand the proposed
mechanism better.

Regards,

-- 
Terry Allen  (terry@ora.com)
Editor, Digital Media Group
O'Reilly & Associates, Inc.
Sebastopol, Calif., 95472



From dsr@hplb.hpl.hp.com  Thu Jan 13 15:58:11 1994 GMT
Message-Id: <9401131558.AA25708@manuel.hpl.hp.com>
Date: Thu, 13 Jan 94 15:58:11 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Whitespace

> I don't think that the Web should be used only to present text-based 
> documents -- certainly not!  The Web is a joy to view over Mosaic, and 
> perhaps the first thing that really makes the Internet into an exciting 
> and accessible resource for people.  As a matter of fact, I don't think 
> anyone is really arguing for that.

Too right! I am keenly looking foward to bolting in support for
realtime video and  audio streams using the new isochronous protocols
now becoming available. Just think of browsing thru web documents to
find the tv channel of your choice, or perhaps requesting a rerun of
a favorite episode of Red Dwarf. Instead of snapshots of a coffee pot,
we could be tapping into video coverage direct from NASA ...

> I don't think we should limit the scope, but I also don't think we should 
> forget them, either.

Thats the justification for the FIG element in HTML+. It allows
for shaped buttons etc. but also provides an effective way of viewing
them on vt100 terminals.

Dave Raggett




From kims@ncsa.uiuc.edu  Thu Jan 13 09:13:51 1994 -0600
Message-Id: <9401131513.AA08537@void.ncsa.uiuc.edu>
Date: Thu, 13 Jan 1994 09:13:51 -0600
From: kims@ncsa.uiuc.edu (Kim Stephenson)
Subject: Re: status of clients?

The next revision 1.0.3 (a maintaince upgrade) of NCSA Mosaic for the Mac
is coming along nicely and will be released fairly soon.

Kim Stephenson

>Hi...
>
>Someone posted a survey of www clients a while back in a nice chart format
>that listed their capabilities.  Could someone point me to the latest rev
>of such a beast?  Thanks.
>
>Also, how's the NCSA Mac/PC groups doing on the next rev?
>
>-bob kaehms





From shelden@fatty.law.cornell.edu  Thu Jan 13 11:19:08 1994 -0500
Message-Id: <9401131619.AA01837@fatty.law.cornell.edu>
Date: Thu, 13 Jan 1994 11:19:08 -0500
From: shelden@fatty.law.cornell.edu (Brian T. Shelden)
Subject: Re: Cello Beta v.8 Released (longish) 

 
>Cello Beta Version .8 is hereby released.  You can obtain it 
>from ftp.law.cornell.edu, directory /pub/LII/Cello.

Whoops.  Actually it's not.  I just crashed fatty,
and upon reboot, the listserv thought that it had 
yet to send that message to our cello-l and
teknoids lists.  Since Tom originally sent copies to
unite and www-talk, the listserv CC'd them as
well.

Sorry for the inconvenience.

Sheepishly,
Brian Shelden
bts1@cornell.edu
Unix Systems Coordinator
Cornell Law School




From tom@fatty.law.cornell.edu  Fri Nov  5 13:22:40 1993 -0500 (EST)
Message-Id: <9311051822.AA13860@fatty.law.cornell.edu>
Date: Fri, 5 Nov 1993 13:22:40 -0500 (EST)
From: tom@fatty.law.cornell.edu (Thomas R. Bruce)
Subject: Cello Beta v.8 Released (longish)

Folks:

Cello Beta Version .8 is hereby released.  You can obtain it 
from ftp.law.cornell.edu, directory /pub/LII/Cello.

A note on the version numbers; Distinct version discontinued
------------------------------------------------------------
Beta version r8 is the successor to Winsock version r6, and to 
the Distinct version beta 7.  There will be no continuation of 
the Distinct version;  Cello is now exclusively Winsock-based.
The files and messages on ftp.law.cornell.edu have been 
appropriately adjusted.

Besides, we thought it was time to make it a beta.

PLEASE NOTE that documentation in the online help system still 
incorporates the installation instructions for the Distinct 
version, a  problem which will be attacked next (this is 
getting us a lot of mail).

What it is:
-----------
Cello is a multipurpose Internet browser which permits you to 
access information from many sources in many formats.  
Technically, it's a WorldWideWeb client application.  This 
means that you can use Cello to access data from WorldWideWeb, 
Gopher, FTP, and CSO/ph/qi servers, as well as X.500 directory 
servers, WAIS servers, HYTELNET, TechInfo, and others through external 
gateways.  You can also use Cello and the WWW-HTML hypertext 
markup standard to build local hypertext systems on LANS, on 
single machines, and so on.  Cello also permits the 
postprocessing of any file for which you've set up an 
association in the Windows File Manager -- for example, if you 
download an uncompressed Microsoft Word file from an FTP site, 
and the appropriate association exists in File Manager, Cello 
will run MS-Word on it for you.  This same capability is used 
to view graphics and listen to sound files you get from the Net.

Cello runs atop a WINSOCK network layer.  We recommend the use 
of Peter Tattam's Trumpet Winsock, available from 
ftp.utas.edu.au, if you don't have a Winsock already.

Repaired in this release:
------------------------

-- FTP timeouts adjusted to reflect reality for those on slow 
links.

-- Lagging-dot in DNS names is now removed by Cello, and won't 
interfere with those Winsocks which don't understand it.

-- Several very minor bugs involving display.

Not yet repaired/enhanced:
--------------------------

--Numerous user-requested enhancements to FTP handling, including 
non-anonymous login and the ability to get short ("ls" not 
"ls -l") listings for servers whose long listings don't make 
sense.

--Printing bugs.  In particular, inlined images don't print 
yet.  Some minor bugs with font sizes and whatnot were repaired 
in the current release, but the whole thing needs a good 
going-over.


Added/enhanced in this version:
------------------------------

-- Inline image support.

 Cello supports inlining of .GIF, .XBM (X bitmap), .BMP, and 
.PCX images.  The latter should be particularly useful to 
information providers who wish to serve local files only or who 
have a PC-based graphics library.  Autofetching of graphics can 
be turned off by menu choice, and dithering may be substituted 
for quantizing in the case of 24-bit images, which ought to 
speed up rendition quite a bit.

There are still a few glitches having to do with color.  Cello 
resolves palette differences between images by loading a 
scaled, representative 256-color palette and essentially 
insisting that everyone adhere to it.  Most of the time this 
provides fairly accurate color rendition, but experimentation 
shows that some shades don't do well.  The subtle oranges used 
in some of the O'Reilly GNN icons seem to suffer badly, for 
instance.  The future alternative to this scheme will be to 
mutually-resolve all images onscreen at any particular moment, 
but that won't be in place for a couple of weeks yet, and is 
likely to slow performance in any case.  Art-book publishers 
will probably want to wait.

-- DDE server support.

You can invoke Cello from other applications which support the 
DDE execute command.  Here's how you'd do it with an MS-Word 
macro:

Sub MAIN
ChanNum = DDEInitiate("Cello", "URL")
DDEExecute(ChanNum, "http://www.law.cornell.edu")
DDETerminate(ChanNum)
End Sub

As you can see, the DDE service name is "Cello", the topic is 
"URL", and the data sent in the execute command is a URL.

Needless (perhaps) to say, OLE support and DDE client support 
are planned in the near future.

-- TN3270 via external application.

Cello now supports TN3270 via an external application, a 
feature which was prompted by the appearance of a 
freely-distributed TN3270 for Windows (see 
comp.protocols.tcp-ip.ibmpc for announcements and location).  
Cello expects the same #h and #p parameters used in the 
"Use your own Telnet client" menu choice.


-- Locatable style and bookmark files

 By popular request of long-suffering network administrators, 
there are now options in the CELLO.INI file to set the location 
for style and bookmark files.  These are also menu-accessible.
Examples:

BookmarkFile=c:\somedir\cello.bmk
StyleFile=c:\another\dir\cello.sty


-- Bookmark-dump-to-file feature.

There is now a button in the Bookmark dialog which will dump 
your entire bookmark list to an HTML file for subsequent 
editing into whatever you like, possibly an alternate home page.


Coming attractions:
------------------

The sheer volume of user mail has convinced me that it's 
probably time to have an online help system which reflects some 
small corner of reality (grin).  Documentation will be fixed 
before anything else.

OLE, FTP enhancements, and DDE client features will follow.

Have fun with it.

Tb.



-- 
+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
|  Thomas R. Bruce                           trb2@cornell.edu |
|  Research Associate                                         |
|  Cornell Law School                     Voice: 607-255-1221 |
|  Myron Taylor Hall                        FAX: 607-255-7193 |
|  Ithaca, NY 14853                                           |
+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+



From dsr@hplb.hpl.hp.com  Thu Jan 13 16:45:32 1994 GMT
Message-Id: <9401131645.AA25811@manuel.hpl.hp.com>
Date: Thu, 13 Jan 94 16:45:32 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Paths, HTML+, SGML

> This implies that one can parse the virtual doc by using the SGML
> SUBDOC feature.  To do this in general, in your SGML declaration set 
> SUBDOC to YES n, where n is the "maximum number of subdocuments that
> will be open at any point in the document" (purists please forgive
> me for citing Martin Bryan, *SGML, an Author's Guide,* p. 199,
> instead of the SGML Handbook).  Then set up the nodes to be 
> included as entities with the SUBDOC keyword:

Thanks for the explanation of how to do this. I'm don't expect that
web browsers would bother with this though. Instead the path document
is kept in a cache and searched when the user clicks Forward (or Next) or
whatever (you might want the browser to read ahead, see below).

> Now the nodes may include anything, from GIFs to (shudder) RTF
> to full HTML instances (complete with HTML, HEAD, and BODY tags),
> to HTML fragments, such as:

Thats the idea. There is quite a bit of interest in using the web
to view scanned images of paper documents. The path (or tour)
mechanism would support this nicely. There is no need to show the
HTML node containing the path unless the user starts with it, since
you can use the LINK element passed via an HTTP header to point
to the path (REL=Path).

> Is the TITLE of a subdoc'd node to be suppressed, for example?

You might want to design the browser to do read ahead along the path
and load the titles into a menu. A shortcut is to specify the title
in the link with the TITLE attribute.

> I'm also unclear about how a REL=path is to be displayed,
> as opposed to a subdocument.  Are paths to be indicated as
> anchors?  And how does that square with the following?

If you are viewing the document containing the anchor with REL=path
then the anchor works exactly as normal, so that clicking it takes
you to the document that specifies the subpath. The beauty of this
scheme is that paths are also ordinary HTML and can be viewed in
the same way as normal documents. Anchors with REL=Subdocument
or REL=Path work in the normal way, except when being interpreted
as a path specification for another document.

Dave Raggett



From dolesa@smtp-gw.spawar.navy.mil  Thu Jan 13 12:10:23 1994 EDT
Message-Id: <9400137584.AA758491823@smtp-gw.spawar.navy.mil>
Date: Thu, 13 Jan 94 12:10:23 EDT
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: Solution Found: A/UX & Inline Images (Gifs)


     Thanks to all who have helped.  I figure it is worth the post so that 
     someone else doesn't run into the same problem, and if they do, will 
     have a solution. 
     
     Mac A/UX keeps the header info all in one file format.  You need to 
     split the data fork.
     
     fcnvt -d inimage.gif outimage.gif
     
     This will take inimage.gif and produce 2 output files: outimage.gif 
     and %outimage.gif.  Images will then display inline.  It's an A/UX 
     thing.
     
     Thanks to Jay Budzik for finding the answer and all his support of 
     A/UX and www.  Salute!
     
                                Andre'



From peveritt@pandora.ncts.navy.mil  Thu Jan 13 11:15:45 1994 +0600
Message-Id: <9401131715.AA02572@voltaire.ncts.navy.mil>
Date: Thu, 13 Jan 1994 11:15:45 +0600
From: peveritt@pandora.ncts.navy.mil (Paul Everitt)
Subject: On-the-fly indexing difference between lynx and mosaic


Hi folks.  I am trying to allow browsing of directory structures
on my ftp server by going in through ftp, to wit:

ftp://ftp.ncts.navy.mil/pub/navpalib/navnews/somefile.txt

changes to (www and ftp are the same machine):

http://ftp.ncts.navy.mil/navpalib/navnews/somefile.txt

I used the ncsa httpd to alias /navpalib to /home/ftp/pub/navpalib.

In any event, this URL retrieves the textfile under Mosaic, but lynx
prompts for a save file.  Any ideas?  Is it the .txt extensions?

--Paul



From germuska@casbah.acns.nwu.edu  Thu Jan 13 11:36:28 1994 -0600 (CST)
Message-Id: <9401131736.AA28033@casbah.acns.nwu.edu>
Date: Thu, 13 Jan 1994 11:36:28 -0600 (CST)
From: germuska@casbah.acns.nwu.edu (Joe Germuska)
Subject: Re: Whitespace (fwd)

> The fundamental paradox of this issue is
> * Document authors want to specify the appearance of their documents.
> * Users want to configure their browsers.

I understand the sense, because I often want to control the appearance of
the documents I serve, but _this is not a paradox_ because authors specify
the content of their documents.  

Clients should have _total_ control over how HTML is rendered.  Even if
rendering options were limited (like they are in MacMosaic) there's no way
an author can test his/her HTML on every possible platform.  Therefore, the
limitations do nothing but frustrate (me, at least) users who don't like
the hard coded choices.

I think a good graphical browser would define two or three "root" styles --
they would be at least:
"header"
"normal"

All variants would, by default, include the characteristics of their root
style -- that way, I could say "I want all my headers in Helvetica" and not
have to redefine each level; "I want all my body text in palatino" and the
same...  You should at least be able to define "character" level styles
(font, size, style...) and maybe even "paragraph" level. (line spacing,
blank line between paragraphs)

My model is the way a good word processor handles "styles" -- MS Word, for
example.  If I wanted H1 to be in Zapf Chancery, and H2-5 in Helvetica, I
should be able to choose that without having to set every heading.  

Is this just a low priority, or are client authors opposed to giving so
much control to users?

	Joe 
-- 
joe germuska * j-germuska@nwu.edu * www * res hall net * instruct tech
      academic computing & network services * northwestern univ
"Free the people with music..." - Bob Marley



From swb1@cornell.edu  Thu Jan 13 12:41:08 1994 -0500
Message-Id: <199401131737.AA28798@postoffice2.mail.cornell.edu>
Date: Thu, 13 Jan 1994 12:41:08 -0500
From: swb1@cornell.edu (Scott W Brim)
Subject: Re: Whitespace

At  9:47 1/13/94 -0600, Kenneth Chang wrote:
  >The fundamental paradox of this issue is
  >* Document authors want to specify the appearance of their documents.
  >* Users want to configure their browsers.

Just to add a third category: I'm a document author who is just very
happy to find one format that everyone can use and has flexibility to
do unexpected things in the future.  I come from dealing with
situations like RFCs, where we currently either use 7-bit text or
incompatible postscript.  The lowest common denominator for HTML's
final appearance looks pretty good to me (well, RFCs do need FIGs).
Give me Headings and a small set of emphases and I'll be thoroughly
satisfied.

  >This results in strange contradictions, such as in MacMosaic, where for
  >each heading tag, you can specify the font and font size (control by user),
  >but _not_ the font style (because the developers thought this should be
  >controlled by the document author through <B> and <I>).

I think this is the right approach.

  >If you want a system where a document can be rendered reasonably across not
  >only a range of different platforms, but different _configurations_ of a
  >given browser, then document authors have to accept limited layout tools
  >and trust the browser to do the presentation.

Exactly, and I am impressed with the high quality of the major
browsers.  They have earned your trust.

  >At some point, WWW developers need to come to some sort of consensus about
  >which of these two is more important.

Page designers are frustrated that they can't control the psychological
effect of their work on the reader.  The Web and browsers shouldn't
have to change -- the page designers should change, should see the new
situation as an opportunity for doing things differently.  We might
actually get back to a situation where content is more important than
presentation.  Let's stand our ground.

... Scott





From lentz@rossi.astro.nwu.edu  Thu Jan 13 12:13:22 1994 -0600 (CST)
Message-Id: <9401131813.AA07810@rossi.astro.nwu.edu>
Date: Thu, 13 Jan 1994 12:13:22 -0600 (CST)
From: lentz@rossi.astro.nwu.edu (Robert Lentz)
Subject: Re: Whitespace

Hello,
	All this talk about HTML not being a layout language always
frustrates me. We must realize that it does have some layout capabilities,
and should (especially in aligning text wrt graphics). In fact, great
frustration has been derived from how it forces one to accept certain layout
characteristics: the way headers have and implicit <p> which cannot be
overridden, thus if I want to subtitle something, I either have to do it
within the same header tag, or have it be substantially separated from the
main title. With <p> and <br> it would be nice if we were allowed the
freedom to decide all breaks and paragraph spacing ourselves.

	Another example is the <address> tag. I once wanted to include an
email address in a document, and tried tagging it according to type, but
much to my consternation found that the <address> tag then forced a <br> in
my document.

	If we really want to emphasize content type we probably really need
an <net-address> or <address type=net,mail,etc> tag/attribute, especially
for sending authors email from within the clients.

	My biggest beef has been not being able to use headers within lists.
I was thus encouraged to just work around it by using <b>, and thus my
documents are not as well "content labelled" as they would be if HTML were
more flexible. Or will "content parsers" treat levels of lists as different
levels of headers? (Can we hint that browsers should at least do this in
their presentations?)

	HTML+ will give us a big chance if we see things that need to be
"clarified". Anything wihtout the proper Doctype could be interpreted based
upon the old rules.

	Not as neatly stated as I would like, but I think it gets my point
across.

-Robert Lentz
-- 
lentz@rossi.astro.nwu.edu            http://www.astro.nwu.edu/lentz/plan.html
	"You have to push as hard as the age that pushes against you."
					-Flannery O'Connor



From phillips@cs.ubc.ca  Mon Jan 13 10:37:00 1994 -0800
Message-Id: <7168*phillips@cs.ubc.ca>
Date: 13 Jan 94 10:37 -0800
From: phillips@cs.ubc.ca (George Phillips)
Subject: Slow HTTP Responses

Sometimes a server can take a long time to return a document because
it is doing something slow like an Archie search.  It would be nice
if the server could give the client some kind of status information.
I have two suggestions on how to do that:

1) Add a new response header called "Status:".  Whenever the browser
sees "Status:", it will report the string to the user.  For example:

client says:
	GET /archie?lynx HTTP/1.0

server responds:
	HTTP/1.0 200 Document Follows
	MIME-Version: 1.0
	Status: 0 % of search done
	  ... and after a bit
	Status: 20 % of search done
	 ... and a bit later
	Status: 100 % done!

	<TITLE>etc.etc.etc.

This is simple and simple to implement.  Unfortunately, the server
can report a HTTP level-error if something goes wrong -- it has
already said that things are OK.

2) Make a new Content-Type: for HTTP responses (http/response?).
And a new reponse code 205 that triggers the browser to report
the status message.  So you get server reponses like:

	HTTP/1.0 205 Starting search...
	MIME-Version: 1.0
	Content-Type: http/response

	HTTP/1.0 205 Search 10 % complete
	MIME-Version: 1.0
	Content-Type: http/response
	etc.

A little trickier to implement, would probably enable libwww to
handle .http files, makes the reponses a bit verbose.  Don't
take the "http/reponse" Content-Type: name too seriously - the
real solution will do better.

Any preferences or better ideas?  I think the "http/reponse"
Content-Type: could have many collateral benefits as it frees
HTTP request/reponse from TCP/IP transports (mail servers, anyone?).
Both ideas are at least compatible with current practice
while not being too hard to do.  I thought about a multipart/alternative
solution but quickly got scared away.



From sanders@BSDI.COM  Thu Jan 13 12:46:16 1994 -0600
Message-Id: <199401131846.MAA02762@austin.BSDI.COM>
Date: Thu, 13 Jan 1994 12:46:16 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Slow HTTP Responses 

> 	Status: 20 % of search done
> 	 ... and a bit later
> 	Status: 100 % done!

WOW!!!!!! Does this mean that you have solved the halting problem?????
Would you please explain how you are supposed to know what 20% done means?  :-)

Anyway, the point is that you could do this for timeout related functions
but in that case you would be much better off just returning the timeout
value to the client.

The *only* useful thing you could do is print warm fuzzies, but then the
browser could really just fake that and everyone would be happy 99% of the
time with 1% of the effort.

--sanders



From vandyk@roses.rockwell.com  Thu Jan 13 10:50:24 1994 PST
Message-Id: <9401131847.AA11747@slopok.roses.rockwell.com>
Date: Thu, 13 Jan 94 10:50:24 PST
From: vandyk@roses.rockwell.com (R. Michael Van Dyk)
Subject: Whitespace | Fonts | HTML vs. page layout

content-type:text/plain;charset=us-ascii
mime-version:1.0


Since anyone with an e-mail address seems to be voicing their 2-cents 
on this subject, who am I not to join the fray?

Consider this one vote for:

HTML, or some extension to HTML, or some OTHER THING allowed inside HTML,
which allows document authors to at least HINT AT, if not CONTROL some
aspects of page layout.

Strictly from my perspective (although not disregarding yours), I am happy
to suggest to the readers of my documents that:

	IF they want to get the full value out of my efforts to have
	   STYLE express some extra hints about CONTENT
	 
	THEN they should use a browser that supports the above extensions.

I don't really mind at all if people want to use simpler browsers or
(E-macs on ANSI terminals; no flames please) to read my documents.  They
simply won't see the extra work that I did to help them understand my stuff.
				THEIR CHOICE.

I do mind if those people insist on divorcing the ability to use fancy 
bit-mapped capabilities in my primary browser (Mosaic) from the HTML which
is the basis for accessing webs of information.


I'm sorry if I offend anyone, but this particular thread seems to be one
where people air their viewpoints; this is mine, and mine alone. 
<!include standard-employer-disclaimers>

Mike






From FisherM@is3.indy.tce.com  Thu Jan 13 12:25:00 1994 PST
Message-Id: <2D35AE64@MSMAIL.INDY.TCE.COM>
Date: Thu, 13 Jan 94 12:25:00 PST
From: FisherM@is3.indy.tce.com (Fisher Mark)
Subject: RE: Whitespace


The central problem is that different browsers have different levels of 
rendering ability, which is a situation likely to continue for a long (5-10 
years?) time.  Case in point:  at Thomson I use Windows and Windows NT to 
run Cello.  Once I have my Internet connection at home, I will use a 
text-based browser because Windows on a 286 is unacceptably slow.  Many 
companies in the business world (a world that needs to become part of the 
Web for the Web to really become World-Wide) are still depreciating their 
286 and 8088 machines, much less having switched over to video-accelerated 
486s (my work configuration).  Still fewer in the world are people using 
Suns, RS/6000s, HPs, ....

I like Brian Oakley's FACE idea, as an HTML editor could automagically allow 
users to have bold, italics, etc. while concealing the fact (to novices) 
that these are just hints to the browser.  My suspicion is that if a 
document is confusing when read in a single-font browser, it probably also 
is confusing in a 32767-fonts with 28 different sizes and 10 different 
emphasises (sp?) browser.

I see a need for both simple documents (as can be specified under HTML 
Classic :) as well as glossy magazines, weather maps, mathematical papers, 
road atlases, and so on.  It may be that HTML will have to evolve beyond an 
SGML document type to something in the same class as word-processor formats 
to support the multimedia/intermedia (as in FORMS and ISMAP) documents 
future authors will want to write.
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From phillips@cs.ubc.ca  Mon Jan 13 11:19:00 1994 -0800
Message-Id: <7171*phillips@cs.ubc.ca>
Date: 13 Jan 94 11:19 -0800
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: Slow HTTP Responses 

Tony says:
>The *only* useful thing you could do is print warm fuzzies, but then the
>browser could really just fake that and everyone would be happy 99% of the
>time with 1% of the effort.

Yes, exactly.  You got it on the second guess.  Thing is, browsers
can't fake the warm fuzzies -- they can only wait.  But the server
often can give a progress indicator.  It can say "I'm half-way
through all the files".  Telnet-based Archie servers do this today,
gopher servers tend to manage it because clients display data
as soon as they get it.  WWW can't do this -- your only comfort
that anything is happening is Mosaics spinning globe.  That
makes for a lousy interface.

You may not think warm fuzzies are as important as, say, whitespace,
but I sure think a mechanism like this would let HTTP server
admins provide an even better service.

You seem to worry an awful lot about effort when there's no effort involved
here.  It all fits nicely and NOBODY has to write any code unless
they want to.  They just need agree that the mechanism would be nice
to have.



From henrich@crh.cl.msu.edu  Thu Jan 13 14:35:44 1994 -0500 (EST)
Message-Id: <9401131935.AA20547@crh.cl.msu.edu>
Date: Thu, 13 Jan 1994 14:35:44 -0500 (EST)
From: henrich@crh.cl.msu.edu (Charles Henrich)
Subject: Re: Slow HTTP Responses

> You seem to worry an awful lot about effort when there's no effort involved
> here.  It all fits nicely and NOBODY has to write any code unless
> they want to.  They just need agree that the mechanism would be nice
> to have.

Welcome to the wonderful world of standards!

:)

-Crh

    Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu

                     http://rs560.msu.edu/~henrich/



From jforbes@sqi.com  Thu Jan 13 12:13:19 1994 +0800
Message-Id: <9401132013.AA02056@cochem.sqi.com>
Date: Thu, 13 Jan 1994 12:13:19 +0800
From: jforbes@sqi.com (Jason E. Forbes)
Subject: subscribe




From montulli@stat1.cc.ukans.edu  Thu Jan 13 14:16:57 1994 CST
Message-Id: <9401132016.AA27671@stat1.cc.ukans.edu>
Date: Thu, 13 Jan 94 14:16:57 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Whitespace

> > As stated before, HTML documents are intended to be viewed
> > on a variety of platforms in viewers of varying
> > capabilities.  Delivering fully formatted documents (let
> > alone multimedia) in a platform-independent format is a
> > very tough problem.  But Adobe has taken a crack at it with
> > Acrobat.  Instead of trying to get HTML to do things
> > it wasn't designed for, why not deliver a PDF Content-type
> > and spawn an Acrobat reader?  Let's leverage the work
> > that has already been done.
> 
> Acrobat isn't free and/or ubiquitous, and Acrobat doesn't provide
> network-wide hypermedia.
> 
> Marc
> 
Right, replica from Faralon is a much better solution!  Their
readers are absolutly free and they are willing to work with
the web communtity to add network-wide hyperlinks. (they are
already adding local hyperlinks).  It only costs money
to buy a replica document creator, which runs about $50, far
cheaper than Acrobat.

The only problem with replica is that they don't currently
have an Xwindows browser. :(  I had heard that they were
talking to NCSA about contracting to write an Xwindows version.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From bobs@sco.com  Thu Jan 13 10:11:06 1994 PST
Message-Id: <9401131011.aa03512@scobob.sco.com>
Date: Thu, 13 Jan 94 10:11:06 PST
From: bobs@sco.com (Bob Stayton)
Subject: Re: Whitespace

> From: Paul "S." Wain <Paul.Wain@brunel.ac.uk>
> 
> I think that to now turn around and say, "No HTML should only be used to
> design text based documents" is, well to say the least, it will
> certainly slam the breaks on the rapid advance of the Web. 

I didn't mean to imply that *WWW* should not support
multimedia.  It does so now.  But it does so by sending
different content types and spawning external viewers
specialized for that content-type, not by directly
supporting all content-types in the HTML syntax.  HTML
provides pointers to the other data types, but doesn't
actually contain the data.

I believe page layout languages are a different
content-type from SGML-based languages like HTML.  Page
layout includes precise typography, multiple columns, mixed
text and graphics frames, rotated text, spot and process
color, and lots of other stuff that is tough to do in a
platform-independent fashion.  And since HTML is
an SGML application, there is no way in SGML to
express those formatting features.  It requires
another language.  Like PDF.

@ Acrobat isn't free and/or ubiquitous,

I should have said Acrobat-compatible.  I believe someone
will write a free PDF viewer, similar to ghostscript.

@ and Acrobat doesn't provide network-wide hypermedia.

Since PDF documents handle hyperlinks, I could see Adobe
adding URLs to the spec and someone merging a WWW viewer
with a PDF viewer by making it work with URLs.  A PDF
document with URLs that link to other media viewers *could*
provide fully formatted multimedia like a CDROM.

bobs



From WIGGINS@msu.edu  Thu Jan 13 17:21:27 1994 EST
Message-Id: <9401132228.AA27050@dxmint.cern.ch>
Date: Thu, 13 Jan 94 17:21:27 EST
From: WIGGINS@msu.edu (Rich Wiggins)
Subject: Re: Whitespace

>I think trying to get HTML to deliver fully-formatted
>slick magazines is like trying to make your car fly.
>You could probably do it, but why bother when there
>are perfectly good airplanes out there?  And you
>probably wouldn't have a very good car when you were done.
>
>As stated before, HTML documents are intended to be viewed
>on a variety of platforms in viewers of varying
>capabilities.  Delivering fully formatted documents (let
>alone multimedia) in a platform-independent format is a
>very tough problem.

HTTP/HTML already is delivering multimedia.  It this pig (or car)
don't learn to fly, some other tool that does fly will take
over the exponential growth the Web now enjoys.  (Indeed, the
Web's growth curve is largely due to clicking on huge image,
sound and video documents!)

People want to deliver documents, and documents are going
multimedia.  Notions of purity or "What HTML was intended
to do" make sense in the abstract but may not be compelling
in every specific case.

You may be right -- PDF, which has precise rendering and
multimedia and hypertext hooks, or a competitor, may be the
way that really elaborate networked multimedia gets delivered.
But then HTML will be rendered (pun intended) a footnote.

/rich



From montulli@stat1.cc.ukans.edu  Thu Jan 13 16:29:05 1994 CST
Message-Id: <9401132229.AA19058@stat1.cc.ukans.edu>
Date: Thu, 13 Jan 94 16:29:05 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Slow HTTP Responses

> 
> > 	Status: 20 % of search done
> > 	 ... and a bit later
> > 	Status: 100 % done!
> 
> WOW!!!!!! Does this mean that you have solved the halting problem?????
> Would you please explain how you are supposed to know what 20% done means?  :-)
> 
> Anyway, the point is that you could do this for timeout related functions
> but in that case you would be much better off just returning the timeout
> value to the client.

Only if the timeout value is known.  Setting a maximum timeout is
next to useless for a sophisticated database.  Only after the
query is parsed, and in many cases, the search begun can a
resonable estimate of time be given.  This time value may
change dynamically due to limitless parameters.

> 
> The *only* useful thing you could do is print warm fuzzies, but then the
> browser could really just fake that and everyone would be happy 99% of the
> time with 1% of the effort.
> 
> --sanders
> 
The Z39.50 people seem to think its pretty important, regular
status messages are an integral part of the newer specs.

This sort of thing becomes VERY important for any sort of
long database search.  As forms really take off I expect to
see more and more long searches, some indication of server
progress is important, even if its nothing more that a message
saying: Still processing.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From kaehms@sedbsvr.se.ssd.lmsc.lockheed.com  Thu Jan 13 14:34:00 1994 PST
Message-Id: <9401132234.AA26611@eagle.is.lmsc.lockheed.com>
Date: Thu, 13 Jan 94 14:34:00 PST
From: kaehms@sedbsvr.se.ssd.lmsc.lockheed.com (Bob Kaehms)
Subject: <HTML> tags... a dead issue?

Hi..
I went back and forth a bit with celloTom and mosaicJon a bit today (sorry to
drag you guys into this) because I was trying to figure out a solution to
going from Mosaic/Cello --> gopher -- WAIS, and returning an html doc.

Doing this on a unix box, I took a lucky guess and put a <HTML> tag at the
top of a html doc being returned.  When I viewed this on the unix box, the
thing viewd fine.  Tom said that it was an unofficial tag that was being 
thrown around a while back, and that some viewers, including cello handle it
OK.  MosaicWIN, and I assume MosaicMac, ignore the tag and view the doc as
plain text... 

Is this a dead issue?  Isn't this a simple way to handle some funky gateway
problems that will continually pop up? Is there another way such as filename
extensions that would work as well.  Is there a decision near? Was there already
a decision made?

Sorry to beat a dead horse...
I'll take my answers off the air.

bob



From WIGGINS@msu.edu  Thu Jan 13 17:34:25 1994 EST
Message-Id: <9401132242.AA01462@dxmint.cern.ch>
Date: Thu, 13 Jan 94 17:34:25 EST
From: WIGGINS@msu.edu (Rich Wiggins)
Subject: Re: Slow HTTP Responses

I vehemently disagree.  An odometer should be a sign that some real
work is being done.  Put up an hourglass if all you want to show
is "yes the client is still on a running computer but we're waiting."
Would you have "option hash" in FTP fake the fact that transfer is
taking place?

The idea seems rather nifty to me.  Status might be amount of
searching accomplished instead of percent complete, for those
cases where 100% complete is not easily defined.   In a fickle
network a user can benefit by noting that progress has halted
on a long request.

Maybe this is too burdensome for HTTP, maybe not.  But don't do it at all
if your idea of doing it is a fake status report.

/Rich Wiggins, CWIS Coordinator, Michigan State U


>> 	Status: 20 % of search done
>> 	 ... and a bit later
>> 	Status: 100 % done!
>
>WOW!!!!!! Does this mean that you have solved the halting problem?????
>Would you please explain how you are supposed to know what 20% done means?  :-)
>
>Anyway, the point is that you could do this for timeout related functions
>but in that case you would be much better off just returning the timeout
>value to the client.
>
>The *only* useful thing you could do is print warm fuzzies, but then the
>browser could really just fake that and everyone would be happy 99% of the
>time with 1% of the effort.
>
>--sanders



From henrich@rs560.cl.msu.edu  Thu Jan 13 18:34:36 1994 -0500 (EST)
Message-Id: <9401132335.AA47118@rs560.cl.msu.edu>
Date: Thu, 13 Jan 1994 18:34:36 -0500 (EST)
From: henrich@rs560.cl.msu.edu (Charles Henrich)
Subject: Re: Slow HTTP Responses

> This sort of thing becomes VERY important for any sort of
> long database search.  As forms really take off I expect to
> see more and more long searches, some indication of server
> progress is important, even if its nothing more that a message
> saying: Still processing.

You are absolutely correct!  I happen to be one of those folks that dont trust
computers at all, if I dont see output, something is probably broken.
Especially for long searches, or other activites that are take a fair amount of
time, the server should be able to send a simple message "Yup, Im still here,
still working, gimme another 15 minutes"

-Crh

    Charles Henrich     Michigan State University     henrich@crh.cl.msu.edu

                     http://rs560.msu.edu/~henrich/




From sanders@BSDI.COM  Thu Jan 13 17:58:49 1994 -0600
Message-Id: <199401132358.RAA04736@austin.BSDI.COM>
Date: Thu, 13 Jan 1994 17:58:49 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Slow HTTP Responses 

> I vehemently disagree.  An odometer should be a sign that some real
> work is being done.  Put up an hourglass if all you want to show
> is "yes the client is still on a running computer but we're waiting."
> Would you have "option hash" in FTP fake the fact that transfer is
> taking place?
hash indicates *REAL* work being done, Mosaic already does this.  Mosaic
also changes the cursor.  Neither of these address the issue, The issue
being, what and how should the remote server be able to tell us about the
status of the request being processed.  It's not clear to me at this point
that "Status:" is the right solution, HTTP is simply not designed as an
interactive protocol TTBOMK.

My current thinking on this general problem is that we need "protocol
callbacks", reply addresses for services provided by the client encoded
in the client's request to the server.  For example, status messages could
be passed back via:
	Services: Status/1.0 port=9994
Or, you might have the status monitor on another host:
	Services: Status/1.0 port=9994,host=otherhost.dom
Or support an interactive audio/video service:
	Services: AVsync/1.1 port=9995,chn=32,mode=HDTV.3

The status service should probably be UDP based, another reason
not to run it inside HTTP.

We could/should also define an interface standard for clients wanting to
run these services externally so, e.g., you could just get the AVsync
program and "hook it up" to your favorite client.  Much thought still
needs to go into this, of course, but it seems a must for the long term.
Same goes for external content-type viewers.  We need a high-end interface
language for talking about Hypertext objects between the client and
the viewer.

CGI has been a step in the right direction for the server end.  Much
thanks to RobM for his effort there.

--sanders



From strata@fenchurch.MIT.EDU  Thu Jan 13 19:48:05 1994 EST
Message-Id: <CMM.0.90.0.758508485.strata@fenchurch>
Date: Thu, 13 Jan 94 19:48:05 EST
From: strata@fenchurch.MIT.EDU (M. Strata Rose)
Subject: Search feedback (was: Re: Slow HTTP Responses)


I think many people would be happy with (as a minimum) some combination
of "I found N hits" and "I know about X amount of data and am Y the way
through it".  This conveys progress while not attempting to solve the halting
problem, especially if Y is simply a number and not a percentage.  This
method can peacefully coexist with discovery-style searches, where the user
might see the following status info over time:

	Data pool: 25 Mb in 18 files
	Searched: 14 Mb in 11 files
	Hits: 3
	Current data: http:/foo.bar.baz:splat  (3.2Mb)
	Timestamp: Thu Jan 13 16:37:50 PST 1994

	...

	Data pool: 43 Mb in 25 files
	Searched: 31 Mb in 18 files
	Hits: 8
	Current data: http:/foo.bar.waldo:roadkill  (12Mb)
	Timestamp: Thu Jan 13 16:57:20 PST 1994

	...

	Data pool: 2011 Mb in 62 files
	Searched: 87 Mb in 43 files
	Hits: 17
	Current data: ftp:/splat.foo.woo-woo:bigdata (1029 Mb)
	Timestamp: Thu Jan 13 17:12:20 PST 1994

	...

And so on.  Additional features of the search could be a [STOP]
button that can be hit to bypass a particular document, such as
the mythical "bigdata" in the last example.  Users could have more
control still with a "Current Hits:" status line that updates hits
from that particular document and then adds them to the "Hits" display
only when that document is finished.  Our hypothetical user might
change his mind about "bigdata" if he/she could see that 7 of those
17 hits alone came from that document and we still weren't done
searching it.

This is more UI stuff then Web-specific, but I consider it an example
of features people are likely to want, and things which are desirable
and could be relatively easy to provide if we support an infrastructure
for them.

Comments?  UI-specific comments might go to me and not the list.

_Strata



M. Strata Rose
Unix & Network Consultant, SysAdmin & Internet Information 
Virtual City Network (tm)
strata@virtual.net | strata@hybrid.com | strata@fenchurch.mit.edu



From jonm@ncsa.uiuc.edu  Thu Jan 13 19:15:04 1994 CST
Message-Id: <9401140115.AA16834@void.ncsa.uiuc.edu>
Date: Thu, 13 Jan 94 19:15:04 CST
From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
Subject: Re: Slow HTTP Responses 

At 05:58 PM 1/13/94 -0600, Tony Sanders wrote:

>It's not clear to me at this point
>that "Status:" is the right solution, HTTP is simply not designed as an
>interactive protocol TTBOMK.

It's fairly clear to me that it is _not_ the correct solution.  I was
thinking along the same lines as Tony w/ regards to using a separate
UDP connection.

Maybe I was just missing something, but I didn't see how either of
the initial suggestions would actually work.  

The first method forced all of the status info to come before any of
the data was transferred which seems silly.  If I have a server 
generating information, it may as well be sending it as it is generated.  

The second method, I don't understand at all. I've already received one 
HTTP/1.0 response so how/where am I getting the "HTTP/1.0 205 Starting 
search..." messages?

>
>My current thinking on this general problem is that we need "protocol
>callbacks", reply addresses for services provided by the client encoded
>in the client's request to the server.  For example, status messages could
>be passed back via:
>	Services: Status/1.0 port=9994
>Or, you might have the status monitor on another host:
>	Services: Status/1.0 port=9994,host=otherhost.dom
>Or support an interactive audio/video service:
>	Services: AVsync/1.1 port=9995,chn=32,mode=HDTV.3
>
>The status service should probably be UDP based, another reason
>not to run it inside HTTP.

Agreed.  It is silly to use TCP for information which is obviously
ideally suited for UDP.  

-Jon

---
Jon E. Mittelhauser (jonm@ncsa.uiuc.edu)
Research Programmer, NCSA                          (NCSA Mosaic for MS Windows)
More info <a href="http://www.ncsa.uiuc.edu/SDG/People/jonm/jonm.html">here</a>





From janssen@parc.xerox.com  Thu Jan 13 17:13:52 1994 PST
Message-Id: <chBT7EAB0KGW03cecY@holmes.parc.xerox.com>
Date: Thu, 13 Jan 1994 17:13:52 PST
From: janssen@parc.xerox.com (Bill Janssen)
Subject: Re: Whitespace

Or build a PDF viewer into  your browser, along with the HTML viewer. 
How do you handle links?

Bill



From robm@ncsa.uiuc.edu  Thu Jan 13 23:17:05 1994 -0600
Message-Id: <9401140517.AA18910@void.ncsa.uiuc.edu>
Date: Thu, 13 Jan 1994 23:17:05 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Slow HTTP Responses

/*
 * Re: Slow HTTP Responses  by Tony Sanders (sanders@BSDI.COM)
 *    written on Jan 13,  5:58pm.
 *
 * hash indicates *REAL* work being done, Mosaic already does this.  Mosaic
 * also changes the cursor.  Neither of these address the issue, The issue
 * being, what and how should the remote server be able to tell us about the
 * status of the request being processed.  It's not clear to me at this point
 * that "Status:" is the right solution, HTTP is simply not designed as an
 * interactive protocol TTBOMK.

I agree, fitting status messages into the current protocol will either be an
ugly hack, or require two responses like in the 205 proposal. I'm not fond
of either.

 * My current thinking on this general problem is that we need "protocol
 * callbacks", reply addresses for services provided by the client encoded
 * in the client's request to the server.  For example, status messages could
 * be passed back via:
 * 	Services: Status/1.0 port=9994
 * Or, you might have the status monitor on another host:
 * 	Services: Status/1.0 port=9994,host=otherhost.dom
 * Or support an interactive audio/video service:
 * 	Services: AVsync/1.1 port=9995,chn=32,mode=HDTV.3
 * 
 * The status service should probably be UDP based, another reason
 * not to run it inside HTTP.
 * 
 * We could/should also define an interface standard for clients wanting to
 * run these services externally so, e.g., you could just get the AVsync
 * program and "hook it up" to your favorite client.  Much thought still
 * needs to go into this, of course, but it seems a must for the long term.

This is an interesting proposal, and very open-ended which is why I like it.
It provides flexibility in that the Services: line could specify anything
(like the access authorization does now, the PEM and PGP experimental auth
protocols I just added to Mosaic and httpd are remarkably similar to this
proposal but I digress). Of all the proposals made thus far, this sounds
like the most logical. I think we should be looking at long-term solutions
instead of short-term HTTP hacks and kludges so we're not regretting our
actions five years from now.

 * Same goes for external content-type viewers.  We need a high-end interface
 * language for talking about Hypertext objects between the client and
 * the viewer.
 * 
 * CGI has been a step in the right direction for the server end.  Much
 * thanks to RobM for his effort there.
 */

CGI will help in that the Status: line could be made available in the
environment for the gateways to interpret. This raises the question again of
whether CGI should send all of the header lines to the script regardless of
whether it knows their meaning, but that will require more thought.

--Rob



From robm@ncsa.uiuc.edu  Fri Jan 14 05:04:15 1994 -0600
Message-Id: <9401141104.AA22330@void.ncsa.uiuc.edu>
Date: Fri, 14 Jan 1994 05:04:15 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Redirection: "Location" or "Uri" ?

/*
 * Redirection: "Location" or "Uri" ?  by Martijn Koster (m.koster@nexor.co.uk)
 *    written on Jan  6,  7:15pm.
 *
 * I was playing around with redirection today, and got conflicting info
 * on how it is done: both draft-ietf-iiir-http-00.txt and 
 * http://info.cern.ch/hypertext/WWW/Technical.html talk about
 * 
 * 	Uri: <url> String CrLf
 * 
 * but Mosaic for X and httpd seem to use
 * 
 * 	Location: <url>
 * 
 * Why are these contradictory? Which one is right?
 */

The PS document we based it on is from June 1993... I'm not sure when it was
changed or who changed it, but now that we have a huge installed user base I
think Location should be used.

--Rob



From m.koster@nexor.co.uk  Fri Jan 14 13:22:30 1994 +0000
Message-Id: <9401141323.AA16695@dxmint.cern.ch>
Date: Fri, 14 Jan 1994 13:22:30 +0000
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: Redirection: "Location" or "Uri" ?

> now that we have a huge installed user base I think Location should be used.

Agreed. Can the spec be ammended?


-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From hgs@research.att.com  Fri Jan 14 09:46:07 1994 EST
Message-Id: <9401141447.AA05029@dxmint.cern.ch>
Date: Fri, 14 Jan 94 09:46:07 EST
From: hgs@research.att.com (Henning G. Schulzrinne)
Subject: Slash or no slash?

This may have been beaten to death several times over, but here goes:

According to the BNF description 

  http://info.cern.ch/hypertext/WWW/Addressing/URL/5_BNF.html,

http URL's don't need a trailing slash after the host, i.e.,
http://www.research.att.com (without the trailing slash) is legal.

Furthermore, relative URLs are supposed to not start with a slash.
(see Relative.html, same place).

However, if lynx (2.1) is fed the non-trailing-slash HTTP and tries to
access a relative URL (say "mbone-faq.html"), it simply concatenates
the two, with dire results, into
http://www.research.att.commbone-faq.html. Mosaic automatically appends
a slash and thus has no problem.

Is it just that Lynx is broken or is something else wrong?

---
Henning Schulzrinne (hgs@research.att.com)
AT&T Bell Laboratories  (MH 2A-244)
600 Mountain Ave; Murray Hill, NJ 07974
phone: +1 908 582-2262; fax: +1 908 582-5809




From tkevans@fallst.es.dupont.com  Fri Jan 14 10:05:04 1994 -0500 (EST)
Message-Id: <m0pKq5A-0000Z0C@fallst.es.dupont.com>
Date: Fri, 14 Jan 1994 10:05:04 -0500 (EST)
From: tkevans@fallst.es.dupont.com (Tim Evans)
Subject: Large Pre-formatted Files

We'd like to put our local installation's phone book, about 300KB
in size and more than 4,000 lines long, up on our internal server.
The file is currently formatted like this:

name<tab><tab><tab>number<tab>department<tab>building/room

Using the raw file fouls up the formatting, generating a single continuous
"paragraoh" of unformatted data.  I tried surrounding the entire
document with <pre> and </pre> markup, but the file--or more likely
the pre-formatted part of the file--seems too large for Mosaic to
handle.  (It's been 15 minutes since I asked Mosaic 2.1 to load the
file, on a Sun 4/110, and I'm still looking at the little watch.)
I've tried simply putting <p> at the end of each line/entry, and
this works, but "double-spaces" the output, something I'd rather
not have.

How can this sort of file be marked up in HTML to display it properly
in a reasonable amont of time?

-- 
INTERNET:	tkevans@fallst.es.dupont.com
Tim Evans	2201 Brookhaven Ct, Fallston, MD 21047



From m.koster@nexor.co.uk  Fri Jan 14 15:41:44 1994 +0000
Message-Id: <9401141544.AA17486@dxmint.cern.ch>
Date: Fri, 14 Jan 1994 15:41:44 +0000
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: Large Pre-formatted Files


> We'd like to put our local installation's phone book, about 300KB
> in size and more than 4,000 lines long, up on our internal server.

Why put all the data in a single document? Why not write a simple gateway
to search through that file for names or whatever? That will be a lot
faster than loading the whole document and searching through it yourself.

> name<tab><tab><tab>number<tab>department<tab>building/room

Especially with a nice format like that it's trivial...

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From sanders@BSDI.COM  Fri Jan 14 10:39:37 1994 -0600
Message-Id: <199401141639.KAA07042@austin.BSDI.COM>
Date: Fri, 14 Jan 1994 10:39:37 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Redirection: "Location" or "Uri" ? 

There are quite a few changes in the HTTP spec.  Time for a re-read!

>  * 	Location: <url>
[changed to the following by TimBL a few months ago]
>  * 	Uri: <url> String CrLf

What needs to happen is all clients should be changed to support both.
When that happens we should start converting servers to send URI: instead
of Location:.  Clients will probably need to support Location: for a long
time, but it's cost is very low (it's really just a special case of URI:).

HTTP SPEC CHANGE REQUEST:
    The HTTP spec should be changed to mention Location: as the depreciated
    URI: specifier without the vary parameter.

    Also "Content-Language:" gives examples "Language: En_US".  So is it
    Content-Language or just Language.  If it's Content-Language: then
    it should be Content-Version:.  I vote for using a Content- prefix
    to reduce namespace conflicts (you still have a few, Content-Length
    for example).


URI: was chosen for a generic object location identifier and it includes
the "vary" parameter which allows you to specify things about the object.
For example, you can point to an *exact* object, or an object in which
the "version" varies, or an object in which the "language" varies,
or an object in which the "content-type" may vary.

There are headers to go along with each of the dimension of variability
(namely Language:, Version:, and Content-Type:).  Version: isn't fully
spec'ed out yet.  Of course, this is an extensible list.

--sanders



From jonm@ncsa.uiuc.edu  Fri Jan 14 11:45:19 1994 CST
Message-Id: <9401141745.AA26689@void.ncsa.uiuc.edu>
Date: Fri, 14 Jan 94 11:45:19 CST
From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
Subject: Re: Large Pre-formatted Files

Well, my first suggestion is what Martijn suggested, break it up and
make it searchable through a form interface to a gateway...

However, if you are just looking for a quick "hack"...

>I've tried simply putting <p> at the end of each line/entry, and
>this works, but "double-spaces" the output, something I'd rather
>not have.

Use <BR> at the end of each line (Line Break).  This won't result in
the double spacing effect and will give you what you desire...

However, the file will still be to large to be useful... :^)

-Jon

---
Jon E. Mittelhauser (jonm@ncsa.uiuc.edu)
Research Programmer, NCSA                          (NCSA Mosaic for MS Windows)
More info <a href="http://www.ncsa.uiuc.edu/SDG/People/jonm/jonm.html">here</a>




From robm@ncsa.uiuc.edu  Fri Jan 14 11:56:18 1994 -0600
Message-Id: <9401141756.AA26885@void.ncsa.uiuc.edu>
Date: Fri, 14 Jan 1994 11:56:18 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: More CGI Comments

/*
 * Re: More CGI Comments  by rhb@hotsand.att.com (rhb@hotsand.att.com)
 *    written on Jan  9,  8:04pm.
 *
 * 
 * > The other, more serious, complication is that if some binary data file
 * > happens to begin with a magic number, the server would refuse to serve it
 * > up (instead trying, and failing, to run it).  If I'm not mistaken, most
 * > common binary data formats (Sun .au, AIFF, JPEG (JFIF), MPEG, tar,
 * > compressed data...) define their own series of magic numbers which are
 * > unlikely to conflict, but relying on this would be dicey.
 * 
 * This in fact points up the need to have a "smart" server.  It is important
 * that the server be able to identify the files it's serving not just by their
 * suffix (i.e., in the mime type file) but via any available attribute
 * (such as magic numbers).  The information that's available differs
 * among platforms and application (server) needs.  I'm in agreement that
 * we need a flexible method that allows the server to use it's "smarts"
 * and the users' needs (e.g., the scripts I am interested in providing
 * are all perl and tcl, so the #! would work for my application...).

I am really leery of doing anything with magic numbers. There is the binary
data problem (my machine's file command just identified a GIF as a C
program). Also, what is the magic number of an HTML document? A plaintext
document? 

 * Now a more general question that turns the previous question inside
 * out.  The original concern was in adding executables to directories
 * that were not indicated as having executables.  Well it seems that
 * I can certainly have "regular" (non script) html files in executable
 * directories.  
 
Sure but when you try to retrieve them you'll get hit with a 500.
 
 * What do I gain from having a directory "not executable"?
 * Why aren't all directories cgi-bin directories?  

Managability.... at the time we made a design decision to split scripts
apart from regular documents. I felt it was important to make no suffix
restrictions on script names and I didn't want to use magic numbers. It's
important to remember also that when these design decisions were made there
was no access control in the server except that provided by config files. 

We made this decision because we felt it was necessary to give the server
administrator as much control as possible over script execution since it is
an inherently dangerous process.

It is extremely easy to write a careless script which gives intruders full
access to your machine. I should know, someone just executed an xterm from
my machine to theirs because of a careless misconfiguration I had. An
extremely popular CGI gateway which was released recently can be exploited
rather easily as well. Yes, fixes are on the way, yes, I am being
deliberately vague.

Now let's consider Joe Shmoe who writes a CGI script for his/her user script
directory. It won't be their fault, but they could easily seriously
compromise a system with a relatively innocent script.

I plan to add the ability to have script execution by suffix to 1.2 (since
1.1 is too close to being out the door). Access control will allow you to
turn script execution off in certain directories, as will simply not adding
the suffix maps to your mime configuration.

 * I can restrict
 * GET in the directories in which I really have scripts if I'm concerned
 * about reading scripts...
 *
 * One point brought to my attention is that all directories are already
 * "executable" in the NCSA server environment via the <inc var> directive
 * (i.e., this takes care of my original problem of getting to the
 * user variable in authentication), but it doesn't take care of things
 * like forms.
 */

The server-side includes are a whole different ball of wax. 

--Rob



From masinter@parc.xerox.com  Fri Jan 14 13:01:47 1994 PST
Message-Id: <94Jan14.130158pst.2732@golden.parc.xerox.com>
Date: Fri, 14 Jan 1994 13:01:47 PST
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Redirection: "Location" or "Uri" ? 

I remember a discussion in one of the IETF mailing lists of a proposal
to generalize `content-type' for MIME to be `media-type'.
`content-type' was appropriate for mail messages, to describe the type
of the content of the mail, and may be inapropriate for other
contexts.

Just FYI, if you're trying to regularize attribute names.




From masinter@parc.xerox.com  Fri Jan 14 13:30:26 1994 PST
Message-Id: <94Jan14.133029pst.2732@golden.parc.xerox.com>
Date: Fri, 14 Jan 1994 13:30:26 PST
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Slow HTTP Responses

I uneasy having to supply a separate information channel when you
already have a channel established. The only problem is that you want
to interpolate asynchronous information in the middle of a channel of
other information. Telnet already does this by having some escape
codes.

Why not use an escape code mechanism for those information channels
that want to send status information back? This won't be MOST servers,
only a few. The client just would have to look for the escape codes in
the data stream.




From WIGGINS@msu.edu  Fri Jan 14 17:00:19 1994 EST
Message-Id: <9401142209.AA24323@dxmint.cern.ch>
Date: Fri, 14 Jan 94 17:00:19 EST
From: WIGGINS@msu.edu (Rich Wiggins)
Subject: Re: Slow HTTP Responses

>> I vehemently disagree.  An odometer should be a sign that some real
>> work is being done.  Put up an hourglass if all you want to show
>> is "yes the client is still on a running computer but we're waiting."
>> Would you have "option hash" in FTP fake the fact that transfer is
>> taking place?
>hash indicates *REAL* work being done, Mosaic already does this.  Mosaic
>also changes the cursor.  Neither of these address the issue, The issue
>being, what and how should the remote server be able to tell us about the
>status of the request being processed.  It's not clear to me at this point
>that "Status:" is the right solution, HTTP is simply not designed as an
>interactive protocol TTBOMK.

Regardless of the merits of the Status proposal, the spinning globe
does NOT equate to hash: sometimes the globe spins when nothing is
being transferred -- I've had it spin in WinMosaic while a dialog
box sits telling me the fetch failed.  Moreover, Option Hash gives
you a visual indication as to the amount of work that has been
accomplished -- it's a count of blocks transferred.  Users can't
count globe revolutions as a measure of work done.

I can picture lots of situations where it'd be very nice to have
a standard status mechanism.  Suppose the info you want is offline
and must be fetched with an unknown wait for posting online.  Suppose
the server is hunting around a huge database (or the Internet) for
your answer.  An odometer reflecting real work done (which is not
necessarily the same thing as amount of data transferred to the
user, by the way) could be extremely useful.

Whether the particular proposal for communicating back status info
is the best scheme or not, having the client fake a "warm fuzzy"
indicator does not address the need expressed.


/rich



From sanders@BSDI.COM  Fri Jan 14 16:15:07 1994 -0600
Message-Id: <199401142215.QAA08634@austin.BSDI.COM>
Date: Fri, 14 Jan 1994 16:15:07 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Slow HTTP Responses 

Larry Masinter writes:
> Telnet already does this by having some escape codes.
Blah, some serious uglyness is due to this.  Muxing control information
into a data stream is a very ugly proposition.  It's best to let the
low-level protocol code do it since *lots* of people have spent *lots* of
time making that perfect.

> Why not use an escape code mechanism for those information channels
> that want to send status information back? This won't be MOST servers,
> only a few. The client just would have to look for the escape codes in
> the data stream.
It makes the protocol two or three orders of magnitude more complex, which
is exactly what HTTP tries to avoid in the first place.  It doesn't matter
if only a few servers need to do it, every client would have to support it
and, for practical reasons, so would most servers.

We've already pointed out that the status protocol should be UDP-based
anyway.  The nice thing about doing it with "Services:" is that you could
even write a little stand-alone program that sends a packet to the specified
port and call it from a shell script!  That way *every* server can easily
provide this service if they want.  Also, if it's UDP-based then it would
be faster *and* require less network resources then muxing on a TCP
connection.  It's also far more likely to be correctly implemented in the
next 10 years, unlike other solutions that involve radically changing
the nature of HTTP.

And last, but not least, it's an extensible solution.

--sanders



From hgs@research.att.com  Fri Jan 14 17:37:32 1994 EST
Message-Id: <9401142238.AA25720@dxmint.cern.ch>
Date: Fri, 14 Jan 94 17:37:32 EST
From: hgs@research.att.com (Henning G. Schulzrinne)
Subject: Re: Slow HTTP Responses

> We've already pointed out that the status protocol should be UDP-based
> anyway.  The nice thing about doing it with "Services:" is that you could
> even write a little stand-alone program that sends a packet to the specified
> port and call it from a shell script!  That way *every* server can easily
> provide this service if they want.  Also, if it's UDP-based then it would
> be faster *and* require less network resources then muxing on a TCP
> connection.  It's also far more likely to be correctly implemented in the
> next 10 years, unlike other solutions that involve radically changing
> the nature of HTTP.
> 
> And last, but not least, it's an extensible solution.

Keep in mind that UDP means that many behind-a-firewall machines
won't be able to make any use of any UDP-based mechanism.

In general, there seem to be four situations with long waits where
some user indication would be helpful:

- long data transfer (the current numeric indicator in Mosaic handles that,
  although a graphical indication would probably be nicer). If I
  understand ftp right, the hash marks are generated purely locally,
  simply by counting bytes, without any involvement of the other party.

- long search, followed by burst of data (archie model); all
  status messages would precede the actual data (no need to send
  escape codes, simply more header fields, for example)

- the tough one: bursts of data, interspersed with "think pauses",
  probably without any indication how much more work needs to be done.

- one page consisting of text plus inlined images; the current byte
  counts give no indication how long it will be until the whole page
  is ready since there could be tens of inlined images

> 
> --sanders
> 




From mcrae@ora.com  Fri Jan 14 14:55:45 1994 -0800
Message-Id: <199401142255.AA15865@rock.west.ora.com>
Date: Fri, 14 Jan 94 14:55:45 -0800
From: mcrae@ora.com (Christopher J. McRae)
Subject: ANNOUNCEMENT: SIGWEB #4 (Sunnyvale, CA)



				SIGWEB #4
		       Where Are We Going, and Why?


A panel discussion in two parts, featuring...
  Dale Dougherty        O'Reilly & Associates
  Barry Leiner          Internet Activities Board and USRA
  Clifford Lynch        UC Division of Library Automation
  Marty Tenenbaum       Enterprise Integration Technologies
  Terry Winograd        Stanford University

Part 1: The Global/National Internet Community
The first part of the meeeting will be a general discussion about the 
state-of-the-art in (internet-based) information systems and where it 
is/should-be/will-be going.  How do the Internet community and Internet-based 
information tools fit into the development of a National Information 
Infrastructure in the U.S.?  What is the future of Internet-based information 
systems?  What are the biggest obstacles facing us in the next five
years?

Part 2: The S.F. Bay Area Internet Community (i.e. SIGWEB)
The second part of this meeting will focus more narrowly on how the local
community, i.e. SIGWEB, fits into the vision(s) elucidated in the first part.
Our intention is for the meeting to help forge a more solid identity/mission
for the group.  Who are we?  Why do we show up for these meetings?  Where are
we each/all going?  What is the role of SIGWEB?  If there's anything else we
should be doing, who is willing/able to do it and what kind of help do you
need?

Prepared statements will be kept to a minimum, this is an audience
participation free-for-all.  Please come prepared to join in the discussion.

Location: Amdahl Corporation - Sunnyvale, CA
    Date: February 3rd, 1994
    Time: 2:00 - 5:00 PM

Directions to be announced shortly.

-----------------------------------------------------------------------
Christopher McRae			            chrism@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036



From jonm@ncsa.uiuc.edu  Fri Jan 14 17:09:21 1994 CST
Message-Id: <9401142309.AA01717@void.ncsa.uiuc.edu>
Date: Fri, 14 Jan 94 17:09:21 CST
From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
Subject: Re: Slow HTTP Responses

At 01:30 PM 1/14/94 PST, Larry Masinter wrote:
>I uneasy having to supply a separate information channel when you
>already have a channel established. The only problem is that you want
>to interpolate asynchronous information in the middle of a channel of
>other information. Telnet already does this by having some escape
>codes.
>

Uggggggggh!  Ugggggggggh!  Yuck! 
Please tell me you meant to include a smiley face here?!?

How do you plan on dealing with arbitrary binary data with status
information intermixed?  Take a simple GIF file.  How would you
recognize the escape codes intermixed with the binary data?  I 
presume you are envisioning some crazy magic number scheme but this
simply is impractical...

The telnet example just is not at all relavant as to how the
Web should work.  It doesn't work that well in the first place and
is not at all extensible...

>Why not use an escape code mechanism for those information channels
>that want to send status information back? This won't be MOST servers,
>only a few. The client just would have to look for the escape codes in
>the data stream.
>

In _any arbitrary_ data stream.  Why bother?

Other than the implementation work involved, what are 
the problems that people see with the UDP scheme that Tony and I 
were suggesting?

-Jon

---
Jon E. Mittelhauser (jonm@ncsa.uiuc.edu)
Research Programmer, NCSA                          (NCSA Mosaic for MS Windows)
More info <a href="http://www.ncsa.uiuc.edu/SDG/People/jonm/jonm.html">here</a>




From luotonen@ptsun00.cern.ch  Sat Jan 15 00:24:07 1994 +0100
Message-Id: <9401142324.AA08782@ptsun03.cern.ch>
Date: Sat, 15 Jan 94 00:24:07 +0100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: Slow HTTP Responses


> - one page consisting of text plus inlined images; the current byte
>   counts give no indication how long it will be until the whole page
>   is ready since there could be tens of inlined images

Hey, what if Mosaic would start saying

	.... bytes of inlined image data (3/7)

That is, indicating the number of inlined images and the
current one.

-- Cheers, Ari --




From ses@tipper.oit.unc.edu  Fri Jan 14 18:15:52 1994 -0500
Message-Id: <9401142315.AA00735@tipper.oit.unc.edu>
Date: Fri, 14 Jan 94 18:15:52 -0500
From: ses@tipper.oit.unc.edu (Simon E Spero)
Subject: Re: ANNOUNCEMENT: SIGWEB #4 (Sunnyvale, CA) 

Chris-
 Is this one going to be multicast?
Simon



From janssen@parc.xerox.com  Fri Jan 14 16:27:16 1994 PST
Message-Id: <ohBnVYgB0KGWM917cI@holmes.parc.xerox.com>
Date: Fri, 14 Jan 1994 16:27:16 PST
From: janssen@parc.xerox.com (Bill Janssen)
Subject: Re: Slow HTTP Responses

Do MacTCP and WinSock support UDP?

Bill



From masinter@parc.xerox.com  Fri Jan 14 17:11:44 1994 PST
Message-Id: <94Jan14.171144pst.2732@golden.parc.xerox.com>
Date: Fri, 14 Jan 1994 17:11:44 PST
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Slow HTTP Responses

I wasn't joking, and so didn't include a smiley. Using in-band status
information is most appropriate when the data stream *isn't* binary,
but it is just as possible to add escape codes to a binary gif stream
as it is to on the fly LZW compress a .tar file. 

It is neither crazy nor impractical; it uses little additional
bandwidth, or processing speed, doesn't rely on the TCP transport,
works through proxy gateways, doesn't require the client to fork a
separate process to listen for asynchronous UDP packets.

Certainly, for static data that is precomputed, you WOULDN'T want to
use inline escape codes, since you can merely tell the client how big
it is in the first place. This would only be appropriate for those
HTTP requests that were actually computing something where the size of
result and time of completion weren't known in advance.

I admit that there is a perspective from which this is unaesthetic,
but I suggest you squint at it a little harder.





From masinter@parc.xerox.com  Fri Jan 14 18:20:42 1994 PST
Message-Id: <94Jan14.182056pst.2732@golden.parc.xerox.com>
Date: Fri, 14 Jan 1994 18:20:42 PST
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Search feedback (was: Re: Slow HTTP Responses)

There are two independent issues:

1)  what channel is used for sending status information back.
   Async UDP
   In-band escape sequences
   ...
   (maybe this is something that you specify in your 'GET' request
   if you're willing to accept them, and something that comes
   back in the header if there's a place to recieve them)

2) What is the nature of the progress report:
    percentage complete
    amount of time remaining
    X out of Y as Strata Rose indicated


I think we can manage to support all combinations of these,
optionally, in the protocol, and then see what actually gets
implemented and supported.



From swb1@cornell.edu  Sat Jan 15 00:09:12 1994 -0500
Message-Id: <199401150507.AA12881@postoffice2.mail.cornell.edu>
Date: Sat, 15 Jan 1994 00:09:12 -0500
From: swb1@cornell.edu (Scott W Brim)
Subject: Re: Slow HTTP Responses

Yes.

At 16:27 1/14/94 -0800, Bill Janssen wrote:
  >Do MacTCP and WinSock support UDP?
  >
  >Bill





From fenner@cmf.nrl.navy.mil  Sat Jan 15 01:51:43 1994 -0500
Message-Id: <9401150651.AA03481@riker.cmf.nrl.navy.mil>
Date: Sat, 15 Jan 1994 01:51:43 -0500
From: fenner@cmf.nrl.navy.mil (William C Fenner)
Subject: Re: sd access through a WWW server 

[For those on the www-talk mailing list: the idea is to have the ability to
 use WWW for a session directory for multicast conferences.  Many multicast
 conferences have a limited lifetime, so the use of a less dynamic system
 such as WWW may not be preferable.]

On Fri, 14 Jan 94 16:30:56 PST  Yee-Hsiang Chang wrote:
> If a wide-area directory service such as 
> the Mosaic can be tailored for the conference announcement and discovery, 
> we do not need to send event message periodically.

One of the other ideas that I've been tossing around is to create an
application/x-sd type in HTTP, which simply passes the sd entry to a
program that does whatever sd would do with that entry - then people
running Mosaic with a proper ~/.mailcap could actually launch multicast
applications by clicking on a hypertext link.

I prefer application/x-sd to application/x-csh because it allows
the client to decide what application to run for each session type,
instead of forcing the server to make assumptions.

If there is interest for this as well, I might crank it up a couple of
notches on my priority list.

  Bill



From fielding@simplon.ICS.UCI.EDU  Sat Jan 15 00:16:14 1994 -0800
Message-Id: <9401150016.aa19557@paris.ics.uci.edu>
Date: Sat, 15 Jan 1994 00:16:14 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Announcing wwwstat-0.1 -- an access log summary program


This message is to announce the availability of wwwstat Version 0.1 --
a new program for analyzing NCSA httpd_1.0 server access logs and printing
an HTML-formatted summary report.  The program is written in Perl
and, once customized for your site, should work on any UNIX-based system
with Perl 4.019 or better.  The program is in the public domain (i.e. FREE).

As an example of what wwwstat can do for you, look
    <A HREF="http://www.ics.uci.edu/Admin/wwwstats.html"> here </A>
to see UC Irvine's Department of Information & Computer Science
WWW server statistics.

For more information and access to the wwwstat-0.1 distribution,
point your WWW client at

    <A HREF="http://www.ics.uci.edu/WebSoft/wwwstat/"> wwwstat-0.1 </A>.

The intention is that wwwstat be run as a crontab entry just before
midnight, with its output redirected to the site's summary file.
It could easily be modified to run as a CGI script, but that
is not recommended for slow processors or heavily utilized servers.

Obviously, versions of this program would also be nice for the Plexus
and CERN servers.  However, I found that much of the logic for finding
file names was just too specific to the NCSA server to justify all the
other work of making this general.  Feel free to do so yourself.


Version History:

Known problems
     Cannot estimate the transmission of bytes from scripts.
     Ignores the fact that some accesses may have been denied.

Version 0.1                                             January 14, 1994 
     Added support for HTML output.
     Added reversed subdomain statistics.
     Added the logic for grouping files in archive sections.
     Rewrote the whole damn thing.

Version 0.0
     Originally from fwgstat 0.35 (jem@sunsite.unc.edu) with all the
     extra options stripped out and many bugs fixed.  In turn,
     fwgstat was heavily based on xferstats, which is packaged with
     the Wuarchive FTP daemon.  Fwgstat is good for multi-server stats.


If you have any suggestions, bug reports, fixes, enhancements,
or praise, send them to me at <fielding@ics.uci.edu>.

Have fun,


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From robm@ncsa.uiuc.edu  Sat Jan 15 04:02:09 1994 -0600
Message-Id: <9401151002.AA06465@void.ncsa.uiuc.edu>
Date: Sat, 15 Jan 1994 04:02:09 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Search feedback (was: Re: Slow HTTP Responses)

/*
 * Re: Search feedback (was: Re: Slow HTTP Responses)  by Larry Masinter (masinter@parc.xerox.com)
 *    written on Jan 14,  6:20pm.
 *
 * There are two independent issues:
 * 
 * 1)  what channel is used for sending status information back.
 *    Async UDP
 *    In-band escape sequences
 *    ...

The beauty of Tony's suggestion is that it defines no channel... it only
defines that the channel used is not the current HTTP connection. 

 *    (maybe this is something that you specify in your 'GET' request
 *    if you're willing to accept them, and something that comes
 *    back in the header if there's a place to recieve them)

Exactly.

 * 2) What is the nature of the progress report:
 *     percentage complete
 *     amount of time remaining
 *     X out of Y as Strata Rose indicated

Again, Tony's proposal allows any of these (and the ones people come up with
years from now) to be integrated easily.

 * I think we can manage to support all combinations of these,
 * optionally, in the protocol, and then see what actually gets
 * implemented and supported.
 */

Exactly.

--Rob



From atotic@ncsa.uiuc.edu  Sat Jan 15 13:38:25 1994 -0600 (CST)
Message-Id: <9401151938.AA09262@void.ncsa.uiuc.edu>
Date: Sat, 15 Jan 1994 13:38:25 -0600 (CST)
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: Slow HTTP Responses

> Do MacTCP and WinSock support UDP?
> 
> Bill

MacTCP does. One problem that I see with using UDP is that wwwlib would 
become more machine dependent, since there is no standard UDP interface (like
sockets for TCP).

Aleks




From atotic@ncsa.uiuc.edu  Sat Jan 15 13:48:05 1994 -0600 (CST)
Message-Id: <9401151948.AA09323@void.ncsa.uiuc.edu>
Date: Sat, 15 Jan 1994 13:48:05 -0600 (CST)
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: Slow HTTP Responses

Another idea for slow response handling:

- Keep on sending a "Redirect URL" (204?)directive back to the client, until 
server is ready to ship back the whole search.

- The only thing that needs to be done is to add another line to "Redirect"
  response, maybe something like "Redirect-message:".

And this would require very little work on everyones part, except the server
which would have to keep track of the queries.

Aleks



From beard@kcmetro.cc.mo.us  Sun Jan 16 20:08:21 1994 -0600 (CST)
Message-Id: <Pine.3.88.9401162051.B49698-0100000@kcmetro>
Date: Sun, 16 Jan 1994 20:08:21 -0600 (CST)
From: beard@kcmetro.cc.mo.us (beard@kcmetro.cc.mo.us)
Subject: subscribe

SUBSCRIBE Scott Beard




From Bas.Rokers@phil.ruu.nl  Mon Jan 17 15:49:53 1994 +0100
Message-Id: <9401171449.AA00696@laurel.phil.ruu.nl>
Date: Mon, 17 Jan 1994 15:49:53 +0100
From: Bas.Rokers@phil.ruu.nl (Bas Rokers)
Subject: list

please send me some info on list




From kevinh@eit.COM  Mon Jan 17 13:48:15 1994 PST
Message-Id: <9401172148.AA24422@eit.COM>
Date: Mon, 17 Jan 94 13:48:15 PST
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: ANNOUNCE: Announcing EIT's World-Wide Web Server


	Enterprise Integration Technologies (EIT) is pleased to announce
its renovated World-Wide Web server of corporate, technical, and network
information. EIT is a rapidly growing R&D and consulting company specializing
in information technology for electronic commerce, collaborative engineering,
and agile manufacturing. Recently, EIT was awarded a U.S. Technology
Reinvestment Program grant to lead the development of CommerceNet,
an $8 million project designed to help Silicon Valley businesses make
use of the coming "information superhighway".
	EIT's WWW server contains a broad range of information about the
company, its people, projects, and technologies, and some useful network
resources. Much of it is unique in content or in presentation.
	Highlights include:

          * a WAIS to HTML gateway,which lets you search the entire
            WWW server and then sort the output in a variety of ways,
          * a corporate overview and fact sheet, and information about
            our location and office (including a 3D-ish interactive
            office map),
          * pictures and biographies of EIT personnel,
          * MBONE and MPEG technical information,
          * software archives, including a multicast version of Mosaic
            for X, Mosaic remote-control programs, and the latest
            version of getsites (a Web server log analyzer),
          * information on folk music and Zoomer PDAs,
          * Kevin Hughes' Internet resources list,
          * public domain Gopher/FTP icons (which are currently used in
            Mosaic for X),
          * the updated report of Hypertext '93 Web happenings, including
            the fixed HTML DTD and more information on Ted Nelson,
          * hypertext archives of the Zoomer PDA, www-courseware, and
            www-literature mailing lists,
          * and much more!

	The URL is:

	http://www.eit.com/

	Previous links anyone may have had to MPEG information, getsites,
Hypertext '93 information or the Internet resources list should be changed to:

	http://www.eit.com/techinfo/mpeg/mpeg.html
	http://www.eit.com/software/getsites/getsites.html
	http://www.eit.com/reports/ht93/ht93.report.html
	http://www.eit.com/web/netservices.html

	"Entering the World-Wide Web: A Guide to Cyberspace" (the
October 1993 version) is available in HTML and PostScript form at:

	ftp://ftp.eit.com/pub/web.guide/

	Getsites is at:

	ftp://ftp.eit.com/pub/web.software/

	Public domain icons:

	ftp://ftp.eit.com/pub/web.icons/

	Questions involving the site should go to webmaster@eit.com;
questions concerning EIT and its projects should be emailed to info@eit.com.
	Enjoy!

        -- Brian Smithson
           Hypermedia Project Manager
           brian@eit.com

	-- Kevin Hughes
           EIT Webmaster
           kevinh@eit.com



From mcrae@ora.com  Mon Jan 17 14:45:23 1994 -0800
Message-Id: <199401172245.AA27768@rock.west.ora.com>
Date: Mon, 17 Jan 94 14:45:23 -0800
From: mcrae@ora.com (Christopher J. McRae)
Subject: Re: ANNOUNCEMENT: SIGWEB #4 (Sunnyvale, CA) 


> To: "Christopher J. McRae" <mcrae@ora.com>
> Subject: Re: ANNOUNCEMENT: SIGWEB #4 (Sunnyvale, CA) 
> In-Reply-To: Your message of "Fri, 14 Jan 94 14:55:45 PST."
>              <199401142255.AA15865@rock.west.ora.com> 
> Date: Fri, 14 Jan 94 18:15:52 -0500
> From: Simon E Spero <ses@tipper.oit.unc.edu>
> 
> Chris-
>  Is this one going to be multicast?
> Simon

  Well, I'm not sure if they have the facilities but we're working on
it.  It *will* be videotaped and so could be published after the fact.
However, I believe we have several recordings now which we have not yet
become available.  It's really just a matter of finding the time.
Hopefully, we'll soon have more support for doing such broadcasts.

Chris
-----------------------------------------------------------------------
Christopher McRae			            chrism@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036



From ses@tipper.oit.unc.edu  Mon Jan 17 17:55:50 1994 -0500
Message-Id: <9401172255.AA23546@tipper.oit.unc.edu>
Date: Mon, 17 Jan 94 17:55:50 -0500
From: ses@tipper.oit.unc.edu (Simon E Spero)
Subject: Re: ANNOUNCEMENT: SIGWEB #4 (Sunnyvale, CA) 

Care to put the videos (or at least the sound tracks) up as mpegs?

Simon



From mcrae@ora.com  Mon Jan 17 15:04:59 1994 -0800
Message-Id: <199401172304.AA28423@rock.west.ora.com>
Date: Mon, 17 Jan 94 15:04:59 -0800
From: mcrae@ora.com (Christopher J. McRae)
Subject: ADDENDUM: SIGWEB #4 (Sunnyvale, CA)


(See below for a copy of the original announcement)

Directions to SIG-WEB #4 at Amdahl Building 7 in Santa Clara:
------------------------------------------------------------------------------

Building 7 is in Amdahl's Santa Clara complex at the intersection of Central
and San Tomas Expressways.  The expressway interchange makes the directions
more complicated than they should be.

The address is 2251 Lawson Lane in Santa Clara.  Lawson Lane is the off-ramp
between northbound San Tomas and westbound Central.  From some directions it
may be more convenient to use the other entrance on Scott Blvd.

Enter at the Building 7 Lobby.  The SIG-WEB meeting will be in the cafeteria.

>From the East Bay:
   * Use I-880 or I-680 to southbound Montague Expressway
   * Get in the left lane before you reach 101. (you won't have time after it.)
   * Montague becomes San Tomas Expressway at Hwy 101
   * Take the next left at the light immediately after 101 at Scott Blvd
   * Turn right at the first driveway (Amdahl Bldg 8)
   * Turn right and drive around the Building 8/7 complex to the front of
     Building 7

>From the peninsula: 
   * Use 101 "southbound" (it's really eastbound).  The San Tomas Expressway
     exit is just after Great America in Santa Clara.  Exit right to
     southbound San Tomas Expressway.  (The other direction is called Montague
     so you can only go southbound on San Tomas.)
   * Unless the road is clear, there isn't time to safely get in the left
     lane for Scott Blvd so STAY IN THE RIGHT LANE
   * Go to the interchange at Central Expressway immediately after Scott
     Blvd.
   * Go underneath Central Expressway and exit right to eastbound Central
   * Exit again to northbound San Tomas.  Go underneath Central again.
   * Exit again... this off-ramp is Lawson Lane.  Amdahl Building 7 is the
     first driveway on the left.

>From San Jose:
   * Use 101 "northbound" to San Tomas Expressway in Santa Clara.  It's past
     the San Jose Airport and Trimble/De La Cruz exit.
   * Unless the road is clear, there isn't time to safely get in the left
     lane for Scott Blvd so stay in the right lane
   * follow instructions as above from the peninsula

>From parts of Santa Clara, Cupertino, etc:
   * Use northbound San Tomas Expressway
   * Go underneath Central Expressway and exit right immediately after it,
     as if you were going to westbound Central Expressway.
   * The off-ramp is Lawson Lane.  Make the first left to the Amdahl Bldg 7.





To: sig-web@library.ucsf.edu
Cc: gopher-news@boombox.micro.umn.edu, wais-talk@think.com,
    www-talk@info.cern.ch
Subject: ANNOUNCEMENT: SIGWEB #4 (Sunnyvale, CA)
Date: Fri, 14 Jan 94 14:55:45 -0800
From: "Christopher J. McRae" <mcrae>



				SIGWEB #4
		       Where Are We Going, and Why?


A panel discussion in two parts, featuring...
  Dale Dougherty        O'Reilly & Associates
  Barry Leiner          Internet Activities Board and USRA
  Clifford Lynch        UC Division of Library Automation
  Marty Tenenbaum       Enterprise Integration Technologies
  Terry Winograd        Stanford University

Part 1: The Global/National Internet Community
The first part of the meeeting will be a general discussion about the 
state-of-the-art in (internet-based) information systems and where it 
is/should-be/will-be going.  How do the Internet community and Internet-based 
information tools fit into the development of a National Information 
Infrastructure in the U.S.?  What is the future of Internet-based information 
systems?  What are the biggest obstacles facing us in the next five
years?

Part 2: The S.F. Bay Area Internet Community (i.e. SIGWEB)
The second part of this meeting will focus more narrowly on how the local
community, i.e. SIGWEB, fits into the vision(s) elucidated in the first part.
Our intention is for the meeting to help forge a more solid identity/mission
for the group.  Who are we?  Why do we show up for these meetings?  Where are
we each/all going?  What is the role of SIGWEB?  If there's anything else we
should be doing, who is willing/able to do it and what kind of help do you
need?

Prepared statements will be kept to a minimum, this is an audience
participation free-for-all.  Please come prepared to join in the discussion.

Location: Amdahl Corporation - Sunnyvale, CA
    Date: February 3rd, 1994
    Time: 2:00 - 5:00 PM

Directions to be announced shortly.

-----------------------------------------------------------------------
Christopher McRae			            chrism@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036



From mcrae@ora.com  Mon Jan 17 15:15:17 1994 -0800
Message-Id: <199401172315.AA28742@rock.west.ora.com>
Date: Mon, 17 Jan 94 15:15:17 -0800
From: mcrae@ora.com (Christopher J. McRae)
Subject: Re: ANNOUNCEMENT: SIGWEB #4 (Sunnyvale, CA) 


Simon Spero writes:
> Care to put the videos (or at least the sound tracks) up as mpegs?

  That's what I meant (even if not what I said :) when I wrote that
it's just a matter of finding the time (or a volunteer...).  
  Meanwhile, you can check out the On-Demand web server which Anders
Klemets has put up (MICE On-Demand seminars, or something like that).
I don't have the URL handy right now, but maybe one of you out there
does.

I wrote (in response to your initial message):
| > Chris-
| >  Is this one going to be multicast?
| > Simon
| 
| Well, I'm not sure if they have the facilities but we're working on
| it.  It *will* be videotaped and so could be published after the fact.
| However, I believe we have several recordings now which we have not yet
| become available.  It's really just a matter of finding the time.
| Hopefully, we'll soon have more support for doing such broadcasts.
| 

Chris
-----------------------------------------------------------------------
Christopher McRae			            chrism@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036



From chewleng@iti.gov.sg  Tue Jan 18 08:39:00 1994 +0800
Message-Id: <9401180034.AB11736@iti.gov.sg>
Date: Tue, 18 Jan 1994 08:39:00 +0800
From: chewleng@iti.gov.sg (ChewLeng BEH - Tel:)
Subject: binary file for Mac A/UX httpd

Please advise me where I can get a copy of the binary file for Mac A/UX
httpd server.  Thanks.





From J.Larmouth@iti.salford.ac.uk  Mon Jan 18 08:55:00 1994
Message-Id: <9401180907.AA03122@dxmint.cern.ch>
Date: 18 Jan 94 8:55
From: J.Larmouth@iti.salford.ac.uk (J.Larmouth@iti.salford.ac.uk)
Subject: Another pennyworth on layout issues

=========================================================================
E-mail from: Prof J Larmouth              J.Larmouth @ ITI.SALFORD.AC.UK
             Director                       Telephone: +44 61 745 5657
             IT Institute                         Fax: +44 61 745 8169
             University of Salford              Telex: 668680 (Sulib)
             Salford M5 4WT
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

To:     www-talk@info.cern.ch

Subject:      Another pennyworth on layout issues

I fully support trying to remain at the semantic level in mark-up,  but I
believe that horizontal and vertical white space is frequently used to
indicate semantic boundaries (for example,  "have a nice day" is very
different from a skating rink advert saying "have an ice day") not just
for "prettiness".

I recently complained on a different list that

                some text <i>and italic text</i> some more text

was rendered with one space after the italic text by Mosaic,  and with
two by Cello.   (I was asserting that Cello in this case had it wrong).
I was told that I should remove the space after the </i>!

At the very least,  browser writers need to agree on things such as this,
but it would actually be better if the HTML+ spec addressed this area in
at least terms of guidance.

So I reinforce the need to clearly specify when an element causes a
vertical break before and/or after itself.   Also to make clear that
"<P><P><P>" is exactly equivalent to "<P>",  and that a "<P>" at a point
where a vertical break has been forced by an adjacent element has no
effect,  and that elements like <b> and </i> do not in themselves cause
word (horizontal) breaks.

The vertical break issue produces another difference between Cello and
Mosaic in the formatting of:

                Some text <pre>      spaces      </pre> some more text.

In this case I think I regard Cello as correct.

Anyway,  my plea is for recognition that consistency on the insertion of
vertical breaks and/or horizontal breaks (spaces) around elements is
needed,  and that clear guidance should be present in the HTML+
definition,  irrespective of "religious" issues of what is page layout
and what is semantically significant formatting.

John L




From fenner@cmf.nrl.navy.mil  Tue Jan 18 14:37:03 1994 EST
Message-Id: <9401181937.AA11402@herman.cmf.nrl.navy.mil>
Date: Tue, 18 Jan 94 14:37:03 EST
From: fenner@cmf.nrl.navy.mil (William C Fenner)
Subject: First shot at sd via WWW

Well, I only have a directory, and only a rough one at that, but you can
point your browser at http://herman.cmf.nrl.navy.mil:8001/sd/ and see what
you think of my first shot.  Thanks to Tom Pusateri for his sd-listen code.

  Bill



From stumpf@informatik.tu-muenchen.de  Tue Jan 18 22:34:22 1994 +0100
Message-Id: <94Jan18.223435mesz.311353@hprbg5.informatik.tu-muenchen.de>
Date: Tue, 18 Jan 1994 22:34:22 +0100
From: stumpf@informatik.tu-muenchen.de (Markus Stumpf)
Subject: CGI and REMOTE_USER

Hello!

I have hacked rfc931 identification into the NCSA httpd-1.0 code.
No big deal :) I've used the rfc931.c from tcp-wrappers package and added
one line to httpd.c.

For those unfamiliar with rfc931: this makes a connection back to
the client host and tries to contact an indent daemon which provides
the username of the user owning the client socket.
We want to use this feature to gain more information on (mostly local)
users using scripts (for sending mail to webmaster, etc.).

I'd like to provide this information within the CGI scripts, but it looks
like REMOTE_USER is the wrong variable. To be true, I think REMOTE_USER
would be the correct one but the current name for what it is used for
is misleading and should be changed to AUTH_USER  :)
As like I read the spec the real user on the client side and the user
that should be authenticated mustn't necessarily be the same, right?

Any ideas, comment?

	\Maex
-- 
______________________________________________________________________________
 Markus Stumpf                        Markus.Stumpf@Informatik.TU-Muenchen.DE 
                                http://www.informatik.tu-muenchen.de/~stumpf/



From vinay@eit.COM  Tue Jan 18 13:54:31 1994 PST
Message-Id: <9401182154.AA08196@eit.COM>
Date: Tue, 18 Jan 94 13:54:31 PST
From: vinay@eit.COM (Vinay Kumar)
Subject: Re: sd access through a WWW server

In terms of UI issues, what is the difference between showing a Mosaic
Page full of sd announcements, and seeing the same by cranking up "sd" as
an external viewer. "sd" can take care of starting up appropriate media 
tools like vat, wb etc..as well. Maybe i am missing something here.

--
  Vinay Kumar
vinay@eit.com

---------------------------
> From www-talk-request@www0.cern.ch Fri Jan 14 23:01:12 1994
> 
> One of the other ideas that I've been tossing around is to create an
> application/x-sd type in HTTP, which simply passes the sd entry to a
> program that does whatever sd would do with that entry - then people
> running Mosaic with a proper ~/.mailcap could actually launch multicast
> applications by clicking on a hypertext link.
> 
> I prefer application/x-sd to application/x-csh because it allows
> the client to decide what application to run for each session type,
> instead of forcing the server to make assumptions.
> 
> If there is interest for this as well, I might crank it up a couple of
> notches on my priority list.
> 
>   Bill
> 



From phillips@cs.ubc.ca  Mon Jan 18 13:53:00 1994 -0800
Message-Id: <7208*phillips@cs.ubc.ca>
Date: 18 Jan 94 13:53 -0800
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: CGI and REMOTE_USER

Well, you could set AUTH_TYPE to RFC931 and REMOTE_USER to the
string returned by the ident daemon but that would be wrong.
While many ident daemons return the user name, that is not
a requirement.

How about sticking the ident daemon response into an environment
variable like REMOTE_IDENT.  You should base64 or otherwise
encode it in case it contains binary data which can be a headache
for shell CGI scripts and even C CGI programs can't handle nulls.

			-- George



From robm@ncsa.uiuc.edu  Tue Jan 18 15:57:47 1994 -0600
Message-Id: <9401182157.AA08158@void.ncsa.uiuc.edu>
Date: Tue, 18 Jan 1994 15:57:47 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI and REMOTE_USER

/*
 * CGI and REMOTE_USER  by Markus Stumpf (stumpf@informatik.tu-muenchen.de)
 *    written on Jan 18, 10:34pm.
 *
 * I have hacked rfc931 identification into the NCSA httpd-1.0 code.
 * No big deal :) I've used the rfc931.c from tcp-wrappers package and added
 * one line to httpd.c.
 * 
 * For those unfamiliar with rfc931: this makes a connection back to
 * the client host and tries to contact an indent daemon which provides
 * the username of the user owning the client socket.
 * We want to use this feature to gain more information on (mostly local)
 * users using scripts (for sending mail to webmaster, etc.).
 * 
 * I'd like to provide this information within the CGI scripts, but it looks
 * like REMOTE_USER is the wrong variable. To be true, I think REMOTE_USER
 * would be the correct one but the current name for what it is used for
 * is misleading and should be changed to AUTH_USER  :)
 * As like I read the spec the real user on the client side and the user
 * that should be authenticated mustn't necessarily be the same, right?
 */

Yes, that's right. Despite the fact that the name is misleading I would
prefer not to change the usage of REMOTE_USER since it is not backward
compatible change and we promised no more of those. What about using
REMOTE_LOGNAME or something like that for the identd-given username?

--Rob



From fenner@cmf.nrl.navy.mil  Wed Jan 19 11:39:35 1994 EST
Message-Id: <9401191639.AA15330@herman.cmf.nrl.navy.mil>
Date: Wed, 19 Jan 94 11:39:35 EST
From: fenner@cmf.nrl.navy.mil (William C Fenner)
Subject: Announcing sd-launch v1.0

Here's the README for sd-launch v1.0, a tool to integrate multicast
session launching with Mosaic.

sd-launch, v1.0

sd-launch is a helper for Mosaic (and other WWW browsers, and, indeed,
metamail) to allow launching of MBONE sessions via WWW (or MIME).

sd-launch is simply a C wrapper for two tcl programs:

sd_start.tcl, as supplied with sd v1.13 from LBL (available in
  ftp://ee.lbl.gov/conferencing/sd/)
sd_load.tcl, which parses an .sd_cache entry (or an application/x-sd
  datastream as supplied by my sd extension to Plexus) into the form
  that sd_start requires

To compile, you need tcl2c, which is part of the nv distribution
  (available in ftp://ftp.parc.xerox.com/pub/net-research/), or
  you could ask me for the already-processed .c files.

A binary is available for SunOS 4.1.3, the only platform I have
conveniently available to me.  I will make binaries available for
other platforms if they are supplied to me.

To use, put something similar to the following in your ~/.mailcap :

application/x-sd; sd-launch %s

William C. Fenner <fenner@cmf.nrl.navy.mil>

sd-launch is available via the web from
http://www.cmf.nrl.navy.mil/people/fenner/dist/sd-launch .
Make sure to use binary transfer mode if you're getting a
tar file or an executable.



From bajan@bunyip.com  Wed Jan 19 12:18:11 1994 -0500
Message-Id: <9401191718.AA14394@mocha.bunyip.com>
Date: Wed, 19 Jan 1994 12:18:11 -0500
From: bajan@bunyip.com (Alan Emtage)
Subject: New Gopher Indexing Service

[ This mail is being cross-posted to several groups... please excuse
  the possible multiple copies ]

As you might know, the archie Internet service indexes the contents of
anonymous FTP sites that wish to have their information advertised to the
Internet community. Each archie server is owned and operated by individual
volunteer sites around the Internet, using server software originally
developed at McGill University in Montreal. There are already well over 30
archie servers installed around the world and continued support for the
service is now provided through Bunyip Information Systems, a company
formed to ensure the availability of quality operational and software
support for Internet services. 

Over the past year we have made a number of improvements to the archie
system. In particular we've added support for a range of additional
information collections. With these new capabilities, it will be possible
in the coming months for archie server operators to offer a range of
additional useful information.

Perhaps the single most important addition we have planned for the near
future is a fully supported index of Gopher menu items. The code for this
new collection has been written and tested and will soon be made available
as a free upgrade to all existing archie servers. This upgrade will be
accompanied by a new direct Gopher frontend onto both the original archie
anonymous FTP database and the new Gopher menu item collection (and the
other collections we are planning as they become available). It is our
hope that a number of archie operators will choose to offer this new
Gopher index collection, bringing a new level of availability and
operational support to such services.

We are attempting to reach as many Gopher site operators as possible,
because we would like permission to include an index of their gopher tree
in the new collection. It has been our policy with the anonymous FTP
database to not include a site unless we have positive acknowlegement
from the site operator, and we intend to continue the policy with this
new collection. 

We have already contacted several hundred operators and have had very
positive feedback from them (2 refusals in 400 responses so far).

At the same time, we intend to follow existing practice in gopher
indexing where possible. Thus, we will try to work with site
administrators and tool builders to use existing indexing files where
possible, rather than place any additional load on the server by scanning
the entire tree. We will be working in concert with the community to
develop and open standard for such indices.

If you are a Gopher site operator, and if you give us your permission, we
will access your gopher site on a periodic basis to create an index of
the menu items accessible from your site. The index will be built by one
of the cooperating archie sites, that will in turn make it available to
all the other archie sites. Note that the archie system permits this
sharing of collected information in a predigested format, lowering the
load we place on the network and also minimizing the interactions needed
at your site.

We hope that this new index will be of use both to gopher site
administrators (in advertising the existence and contents of your site)
and other members of the Internet community (in providing a localized,
reliable directory of gopherspace which enjoys full operational support).
Your comments and feedback on the service as we bring it up are most
welcome.


TO SAY YES (OR ASK QUESTIONS):
-----------------------------

If you would like to have your site indexed, please send the particulars
of your gopher site to:

    gopher-index-ack@bunyip.com

We're looking for the following information:

     <gopher name>
     Host: <fully qualified internet address>
     Port: 
     Path: 
     Admin: <name, e-mail address>


TO SEND COMMENTS:
----------------

If you have any particular concerns with this concept and/or would like
to let us know why you don't want your gopher site indexed, please feel
free to send comments to

   gopher-index-concern@bunyip.com



Thanks in advance!

The Gang at Bunyip Information Systems



From robm@ncsa.uiuc.edu  Wed Jan 19 13:39:51 1994 -0600
Message-Id: <9401191939.AA22244@void.ncsa.uiuc.edu>
Date: Wed, 19 Jan 1994 13:39:51 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI and REMOTE_USER

/*
 * Re: CGI and REMOTE_USER  by Markus Stumpf (stumpf@informatik.tu-muenchen.de)
 *    written on Jan 19, 12:08am.
 *
 * 
 * |> REMOTE_LOGNAME or something like that for the identd-given username?
 * 
 * I have used the REMOTE_LOGNAME variable.
 * 
 */

Is there anyone who objects to REMOTE_LOGNAME being added to the CGI spec as
the RFC931 username of the remote user? This variable will be completely
optional as to whether a server wants to support it or not.

--Rob



From jim@eies.njit.edu  Thu Jan 20 13:48:08 1994 EST
Message-Id: <9401201848.AA07167@eies.njit.edu>
Date: Thu, 20 Jan 94 13:48:08 EST
From: jim@eies.njit.edu (James Whitescarver)
Subject: CCCC @ NJIT CMC/CSCW

I'm back........

I've been out of this group for nearly a year.  It's been an exciting year, 
where many of our visions have been realized.  We had serious hacker problem
and were isolated for some time.  Fortunatly our public access at
telnet:www@www.njit.edu remained active most of the time.

My promises to release a new version of the NJIT screen mode browser have
not been kept.  fortunately, linus has filled the gap nicely.  We will
have a new release shorly, but still cannot release source code.

I'm writing for two reasons. First, we have a perminant new home page
at <http://it.njit.edu/njIT/Department/CCCC/njit_info.html>.  This
includes links to the experimental gateway to the distributed communications
data base for our Electronic Information Exchange System.  We expect to annouce
NJIT entry points soon (boy are we slow).

In addition, we have many projects involving www at this time.  We are
seeking consultants to help set up authentication gateways, servers and clients
with encription for various platforms.  Please send me product information,
resumes or proposals and pointers to relivant information.  Thanks!

Nearly 20 years of research and delvelopment of group systems at NJIT is
the primary resourse we have to offer the WWW community.  Please involve
me in any CMC/CSCW related activities on the web.  WWW is enabling us to
achieve many of our long term objectives.  We are determined to return some
of this value to the web community at large.  We are witnessing the 
blossoming of the information garden-- the integration of computer-mediated
communications with WWW we invision as the fertilizer to make it grow,
and reduce the entropy of our group memory.

On a personal note, I am looking for funding to attend the WWW conference.
If you have, or know of, an opportunity to consult there, I'd appreciate
any leads-- double thanks!  My expertise includes interfacing anything to
the web, Smalltalk, object-oriented methodology, frameworks for the
collective intelligence, and computer-mediated communications-- not to
mention humility :)

Do check into our new home page and send me any feedback.  (the EIES
gateway is unavailable 4 to 5 AM eastern standard time).

Kudos to all for an exceedingly fruitful year of web development!

James Whitescarver, Asst. Dir., CCCC, NJIT, Newark NJ 07102, USA, 201-596-2937
jim@eies.njit.edu



From kchang@ncsa.uiuc.edu  Thu Jan 20 13:34:38 1994 CST
Message-Id: <9401201934.AA07849@landrew.ncsa.uiuc.edu>
Date: Thu, 20 Jan 94 13:34:38 CST
From: kchang@ncsa.uiuc.edu (Kenneth Chang)
Subject: Looking for a list of special characters supported by HTML

I'm looking for a list of non-ASCII characters supported by HTML. I found
the ISO Latin 1 character entities list at CERN, but I also need characters
such as em dashes, en dashes, etc. Can you include these into HTML
documents??

Thanks.
--ken chang
  NCSA Publications





From montulli@stat1.cc.ukans.edu  Thu Jan 20 13:50:02 1994 CST
Message-Id: <9401201950.AA27008@stat1.cc.ukans.edu>
Date: Thu, 20 Jan 94 13:50:02 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Looking for a list of special characters supported by HTML

> 
> I'm looking for a list of non-ASCII characters supported by HTML. I found
> the ISO Latin 1 character entities list at CERN, but I also need characters
> such as em dashes, en dashes, etc. Can you include these into HTML
> documents??
> 
> Thanks.
> --ken chang
>   NCSA Publications
> 
> 
> 

Here you go,
:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************

<html>
<Head>
<title>Test of ISO LATIN1 charactor set</title>
</Head>
<body>

<h1>ISO LATIN1 text entities</h1>
<ul>
  <li>"&AElig;",	/* capital AE diphthong (ligature) */ 
  <li>"&Aacute;",	/* capital A, acute accent */ 
  <li>"&Acirc;",	/* capital A, circumflex accent */ 
  <li>"&Agrave;",	/* capital A, grave accent */ 
  <li>"&Aring;",	/* capital A, ring */ 
  <li>"&Atilde;",	/* capital A, tilde */ 
  <li>"&Auml;", 	/* capital A, dieresis or umlaut mark */ 
  <li>"&Ccedil;",	/* capital C, cedilla */ 
  <li>"&ETH;", 		/* capital Eth, Icelandic */ 
  <li>"&Eacute;",	/* capital E, acute accent */ 
  <li>"&Ecirc;",	/* capital E, circumflex accent */ 
  <li>"&Egrave;",	/* capital E, grave accent */ 
  <li>"&Euml;",		/* capital E, dieresis or umlaut mark */ 
  <li>"&Iacute;",	/* capital I, acute accent */ 
  <li>"&Icirc;",	/* capital I, circumflex accent */ 
  <li>"&Igrave;",	/* capital I, grave accent */ 
  <li>"&Iuml;",		/* capital I, dieresis or umlaut mark */ 
  <li>"&Ntilde;",	/* capital N, tilde */ 
  <li>"&Oacute;",	/* capital O, acute accent */ 
  <li>"&Ocirc;",	/* capital O, circumflex accent */ 
  <li>"&Ograve;",	/* capital O, grave accent */ 
  <li>"&Oslash;",	/* capital O, slash */ 
  <li>"&Otilde;",	/* capital O, tilde */ 
  <li>"&Ouml;",		/* capital O, dieresis or umlaut mark */ 
  <li>"&THORN;",	/* capital THORN, Icelandic */ 
  <li>"&Uacute;",	/* capital U, acute accent */ 
  <li>"&Ucirc;",	/* capital U, circumflex accent */ 
  <li>"&Ugrave;",	/* capital U, grave accent */ 
  <li>"&Uuml;",		/* capital U, dieresis or umlaut mark */ 
  <li>"&Yacute;",	/* capital Y, acute accent */ 
  <li>"&aacute;",	/* small a, acute accent */ 
  <li>"&acirc;",	/* small a, circumflex accent */ 
  <li>"&aelig;",	/* small ae diphthong (ligature) */ 
  <li>"&agrave;",	/* small a, grave accent */ 
  <li>"&amp;",		/* ampersand */ 
  <li>"&aring;",	/* small a, ring */ 
  <li>"&atilde;",	/* small a, tilde */ 
  <li>"&auml;",		/* small a, dieresis or umlaut mark */ 
  <li>"&ccedil;",	/* small c, cedilla */ 
  <li>"&eacute;",	/* small e, acute accent */ 
  <li>"&ecirc;",	/* small e, circumflex accent */ 
  <li>"&egrave;",	/* small e, grave accent */ 
  <li>"&emsp;",		/* emsp, em space - not collapsed */
  <li>"&ensp;",		/* ensp, en space - not collapsed */
  <li>"&eth;",		/* small eth, Icelandic */ 
  <li>"&euml;",		/* small e, dieresis or umlaut mark */ 
  <li>"&gt;",		/* greater than */ 
  <li>"&iacute;",	/* small i, acute accent */ 
  <li>"&icirc;",	/* small i, circumflex accent */ 
  <li>"&igrave;",	/* small i, grave accent */ 
  <li>"&iuml;",		/* small i, dieresis or umlaut mark */ 
  <li>"&lt;",		/* less than */ 
  <li>"&nbsp;",		/* nbsp, non breaking space */
  <li>"&ntilde;",	/* small n, tilde */ 
  <li>"&oacute;",	/* small o, acute accent */ 
  <li>"&ocirc;",	/* small o, circumflex accent */ 
  <li>"&ograve;",	/* small o, grave accent */ 
  <li>"&oslash;",	/* small o, slash */ 
  <li>"&otilde;",	/* small o, tilde */ 
  <li>"&ouml;",		/* small o, dieresis or umlaut mark */ 
  <li>"&quote;",	/* quote, '"' */
  <li>"&szlig;",	/* small sharp s, German (sz ligature) */ 
  <li>"&thorn;",	/* small thorn, Icelandic */ 
  <li>"&uacute;",	/* small u, acute accent */ 
  <li>"&ucirc;",	/* small u, circumflex accent */ 
  <li>"&ugrave;",	/* small u, grave accent */ 
  <li>"&uuml;",		/* small u, dieresis or umlaut mark */ 
  <li>"&yacute;",	/* small y, acute accent */ 
  <li>"&yuml;",		/* small y, dieresis or umlaut mark */ 
</ul>

<address><a href="http://info.cern.ch/hypertext/WWW/People.html#Montulli">LM</a></address>
<body>
</html>



From montulli@stat1.cc.ukans.edu  Thu Jan 20 15:13:57 1994 CST
Message-Id: <9401202113.AA43077@stat1.cc.ukans.edu>
Date: Thu, 20 Jan 94 15:13:57 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: CCCC @ NJIT CMC/CSCW

> 
> On a personal note, I am looking for funding to attend the WWW conference.
> If you have, or know of, an opportunity to consult there, I'd appreciate
> any leads-- double thanks!  My expertise includes interfacing anything to
> the web, Smalltalk, object-oriented methodology, frameworks for the
> collective intelligence, and computer-mediated communications-- not to
> mention humility :)
> 
I'm looking for travel funding too, although I'm not as sure
about my qualifications. :)  A web consortium would sure be
handy for this sort of thing. 

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From robm@ncsa.uiuc.edu  Thu Jan 20 15:22:16 1994 -0600
Message-Id: <9401202122.AA07829@void.ncsa.uiuc.edu>
Date: Thu, 20 Jan 1994 15:22:16 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI and REMOTE_USER

/*
 * Re: CGI and REMOTE_USER
 *    written on Jan 19, 11:49am.
 *
 * I only object to the name.  It should really be something like
 * REMOTE_IDENT.  We don't want to promote the fallacies that
 * RFC931 gives you the username and, especially, that the information
 * it gives you is useful for anything but audit information.

Hmmm, a good point. It is trivial to forge this information and LOGNAME
implies that the variable is really the remote username.

Okay, so how about REMOTE_IDENT? Anyone object to that?

 * Along those lines, it should probably be base64 encoded since
 * it could be binary.
 */

Why? I see nothing in the RFC which implies that it could be binary, in
fact, the reply is specified to be terminated by a CR/LF pair. Is there
something I'm not reading correctly? Similarly, rfc931.c uses fgets to read
the server's reply which would probably be munged horribly by binary output
from the server.

--Rob



From dsr@hplb.hpl.hp.com  Fri Jan 21 12:56:14 1994 GMT
Message-Id: <9401211256.AA19233@manuel.hpl.hp.com>
Date: Fri, 21 Jan 94 12:56:14 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Looking for a list of special characters supported by HTML

> I'm looking for a list of non-ASCII characters supported by HTML. I found
> the ISO Latin 1 character entities list at CERN, but I also need characters
> such as em dashes, en dashes, etc. Can you include these into HTML
> documents??

These characters are part of the HTML+ spec, as described  in:

        ftp://15.254.100.100/pub/draft-raggett-www-html-00.ps

        ftp://15.254.100.100/pub/htmlplus.dtd.txt

The Postscript write-up is some what dated, see the DTD for an upto date
description of HTML+. A new write-up is in preparation, and any feedback
is welcome.

Dave Raggett



From guenther.fischer@hrz.tu-chemnitz.de  Fri Jan 21 09:27:54 1994 +0100 (MET)
Message-Id: <9401210827.AA16053@flash1.hrz.tu-chemnitz.de>
Date: Fri, 21 Jan 1994 09:27:54 +0100 (MET)
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: I didn't found another address

Hi,
a question about the www cern servers. There are many important
documents and I want to cache those servers but they don't
give the Last-modified field. 
Can I hope for a server upgrade next time?

Please bounce this mail to the right place - thanks.

	~Guenther

-- 
Name:      Guenther Fischer / Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361     / mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>




From stumpf@informatik.tu-muenchen.de  Fri Jan 21 20:51:05 1994 +0100
Message-Id: <94Jan21.205110mesz.311353@hprbg5.informatik.tu-muenchen.de>
Date: Fri, 21 Jan 1994 20:51:05 +0100
From: stumpf@informatik.tu-muenchen.de (Markus Stumpf)
Subject: Using  ftp:// vs http://

Hello!

I know this is a very tricky point ...

First some explanations:
We have a workstation cluster with about 150 machines that have
full email, news, irc, http and gopher access (via gateways) but no
telnet or ftp. We also run a ftp archive with about 3 GB (and growing)
software that we try to maintain very well and keep up to date.
This cluster is used for students in their first/second year. 
We don't want to allow the users in this cluster ftp access until they
know more about the net and use the resources more "seriously". All
students after the second year (sometimes earlier) get access to fully
connected machines.
Therefor we do not gate ftp URLs but only  gopher and http.

Now I have come across the problem that some sites allow complete
access to their FTP areas via http URLs.
The funny thing about that is that some of the sites are really restricted
about access to their anonymous ftp archive and one can easily bypass
this using the httpd instead of the ftpd.

Now my plea: Could we please use each protocol for what it was designed
for? I.e. use http for "information" and ftp for "files". I know that
the difference is rather fluent ...

	\Maex
-- 
______________________________________________________________________________
 Markus Stumpf                        Markus.Stumpf@Informatik.TU-Muenchen.DE 
                                http://www.informatik.tu-muenchen.de/~stumpf/



From luotonen@ptsun00.cern.ch  Fri Jan 21 21:33:44 1994 +0100
Message-Id: <9401212033.AA14960@ptsun03.cern.ch>
Date: Fri, 21 Jan 94 21:33:44 +0100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: I didn't found another address


> Hi,
> a question about the www cern servers. There are many important
> documents and I want to cache those servers but they don't
> give the Last-modified field. 
> Can I hope for a server upgrade next time?

Yes, next one will give that, and also Server:, Content-Length:,
have HEAD, CGI/1.0 scripts with both GET and POST, will do setuid
to authenticated user uid as an option, and a lot more.

Cheers,
--
Ari Luotonen		 |
World-Wide Web Project	 |
CERN			 | phone: +41 22 767 8583
CH - 1211 Geneve 23	 | email: luotonen@dxcern.cern.ch



From tkevans@fallst.es.dupont.com  Fri Jan 21 15:49:41 1994 -0500 (EST)
Message-Id: <m0pNSnV-0000ZAC@fallst.es.dupont.com>
Date: Fri, 21 Jan 1994 15:49:41 -0500 (EST)
From: tkevans@fallst.es.dupont.com (Tim Evans)
Subject: Problems w/WAIS Interface

I'm having some problems with a newly built WAIS index (previously
built ones are working ok, so this is sorta wierd).  I've built the
new index on both a Sun 4 and an RS/6000, and am having trouble
with both indices.  I'm using freeWAIS 0.202 and Mosaic 2.1 with
direct WAIS support built in.

Attempting to access the index built on the RS/6000 fails with the
following in the server log file:

cannot set readonly PWD=/home/tkevans
15527: 2: Jan 21 14:02:44 1994: 100: INFO locked, waiting since 1 seconds
15527: 3: Jan 21 14:02:44 1994: -2: Warning - lockfile /tmp/INFO.lock won't go away after 1 seconds, not reindexing.
15527: 4: Jan 21 14:02:44 1994: 1: Accepted connection from: fallst.es.dupont.com [138.196.252.3], freeWAIS Release 0.2 beta
15527: 5: Jan 21 14:02:44 1994: 3: Search! Database: lookup.src, Seed Words: brik
15527: 6: Jan 21 14:02:44 1994: 100: query lock can't be set
15527: 7: Jan 21 14:02:44 1994: -1: can't open the word hash file /nus-sources/lookup.dct

15527: 8: Jan 21 14:02:44 1994: 4: Returned 0 results.
15527: 9: Jan 21 14:02:44 1994: 4: Returned 0 results.  Aww.
15527: 10: Jan 21 14:02:44 1994: 2: Done handling client

Note the gobblydegook in the line referring the the hash file.

Now, when I access the index built on the Sun 4, the search also fails
but different errors appear in the waisserver logfile:

2881: 0: Jan 21 15:24:31 1994: 1: Accepted connection from: fallst [138.196.252.3], freeWAIS Release 0.2 beta
2881: 1: Jan 21 15:24:31 1994: 3: Search! Database: lookup.src, Seed Words: brik
2881: 2: Jan 21 15:24:31 1994: -2: Warning: couldn't open /data/wais-sources/lookup.syn - translation disabled
2881: 3: Jan 21 15:24:31 1994: 100: Word brik Syn brik
2881: 4: Jan 21 15:24:31 1994: -2: Dangling File /nu/internet/database/anomaly.sbs.risc.net in database /data/wais-sources/lookup.
2881: 5: Jan 21 15:24:31 1994: 4: Returned 0 results.
2881: 6: Jan 21 15:24:31 1994: 4: Returned 0 results.  Aww.
2881: 7: Jan 21 15:24:32 1994: 2: Done handling client

Note the reference to the "dangling file".

Anybody have any ideas?  Thanks.

-- 
Tim Evans                     |    E.I. du Pont de Nemours & Co.
tkevans@eplrx7.es.dupont.com  |    Experimental Station
(302) 695-9353/7395           |    P.O. Box 80357
EVANSTK AT A1 AT ESVAX        |    Wilmington, Delaware 19880-0357



From bianco@giant.larc.nasa.gov  Fri Jan 21 21:00:46 1994 GMT
Message-Id: <199401212100.VAA07927@MiSTy.larc.nasa.gov>
Date: Fri, 21 Jan 1994 21:00:46 GMT
From: bianco@giant.larc.nasa.gov (David Bianco)
Subject: Using  ftp:// vs http://

Markus Stumpf writes:
 > 
 > Now my plea: Could we please use each protocol for what it was designed
 > for? I.e. use http for "information" and ftp for "files". I know that
 > the difference is rather fluent ...
 > 

"Designed for?"  Well, we do operate a pretty large software and file
repository here (although the total capacity is measured in
terrabytes, not mere gigs 8-) I provide access to it via http for pure
convenience's sake.  Different sites have different needs, so simply
asking everyone to do the same thing is not likely to succeed.  I
certainly wouldn't be willing to give up my HTTP gateway.  It's too
useful...

	David




From FisherM@is3.indy.tce.com  Fri Jan 21 17:40:00 1994 PST
Message-Id: <2D4085A9@MSMAIL.INDY.TCE.COM>
Date: Fri, 21 Jan 94 17:40:00 PST
From: FisherM@is3.indy.tce.com (Fisher Mark)
Subject: Configuring CERN httpd v2.14 as WAIS Gateway


I must have my stupid cap on, but I can't figure out how to properly 
configure the CERN v2.14 httpd ("WWWDaemon.. 2.14" and "WWWLibrary ... 
2.14") as a WAIS gateway on my SunOS 4.1.3 system after upgrading from 
v2.07.  It recognizes my URL as a WAIS gateway URL, but then tries to 
process it as an "htbin" search request.  Any help?
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From jonm@ncsa.uiuc.edu  Fri Jan 21 18:29:35 1994 CST
Message-Id: <9401220029.AA02792@void.ncsa.uiuc.edu>
Date: Fri, 21 Jan 94 18:29:35 CST
From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
Subject: Re: Using  ftp:// vs http://

I tried to resist!  Honestly... :^)   I'll be nice...

At 08:51 PM 1/21/94 +0100, Markus Stumpf wrote:

>Now my plea: Could we please use each protocol for what it was designed
>for? I.e. use http for "information" and ftp for "files". I know that
>the difference is rather fluent ...

The difference is non-existent in many cases and should _never_ be 
emphasized (IMHO).  The whole point of the Web is to make information
(often necessarily contained in files) transparently available to people.
This is especially true for novice users since you can surround the links
with useful text... 

Why don't use just use access authorization to only allow http 
accesses to the FTP data from the fully connected ("trusted" 2nd year student)
machines?  This would give you all the potential benefits without losing 
what you seem to consider a necessary restriction...

-Jon

---
Jon E. Mittelhauser (jonm@ncsa.uiuc.edu)
Research Programmer, NCSA                          (NCSA Mosaic for MS Windows)
More info <a href="http://www.ncsa.uiuc.edu/SDG/People/jonm/jonm.html">here</a>




From fielding@simplon.ICS.UCI.EDU  Fri Jan 21 22:19:55 1994 -0800
Message-Id: <9401212220.aa15039@paris.ics.uci.edu>
Date: Fri, 21 Jan 1994 22:19:55 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Announcing wwwstat-0.2 -- an access log summary program 


This message is to announce the availability of wwwstat Version 0.2 --
a program for analyzing NCSA httpd_1.0 server access logs and printing
an HTML-formatted summary report.  The program is written in Perl
and, once customized for your site, should work on any UNIX-based system
with Perl 4.019 or better.  The program is in the public domain (i.e. FREE).

As an example of what wwwstat can do for you, look
    <A HREF="http://www.ics.uci.edu/Admin/wwwstats.html"> here </A>
to see UC Irvine's Department of Information & Computer Science
WWW server statistics.

For more information and access to the wwwstat-0.2 distribution,
point your World-Wide Web client at

    <A HREF="http://www.ics.uci.edu/WebSoft/wwwstat/"> wwwstat-0.2 </A>.

For those of you without offsite http access but with ftp access, wwwstat is
also available via anonymous ftp at:

    <ftp://liege.ics.uci.edu/pub/arcadia/wwwstat/wwwstat-0.2.tar.Z>

Version History:

Known problems            [Due to limits of info in access_log]
     Assumes that a file's size has not changed since the access.
     Cannot estimate the transmission of bytes from scripts.
     Ignores the fact that some accesses may have been denied.
     Ignores any Redirection directives.

Version 0.2                                             January 21, 1994 
     Added support for the /~username form of files. 
     Added general support for Alias and ScriptAlias configurations. 
     Now reads the server config file to get site configuration. 
     Sped up the process by caching file sizes (fewer file stats). 
     Added options to display full IP addresses in subdomain listing. 
     Expanded some form field sizes.   Now sorts archive section by name.

Version 0.1                                             January 14, 1994 
     Added support for HTML output.
     Added reversed subdomain statistics.
     Added the logic for grouping files in archive sections.
     Rewrote the whole damn thing.

Version 0.0
     Originally from fwgstat 0.35 (jem@sunsite.unc.edu) with all the
     extra options stripped out and many bugs fixed.  In turn,
     fwgstat was heavily based on xferstats, which is packaged with
     the Wuarchive FTP daemon.  Fwgstat is good for multi-server stats.


If you have any suggestions, bug reports, fixes, or enhancements,
send them to me at <fielding@ics.uci.edu>.  Unless some serious bugs
are discovered, I do not anticipate making any more releases until
the NCSA httpd access_log format changes.

Have fun,


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From pini@bgumail.bgu.ac.il  Sun Jan 23 16:10:20 1994 +0200 (IST)
Message-Id: <Pine.3.85.9401231620.A14818-0100000@bgumail.bgu.ac.il>
Date: Sun, 23 Jan 1994 16:10:20 +0200 (IST)
From: pini@bgumail.bgu.ac.il (Pini Albilia)
Subject: Inlined image format

Hello,
Is there a way to include Inlined image not in gif format?
-Pini






From tkevans@fallst.es.dupont.com  Sun Jan 23 09:30:48 1994 -0500 (EST)
Message-Id: <m0pO5px-0000Z8C@fallst.es.dupont.com>
Date: Sun, 23 Jan 1994 09:30:48 -0500 (EST)
From: tkevans@fallst.es.dupont.com (Tim Evans)
Subject: Problems Accessing ismap links Outside Local Domain

We have an internal Web running (not accessible to outside
world) and have run into a curious problem.  The server is the NCSA
httpd server, last-but-one-to-current release.  We have made no effort to
impose any sort of access limits and are using the default 
access.conf file, as distributed with the NCSA server.  The server
is an RS/6000 running AIX 3.2.5.

All hyperlinks on the server, as well as links to other servers
within our corporate network, work perfectly well *EXCEPT* links
to an 'ismap' document.  Except for this one document, all other
hyperlinks work from anywhere in the corporate network.
This ismap document, however, seems only to be accessible
within the immediate local subdomain, and cannot be accessed outside it.
No errors appear in the error log when users in other subdomains
attempt to access it, and no attempts to access this
document from outside the local subdomain appear in the access
log.

If anyone has ideas, please send them along.  Thanks.

-- 
Tim Evans                     |    E.I. du Pont de Nemours & Co.
tkevans@eplrx7.es.dupont.com  |    Experimental Station
(302) 695-9353/7395           |    P.O. Box 80357
EVANSTK AT A1 AT ESVAX        |    Wilmington, Delaware 19880-0357



From relihanl@ul.ie  Sun Jan 23 16:54:20 1994 +0000 (WET)
Message-Id: <Pine.3.05.9401231620.A15784-b100000@itdsrv1.ul.ie>
Date: Sun, 23 Jan 1994 16:54:20 +0000 (WET)
From: relihanl@ul.ie (Liam Relihan)
Subject: Anchors/Fragments ?

hello all...

I'm doing some work in formalizing W3 by looking at it from an IR point of
view. I reckon that the objects types in W3 are those types that are
addressable by URLs:

1. nodes (which include ordinary nodes and index nodes) which may be
   addressed with a path

2. virtual nodes which may be addressed with a "?"

3. fragments of nodes which may be addressed with a "#"

Now, I am unsure about the status of the "fragment-id" (or "#"). TBL's URL
spec states...

"This represents a part of, fragment of, or a sub-function within, an
object or object. Its syntax and semantics are defined by the application 
responsible for the object, or the specification of the content type of
the object."

Now, if I have a URL such as the following 

http://qwerty.ie/abc.html#parta

, from an IR point of view, retrieval of the (sub-)object identified by that
URL means the following:

1. retrieve the document identified by "http://qwerty.ie/abc.html" from
   the server.
2. Search the document until the anchor named "parta" is found
3. Display the document and move the pointer to "parta"


My questions are...
is this sequence valid only for the HTTP scheme ?
if a fragment-id were used for another scheme (say NNTP), might the
retrieval sequence be different ?

Is fragment-id always meant for the identification of parts of objects
(sub-objects) within the context of a whole enclosing object ?

If not, why isn't "?" good enough for acquiring addressable sub-objects ?



Apologies if my questions are slightly unclear. 
Unfortunately this stuff can be subtle at times :-)

Liam

 --
 Liam Relihan,               |               Voice: +353-61-333644 ext.5015
 CSIS, Schumann Building,    |/|                        Fax: +353-61-330876
 University Of Limerick,       |                     E-mail: relihanl@ul.ie
 Ireland.                      http://itdsrv1.ul.ie/PERSONNEL/lrelihan.html




From P.Lister@cranfield.ac.uk  Mon Jan 24 11:06:28 1994 GMT
Message-Id: <9401241106.AA02151@xdm039.ccc.cranfield.ac.uk>
Date: Mon, 24 Jan 94 11:06:28 GMT
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Re: Anchors/Fragments ?


I asked a connected question some time ago. The net.wisdom was that
'//fred/foo?bar' implies a query 'bar' sent to server 'fred' in the
context of path 'foo'; interpretation of 'bar' is up to fred, and the
browser is not expected to do anything special to the reply, other than
display it normally.

'//fred/foo#bar' means that the browser requests 'foo' from fred
(without mentioning 'bar'), and displays the reply in manner
illustrating the part of the reply marked 'bar' (e.g. an HTML anchor
with includes 'name="bar"' ).

Fragments (#) are interpreted by the browser, queries (?) by the
server. The difference is *not* semantic in terms of document
structure: a 'query' need not be a question per se, it could
semantically be a logical 'fragment'; equally, a 'fragment' could be
something more normally associated with a question,  (e.g. if the reply
were a plain text file, the browser might insert a linked list of
anchors around each occurence of 'bar').

Peter Lister                             Email: p.lister@cranfield.ac.uk
Computer Centre, Cranfield University    Voice: +44 234 754200 ext 2828
Cranfield, Bedfordshire MK43 0AL UK        Fax: +44 234 750875
--- Widget. It's got a widget. A lovely widget. A widget it has got. ---



From appel@cih.hcuge.ch  Mon Jan 24 18:02:26 1994 +0100
Message-Id: <1479*/S=appel/OU=cih/O=hcuge/PRMD=switch/ADMD=arcom/C=ch/@MHS>
Date: Mon, 24 Jan 1994 18:02:26 +0100
From: appel@cih.hcuge.ch (Ron D. Appel)
Subject: looking for HTML page




From mcrae@ora.com  Mon Jan 24 11:56:56 1994 -0800
Message-Id: <199401241956.AA16419@rock.west.ora.com>
Date: Mon, 24 Jan 94 11:56:56 -0800
From: mcrae@ora.com (Christopher McRae)
Subject: Re: Inlined image format 

Pini Albilia writes:
> Is there a way to include Inlined image not in gif format?

  There used to be a patch for Xmosaic which allowed for inlining TIFF
images.  The work was done by Cheong Ang at UCSF.  I think you should be
able to get it via ftp to ftp.library.ucsf.edu.  If you're interested in
using some other image format, you might look at Cheong's code as an
example of how to modify Xmosaic.

Chris
-----------------------------------------------------------------------
Christopher McRae			            mcrae@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036





From jonm@ncsa.uiuc.edu  Mon Jan 24 14:12:02 1994 CST
Message-Id: <9401242012.AA11417@void.ncsa.uiuc.edu>
Date: Mon, 24 Jan 94 14:12:02 CST
From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
Subject: Re: Inlined image format 

At 11:56 AM 1/24/94 -0800, Christopher McRae wrote:
>Pini Albilia writes:
>> Is there a way to include Inlined image not in gif format?
>
>  There used to be a patch for Xmosaic which allowed for inlining TIFF
>images.  The work was done by Cheong Ang at UCSF.  I think you should be
>able to get it via ftp to ftp.library.ucsf.edu.  If you're interested in
>using some other image format, you might look at Cheong's code as an
>example of how to modify Xmosaic.

The advantage to using Gifs, however, is that the document will be viewable
in all browsers that support inlined images.  If you add a new format, the
document will only be viewble by a very limited set of people...

At least in terms of Mosaic, we have currently only agreed to support gifs 
and xbms on all three platforms...

-Jon

---
Jon E. Mittelhauser (jonm@ncsa.uiuc.edu)
Research Programmer, NCSA                          (NCSA Mosaic for MS Windows)
More info <a href="http://www.ncsa.uiuc.edu/SDG/People/jonm/jonm.html">here</a>




From relihanl@ul.ie  Mon Jan 24 23:03:10 1994 +0000 (WET)
Message-Id: <Pine.3.05.9401242309.A28649-a100000@itdsrv1.ul.ie>
Date: Mon, 24 Jan 1994 23:03:10 +0000 (WET)
From: relihanl@ul.ie (Liam Relihan)
Subject: DELETE Method

Hi...

Has anyone implemented the DELETE method ?

Is it possible to delete (according to the HTTP spec):

1. virtual nodes ?

2. index nodes ?

The best HTTP spec I have is that written by TBL, dated "5 Nov 1993" and
marked "internet draft". There are just 3 lines on the DELETE method.

Is there a more up-to-date version around ?

Liam

--
 Liam Relihan,               |               Voice: +353-61-333644 ext.5015
 CSIS, Schumann Building,    |/|                        Fax: +353-61-330876
 University Of Limerick,       |                     E-mail: relihanl@ul.ie
 Ireland.                      http://itdsrv1.ul.ie/PERSONNEL/lrelihan.html




From DLCROSS@ucs.indiana.edu  Mon Jan 24 18:51:07 1994 EST
Message-Id: <9401242351.AA16948@dxmint.cern.ch>
Date: Mon, 24 Jan 94 18:51:07 EST
From: DLCROSS@ucs.indiana.edu (DLCROSS@ucs.indiana.edu)
Subject: NCSA Mosaic For MS Windows 1.0

Does anyone know...
ew{Ph|E:e Print... option from the FILE menu is fuxJ{



From robm@ncsa.uiuc.edu  Mon Jan 24 17:40:16 1994 -0600
Message-Id: <9401242340.AA17395@void.ncsa.uiuc.edu>
Date: Mon, 24 Jan 1994 17:40:16 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: NCSA httpd 1.1

Come 'n' get it kids... NCSA httpd 1.1 is now available at URL
http://hoohoo.ncsa.uiuc.edu/docs/ or via FTP from ftp.ncsa.uiuc.edu in
/Web/ncsa_httpd/httpd_1.1.

What's new? A bunch of bug fixes, a couple of performance
enhancements, new CGI docs, fancy directory indexing, experimental
two-way encrypted authentication (look for Mosaic 2.2 Real Soon Now
for the other half of it). It also includes WebReport 2.2.

Here's a more specific list:
 * Introduced experimental PEM/PGP based encrypted user authentication 
 * Improved directory indexing 
 * Cleaned up error output and fixed horrible output when server-side include
   error occurrs
 * Fixed slight bug in buffering code 
 * Directory indexing for user-supported directories fixed 
 * Retrieval of user supported directory with no trailing slash issues 
   redirect again 
 * Now supports RFC931 identd for logging purposes 
 * stderr for scripts and server side includes now sent to error_log 
 * Fixed bug in NCSA script code which would cause Location: to be ignored for
   local files 
 * Removed misfeature wherein Location: url's were being escaped by the server
 * Args to INC SRVURL escaped to avoid unpleasant surprises 
 * Location: /cgi-bin/foo?arg now works 
 * HEAD only for CGI scripts now ignores body put out by stupid
   scripts 
 * Options information now propogated instead of always defaulting to
   Options All
 * Average case for CGI code finding script is now one stat() call.

As usual, comments/questions to httpd@ncsa.uiuc.edu

Incidentally, I still have two unresolved bugs on my list. Apparently
on some systems, scripts can somehow have a character 255 appended to
their output, and apparently under some versions of Solaris the server
ignores .htaccess files. If anybody continues seeing these bugs with
1.1, please let me know so we can get together and try and work out
what the problem is.

Have fun
--Rob





From dcmartin@library.ucsf.edu  Mon Jan 24 16:12:43 1994 PST
Message-Id: <199401250012.AA24173@library.ucsf.edu >
Date: Mon, 24 Jan 1994 16:12:43 PST
From: dcmartin@library.ucsf.edu (David C. Martin)
Subject: Re: Inlined image format 

Regardless of format, sometimes you have information in format A and due
to limits on computation and/or storage you can only deliver it in that
format.  Additional formats are not bad and it is only the design
limitations of Mosaic that make supporting more formats problematic.

dcm
--------
Jon E. Mittelhauser writes:

At 11:56 AM 1/24/94 -0800, Christopher McRae wrote:
>Pini Albilia writes:
>> Is there a way to include Inlined image not in gif format?
>
>  There used to be a patch for Xmosaic which allowed for inlining TIFF
>images.  The work was done by Cheong Ang at UCSF.  I think you should be
>able to get it via ftp to ftp.library.ucsf.edu.  If you're interested in
>using some other image format, you might look at Cheong's code as an
>example of how to modify Xmosaic.

The advantage to using Gifs, however, is that the document will be viewable
in all browsers that support inlined images.  If you add a new format, the
document will only be viewble by a very limited set of people...

At least in terms of Mosaic, we have currently only agreed to support gifs 
and xbms on all three platforms...

-Jon

---
Jon E. Mittelhauser (jonm@ncsa.uiuc.edu)
Research Programmer, NCSA                          (NCSA Mosaic for MS Windows)
More info <a href="http://www.ncsa.uiuc.edu/SDG/People/jonm/jonm.html">here</a>





From Tim.Finin@cs.umbc.edu  Mon Jan 24 20:27:33 1994 -0500
Message-Id: <199401250127.AA01927@cujo.cs.umbc.edu>
Date: Mon, 24 Jan 1994 20:27:33 -0500
From: Tim.Finin@cs.umbc.edu (Timothy Finin)
Subject: Workshop on "Digital Libraries: Current Issues", May 19-20, Newark NJ


				   
			     Workshop on
		  Digital Libraries: Current Issues
				   
		 Thursday, May 19 and Friday 20, 1994

SPONSORS: Rutgers University, Purdue University, Bellcore, and AT&T

LOCATION}: Rutgers University, Ackerson Hall
           180 University Ave, Newark, NJ

WORKSHOP Co-CHAIRS: Nabil R. Adam, Bharat K. Bhargava, Yelena Yesha

PROGRAM COMMITTEE: S. Amarel, Rutgers Univ;   T. Finin, UMBC; 
                   P. Kanellakis, Brown Univ; S. Naqvi,Bellcore;
                   G.Schlageter, Fern Univ.;  B. Wah, Univ. of Illinois-UC


Digital libraries are destined to be the first true archives of
integrated multimedia data. Currently, we have corpora of text, image,
and sound files. Soon we expect to have corpora of electronic books
which are capable of subsuming most of the media we use today and have
the potential for added functionality by being interactive.

A digital library is a unification of all available data repositories
through a single interface. That interface should be expected to cope
with whatever kind of data, e.g., text, image, and sound that is being
retrieved. Furthermore, this interface should serve as an effective
access means for scientists, engineers, managers as well as everyday
individuals.

With the new initiative to build a better information infrastructure,
it has become even more important to develop better means for
managing, locating, and retrieving information and knowledge bases.

We believe that the technologies of Parallelism, Networking/
Communications, Multi-media, User-interfaces, Database Management, and
Information Management will play an important role in making digital
libraries a reality.


OBJECTIVE: The aim of the workshop is to provide an international
forum to discuss evolving research issues and applications in digital
libraries.  Invited speakers from industry, universities, and
government will present their experiences and vision for the future.

The following are among  the invited speakers and panelists.

 - Ron Ashany, Program Director, Database and Expert Systems, NSF
 - Ed Fox, Virginia Tech  
 - Milton Halem, Chief of Space Data and Computing Division, NASA
 - Mike Lesk, Bellcore
 - William Steele, Director, Business Development, GTE
 - Don Tiedeman, Vice-President, AT&T 

INFORMATION TO AUTHORS: Authors interested in participating in the
workshop are invited to submit a 3 page abstract by Feb. 28, 1994 to:

  Prof. Nabil R. Adam
  Rutgers University
  180 University Ave.
  Newark, NJ 07102
  (201) 648-5239
  adam@adam.rutgers.edu

Notification of acceptance will be sent by March 15, 1994.

An edited book and special issue of a journal are planned.

ACCOMMODATION: All attendees are welcome to stay at the Hilton and
Towers (Gateway Center, Raymond Blvd., Newark, NJ) (201) 622-5000. A
block of rooms will be reserved and a special rate will be arranged
for the attendees (ask for the Rutgers Group. For a Courtesy bus from
Newark Airport call the Hilton).



From eytan@MIT.EDU  Mon Jan 24 21:28:59 1994 EST
Message-Id: <9401250228.AA21467@ne43-513-2.MIT.EDU>
Date: Mon, 24 Jan 94 21:28:59 EST
From: eytan@MIT.EDU (eytan@MIT.EDU)
Subject: Mosaic for Linux

Has anyone sucessfuly ported any of the newer versions of xmosaic for
linux (anything after v. 1.2)?

Also, is anyone aware of any other viewers for Linux?

Thanks,

Eytan Adar
eytan@mit.edu



From ESullivan@pcweek.ziff.com  Mon Jan 24 23:26:00 1994 -0800 (PST)
Message-Id: <2D44CA51@MSGate.PCWeek.Ziff.Com>
Date: Mon, 24 Jan 1994 23:26:00 -0800 (PST)
From: ESullivan@pcweek.ziff.com (Eamonn Sullivan)
Subject: RE: Mosaic for Linux


I use Lynx 2.1 (a vt-100-based Web browser) on Linux without any problems. 
It was developed by the University of Kansas' Distributed Computing Group. I 
found it off of one of the CERN Web overview pages.
 ----------
From: www-talk-request
To: www-talk
Subject: Mosaic for Linux
Date: Monday, January 24, 1994 9:28PM

Has anyone sucessfuly ported any of the newer versions of xmosaic for
linux (anything after v. 1.2)?

Also, is anyone aware of any other viewers for Linux?

Thanks,

Eytan Adar
eytan@mit.edu



From montulli@stat1.cc.ukans.edu  Mon Jan 24 23:15:36 1994 CST
Message-Id: <9401250515.AA39124@stat1.cc.ukans.edu>
Date: Mon, 24 Jan 94 23:15:36 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: WWW support from bunyip (fwd)

Here's a note I just got back from bunyip.
It looks like pretty good news.

:lou

Forwarded message:
> From peterd@expresso.bunyip.com Mon Jan 24 22:36:14 1994
> Message-Id: <9401250424.AA05226@expresso.bunyip.com>
> From: Peter Deutsch <peterd@bunyip.com>
> Date: Mon, 24 Jan 1994 23:24:41 -0500
> In-Reply-To: Lou Montulli's message as of Jan 19, 12:48
> X-Mailer: Mail User's Shell (7.2.4 2/2/92)
> To: montulli@stat1.cc.ukans.edu (Lou Montulli)
> Subject: Re: WWW support :)
> Cc: bajan@bunyip.com, goph-ans@bunyip.com
> 
> Hi,
> 
> [ You wrote: ]
> 
> > As you probably know, WWW offers a gateway service to archie
> > using the new forms interface.  This interface offers a
> > large improvement over the previous forms of archie access.
> .  .  .
> > I was also wondering if you have plans to index HTTP (WWW specific
> > protocal) servers as well as Gopher servers.  It should be
> > very similar to what you have already done for gopher, except
> > that the web comunity would strongly prefer the use of prepared
> > index files exclusively.
> 
> We're certainly considering some form of WWW indexing,
> although the lack of any real meaningful information in
> many of the HTTP links themselves makes the current
> automated approach somewhat problematic. We _have_
> completed initial tests of a new direct WWW frontend onto
> the current system, which will make both the current
> anonFTP and the new Gopher index collections available to
> the WWW community without any need for gateways. This will
> be part of a new augmented archie service which we hope to
> deploy in the near future.
> 
> We'd be happy to work with the WWW community as it defines
> a standardized mechanism for indexing files but such a
> project will have to be held up until we can get the new
> gopher index service deployed and stabilized. Hopefully
> this will take no more than a couple of months (I say with
> a silly grin on my face :-) at which point we can look to
> the next collection to target.
> 
> We'll certainly be keeping your suggestion in mind and if
> in the meantime the WWW community can agree upon the
> format of indexing files and start deploying them onto the
> net, we'd be happy to mine the data as soon as we have the
> resources to do so.
> 
> 					- peterd
> 
> -- 
> ------------------------------------------------------------------------------
>  "I don't break out into warm glows thinking about technologies, I break
>   out into warm glows thinking about how customers will use our stuff to
>   improve their businesses."
>                               - Jim Manzi, quoted in Communications Week...
> ------------------------------------------------------------------------------
> 


-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From Guido.van.Rossum@cwi.nl  Tue Jan 25 09:54:10 1994 +0100
Message-Id: <9401250854.AA24502=guido@voorn.cwi.nl>
Date: Tue, 25 Jan 1994 09:54:10 +0100
From: Guido.van.Rossum@cwi.nl (Guido.van.Rossum@cwi.nl)
Subject: Re: Inlined image format 

> Additional formats are not bad and it is only the design
> limitations of Mosaic that make supporting more formats problematic.

I don't know if the fact that thousands of sites have a copy of Mosaic
is part of its design :-), but I presume that the trouble in getting
all sites to upgrade is much more of a concern...

--Guido van Rossum, CWI, Amsterdam <Guido.van.Rossum@cwi.nl>
URL:  <http://www.cwi.nl/cwi/people/Guido.van.Rossum.html>



From kay@rzri6f.gsi.de  Tue Jan 25 11:11:35 1994 +0100
Message-Id: <9401251011.AA79219@rzri6f.gsi.de>
Date: Tue, 25 Jan 94 11:11:35 +0100
From: kay@rzri6f.gsi.de (Kay Winkler)
Subject: UNsubscribe

Could you please unsubscribe me?



From m.koster@nexor.co.uk  Tue Jan 25 08:46:13 1994 +0000
Message-Id: <9401251345.AA01201@dxmint.cern.ch>
Date: Tue, 25 Jan 1994 08:46:13 +0000
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: WWW support :)


In <9401250424.AA05226@expresso.bunyip.com> Peter Deutsch wrote:

> Lou Montulli wrote:
>
> > I was also wondering if you have plans to index HTTP (WWW specific
> > protocal) servers as well as Gopher servers.  It should be
> > very similar to what you have already done for gopher, except
> > that the web comunity would strongly prefer the use of prepared
> > index files exclusively.
> 
> We're certainly considering some form of WWW indexing,
> although the lack of any real meaningful information in
> many of the HTTP links themselves makes the current
> automated approach somewhat problematic.

True, just looking at links/titles is not enough.

> We _have_ completed initial tests of a new direct WWW frontend onto
> the current system, which will make both the current anonFTP and the
> new Gopher index collections available to the WWW community without
> any need for gateways. This will be part of a new augmented archie
> service which we hope to deploy in the near future.

I'm looking forward to see this...

> We'd be happy to work with the WWW community as it defines a
> standardized mechanism for indexing files but such a project will
> have to be held up until we can get the new gopher index service
> deployed and stabilized. Hopefully this will take no more than a
> couple of months (I say with a silly grin on my face :-) at which
> point we can look to the next collection to target.

Of course there is no reason the WWW community can't start using
such a service now.

> We'll certainly be keeping your suggestion in mind and if
> in the meantime the WWW community can agree upon the
> format of indexing files and start deploying them onto the
> net, we'd be happy to mine the data as soon as we have the
> resources to do so.

Are you aware of ALIWEB? It is currently the only system that is doing
what you want: it retrieves hand-prepered IAFA templates via HTTP, and
rolls them into a searcheable database (which in turn is included in
the W3 Catalog; one of the best WWW catalogs about).  Currently about
20 sites have deployed index files, and the number is growing. I am
hoping to give a talk on the upcoming WWW conference on this, which I
expect will bring it to attention of more sites.

For details see http://web.nexor.co.uk/aliweb/doc/aliweb.html

I would like to expand the discussion on the use of the IAFA templates
-- maybe a new template-type is called for, maybe not. Either way I
have some comments about them, but all attempts at initiaing a
discussion with the IAFA people have failed so far.

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From hgs@research.att.com  Tue Jan 25 08:21:39 1994 EST
Message-Id: <9401251421.AA08696@dxmint.cern.ch>
Date: Tue, 25 Jan 94 08:21:39 EST
From: hgs@research.att.com (Henning G. Schulzrinne)
Subject: Final Call for Papers: JSAC Special Issue on the Global Internet

          IEEE Journal on Selected Areas in Communications (JSAC)
                              Call for Papers
                            The Global Internet

  The Internet has grown from the dozen or so nodes of the original
ARPAnet to a collection of more than 15,000 autonomous networks with around
1,800,000 hosts in 60 countries, forming the largest data network ever in
existence.  Its exponential growth and status as a component of the U.S.
National Information Infrastructure have significantly enhanced interest in
the Internet in the past few years.  It also uniquely combines operational
networks which large numbers of educational, research and commercial users
depend on with an experimental network conducive to the rapid introduction
of new services.    Internet technology has found widespread use even
in networks not physically connected to the Internet itself.   In both
organization and technical details, the Internet marks a departure from
customary telecommunications and data networks, even though the underlying
transmission technology is often similar.
  Many of the issues faced by the Internet today, in particular scaling,
heterogeneity, security over untrusted links and integrated services, will
confront both private and public (data) networks in the near future.
  Technical papers are solicited concerning key Internet problems including
the following:

  o scaling and heterogeneity

  o novel applications for the Internet

  o routing, addressing and naming

  o support for mobility

  o integration of new technologies such as ATM, SMDS, frame relay and
    large public data networks with the Internet

  o information services and resource discovery

  o large-scale multicast

  o internet  multimedia,  such  as  real-time  audio/video  conferencing,
    signaling issues

  o quality-of-service issues in an internet

  o resource accounting and billing

  o privacy and security in internetworks

  Electronic  submissions  in  PostScript,  LaTeX  or  HTML  formats  are
encouraged.  You may retrieve guidelines for authors by anonymous ftp from
gaia.cs.umass.edu, directory pub/hgschulz/jsac, through the World-Wide Web
at http://www.research.att.com/ or by sending electronic mail to Henning
Schulzrinne.   Electronic and paper submissions should be sent to Deborah
Estrin only according to the following schedule:

                 Manuscript submission:   February 1, 1994
                 Acceptance notification: June 1994
                 Final manuscript due:    August 1994
                 Publication date:        1st Quarter 1995
Guest Editors
       Jon Crowcroft             Deborah Estrin
       University College London Computer Science Department mc0781
       J.Crowcroft@cs.ucl.ac.uk  University of Southern California
                                  Los Angeles, CA 90089-0781
                                  Tel.:  +1 213 740 4524
                                  estrin@usc.edu

       Henning Schulzrinne       Michael Schwartz
       AT&T Bell Laboratories    University of of Colorado
       hgs@research.att.com      schwartz@cs.colorado.edu



From dolesa@smtp-gw.spawar.navy.mil  Tue Jan 25 10:10:30 1994 EDT
Message-Id: <9400257595.AA759521430@smtp-gw.spawar.navy.mil>
Date: Tue, 25 Jan 94 10:10:30 EDT
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: A/UX & NCSA_httpd_1.1 compile errors


When I try to compile under A/UX, I get:

dolomite.root # make
        gcc -c -g -DAUX http_config.c
        gcc -c -g -DAUX httpd.c
        gcc -c -g -DAUX http_request.c
        gcc -c -g -DAUX util.c
        gcc -c -g -DAUX http_dir.c
        gcc -c -g -DAUX http_alias.c
        gcc -c -g -DAUX http_log.c
        gcc -c -g -DAUX http_mime.c
        gcc -c -g -DAUX http_access.c
        gcc -c -g -DAUX http_auth.c
        gcc -c -g -DAUX http_get.c
        gcc -c -g -DAUX http_put.c
        gcc -c -g -DAUX http_script.c
        gcc -c -g -DAUX rfc931.c
rfc931.c: In function `fsocket':
rfc931.c:56: warning: assignment makes pointer from integer without a cast
        gcc  -o httpd http_config.o httpd.o http_request.o util.o http_dir.o  ht
tp_alias.o http_log.o http_mime.o http_access.o http_auth.o  http_get.o http_put
.o http_script.o rfc931.o
dolomite.root #

Suggestions?  I have the flag set for A/UX.

                    Andre'



From hotsand!ellson  Tue Jan 25 14:45:36 1994 EST
Message-Id: <9401251945.AA06364@hotsand.dacsand>
Date: Tue, 25 Jan 94 14:45:36 EST
From: hotsand!ellson (John Ellson)
Subject: Re: Inlined image format

> From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
> 
> The advantage to using Gifs, however, is that the document will be viewable
> in all browsers that support inlined images.  If you add a new format, the
> document will only be viewble by a very limited set of people...
> 
> At least in terms of Mosaic, we have currently only agreed to support gifs 
> and xbms on all three platforms...

Perhaps I'm missing something, but why does the capability of the
browser affect the viewability of a document?  Why do all the browsers
have to have the same capabilities?

If a browser is extended to view TIFFs inline why can't I indicate
that in the .mailcap?  If the browser is not capable of viewing inline
then the .mailcap can direct the image to an external viewer.

Beyond TIFFs, what I'd really like to see is an inline window that external
viewers can use for their display so that we can have inline movies,
inline shells, inline audio control panels...   all without hard-coded
extensions to the browser.

John Ellson
AT&T Bell Labs



From jonm@ncsa.uiuc.edu  Tue Jan 25 14:10:09 1994 CST
Message-Id: <9401252010.AA00732@void.ncsa.uiuc.edu>
Date: Tue, 25 Jan 94 14:10:09 CST
From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
Subject: Re: Inlined image format

At 02:45 PM 1/25/94 EST, ellson@hotsand.att.com wrote:
>> From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
>> 
>> The advantage to using Gifs, however, is that the document will be viewable
>> in all browsers that support inlined images.  If you add a new format, the
>> document will only be viewble by a very limited set of people...
>> 
>> At least in terms of Mosaic, we have currently only agreed to support gifs 
>> and xbms on all three platforms...
>
>Perhaps I'm missing something, but why does the capability of the
>browser affect the viewability of a document?  Why do all the browsers
>have to have the same capabilities?

Because they are viewing the document in the browser?  They don't
*have* to have the same capabilities but most document authors want
to know a minimum set that all will support.  I (as an HTML author)
know that if I create a document using inlined gifs that any browsers
which support inlined images (e.g. GUI browsers) will display the 
document as I intend.  The same cannot be said for any arbitrary
format...

>If a browser is extended to view TIFFs inline why can't I indicate
>that in the .mailcap?  If the browser is not capable of viewing inline
>then the .mailcap can direct the image to an external viewer.
>

You are being extremely X-centric here.  Most of the world isn't running
X-windows.  

Also, inlined images and external images have a very different use in
practice.  If I find a document that has 10 inlined images that are
links to other pages or files and they are launched into 10 external
windows how in the world do I know which is which?!?


-Jon

---
Jon E. Mittelhauser (jonm@ncsa.uiuc.edu)
Research Programmer, NCSA                          (NCSA Mosaic for MS Windows)
More info <a href="http://www.ncsa.uiuc.edu/SDG/People/jonm/jonm.html">here</a>




From hgs@research.att.com  Tue Jan 25 15:12:25 1994 EST
Message-Id: <9401252014.AA17288@dxmint.cern.ch>
Date: Tue, 25 Jan 94 15:12:25 EST
From: hgs@research.att.com (Henning G. Schulzrinne)
Subject: Re: Final Call for Papers: JSAC Special Issue on the Global Internet

Due to popular demand and other conflicting deadlines, the deadline
for the JSAC Special Issue on the Internet has been extended until
March 1, 1994.

Henning Schulzrinne




From relihanl@ul.ie  Tue Jan 25 20:23:48 1994 +0000 (WET)
Message-Id: <Pine.3.05.9401252047.A12630-b100000@itdsrv1.ul.ie>
Date: Tue, 25 Jan 1994 20:23:48 +0000 (WET)
From: relihanl@ul.ie (Liam Relihan)
Subject: Re: Inlined image format

On Tue, 25 Jan 1994, Jon E. Mittelhauser wrote:

> At 02:45 PM 1/25/94 EST, ellson@hotsand.att.com wrote:
> 
> >If a browser is extended to view TIFFs inline why can't I indicate
> >that in the .mailcap?  If the browser is not capable of viewing inline
> >then the .mailcap can direct the image to an external viewer.
> >
> 
> You are being extremely X-centric here.  Most of the world isn't running
> X-windows.  
> 
> Also, inlined images and external images have a very different use in
> practice.  If I find a document that has 10 inlined images that are
> links to other pages or files and they are launched into 10 external
> windows how in the world do I know which is which?!?

In addition, external filters are not likely to be aware of things like
overlaid anchors, etc.

Liam

--
 Liam Relihan,               |               Voice: +353-61-333644 ext.5015
 CSIS, Schumann Building,    |/|                        Fax: +353-61-330876
 University Of Limerick,       |                     E-mail: relihanl@ul.ie
 Ireland.                      http://itdsrv1.ul.ie/PERSONNEL/lrelihan.html




From dcmartin@library.ucsf.edu  Tue Jan 25 12:29:01 1994 PST
Message-Id: <199401252029.AA02157@library.ucsf.edu >
Date: Tue, 25 Jan 1994 12:29:01 PST
From: dcmartin@library.ucsf.edu (David C. Martin)
Subject: Re: Inlined image format 

Would you (Jon) stop trying to defend the design limitations of Mosaic
and simply recommend GIF to people who have the choice?

I agree with John Ellson that the browsers should be capable of acting
as MIME compliant applications and be able to make use of external
viewers; in addition, I agree that Mosaic should generalize the ability
of an external application displaying w/in the boundaries of Mosaic.

These are not X-centric positions, nor are the UNIX centric.  These are
design choices and should be respected as such.  Software developers who
hide behind the apparent limitations of their chosen platform are as
short-sighted as those that denigrate those very same platforms for
their inherent limitations.

Finally, it is up to the author to determine how and why content should
be displayed and interacted.  It is up to the software designer to
provide the authors of the world the tools and functionality that they
require to convey their ideas.

dcm

BTW: "It is the X Window System," RWS would like to remind everyone,
"not X-Windows."
--------
Jon E. Mittelhauser writes:

At 02:45 PM 1/25/94 EST, ellson@hotsand.att.com wrote:
>> From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
>> 
>> The advantage to using Gifs, however, is that the document will be viewable
>> in all browsers that support inlined images.  If you add a new format, the
>> document will only be viewble by a very limited set of people...
>> 
>> At least in terms of Mosaic, we have currently only agreed to support gifs 
>> and xbms on all three platforms...
>
>Perhaps I'm missing something, but why does the capability of the
>browser affect the viewability of a document?  Why do all the browsers
>have to have the same capabilities?

Because they are viewing the document in the browser?  They don't
*have* to have the same capabilities but most document authors want
to know a minimum set that all will support.  I (as an HTML author)
know that if I create a document using inlined gifs that any browsers
which support inlined images (e.g. GUI browsers) will display the 
document as I intend.  The same cannot be said forany arbitrary
format...

>If a browser is extended to view TIFFs inline why can't I indicate
>that in the .mailcap?  If the browser is not capable of viewing inline
>then the .mailcap can direct the image to an external viewer.
>

You are being extremely X-centric here.  Most of the world isn't running
X-windows.  

Also, inlined images and external images have a very different use in
practice.  If I find a document that has 10 inlined images that are
links to other pages or files and they are launched into 10 external
windows how in the world do I know which is which?!?


-Jon

---
Jon E. Mittelhauser (jonm@ncsa.uiuc.edu)
Research Programmer, NCSA                          (NCSA Mosaic for MS Windows)
More info <a href="http://www.ncsa.uiuc.edu/SDG/People/jonm/jonm.html">here</a>





From marca@eit.COM  Tue Jan 25 13:03:06 1994 PST
Message-Id: <9401252103.AA26715@eit.COM>
Date: Tue, 25 Jan 94 13:03:06 PST
From: marca@eit.COM (Marc Andreessen)
Subject: Re: Inlined image format 

David C. Martin writes:
> Would you (Jon) stop trying to defend the design limitations of
> Mosaic and simply recommend GIF to people who have the choice?

Who are you to say something like that?  Jesus, lighten up.

> It is up to the software designer to provide the authors of the
> world the tools and functionality that they require to convey their
> ideas.

So demand a refund.

Marc



From dcmartin@library.ucsf.edu  Tue Jan 25 13:06:09 1994 PST
Message-Id: <199401252106.AA02557@library.ucsf.edu >
Date: Tue, 25 Jan 1994 13:06:09 PST
From: dcmartin@library.ucsf.edu (David C. Martin)
Subject: Re: Inlined image format 

If none of the Mosaic software engineers can accept a little criticism
of their software product, they should not be producing software for
distribution, either for free or for sale.

Everyone needs to lighten up as the question of image format is moot at
this point and people will add support for other formats and add support
for displaying w/in Mosaic and add support for on-demand client
conversion, etc, etc, etc...

If, however, Mosaic is going to grow into a strong and robust
implementation that supports the growing needs of its user community,
the software developers at NCSA need to be aware of the real-world
contraints and demands of real-world users.  Otherwise the entire
exercise is merely software for its own sake.

dcm
--------
Marc Andreessen writes:

David C. Martin writes:
> Would you (Jon) stop trying to defend the design limitations of
> Mosaic and simply recommend GIF to people who have the choice?

Who are you to say something like that?  Jesus, lighten up.

> It is up to the software designer to provide the authors of the
> world the tools and functionality that they require to convey their
> ideas.

So demand a refund.

Marc




From sanders@BSDI.COM  Tue Jan 25 15:22:16 1994 -0600
Message-Id: <199401252122.PAA01774@austin.BSDI.COM>
Date: Tue, 25 Jan 1994 15:22:16 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Inlined image format 

> Finally, it is up to the author to determine how and why content should
> be displayed and interacted.  It is up to the software designer to
> provide the authors of the world the tools and functionality that they
> require to convey their ideas.
Creeping featurism alert!

HTML+ has defined a more general function <FIG>, you should be talking about
how to make that do what you want instead of messing with <IMG>.

--sanders



From hotsand!ellson  Tue Jan 25 16:20:13 1994 EST
Message-Id: <9401252120.AA08919@hotsand.dacsand>
Date: Tue, 25 Jan 94 16:20:13 EST
From: hotsand!ellson (John Ellson)
Subject: Re: Inlined image format

> From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
> 
> At 02:45 PM 1/25/94 EST, ellson@hotsand.att.com wrote:
> >
> >Perhaps I'm missing something, but why does the capability of the
> >browser affect the viewability of a document?  Why do all the browsers
> >have to have the same capabilities?
> 
> Because they are viewing the document in the browser?  They don't
> *have* to have the same capabilities but most document authors want
> to know a minimum set that all will support.  I (as an HTML author)
> know that if I create a document using inlined gifs that any browsers
> which support inlined images (e.g. GUI browsers) will display the 
> document as I intend.  The same cannot be said for any arbitrary
> format...

As an author, I should be able to assume that the browser can handle
any format as an inline component.  (text, image, sound, movie,
executable tSipp for local 3D rendering....)

The browser should either handle it directly, or hand it off to an
external viewer.  Isn't that an issue that each browser and/or each user
can deal with independently under the control of .mailcap or similar
mechanism?

> >If a browser is extended to view TIFFs inline why can't I indicate
> >that in the .mailcap?  If the browser is not capable of viewing inline
> >then the .mailcap can direct the image to an external viewer.
> 
> You are being extremely X-centric here.  Most of the world isn't running
> X-windows.  

Am I being X-centric?  I certainly didn't mean to be since I also value the
use of non-X browsers.   In what way are MIME types and the .mailcap
facility specific to X-windows?  Isn't something similar used on PCs,
Macs, Amigas?

> Also, inlined images and external images have a very different use in
> practice.  If I find a document that has 10 inlined images that are
> links to other pages or files and they are launched into 10 external
> windows how in the world do I know which is which?!?

As a proposal couldn't you use the delayed image mechanism of Mosaic?
i.e. if the image requires an external view (according to .mailcap)
then the user has to click on an icon to display that image.

My other proposal, which was perhaps X-centric, was that the browser
just provide an inline window that an external image program would use
for its display.  (But since this is a browser specific proposal that
doesn't affect document authors, it doesn't matter if it is X-centric.
Anyway, couldn't the same concept be used in other windowing schemes?)

> From: Liam Relihan <relihanl@ul.ie>
> 
> In addition, external filters are not likely to be aware of things like
> overlaid anchors, etc.

But the inline icon can handle the overlaid anchors, just as in
delayed image loading.  

John Ellson
AT&T Bell Labs



From relihanl@ul.ie  Tue Jan 25 21:42:23 1994 +0000 (WET)
Message-Id: <Pine.3.05.9401252122.C13177-a100000@itdsrv1.ul.ie>
Date: Tue, 25 Jan 1994 21:42:23 +0000 (WET)
From: relihanl@ul.ie (Liam Relihan)
Subject: Re: Inlined image format

On Tue, 25 Jan 1994 ellson@hotsand.att.com wrote:
> > From: Liam Relihan <relihanl@ul.ie>
> > 
> > In addition, external filters are not likely to be aware of things like
> > overlaid anchors, etc.
> 
> But the inline icon can handle the overlaid anchors, just as in
> delayed image loading.  

The icon can handle the anchors if the browser can display the image the
icon represents --- ie. the image appears inline when the icon is activated.

If the browser must spawn a viewer because it can't handle the format, then
map stuff won't work (unless we are dealing with a very sophisticated
viewer that could return coordinates to the browser which could interpred
them).


Liam

--
 Liam Relihan,               |               Voice: +353-61-333644 ext.5015
 CSIS, Schumann Building,    |/|                        Fax: +353-61-330876
 University Of Limerick,       |                     E-mail: relihanl@ul.ie
 Ireland.                      http://itdsrv1.ul.ie/PERSONNEL/lrelihan.html




From andyh@cogs.susx.ac.uk  Tue Jan 25 21:47:23 1994 GMT
Message-Id: <m0pOvbY-00005iC@csuna.crn.cogs.susx.ac.uk>
Date: Tue, 25 Jan 94 21:47:23 GMT
From: andyh@cogs.susx.ac.uk (Andy Holyer)
Subject: Re: Inlined image format

As a (still   learng) ) HTML  programmer,  could I  just request   one
possible extension for  a future version  of Mosaic,  and  that's JPEG
handling?

For a  specific (but very  useful) category  of  images,  they're very
useful, basically because they can be far smaller as long as you don't
mind a few artefacts. It's not at  all hard to  make up very effective
thumbnails  in  a couple of K.  Since bandwidth will always lag behind
storage, I think this is quite significant for the 'Web.

The code  for handling  JPEG  is freely  available  for a  variety  of
platforms  so it shouldn't be  that hard to  build in with the GIF and
xbm code.

Besides, if we've got  inline JPEGs, we're  about  10% of  the  way to
inline MPEG :-)

Just my two penn'oth
-- 
&ndy Holyer, COGS, University of Sussex, Brighton, UK.## PGP Key by finger. ##
"The English are the most tasteless nation on earth, which is why they
set such store by it" - Joe Orton



From jonm@ncsa.uiuc.edu  Tue Jan 25 16:06:43 1994 CST
Message-Id: <9401252206.AA04297@void.ncsa.uiuc.edu>
Date: Tue, 25 Jan 94 16:06:43 CST
From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
Subject: Re: Inlined image format

At 04:20 PM 1/25/94 EST, ellson@hotsand.att.com wrote:
>> From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
>> 
>> At 02:45 PM 1/25/94 EST, ellson@hotsand.att.com wrote:
>> >
>> >Perhaps I'm missing something, but why does the capability of the
>> >browser affect the viewability of a document?  Why do all the browsers
>> >have to have the same capabilities?
>> 
>> Because they are viewing the document in the browser?  They don't
>> *have* to have the same capabilities but most document authors want
>> to know a minimum set that all will support.  I (as an HTML author)
>> know that if I create a document using inlined gifs that any browsers
>> which support inlined images (e.g. GUI browsers) will display the 
>> document as I intend.  The same cannot be said for any arbitrary
>> format...
>
>As an author, I should be able to assume that the browser can handle
>any format as an inline component.  (text, image, sound, movie,
>executable tSipp for local 3D rendering....)

As an ideal, I agree.  I was simply commenting about the current reality
which is that the only means of handling inlined formats is by the
browser doing the work...

BTW, thank you for responding to my comments rather than simply
flaming away and ignoring what I wrote...

>The browser should either handle it directly, or hand it off to an
>external viewer.  Isn't that an issue that each browser and/or each user
>can deal with independently under the control of .mailcap or similar
>mechanism?
>
>> >If a browser is extended to view TIFFs inline why can't I indicate
>> >that in the .mailcap?  If the browser is not capable of viewing inline
>> >then the .mailcap can direct the image to an external viewer.
>> 
>> You are being extremely X-centric here.  Most of the world isn't running
>> X-windows.  
>
>Am I being X-centric?  I certainly didn't mean to be since I also value the
>use of non-X browsers.   In what way are MIME types and the .mailcap
>facility specific to X-windows?  Isn't something similar used on PCs,
>Macs, Amigas?

Yes, something similar is used.  I was simply trying to point out that
we must consider all platforms.  I'm probably a little oversensitive
about this... :^)

>> Also, inlined images and external images have a very different use in
>> practice.  If I find a document that has 10 inlined images that are
>> links to other pages or files and they are launched into 10 external
>> windows how in the world do I know which is which?!?
>
>As a proposal couldn't you use the delayed image mechanism of Mosaic?
>i.e. if the image requires an external view (according to .mailcap)
>then the user has to click on an icon to display that image.

Yes, something like this could be used.  However, it does still present
a very different feel.  If I have inlined images used as buttons, it
would be a real pain to have to load all (10 for example) into external
players to figure out which one takes me to the home page.  All I was
trying to point out is that by defining a limited set (e.g. Gifs and
Xbms currently), an author can be guarnteed that the doc will look and
feel exactly as intended.

>
>My other proposal, which was perhaps X-centric, was that the browser
>just provide an inline window that an external image program would use
>for its display.  (But since this is a browser specific proposal that
>doesn't affect document authors, it doesn't matter if it is X-centric.
>Anyway, couldn't the same concept be used in other windowing schemes?)

But it does affect the authors because they will assume that everyone has
the same browser... that's happening everywhere already.  They will assume
that the image/etc is inlined and that assumption will be present in
the surrounding text and the layout of the page.

Don't get me wrong (ESP. DCM!!), I *like* the idea of allowing external
programs to display within the window.  That is one of the points of
OLE2 in MS Windows.  However, it is the future and this discussion started
out as a question of why we don't support TIFFs inlined.  All I've done
is suggest the reasons that we currently only support a limited set!

-Jon

---
Jon E. Mittelhauser (jonm@ncsa.uiuc.edu)
Research Programmer, NCSA                          (NCSA Mosaic for MS Windows)
More info <a href="http://www.ncsa.uiuc.edu/SDG/People/jonm/jonm.html">here</a>




From mcrae@ora.com  Tue Jan 25 14:24:07 1994 -0800
Message-Id: <199401252224.AA19595@rock.west.ora.com>
Date: Tue, 25 Jan 94 14:24:07 -0800
From: mcrae@ora.com (Christopher McRae)
Subject: CCI - Common Client Interface (was Re: Inlined image format)

  This whole debate would become a non-issue if we were to define a
"Common Client Interface" analogous to the CGI (Common Gateway Interface)
standard.  Suppose we define a standard way to send/receive both data
and events between the client and external 'viewers'.  Note that the
'client' then takes on some window management functions.
  This would allow for more sophisticated externals to be developed which
would work with any/all browsers - without any porting.  Further, these
external programs could then run remotely as 'conversion services'.  People
would not even have to worry about having a local copy of a particular
external, if there were a site willing to serve them.  Also, we could then
potentially interactively inline any interesting format such as TeX or
postscript.
  This would be particularly useful for such things as visualizing
remote data sets processed on remote supercomputers.  For instance, suppose I
connect to a weather data server, or one of the NASA sites containing planetary
data, and decide I would like to massage the data I've located.  The client
starts up an 'external viewer' on one of the NCSA supercomputers (or wherever
I have access) and passes it the URL of my chosen data set.  The external
sucks the data over, sends an HTML control panel to my browser for display,
and then waits for GUI events indicating which operations should be performed.

Off the top of my head...

Common Client Interface
  Client-to-External
     - Here is some data
       - what the external does with the data of course depends on
       the nature of that service
     - Here is a URL, please retrieve the contents as data
     - Here is an event
       - single click
       - double click
       - refresh (one of your windows has just become exposed)
       - etc.
     - Kill yourself

  External-to-Client
     - Here is an HTML document, please display it
     - Create a new window for me please
     - Delete my window, win1, please
     - Resize my window, win1, to size 5x10
     - interpret links yourself, OR notify me of link activation events

Etc.

Chris
-----------------------------------------------------------------------
Christopher McRae			            mcrae@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036



From sanders@BSDI.COM  Tue Jan 25 16:42:41 1994 -0600
Message-Id: <199401252242.QAA02341@austin.BSDI.COM>
Date: Tue, 25 Jan 1994 16:42:41 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Inlined image format 

> If, however, Mosaic is going to grow into a strong and robust
> implementation that supports the growing needs of its user community,
> the software developers at NCSA need to be aware of the real-world
> contraints and demands of real-world users.  Otherwise the entire
> exercise is merely software for its own sake.

I'm afraid you have the situation confused.  The NCSA is implementing a
protocol designed by the WWW community.  You must convince the WWW community
of developers and users that a feature is worthwhile, not the NCSA.  This
is not to deny the NCSA credit for their contributions, but seriously,
there is more involved here than just the NCSA and I applaud them for
recognizing that fact and being far-sighted enough to try and do the right
things for the long term success of the WWW project.

The goal of WWW is to build a distributed, global, hypertext system; not
to implement every "feature du jour" that comes along.  The correct, and
current, solution to this problem is to simply create a hypertext link to
the object in question.

If you want to do inlined everything then work on the HTML+ project.
Inlined everything is a *BIG* project and deserves much more thought than
we have given it here and if we are going to do it then we need to do it
right.  You can't expect to just say "hey! let's add 100,000 lines of
code to every browser" and have people start jumping up and down to do it.

--sanders



From phillips@cs.ubc.ca  Mon Jan 25 13:31:00 1994 -0800
Message-Id: <7274*phillips@cs.ubc.ca>
Date: 25 Jan 94 13:31 -0800
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: Inlined image format 

In spite of the danger of upsetting the Mosaic people, there is something
that really should be done to their in-line image support.  It should
stop lying to the servers -- when the browser sends out a HTTP request to
get an in-line image it says it will accept anything (including text/html).
If it only listed the formats it really supported, the server writers
could do something about all this.



From burchard@geom.umn.edu  Tue Jan 25 16:50:30 1994 -0600
Message-Id: <9401252250.AA19881@mobius.geom.umn.edu>
Date: Tue, 25 Jan 94 16:50:30 -0600
From: burchard@geom.umn.edu (burchard@geom.umn.edu)
Subject: hyper-TeX and standards for client-side extensions

The self-contained nature of the Mosaic solution is an important part  
of its success.  And I agree that slightly fascist efforts are  
probably necessary to keep the Web from splintering into incompatible  
subcommunities (which would be really sad).

However, I think there is at least one *serious* reason to consider a  
client-side extension standard: namely TeX.  For mathematics,  
physics, and other technical disciplines, HTML is just fundamentally  
inadequate (with all due respect to the impressive LaTeX2HTML  
converter).  People in these disciplines have just as much need for  
hypertext as everyone else---for example, references to other online  
papers, or inline images/animations.  What they need is "hyper-TeX".

Now, it seems unlikely that DVI previewing will be built into Mosaic  
any time soon.  Aside from the attendant code bloat, DVI previewing  
requires a lot of external resources (i.e. fonts), which would badly  
break the self-contained model that Mosaic wants to present.

A more realistic scenario is for NCSA and the Web community to define  
a standard interface (analogous to the CGI standard for server  
extension scripts) which allows client-side external viewers to act  
as Hypertext Extensions.

First, such a standard should specify a way for external viewers to  
send URLs back to the WWW browser that spawned them.  Second, this  
standard should allow the viewer to specify whether it wants to  
handle the result of accessing the URL itself, or let the browser  
handle it (to allow external viewers to perform their own inlining).

With such a Hypertext Extension standard in place, it would be  
relatively easy to implement hyper-TeX, since the DVI format already  
supports a system of external references through its \special escape  
mechanism (currently used to inline local images).  The \special  
format could be extended to encompass hypertext anchors, which would  
be displayed visually by the DVI previewer.

I recognize the major dangers of opening up the client side---in all  
likelihood we could just end up with the system incompatibility  
nightmare from which the Web has so far let us mercifully escape.   
But if TeX is going to be accommodated (which I think is necessary  
and inevitable), then we should consciously decide whether it's going  
to be a one-time hack or if we want to take the opportunity to define  
a general extension mechanism.

--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------



From marc@library.ucsf.edu  Tue Jan 25 15:19:37 1994 -0800
Message-Id: <199401252319.AA04153@library.ucsf.edu >
Date: Tue, 25 Jan 1994 15:19:37 -0800
From: marc@library.ucsf.edu (Marc Salomon)
Subject: Re: Inlined image format


Tony Sanders <sanders@BSDI.COM> writes:
>... lots of correct stuff deleted ...
> 
> If you want to do inlined everything then work on the HTML+ project.
> Inlined everything is a *BIG* project and deserves much more thought than
> we have given it here and if we are going to do it then we need to do it
> right.  You can't expect to just say "hey! let's add 100,000 lines of
> code to every browser" and have people start jumping up and down to do it.
> 

Let's distinguish inlined anything from inlined IMAGES (non-interactive, non-
time-series based) of any format.  The midaswww browser converts image formats
not supported by the browser into supported formats on-the-fly.  It is not a
*BIG* project to allow the user to specify a set of translation programs for
non-supported image types that can be called when needed.

In a related message:
jonm@ncsa.uiuc.edu (Jon E. Mittelhauser) writes:
>
>All I was
>trying to point out is that by defining a limited set (e.g. Gifs and
>Xbms currently), an author can be guarnteed that the doc will look and
>feel exactly as intended.

Is the HTML author given that level of control over other aspects of the
HTML page, such as font families, sizes, weights, colors or a particular 
external viewer?  This seems to be perched on the same cliff as the markup 
vs formatting debate was.  IMHO, ultimately, and without C-hacking, to the 
maximum extent possible, control of the decisions made in rendering of the 
document should rest with the user, including the full range of source data 
types of inlined images.  

It seems that with a myriad of static image formats (not applicable to
time-series/interactive data types) around, and more importantly, the data
already locked up in those formats, that we would get more data to more people
faster if there were a standard, client-configurable mechanism for converting
foreign types into supported ones.

-marc
//////////////////////////////////////////////////////////////////////////////
// Marc Salomon                                  e-mail: marc@ckm.ucsf.edu  \\ 
\\ Software Engineer                                                        //
// Innovative Software Systems Group             phone:  415.476.9541       \\
\\ UCSF Center for Knowledge Management                                     //
// 530 Parnassus SF, CA 94134-0840               fax:    415.476.4653       \\
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\



From vinay@eit.COM  Tue Jan 25 15:41:06 1994 PST
Message-Id: <9401252341.AA28693@eit.COM>
Date: Tue, 25 Jan 94 15:41:06 PST
From: vinay@eit.COM (Vinay Kumar)
Subject: What's New - EXPE Design Conference

I am sending out this information i received for the benefit of w3 
audience.
Information on this design conference will also be available on
"What's New Page" from NCSA.
--
  Vinay Kumar
vinay@eit.com

----- Begin Included Message -----
>From brereton@sunrise.stanford.edu Tue Jan 25 15:24:57 1994

The Center for Design Research at Stanford would like to announce our Team
Design Conference, EXPE, on WWW. 
Thanks, 
Sincerely, 

Margot Brereton


Stanford University's Center for Design Research is pleased to announce 
EXPE: An Experiential Conference on Team Design, March 24th - 26th, 1994.  
Explore the design process by working with teams from Chrysler, WET design, 
XEROX PARC, Pacific Data Images, Caterpillar, Jet Propulsion Laboratory and 
the Design Division, Department of Mechanical Engineering, Stanford University.
THe URL is:

	http://gummo.stanford.edu/html/EXPE/expe.html

----- End Included Message -----




From hotsand!ellson  Tue Jan 25 18:43:04 1994 EST
Message-Id: <9401252343.AA12425@hotsand.dacsand>
Date: Tue, 25 Jan 94 18:43:04 EST
From: hotsand!ellson (John Ellson)
Subject: Re: Inlined image format

OK I think I've found my answer.

I understand from Tony Sanders that a more general inline image format
is coming with <FIG>.  (Does this include an inlined window?) 

And I understand from Jon that <IMG> is more of a minumum subset that
is guaranteed to be available for navigational icons.  

Thanks everybody.  

John Ellson
AT&T Bell Labs

------------------------------------------------------
> From: Tony Sanders <sanders@BSDI.COM>
> 
> HTML+ has defined a more general function <FIG>, you should be talking about
> how to make that do what you want instead of messing with <IMG>.
> 
> --sanders

-------------------------------------------------------
> From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
> 
> >> Also, inlined images and external images have a very different use in
> >> practice.  If I find a document that has 10 inlined images that are
> >> links to other pages or files and they are launched into 10 external
> >> windows how in the world do I know which is which?!?
> >
> >As a proposal couldn't you use the delayed image mechanism of Mosaic?
> >i.e. if the image requires an external view (according to .mailcap)
> >then the user has to click on an icon to display that image.
> 
> Yes, something like this could be used.  However, it does still present
> a very different feel.  If I have inlined images used as buttons, it
> would be a real pain to have to load all (10 for example) into external
> players to figure out which one takes me to the home page.  All I was
> trying to point out is that by defining a limited set (e.g. Gifs and
> Xbms currently), an author can be guarnteed that the doc will look and
> feel exactly as intended.
> 



From janssen@parc.xerox.com  Tue Jan 25 15:50:05 1994 PST
Message-Id: <IhFP0hoB0KGWMYkwcj@holmes.parc.xerox.com>
Date: Tue, 25 Jan 1994 15:50:05 PST
From: janssen@parc.xerox.com (Bill Janssen)
Subject: Re: hyper-TeX and standards for client-side extensions

I think a more realistic and useful thing would be to add a Hypertext
Extension to Postscript (which, of course, TeX document can convert
into).  Then you could handle all kinds of different document formats,
including PC word processor formats, and you could also handle color,
embedded bitmaps, and other things outside of TeX's scope.  The code to
do the client-side rendering is already available
(ghostscript/ghostview), and I've seen it embedded in X widgets.

Bill



From sanders@BSDI.COM  Tue Jan 25 17:53:31 1994 -0600
Message-Id: <199401252353.RAA02740@austin.BSDI.COM>
Date: Tue, 25 Jan 1994 17:53:31 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Inlined image format 

> In spite of the danger of upsetting the Mosaic people, there is something
> that really should be done to their in-line image support.  It should
> stop lying to the servers -- when the browser sends out a HTTP request to
> get an in-line image it says it will accept anything (including text/html).
> If it only listed the formats it really supported, the server writers
> could do something about all this.

I agree.  Of course, if the URL doesn't return a GIF then it isn't going
to work so this isn't much of an issue right now.  I do think Mosaic
should restrict the Accept: headers in this situation.

--sanders



From montulli@stat1.cc.ukans.edu  Tue Jan 25 18:17:01 1994 CST
Message-Id: <9401260017.AA21162@stat1.cc.ukans.edu>
Date: Tue, 25 Jan 94 18:17:01 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Inlined image format

> 
> > In spite of the danger of upsetting the Mosaic people, there is something
> > that really should be done to their in-line image support.  It should
> > stop lying to the servers -- when the browser sends out a HTTP request to
> > get an in-line image it says it will accept anything (including text/html).
> > If it only listed the formats it really supported, the server writers
> > could do something about all this.
> 
> I agree.  Of course, if the URL doesn't return a GIF then it isn't going
> to work so this isn't much of an issue right now.  I do think Mosaic
> should restrict the Accept: headers in this situation.
> 
> --sanders
> 
How about adding a new header or a new form of the existing
header to specify what kinds of image types can be "inlined"
The server may want to differentiate between a get that is
coming for an image that will be inlined and one that will
be spawned in an external viewer.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From strata@fenchurch.MIT.EDU  Tue Jan 25 19:33:28 1994 EST
Message-Id: <CMM.0.90.0.759544408.strata@fenchurch>
Date: Tue, 25 Jan 94 19:33:28 EST
From: strata@fenchurch.MIT.EDU (M. Strata Rose)
Subject: Re: CGI and REMOTE_USER


WRT REMOTE_IDENT, I want to put in a request that the variable be able
to hold standard PGP or RSA key signatures.  We almost certainly need to
define additional variables to do authentication and decryption with, but
I thought I would just get the ball rolling a little.

Who out there is already working on "authenticated" Mosaic, ie an http server
which knows to serve encrypted pages to only a select set of users whose
clients will know to decrypt them for display & interpretation?

_Strata


M. Strata Rose
Unix & Network Consultant, SysAdmin & Internet Information 
Virtual City Network (tm)
strata@virtual.net | strata@hybrid.com | strata@fenchurch.mit.edu



From robm@ncsa.uiuc.edu  Tue Jan 25 18:39:06 1994 -0600
Message-Id: <9401260039.AA10395@void.ncsa.uiuc.edu>
Date: Tue, 25 Jan 1994 18:39:06 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI and REMOTE_USER

/*
 * Re: CGI and REMOTE_USER  by "M. Strata Rose"
 *    written on Jan 25,  7:33pm.
 *
 * 
 * WRT REMOTE_IDENT, I want to put in a request that the variable be able
 * to hold standard PGP or RSA key signatures.  We almost certainly need to
 * define additional variables to do authentication and decryption with, but
 * I thought I would just get the ball rolling a little.
 
httpd 1.1 puts the name which is associated with the user's key in
REMOTE_USER not in REMOTE_IDENT. REMOTE_IDENT is *not* to be trusted under
any circumstances for anything other than simple logging.
 
 * Who out there is already working on "authenticated" Mosaic, ie an http 
 * server which knows to serve encrypted pages to only a select set of users 
 * whose clients will know to decrypt them for display & interpretation?
 */

I already did it. httpd 1.1 and the upcoming Mosaic 2.2 have support for an
experimental PGP or PEM based encryption/decryption protocol. Read about it
at http://hoohoo.ncsa.uiuc.edu/PEMPGP.html if you're interested.

--Rob



From robm@ncsa.uiuc.edu  Tue Jan 25 18:47:49 1994 -0600
Message-Id: <9401260047.AA10662@void.ncsa.uiuc.edu>
Date: Tue, 25 Jan 1994 18:47:49 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI and REMOTE_USER

/*
 * Re: CGI and REMOTE_USER  by Rob McCool
 *    written on Jan 25,  6:39pm.
 *
 * at http://hoohoo.ncsa.uiuc.edu/PEMPGP.html if you're interested.
 */

Ahem. That's http://hoohoo.ncsa.uiuc.edu/docs/PEMPGP.html

Sorry
--Rob



From robm@ncsa.uiuc.edu  Tue Jan 25 18:54:55 1994 -0600
Message-Id: <9401260054.AA10879@void.ncsa.uiuc.edu>
Date: Tue, 25 Jan 1994 18:54:55 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Inlined image format

/*
 * Re: Inlined image format  by Lou Montulli
 *    written on Jan 25,  6:17pm.
 *
 * How about adding a new header or a new form of the existing
 * header to specify what kinds of image types can be "inlined"
 * The server may want to differentiate between a get that is
 * coming for an image that will be inlined and one that will
 * be spawned in an external viewer.
 */

I would think that the server shouldn't care... otherwise, how would it know
what was an inlined image and what wasn't? It seems to me that Mosaic should
just send different Accept: headers when it's resolving inlined images...

--Rob



From phillips@cs.ubc.ca  Mon Jan 25 16:39:00 1994 -0800
Message-Id: <7281*phillips@cs.ubc.ca>
Date: 25 Jan 94 16:39 -0800
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: Inlined image format

Lou Montulli says:
>How about adding a new header or a new form of the existing
>header to specify what kinds of image types can be "inlined"
>The server may want to differentiate between a get that is
>coming for an image that will be inlined and one that will
>be spawned in an external viewer.

I don't see the need for this right now especially since the
server will probably know where the image is headed (it probably
gave you the document that in-lined the image in the first place).

If you had such a feature, I'd presume it would be some parameter
on the "Accept:" header, but it seems a little bit abstract for
such a purpose.

Besides, we barely have Accept: headers that say the right thing
never mind many servers that pay attention to them not to
mention any of the parameters.

On the other hand, if someone puts "Accept: image/gif; depth=1"
into their browser, I promise to have at least one server which
uses it!



From robm@ncsa.uiuc.edu  Tue Jan 25 18:45:21 1994 -0600
Message-Id: <9401260045.AA10556@void.ncsa.uiuc.edu>
Date: Tue, 25 Jan 1994 18:45:21 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Patch for NCSA httpd 1.1


There is a slight bug in httpd 1.1 which will cause it to append an extra
linefeed to the beginning of a compressed document. Rather than save a fix
for 1.2, I'm just going update the distribution since I just put it out
yesterday.

If you already picked up a copy of httpd 1.1 and are using encoding, either
pick up another binary from /Web/ncsa_httpd/current/bin, or, if you're using
the source, apply the patch at the end of this message.

Sorry for the confusion.

--Rob



*** ../../httpd_1.1/src/http_mime.c     Wed Jan 19 14:31:55 1994
--- http_mime.c Tue Jan 25 18:12:42 1994
***************
*** 309,315 ****
      if(location[0])
          fprintf(fd,"Location: %s%c",location,LF);
      if(content_encoding[0])
!         fprintf(fd,"Content-encoding: %s%c%c",content_encoding,LF,LF);

      fprintf(fd,"%c",LF);
  }
--- 309,315 ----
      if(location[0])
          fprintf(fd,"Location: %s%c",location,LF);
      if(content_encoding[0])
!         fprintf(fd,"Content-encoding: %s%c",content_encoding,LF,LF);

      fprintf(fd,"%c",LF);
  }



From hgs@research.att.com  Tue Jan 25 20:54:32 1994 EST
Message-Id: <9401260156.AA12396@dxmint.cern.ch>
Date: Tue, 25 Jan 94 20:54:32 EST
From: hgs@research.att.com (Henning G. Schulzrinne)
Subject: Re: hyper-TeX and standards for client-side extensions

Isn't PostScript (minus general-purpose computation) + links =
Adobe Acrobat? 



From atotic@ncsa.uiuc.edu  Tue Jan 25 21:29:02 1994 -0600 (CST)
Message-Id: <9401260329.AA13402@void.ncsa.uiuc.edu>
Date: Tue, 25 Jan 1994 21:29:02 -0600 (CST)
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: Inlined image format

> 
> 
> Tony Sanders <sanders@BSDI.COM> writes:
> >... lots of correct stuff deleted ...
> > 
> > If you want to do inlined everything then work on the HTML+ project.
> > Inlined everything is a *BIG* project and deserves much more thought than
> > we have given it here and if we are going to do it then we need to do it
> > right.  You can't expect to just say "hey! let's add 100,000 lines of
> > code to every browser" and have people start jumping up and down to do it.
> > 
> 
> Let's distinguish inlined anything from inlined IMAGES (non-interactive, non-
> time-series based) of any format.  The midaswww browser converts image formats
> not supported by the browser into supported formats on-the-fly.  It is not a
> *BIG* project to allow the user to specify a set of translation programs for
> non-supported image types that can be called when needed.
> 
> In a related message:
> jonm@ncsa.uiuc.edu (Jon E. Mittelhauser) writes:
> >
> >All I was
> >trying to point out is that by defining a limited set (e.g. Gifs and
> >Xbms currently), an author can be guarnteed that the doc will look and
> >feel exactly as intended.
> 
> Is the HTML author given that level of control over other aspects of the
> HTML page, such as font families, sizes, weights, colors or a particular 
> external viewer?  This seems to be perched on the same cliff as the markup 
> vs formatting debate was.  IMHO, ultimately, and without C-hacking, to the 
> maximum extent possible, control of the decisions made in rendering of the 
> document should rest with the user, including the full range of source data 
> types of inlined images.  
> 
> It seems that with a myriad of static image formats (not applicable to
> time-series/interactive data types) around, and more importantly, the data
> already locked up in those formats, that we would get more data to more people
> faster if there were a standard, client-configurable mechanism for converting
> foreign types into supported ones.
> 
> -marc
> //////////////////////////////////////////////////////////////////////////////
> // Marc Salomon                                  e-mail: marc@ckm.ucsf.edu  \\ 
> \\ Software Engineer                                                        //
> // Innovative Software Systems Group             phone:  415.476.9541       \\
> \\ UCSF Center for Knowledge Management                                     //
> // 530 Parnassus SF, CA 94134-0840               fax:    415.476.4653       \\
> \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
> 


-- 
Aleksandar Totic   -- lead MacMosaic programmer --         atotic@ncsa.uiuc.edu
Software Development Group      National Center for Supercomputing Applications



From guenther.fischer@hrz.tu-chemnitz.de  Wed Jan 26 08:12:36 1994 +0100 (MET)
Message-Id: <9401260712.AA27427@flash1.hrz.tu-chemnitz.de>
Date: Wed, 26 Jan 1994 08:12:36 +0100 (MET)
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: Re: Patch for NCSA httpd 1.1

> 
> 
> There is a slight bug in httpd 1.1 which will cause it to append an extra
> linefeed to the beginning of a compressed document. Rather than save a fix
> for 1.2, I'm just going update the distribution since I just put it out
> yesterday.
> 
> If you already picked up a copy of httpd 1.1 and are using encoding, either
> pick up another binary from /Web/ncsa_httpd/current/bin, or, if you're using
> the source, apply the patch at the end of this message.
> 

I was happy about the nice new features to index ftp archives directly
per http (AddIcon, ReadmeName, AddType, ...), but the uncompressing of
files like *.dvi.Z doeesn't work. Now I saw your patch and nothing
is better. In the sources now I found the 
AddEncoding x-compress Z
and now it works. Please put it in the sample srm.conf and in the
documents or is it there?

	~Guenther

-- 
Name:      Guenther Fischer / Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361     / mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From robm@ncsa.uiuc.edu  Wed Jan 26 09:05:06 1994 -0600
Message-Id: <9401261505.AA21398@void.ncsa.uiuc.edu>
Date: Wed, 26 Jan 1994 09:05:06 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Patch for NCSA httpd 1.1

/*
 * Re: Patch for NCSA httpd 1.1  by Guenther Fischer
 *    written on Jan 26,  8:12am.
 *
 * I was happy about the nice new features to index ftp archives directly
 * per http (AddIcon, ReadmeName, AddType, ...), but the uncompressing of
 * files like *.dvi.Z doeesn't work. Now I saw your patch and nothing
 * is better. In the sources now I found the 
 * AddEncoding x-compress Z
 * and now it works. Please put it in the sample srm.conf and in the
 * documents or is it there?
 */

It's in the docs but not in the sample srm.conf... I'll add it.

BTW Everyone who is sending me mail reminding me that I should delete the
extra LF, I KNOW. 

--Rob



From sg04%kesser@gte.com  Wed Jan 26 11:16:05 1994 EST
Message-Id: <9401261616.AA15473@kesser.cisl214>
Date: Wed, 26 Jan 94 11:16:05 EST
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: Mosaic Accessories

I have been giving the thought of www-client extension mechanism a good
deal of thought (especially how to access information from pagers,
PDA's, PPP notebooks etc). I have sent this document out to several WWW
heavies, and gotton some responses.

I don't have the chutzpah to think that one person can grasp all
aspects of this problem, so I am opening this up to the entire WWW
community. I would like to collaborate with design and implementation
with those who feel that this general approach is of merit.

The original HTML document can be read from:
	ftp://ftp.gte.com/pub/circus/accessories.html

-------------------------------------------------------------------------

Mosaic (WWW client) Accessories

External viewers

Currently, add-on functionality to the Mosaic viewer is accomplished by
means of external viewers. A viewer is a leaf process which is invoked by
Mosaic to accomplish a particular "add-on" function. Once invoked the
viewer operates autonomously, with no further connection to Mosaic. The
external viewer mechanism has proven to be a very useful technique for
extending Mosaic. In particular, for adding the ability to view mpeg, jpeg,
and audio files.

Problem statement

Although the external viewer mechanism is very powerful, there are still
are things that one cannot do: 

   News Browsers: A full function news browser needs to have history
   file. The history file would then be used to prune the article list
   (typically a UL list) to hide what one has already seen. Additionally,
   one would like to be able to have kill files, search functions, reply and
   post functions, etc.

   Mail Tools: While browsing a mail archive, one would like to be
   able to invoke standard mail tool functions. For example, after
   selecting a mail message one should be able to reply, forward,
   delete, file or print that article. 

   Form entry: HTML+ supports form entry. However, currently all
   forms must be filled out manually. I frequently find myself filling in
   standard info (name, e-mail address, mailing address, etc.) Would it
   not be nice to be able to create "forms" that fetch directly from files?
   This would certainly assist those who would like to have forms
   where one entry item is a request for a 100KB file.

   Mosaic editors: The ideal Mosaic editor should work symbionically
   with the Mosaic viewer. Allowing one to edit HTML while
   simultaneously seeing it in the viewer. 

Problem Analysis 

Today, if one wants to extend the Mosaic client one has two choices. 

 1. External Viewers: However, a viewer, when invoked, operates
   completely asynchronously with Mosaic. Furthermore, it operates in
   complete isolation from what the Mosaic viewer is doing.

 2. Edit Mosaic: That is, modify the source code for Mosaic. However,
   this is something that people shy away from doing. Either because
   they don't know how, or given the pace of releases from NCSA,
   they might be afraid of trying to keep their edits current. 

Outline of Proposal

I would like to suggest a simple modification of the viewer mechanism, so
that we could have a tool in-between the two above mentioned extreme
choices. That is, I am proposing something I call Accessories. 

An Accessory can be thought of as a variant of an external viewer,
however it operates symbionically and simultaneously with the main
Mosaic client. Furthermore, there is a communication channel between the
accessory and Mosaic so that changes and updates can be sent between
the Mosaic client and its accessories (e.g. the HTML editor).

Examples of Current Accessories

Actually, Mosaic already implements the rudiments of an accessory
construct. For example, the Document Source Viewer, can be thought of
as a symbiotic accessory. Whenever the viewer changes location, so does
the Document Source Viewer. However, the source viewer is: 

 1. An integral piece of the Mosaic Viewer. 
 2. Does not have the necessary reverse communications channels that
   a real HTML editor or Mail tool would need. 

Other current (but even more rudimentary) examples of accessories are:
the Open Document Pop-Up, the HotList Viewer, and the Binary-Save
dialog box. 

Related Work

The concept of an Accessory should also be familiar to people who work
with TK/TCL and to a lessor degree those who work with Microsoft
Windows Applets. In these systems one migrates small constrained
interactive functions to accessory applications. For example, interactive
drawing tools, chart tools, debuggers, paint tools, etc. are implemented as 
Applets or TK tools that operate in tandem with the main applications.

Maintaining a communication channel between application components is
an important part of both of these systems. With Applets, one uses OLE to
invoke and communicate information, while in TK/TCL one uses the send
mechanism (via the X11 property lists, or in TCL-DP via TCP/IP). 

This model of software, where compound applications are fabricated out of
a flock of inter-communicating sub-application tools, is also becoming in
vogue in the CORBA community. 

Proposed Implementation

 Feel free to write me with alternative suggestions. I view this
 proposed implementation as very tentative.

Invocation of Accessories

Applications can be thought of as client side CGI scripts. Thus a
rudimentary way of implementing accessory invocation would be: 

   <A AREF=accessory.ext>Generic Accessory</A> 

However, one would prefer not to specify an actual file, but rather a MIME
type which would then be mapped via the .mailcap file to the accessory.
This would allow one to create documents that specify a generic type of
accessory, and allow the user (via the .mailcap) to choose the version of the
accessory that he/she wishes:

   <A AREF=accessory/x-html-editor>HTML Editor</A> 

The .mailcap entry for this could be: 

   accessory/x-html-editor HTML-ED.exe 

Sometimes one will want the user to manually invoke the accessory.
However, in other cases, for example a news reader, one would like to
have the accessories pop up automatically on certain pages. This could be
done by: 

   <A AREF=accessory/x-news-responder
   MODE=immediate></A> 

One would also like to place some accessories (e.g. the current document
viewer, mail-to, hotlist accessory, print, etc. ) on the Mosaic menu bar.
This could be done via x-resources (as is now done for the old
DOCUMENTS pull-down menu). Perhaps a better solution would be to
create an .appcap file whose entries would be used to create a 
Applications pull-down menu for the Mosaic menu-bar. Entries in the
.appcap file would be of the form: 

   accessory/x-html-editor HTML Editor...
   accessory/x-hotlist Hotlist Accessory... 

Communications to/from Mosaic

Accessories are meant to operate in tandem with the Mosaic viewer.
Minimally, this means that accessories need to know what is currently
showing in the Mosaic viewer. They also need to be able to tell the Mosaic
viewer to switch to a new document. For example, the HTML editor
accessory receives the HTML document from Mosaic, and can return the
updated version of that document.

Communications with Mosaic is very similar to the CGI interface. That is,
there is a small header followed by a MIME type. For example:

   Location: http://www.ncsa.uiuc.edu/index.html
   Content-type: text/html

   --- HTML DOCUMENT --- 

where --- HTML DOCUMENT --- would be the HTML code for the
current document being displayed in the Mosaic viewer. 

It is the job of the accessory to parse the HTML code. For most
accessories this will be trivial. For example, a news accessory just has to
parse a <UL> list of news articles - a very easy task. Most CGI/WWW
programmers are already pretty competent in PERL, TCL, AWK, C-shell,
or C, and there are fairly good tools in most of these languages for scanning
HTML and parsing it. I would expect these to be the prime candidate
languages for writing accessories.

CGI scripts, however, are meant to run once and return. This is not the
case with accessories. They should run in tandem with the Mosaic viewer
- and receive a new message-packet each time the Mosaic viewer
changes. Thus, we cannot use command line arguments or environment
variables. Instead communication occurs via standard input and output. The
actual format of a message-packet is therefore:

   Location: http://www.ncsa.uiuc.edu/index.html
   Length: xxxx
   Content-type: text/html

   --- HTML DOCUMENT --- 

The accessory uses the Location argument to tell when the viewer has
switched to a new document. It can either update itself, or tear itself down if
it is no longer relevant. The Length argument tells the accessory the length
of the body (the HTML document). 

The accessory communicates back to the Mosaic viewer in a similar
manner:

   Location: http://www.ncsa.uiuc.edu/index.html
   Length: xxxx
   Content-type: text/html

   --- HTML DOCUMENT --- 

If the Length, Content-type, and body are left off, then the
message-packet is a standard remote-control directive - telling the
Mosaic viewer to GET to a new document. Otherwise, the Mosaic viewer
is to attempt to execute an HTTP PUT directive and replace the HTML
document.

Some accessories may not need the full HTML document, but instead
could get by with just a list of parameters. Other accessories might want
both start-up parameters and an HTML document. Parameters could be
passed in the body of the HTML document using SGML comments.
However, I believe the following is a cleaner way to handle parameters: 

   <A AREF=accessory/x-mailer
   ARGS="arg1=34&arg2=test"></A> 

The accessory will then receive the following message-packet:

   Location: http://www.ncsa.uiuc.edu/index.html
   Length: xxxx
   Arguments: arg1=34&arg2=test
   Content-type: text/html

   --- HTML DOCUMENT --- 

where the arguments are URL-encoded.

External Communications

Accessories are free to talk to a variety of external ports. These could
include:

   files, e.g. a news diary, a mail alias file, hotlist file. 
   external TCP-IP ports 
   infrared hand held remote device. 
   links to a pager. 
   Closed caption feed from a TV station. 
   links to a packet cellular or cellular phone (CPDP) link. 

For example, one can create a two-way pager remote accessory (or more
likely a PDA such as the Apple Newton). Then one could in effect, run
Mosaic on one's pager, by having the accessory talk to the pager. The
accessory would be responsible for the translation of pager events and
request into Mosaic requests. Personal information (diaries of news articles
read, signatures, security codes, etc.) could be kept on the PDA and
accessed by the accessory to fill in forms or weed out previously read news
articles.

Another idea for PDA accessories would be a personal database
accessory. This database would contain information about what one had
already browsed on the net, personal filtering agents, information about
expense accounts, personal telephone directories, etc. Then, as one moves
from site to site and workstation to workstation, one could plug this
information into the local Mosaic Viewer in order to personalize it.

In general this is the idea behind these client side accessories. That we
should be able to extend the Mosaic client to provide wider access to
Mosaic and to make it work in better harmony with other tools on our hosts.

Communication Issues

The use of standard input/output for communications is less than perfect.
One problem is that it is a stream protocol. Which requires us to either mark
packet boundaries, or to give packet counts. Also, there is no good channel
for signaling events. However, packet protocols such as TCP, UDP, or RPC
are really not meant for such application protocols. In the long run, I think
we should be moving both the CGI interface and the Accessory interface
to one of the emerging applications protocols. That is, either CORBA, OLE
or perhaps a hybrid multi-protocol interface. In any case, the final choice
should be based on it being widely used across platforms and suitable for
both LAN and WAN applications communications.

I have deliberately skirted the synchronization issue. That is, what happens
when there are multiple accessories all attempting to re-direct the same
Mosaic viewer. For the moment, I think we can allow such race conditions
to exist - since most accessories are meant to be controlled by a single live
user via their user interface. However, I would like to hear the viewpoint of
others.

Security Issues

Accessories are meant solve a security problem. One could implement
accessories today, with no changes in the Mosaic client, by means of client
code viewers. For example, a PERL viewer could be loaded with arbitrary
PERL code from a HTML document, and execute accessory functions on
demand. However, this would open up a door to trojan horse code being
uploaded to the client machine and running without the users knowledge or
awareness.

The Accessory mechanism gives the user the ability to: 

   Choose which accessories to use (those he trusts). 
   De-Virus and verify accessories before use. 
   Supply substitute secure accessories of his own creation. 

Implementation Issues

Implementation of accessories seems to be a fairly straight forward and
simple modification of the current viewer mechanism. There is also the
issue of modifying the Anchor directive. However, it does not seem that it
would take a lot of work. I would certainly have gone ahead and modified
Mosaic myself. However, there are issues here that I think the entire
WWW community should be discussing. Furthermore, I would want
anything that is done to become part of the base-line Mosaic system rather
than just a personal variant Mosaic viewer.

Summary of Advantages

Allow OEM'ing of Mosaic GUI tools. 
   Currently tools such as the Document Viewer, Hotlist Viewer,
   Mailer, and Document Saver are all integral parts of Mosaic. NCSA
   staff has to be devoted to maintain these tools as well as write new
   ones such as the HTML editor. The Accessory mechanism will
   allow third party development of these tools. Leading to wider
   customer choice and faster development time. 
Stop Kvetching! 
   NCSA staff is frequently asked to make certain changes to the
   Hotlist viewer, Mailing tool etc. By providing an easy customer
   modifiable mechanism we can give the end user the ability to create
   his own variants of these tools. Only basic PERL, AWK, or C-shell
   script writing skills will be needed. 
Consolidate functionality 
   The accessory mechanism consolidates current tools such as the
   Document viewer, new tools such as the HTML editor, and old
   mechanisms such as the Remote control feature, into one unified
   simple mechanism. 
Remote communications 
   There is an upcoming explosion of personal mobile physical
   accessories. For example, two-way pagers and PDA's such as the
   Apple Newton. The accessory mechanism is a quick way to give a
   subset of Mosaic/WWW access to these devices. 
Database access 
   Accessories allows one to have personal or external databases. The
   accessory can then use these databases to provide automatic form
   filling out, and filtering of mail or of news. 
Security 
   Currently extending the client involves loading code into a code
   viewer (e.g. a PERL or CSH viewer). There are big security
   problems with this approach. The accessory mechanism gives one a
   more secure means of extending a client. 

Future Directions

Ultimately, one could imagine the Mosaic Viewer itself being
re-implemented as an accessory. Mosaic then would become an
background daemon process. This daemon process would act as a master
conductor, informing its accessories when a new document was switched
to, and arbitrating between the responses of the various accessories. I
believe this is the way that TkWWW now works. In such a scheme we
would have effectively OEM'ed the entire Mosaic interface. 

Document location

The HTML version of this document can be found on:
ftp://ftp.gte.com/pub/circus/accessories.html 

Yechezkal-Shimon Gutfreund
GTE Laboratories
40 Sylvan Road
Waltham, MA 02254
+1-617-466-2933
sgutfreund@gte.com


----- End Included Message -----




From amaag@eunet.ch  Wed Jan 26 17:33:07 1994 +0100
Message-Id: <199401261633.RAA25990@chsun.eunet.ch>
Date: Wed, 26 Jan 1994 17:33:07 +0100
From: amaag@eunet.ch (Andreas Maag)
Subject: 

From jfg  Wed Jan 26 08:26:04 1994 remote from dxcern.cern.CH
Received: from dxmint.cern.ch by chsun.eunet.ch (8.6.4/1.34)
	id IAA15240; Wed, 26 Jan 1994 08:26:01 +0100
Received: from dxcern.cern.ch by dxmint.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA28596; Wed, 26 Jan 1994 08:22:52 +0100
Received: by dxcern.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA09026; Wed, 26 Jan 1994 08:22:42 +0100
Received: from dxmint.cern.ch by dxcern.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA09008; Wed, 26 Jan 1994 08:22:37 +0100
Received: from www0.cern.ch by dxmint.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA28008; Wed, 26 Jan 1994 08:16:32 +0100
Received: by www0.cern.ch (5.0/SMI-4.0)
	id AA04716; Wed, 26 Jan 1994 08:13:28 --100
Received: from dxmint.cern.ch by www0.cern.ch (5.0/SMI-4.0)
	id AA04712; Wed, 26 Jan 1994 08:13:25 --100
Received: from obelix.hrz.tu-chemnitz.de by dxmint.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA27791; Wed, 26 Jan 1994 08:12:38 +0100
Received: from attila.hrz.tu-chemnitz.de by obelix.hrz.tu-chemnitz.de 
          with Local SMTP (PP) id <06462-0@obelix.hrz.tu-chemnitz.de>;
          Wed, 26 Jan 1994 08:10:25 +0100
Received: from flash1.hrz.tu-chemnitz.de (flash1-f.hrz.tu-chemnitz.de) 
          by attila.hrz.tu-chemnitz.de (4.1/SMI-4.1) id AA07263;
          Wed, 26 Jan 94 08:10:11 +0100
From: hrz.tu-chemnitz.de!guenther.fischer (Guenther Fischer)
Received: by flash1.hrz.tu-chemnitz.de (4.1/client-1.5) id AA27427;
          Wed, 26 Jan 94 08:12:36 +0100
Message-Id: <9401260712.AA27427@flash1.hrz.tu-chemnitz.de>
Subject: Re: Patch for NCSA httpd 1.1
To: robm@ncsa.uiuc.edu (Rob McCool)
Date: Wed, 26 Jan 1994 08:12:36 +0100 (MET)
Cc: www-talk@www0.cern.ch
In-Reply-To: <9401260045.AA10556@void.ncsa.uiuc.edu> from "Rob McCool" at Jan 25, 94 06:45:21 pm
Reply-To: hrz.tu-chemnitz.de!guenther.fischer
X-Mailer: ELM [version 2.4 PL21]
Mime-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
Content-Length: 1075

> 
> 
> There is a slight bug in httpd 1.1 which will cause it to append an extra
> linefeed to the beginning of a compressed document. Rather than save a fix
> for 1.2, I'm just going update the distribution since I just put it out
> yesterday.
> 
> If you already picked up a copy of httpd 1.1 and are using encoding, either
> pick up another binary from /Web/ncsa_httpd/current/bin, or, if you're using
> the source, apply the patch at the end of this message.
> 

I was happy about the nice new features to index ftp archives directly
per http (AddIcon, ReadmeName, AddType, ...), but the uncompressing of
files like *.dvi.Z doeesn't work. Now I saw your patch and nothing
is better. In the sources now I found the 
AddEncoding x-compress Z
and now it works. Please put it in the sample srm.conf and in the
documents or is it there?

	~Guenther

-- 
Name:      Guenther Fischer / Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361     / mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From peterd@bunyip.com  Wed Jan 26 12:03:09 1994 -0500
Message-Id: <9401261703.AA07086@expresso.bunyip.com>
Date: Wed, 26 Jan 1994 12:03:09 -0500
From: peterd@bunyip.com (Peter Deutsch)
Subject: Re: WWW support :)

[ You wrote: ]

.  .  .
> > We'd be happy to work with the WWW community as it defines a
> > standardized mechanism for indexing files but such a project will
> > have to be held up until we can get the new gopher index service
> > deployed and stabilized. Hopefully this will take no more than a
> > couple of months (I say with a silly grin on my face :-) at which
> > point we can look to the next collection to target.
> 
> Of course there is no reason the WWW community can't start using
> such a service now.

Exactly. We see our strength in providing a relatively
stable service to fetch and serve info, reaching agreement
on needed formats, the deployment of the data and such are
things that clearly belong in the hands of the community
and we wouldn't presume to do anything other than try to
follow your lead here.

> > We'll certainly be keeping your suggestion in mind and if
> > in the meantime the WWW community can agree upon the
> > format of indexing files and start deploying them onto the
> > net, we'd be happy to mine the data as soon as we have the
> > resources to do so.
> 
> Are you aware of ALIWEB? It is currently the only system that is doing
> what you want: it retrieves hand-prepered IAFA templates via HTTP, and
> rolls them into a searcheable database (which in turn is included in
> the W3 Catalog; one of the best WWW catalogs about).  Currently about
> 20 sites have deployed index files, and the number is growing. I am
> hoping to give a talk on the upcoming WWW conference on this, which I
> expect will bring it to attention of more sites.


Thanks for the pointer. It's great to see the IAFA work
starting to be used, I have great hopes for such
approaches in the next little while. Assuming we can keep
to our schedule with gopher, we'll be coming back to the
WWW list ASAP to work out how we might add WWW. 

> 
> For details see http://web.nexor.co.uk/aliweb/doc/aliweb.html
> 
> I would like to expand the discussion on the use of the IAFA templates
> -- maybe a new template-type is called for, maybe not. Either way I
> have some comments about them, but all attempts at initiaing a
> discussion with the IAFA people have failed so far.

<*Embarassed blush*> Sorry about the delays. As one of the
co-chairs I _am_ one of the IAFA people. Unfortuantely
we've been swimming against the tide due to our success
right now. We've been trying to get the gopher project off
the ground while also finishing up several major project
proposals to allow funding for this work. We've now got
things a little bit more under control and both my partner
Alan and I are trying to clear up the backlog (of course I
spent yesterday at the ComNet conference and Alan flies
out today to speak at a Merit Networking seminar but we
are trying!)


				- peterd

-- 
------------------------------------------------------------------------------
 "I don't break out into warm glows thinking about technologies, I break
  out into warm glows thinking about how customers will use our stuff to
  improve their businesses."
                              - Jim Manzi, quoted in Communications Week...
------------------------------------------------------------------------------



From peterd@bunyip.com  Wed Jan 26 12:03:09 1994 -0500
Message-Id: <9401261703.AA07086@expresso.bunyip.com>
Date: Wed, 26 Jan 1994 12:03:09 -0500
From: peterd@bunyip.com (Peter Deutsch)
Subject: Re: WWW support :)

[ You wrote: ]

.  .  .
> > We'd be happy to work with the WWW community as it defines a
> > standardized mechanism for indexing files but such a project will
> > have to be held up until we can get the new gopher index service
> > deployed and stabilized. Hopefully this will take no more than a
> > couple of months (I say with a silly grin on my face :-) at which
> > point we can look to the next collection to target.
> 
> Of course there is no reason the WWW community can't start using
> such a service now.

Exactly. We see our strength in providing a relatively
stable service to fetch and serve info, reaching agreement
on needed formats, the deployment of the data and such are
things that clearly belong in the hands of the community
and we wouldn't presume to do anything other than try to
follow your lead here.

> > We'll certainly be keeping your suggestion in mind and if
> > in the meantime the WWW community can agree upon the
> > format of indexing files and start deploying them onto the
> > net, we'd be happy to mine the data as soon as we have the
> > resources to do so.
> 
> Are you aware of ALIWEB? It is currently the only system that is doing
> what you want: it retrieves hand-prepered IAFA templates via HTTP, and
> rolls them into a searcheable database (which in turn is included in
> the W3 Catalog; one of the best WWW catalogs about).  Currently about
> 20 sites have deployed index files, and the number is growing. I am
> hoping to give a talk on the upcoming WWW conference on this, which I
> expect will bring it to attention of more sites.


Thanks for the pointer. It's great to see the IAFA work
starting to be used, I have great hopes for such
approaches in the next little while. Assuming we can keep
to our schedule with gopher, we'll be coming back to the
WWW list ASAP to work out how we might add WWW. 

> 
> For details see http://web.nexor.co.uk/aliweb/doc/aliweb.html
> 
> I would like to expand the discussion on the use of the IAFA templates
> -- maybe a new template-type is called for, maybe not. Either way I
> have some comments about them, but all attempts at initiaing a
> discussion with the IAFA people have failed so far.

<*Embarassed blush*> Sorry about the delays. As one of the
co-chairs I _am_ one of the IAFA people. Unfortuantely
we've been swimming against the tide due to our success
right now. We've been trying to get the gopher project off
the ground while also finishing up several major project
proposals to allow funding for this work. We've now got
things a little bit more under control and both my partner
Alan and I are trying to clear up the backlog (of course I
spent yesterday at the ComNet conference and Alan flies
out today to speak at a Merit Networking seminar but we
are trying!)


				- peterd

-- 
------------------------------------------------------------------------------
 "I don't break out into warm glows thinking about technologies, I break
  out into warm glows thinking about how customers will use our stuff to
  improve their businesses."
                              - Jim Manzi, quoted in Communications Week...
------------------------------------------------------------------------------



From uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch  Wed Jan 26 12:33:00 1994 PST
Message-Id: <2D46D4C3@MSMAIL.INDY.TCE.COM>
Date: Wed, 26 Jan 94 12:33:00 PST
From: uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch (Fisher Mark)
Subject: Re: Inlined image format


I hate to be MS Windows-centric (although OLE 2.0 is going to be on NT & the 
Mac), but it looks to me like the OLE 2.0 inlined image capability would 
certainly help with this.  As far as I understand it (which may not be far 
enough :( ), under OLE 2.0 a browser that did not know how to process an 
image format could essentially "loan" a child window to an external viewer 
for display of the image, thus producing what looks to the user like the 
current inlined image display but is actually an image displayed by an 
external viewer.  Does anyone know if the OpenDoc specification allows for 
this?
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From broccol@arlut.utexas.edu  Wed Jan 26 13:21:35 1994 -0600
Message-Id: <199401261921.NAA07838@csdsun1.arlut.utexas.edu>
Date: Wed, 26 Jan 1994 13:21:35 -0600
From: broccol@arlut.utexas.edu (Jonathan Abbey)
Subject: Re: Inlined image format

> Date: Wed, 26 Jan 94 12:33:00 PST
> From: Fisher Mark <uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch>
> Subject: Re: Inlined image format
> To: www-talk <www-talk@www0.cern.ch>
> Message-Id: <2D46D4C3@MSMAIL.INDY.TCE.COM>
> 
> I hate to be MS Windows-centric (although OLE 2.0 is going to be on NT & the 
> Mac), but it looks to me like the OLE 2.0 inlined image capability would 
> certainly help with this.  As far as I understand it (which may not be far 
> enough :( ), under OLE 2.0 a browser that did not know how to process an 
> image format could essentially "loan" a child window to an external viewer 
> for display of the image, thus producing what looks to the user like the 
> current inlined image display but is actually an image displayed by an 
> external viewer.  Does anyone know if the OpenDoc specification allows for 
> this?

I have read that OpenDoc is a functional superset of OLE 2.0, so I
believe the answer here is yes.

The X window system's basic architecture is itself very well suited for
the loaning of child windows to other applications.  The problem is that
heretofore there has not been a defined protocol for such sharing.  I believe
that X11R6 (with the Fresco C++ OO toolkit) will provide some explicit
support for this kind of mechanism.

The external-viewer-in-a-browser-window stuff, while conceptually wonderful,
doesn't strike me as so terribly important.  I think that image filters
can do the job just as well (Mosaic or whatever calls a filter with the
data and gets back a GIF file, for instance).  The important thing is to
have some kind of standard way to represent any kind of linear transformations
that such a filter might induce, so that polygonal regions defined as buttons
in the image will be properly mapped to the resulting GIF image.

Of course, if we went to a system where we could have active objects such
as PostScript programs embedded in HTML documents, then we would need to
talk about such an embedding system.

> ======================================================================
> Mark Fisher                            Thomson Consumer Electronics
> fisherm@tcemail.indy.tce.com           Indianapolis, IN

-------------------------------------------------------------------------------
Jonathan Abbey				               broccol@arlut.utexas.edu
Applied Research Laboratories                 The University of Texas at Austin
-------------------------------------------------------------------------------



From sg04%kesser@gte.com  Wed Jan 26 13:45:32 1994 EST
Message-Id: <9401261845.AA17618@kesser.cisl214>
Date: Wed, 26 Jan 94 13:45:32 EST
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: Mosaic Accessories

[I believe I sent this to the wrong address the first time]

I have been giving the thought of www-client extension mechanism a good
deal of thought (especially how to access information from pagers,
PDA's, PPP notebooks etc). I have sent this document out to several WWW
heavies, and gotton some responses.

I don't have the chutzpah to think that one person can grasp all
aspects of this problem, so I am opening this up to the entire WWW
community. I would like to collaborate with design and implementation
with those who feel that this general approach is of merit.

The original HTML document can be read from:
	ftp://ftp.gte.com/pub/circus/accessories.html

-------------------------------------------------------------------------

Mosaic (WWW client) Accessories

External viewers

Currently, add-on functionality to the Mosaic viewer is accomplished by
means of external viewers. A viewer is a leaf process which is invoked by
Mosaic to accomplish a particular "add-on" function. Once invoked the
viewer operates autonomously, with no further connection to Mosaic. The
external viewer mechanism has proven to be a very useful technique for
extending Mosaic. In particular, for adding the ability to view mpeg, jpeg,
and audio files.

Problem statement

Although the external viewer mechanism is very powerful, there are still
are things that one cannot do: 

   News Browsers: A full function news browser needs to have history
   file. The history file would then be used to prune the article list
   (typically a UL list) to hide what one has already seen. Additionally,
   one would like to be able to have kill files, search functions, reply and
   post functions, etc.

   Mail Tools: While browsing a mail archive, one would like to be
   able to invoke standard mail tool functions. For example, after
   selecting a mail message one should be able to reply, forward,
   delete, file or print that article. 

   Form entry: HTML+ supports form entry. However, currently all
   forms must be filled out manually. I frequently find myself filling in
   standard info (name, e-mail address, mailing address, etc.) Would it
   not be nice to be able to create "forms" that fetch directly from files?
   This would certainly assist those who would like to have forms
   where one entry item is a request for a 100KB file.

   Mosaic editors: The ideal Mosaic editor should work symbionically
   with the Mosaic viewer. Allowing one to edit HTML while
   simultaneously seeing it in the viewer. 

Problem Analysis 

Today, if one wants to extend the Mosaic client one has two choices. 

 1. External Viewers: However, a viewer, when invoked, operates
   completely asynchronously with Mosaic. Furthermore, it operates in
   complete isolation from what the Mosaic viewer is doing.

 2. Edit Mosaic: That is, modify the source code for Mosaic. However,
   this is something that people shy away from doing. Either because
   they don't know how, or given the pace of releases from NCSA,
   they might be afraid of trying to keep their edits current. 

Outline of Proposal

I would like to suggest a simple modification of the viewer mechanism, so
that we could have a tool in-between the two above mentioned extreme
choices. That is, I am proposing something I call Accessories. 

An Accessory can be thought of as a variant of an external viewer,
however it operates symbionically and simultaneously with the main
Mosaic client. Furthermore, there is a communication channel between the
accessory and Mosaic so that changes and updates can be sent between
the Mosaic client and its accessories (e.g. the HTML editor).

Examples of Current Accessories

Actually, Mosaic already implements the rudiments of an accessory
construct. For example, the Document Source Viewer, can be thought of
as a symbiotic accessory. Whenever the viewer changes location, so does
the Document Source Viewer. However, the source viewer is: 

 1. An integral piece of the Mosaic Viewer. 
 2. Does not have the necessary reverse communications channels that
   a real HTML editor or Mail tool would need. 

Other current (but even more rudimentary) examples of accessories are:
the Open Document Pop-Up, the HotList Viewer, and the Binary-Save
dialog box. 

Related Work

The concept of an Accessory should also be familiar to people who work
with TK/TCL and to a lessor degree those who work with Microsoft
Windows Applets. In these systems one migrates small constrained
interactive functions to accessory applications. For example, interactive
drawing tools, chart tools, debuggers, paint tools, etc. are implemented as 
Applets or TK tools that operate in tandem with the main applications.

Maintaining a communication channel between application components is
an important part of both of these systems. With Applets, one uses OLE to
invoke and communicate information, while in TK/TCL one uses the send
mechanism (via the X11 property lists, or in TCL-DP via TCP/IP). 

This model of software, where compound applications are fabricated out of
a flock of inter-communicating sub-application tools, is also becoming in
vogue in the CORBA community. 

Proposed Implementation

 Feel free to write me with alternative suggestions. I view this
 proposed implementation as very tentative.

Invocation of Accessories

Applications can be thought of as client side CGI scripts. Thus a
rudimentary way of implementing accessory invocation would be: 

   <A AREF=accessory.ext>Generic Accessory</A> 

However, one would prefer not to specify an actual file, but rather a MIME
type which would then be mapped via the .mailcap file to the accessory.
This would allow one to create documents that specify a generic type of
accessory, and allow the user (via the .mailcap) to choose the version of the
accessory that he/she wishes:

   <A AREF=accessory/x-html-editor>HTML Editor</A> 

The .mailcap entry for this could be: 

   accessory/x-html-editor HTML-ED.exe 

Sometimes one will want the user to manually invoke the accessory.
However, in other cases, for example a news reader, one would like to
have the accessories pop up automatically on certain pages. This could be
done by: 

   <A AREF=accessory/x-news-responder
   MODE=immediate></A> 

One would also like to place some accessories (e.g. the current document
viewer, mail-to, hotlist accessory, print, etc. ) on the Mosaic menu bar.
This could be done via x-resources (as is now done for the old
DOCUMENTS pull-down menu). Perhaps a better solution would be to
create an .appcap file whose entries would be used to create a 
Applications pull-down menu for the Mosaic menu-bar. Entries in the
.appcap file would be of the form: 

   accessory/x-html-editor HTML Editor...
   accessory/x-hotlist Hotlist Accessory... 

Communications to/from Mosaic

Accessories are meant to operate in tandem with the Mosaic viewer.
Minimally, this means that accessories need to know what is currently
showing in the Mosaic viewer. They also need to be able to tell the Mosaic
viewer to switch to a new document. For example, the HTML editor
accessory receives the HTML document from Mosaic, and can return the
updated version of that document.

Communications with Mosaic is very similar to the CGI interface. That is,
there is a small header followed by a MIME type. For example:

   Location: http://www.ncsa.uiuc.edu/index.html
   Content-type: text/html

   --- HTML DOCUMENT --- 

where --- HTML DOCUMENT --- would be the HTML code for the
current document being displayed in the Mosaic viewer. 

It is the job of the accessory to parse the HTML code. For most
accessories this will be trivial. For example, a news accessory just has to
parse a <UL> list of news articles - a very easy task. Most CGI/WWW
programmers are already pretty competent in PERL, TCL, AWK, C-shell,
or C, and there are fairly good tools in most of these languages for scanning
HTML and parsing it. I would expect these to be the prime candidate
languages for writing accessories.

CGI scripts, however, are meant to run once and return. This is not the
case with accessories. They should run in tandem with the Mosaic viewer
- and receive a new message-packet each time the Mosaic viewer
changes. Thus, we cannot use command line arguments or environment
variables. Instead communication occurs via standard input and output. The
actual format of a message-packet is therefore:

   Location: http://www.ncsa.uiuc.edu/index.html
   Length: xxxx
   Content-type: text/html

   --- HTML DOCUMENT --- 

The accessory uses the Location argument to tell when the viewer has
switched to a new document. It can either update itself, or tear itself down if
it is no longer relevant. The Length argument tells the accessory the length
of the body (the HTML document). 

The accessory communicates back to the Mosaic viewer in a similar
manner:

   Location: http://www.ncsa.uiuc.edu/index.html
   Length: xxxx
   Content-type: text/html

   --- HTML DOCUMENT --- 

If the Length, Content-type, and body are left off, then the
message-packet is a standard remote-control directive - telling the
Mosaic viewer to GET to a new document. Otherwise, the Mosaic viewer
is to attempt to execute an HTTP PUT directive and replace the HTML
document.

Some accessories may not need the full HTML document, but instead
could get by with just a list of parameters. Other accessories might want
both start-up parameters and an HTML document. Parameters could be
passed in the body of the HTML document using SGML comments.
However, I believe the following is a cleaner way to handle parameters: 

   <A AREF=accessory/x-mailer
   ARGS="arg1=34&arg2=test"></A> 

The accessory will then receive the following message-packet:

   Location: http://www.ncsa.uiuc.edu/index.html
   Length: xxxx
   Arguments: arg1=34&arg2=test
   Content-type: text/html

   --- HTML DOCUMENT --- 

where the arguments are URL-encoded.

External Communications

Accessories are free to talk to a variety of external ports. These could
include:

   files, e.g. a news diary, a mail alias file, hotlist file. 
   external TCP-IP ports 
   infrared hand held remote device. 
   links to a pager. 
   Closed caption feed from a TV station. 
   links to a packet cellular or cellular phone (CPDP) link. 

For example, one can create a two-way pager remote accessory (or more
likely a PDA such as the Apple Newton). Then one could in effect, run
Mosaic on one's pager, by having the accessory talk to the pager. The
accessory would be responsible for the translation of pager events and
request into Mosaic requests. Personal information (diaries of news articles
read, signatures, security codes, etc.) could be kept on the PDA and
accessed by the accessory to fill in forms or weed out previously read news
articles.

Another idea for PDA accessories would be a personal database
accessory. This database would contain information about what one had
already browsed on the net, personal filtering agents, information about
expense accounts, personal telephone directories, etc. Then, as one moves
from site to site and workstation to workstation, one could plug this
information into the local Mosaic Viewer in order to personalize it.

In general this is the idea behind these client side accessories. That we
should be able to extend the Mosaic client to provide wider access to
Mosaic and to make it work in better harmony with other tools on our hosts.

Communication Issues

The use of standard input/output for communications is less than perfect.
One problem is that it is a stream protocol. Which requires us to either mark
packet boundaries, or to give packet counts. Also, there is no good channel
for signaling events. However, packet protocols such as TCP, UDP, or RPC
are really not meant for such application protocols. In the long run, I think
we should be moving both the CGI interface and the Accessory interface
to one of the emerging applications protocols. That is, either CORBA, OLE
or perhaps a hybrid multi-protocol interface. In any case, the final choice
should be based on it being widely used across platforms and suitable for
both LAN and WAN applications communications.

I have deliberately skirted the synchronization issue. That is, what happens
when there are multiple accessories all attempting to re-direct the same
Mosaic viewer. For the moment, I think we can allow such race conditions
to exist - since most accessories are meant to be controlled by a single live
user via their user interface. However, I would like to hear the viewpoint of
others.

Security Issues

Accessories are meant solve a security problem. One could implement
accessories today, with no changes in the Mosaic client, by means of client
code viewers. For example, a PERL viewer could be loaded with arbitrary
PERL code from a HTML document, and execute accessory functions on
demand. However, this would open up a door to trojan horse code being
uploaded to the client machine and running without the users knowledge or
awareness.

The Accessory mechanism gives the user the ability to: 

   Choose which accessories to use (those he trusts). 
   De-Virus and verify accessories before use. 
   Supply substitute secure accessories of his own creation. 

Implementation Issues

Implementation of accessories seems to be a fairly straight forward and
simple modification of the current viewer mechanism. There is also the
issue of modifying the Anchor directive. However, it does not seem that it
would take a lot of work. I would certainly have gone ahead and modified
Mosaic myself. However, there are issues here that I think the entire
WWW community should be discussing. Furthermore, I would want
anything that is done to become part of the base-line Mosaic system rather
than just a personal variant Mosaic viewer.

Summary of Advantages

Allow OEM'ing of Mosaic GUI tools. 
   Currently tools such as the Document Viewer, Hotlist Viewer,
   Mailer, and Document Saver are all integral parts of Mosaic. NCSA
   staff has to be devoted to maintain these tools as well as write new
   ones such as the HTML editor. The Accessory mechanism will
   allow third party development of these tools. Leading to wider
   customer choice and faster development time. 
Stop Kvetching! 
   NCSA staff is frequently asked to make certain changes to the
   Hotlist viewer, Mailing tool etc. By providing an easy customer
   modifiable mechanism we can give the end user the ability to create
   his own variants of these tools. Only basic PERL, AWK, or C-shell
   script writing skills will be needed. 
Consolidate functionality 
   The accessory mechanism consolidates current tools such as the
   Document viewer, new tools such as the HTML editor, and old
   mechanisms such as the Remote control feature, into one unified
   simple mechanism. 
Remote communications 
   There is an upcoming explosion of personal mobile physical
   accessories. For example, two-way pagers and PDA's such as the
   Apple Newton. The accessory mechanism is a quick way to give a
   subset of Mosaic/WWW access to these devices. 
Database access 
   Accessories allows one to have personal or external databases. The
   accessory can then use these databases to provide automatic form
   filling out, and filtering of mail or of news. 
Security 
   Currently extending the client involves loading code into a code
   viewer (e.g. a PERL or CSH viewer). There are big security
   problems with this approach. The accessory mechanism gives one a
   more secure means of extending a client. 

Future Directions

Ultimately, one could imagine the Mosaic Viewer itself being
re-implemented as an accessory. Mosaic then would become an
background daemon process. This daemon process would act as a master
conductor, informing its accessories when a new document was switched
to, and arbitrating between the responses of the various accessories. I
believe this is the way that TkWWW now works. In such a scheme we
would have effectively OEM'ed the entire Mosaic interface. 

Document location

The HTML version of this document can be found on:
ftp://ftp.gte.com/pub/circus/accessories.html 

Yechezkal-Shimon Gutfreund
GTE Laboratories
40 Sylvan Road
Waltham, MA 02254
+1-617-466-2933
sgutfreund@gte.com


----- End Included Message -----



----- End Included Message -----




From sg04%kesser@gte.com  Wed Jan 26 13:45:32 1994 EST
Message-Id: <9401261845.AA17618@kesser.cisl214>
Date: Wed, 26 Jan 94 13:45:32 EST
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: Mosaic Accessories

[I believe I sent this to the wrong address the first time]

I have been giving the thought of www-client extension mechanism a good
deal of thought (especially how to access information from pagers,
PDA's, PPP notebooks etc). I have sent this document out to several WWW
heavies, and gotton some responses.

I don't have the chutzpah to think that one person can grasp all
aspects of this problem, so I am opening this up to the entire WWW
community. I would like to collaborate with design and implementation
with those who feel that this general approach is of merit.

The original HTML document can be read from:
	ftp://ftp.gte.com/pub/circus/accessories.html

-------------------------------------------------------------------------

Mosaic (WWW client) Accessories

External viewers

Currently, add-on functionality to the Mosaic viewer is accomplished by
means of external viewers. A viewer is a leaf process which is invoked by
Mosaic to accomplish a particular "add-on" function. Once invoked the
viewer operates autonomously, with no further connection to Mosaic. The
external viewer mechanism has proven to be a very useful technique for
extending Mosaic. In particular, for adding the ability to view mpeg, jpeg,
and audio files.

Problem statement

Although the external viewer mechanism is very powerful, there are still
are things that one cannot do: 

   News Browsers: A full function news browser needs to have history
   file. The history file would then be used to prune the article list
   (typically a UL list) to hide what one has already seen. Additionally,
   one would like to be able to have kill files, search functions, reply and
   post functions, etc.

   Mail Tools: While browsing a mail archive, one would like to be
   able to invoke standard mail tool functions. For example, after
   selecting a mail message one should be able to reply, forward,
   delete, file or print that article. 

   Form entry: HTML+ supports form entry. However, currently all
   forms must be filled out manually. I frequently find myself filling in
   standard info (name, e-mail address, mailing address, etc.) Would it
   not be nice to be able to create "forms" that fetch directly from files?
   This would certainly assist those who would like to have forms
   where one entry item is a request for a 100KB file.

   Mosaic editors: The ideal Mosaic editor should work symbionically
   with the Mosaic viewer. Allowing one to edit HTML while
   simultaneously seeing it in the viewer. 

Problem Analysis 

Today, if one wants to extend the Mosaic client one has two choices. 

 1. External Viewers: However, a viewer, when invoked, operates
   completely asynchronously with Mosaic. Furthermore, it operates in
   complete isolation from what the Mosaic viewer is doing.

 2. Edit Mosaic: That is, modify the source code for Mosaic. However,
   this is something that people shy away from doing. Either because
   they don't know how, or given the pace of releases from NCSA,
   they might be afraid of trying to keep their edits current. 

Outline of Proposal

I would like to suggest a simple modification of the viewer mechanism, so
that we could have a tool in-between the two above mentioned extreme
choices. That is, I am proposing something I call Accessories. 

An Accessory can be thought of as a variant of an external viewer,
however it operates symbionically and simultaneously with the main
Mosaic client. Furthermore, there is a communication channel between the
accessory and Mosaic so that changes and updates can be sent between
the Mosaic client and its accessories (e.g. the HTML editor).

Examples of Current Accessories

Actually, Mosaic already implements the rudiments of an accessory
construct. For example, the Document Source Viewer, can be thought of
as a symbiotic accessory. Whenever the viewer changes location, so does
the Document Source Viewer. However, the source viewer is: 

 1. An integral piece of the Mosaic Viewer. 
 2. Does not have the necessary reverse communications channels that
   a real HTML editor or Mail tool would need. 

Other current (but even more rudimentary) examples of accessories are:
the Open Document Pop-Up, the HotList Viewer, and the Binary-Save
dialog box. 

Related Work

The concept of an Accessory should also be familiar to people who work
with TK/TCL and to a lessor degree those who work with Microsoft
Windows Applets. In these systems one migrates small constrained
interactive functions to accessory applications. For example, interactive
drawing tools, chart tools, debuggers, paint tools, etc. are implemented as 
Applets or TK tools that operate in tandem with the main applications.

Maintaining a communication channel between application components is
an important part of both of these systems. With Applets, one uses OLE to
invoke and communicate information, while in TK/TCL one uses the send
mechanism (via the X11 property lists, or in TCL-DP via TCP/IP). 

This model of software, where compound applications are fabricated out of
a flock of inter-communicating sub-application tools, is also becoming in
vogue in the CORBA community. 

Proposed Implementation

 Feel free to write me with alternative suggestions. I view this
 proposed implementation as very tentative.

Invocation of Accessories

Applications can be thought of as client side CGI scripts. Thus a
rudimentary way of implementing accessory invocation would be: 

   <A AREF=accessory.ext>Generic Accessory</A> 

However, one would prefer not to specify an actual file, but rather a MIME
type which would then be mapped via the .mailcap file to the accessory.
This would allow one to create documents that specify a generic type of
accessory, and allow the user (via the .mailcap) to choose the version of the
accessory that he/she wishes:

   <A AREF=accessory/x-html-editor>HTML Editor</A> 

The .mailcap entry for this could be: 

   accessory/x-html-editor HTML-ED.exe 

Sometimes one will want the user to manually invoke the accessory.
However, in other cases, for example a news reader, one would like to
have the accessories pop up automatically on certain pages. This could be
done by: 

   <A AREF=accessory/x-news-responder
   MODE=immediate></A> 

One would also like to place some accessories (e.g. the current document
viewer, mail-to, hotlist accessory, print, etc. ) on the Mosaic menu bar.
This could be done via x-resources (as is now done for the old
DOCUMENTS pull-down menu). Perhaps a better solution would be to
create an .appcap file whose entries would be used to create a 
Applications pull-down menu for the Mosaic menu-bar. Entries in the
.appcap file would be of the form: 

   accessory/x-html-editor HTML Editor...
   accessory/x-hotlist Hotlist Accessory... 

Communications to/from Mosaic

Accessories are meant to operate in tandem with the Mosaic viewer.
Minimally, this means that accessories need to know what is currently
showing in the Mosaic viewer. They also need to be able to tell the Mosaic
viewer to switch to a new document. For example, the HTML editor
accessory receives the HTML document from Mosaic, and can return the
updated version of that document.

Communications with Mosaic is very similar to the CGI interface. That is,
there is a small header followed by a MIME type. For example:

   Location: http://www.ncsa.uiuc.edu/index.html
   Content-type: text/html

   --- HTML DOCUMENT --- 

where --- HTML DOCUMENT --- would be the HTML code for the
current document being displayed in the Mosaic viewer. 

It is the job of the accessory to parse the HTML code. For most
accessories this will be trivial. For example, a news accessory just has to
parse a <UL> list of news articles - a very easy task. Most CGI/WWW
programmers are already pretty competent in PERL, TCL, AWK, C-shell,
or C, and there are fairly good tools in most of these languages for scanning
HTML and parsing it. I would expect these to be the prime candidate
languages for writing accessories.

CGI scripts, however, are meant to run once and return. This is not the
case with accessories. They should run in tandem with the Mosaic viewer
- and receive a new message-packet each time the Mosaic viewer
changes. Thus, we cannot use command line arguments or environment
variables. Instead communication occurs via standard input and output. The
actual format of a message-packet is therefore:

   Location: http://www.ncsa.uiuc.edu/index.html
   Length: xxxx
   Content-type: text/html

   --- HTML DOCUMENT --- 

The accessory uses the Location argument to tell when the viewer has
switched to a new document. It can either update itself, or tear itself down if
it is no longer relevant. The Length argument tells the accessory the length
of the body (the HTML document). 

The accessory communicates back to the Mosaic viewer in a similar
manner:

   Location: http://www.ncsa.uiuc.edu/index.html
   Length: xxxx
   Content-type: text/html

   --- HTML DOCUMENT --- 

If the Length, Content-type, and body are left off, then the
message-packet is a standard remote-control directive - telling the
Mosaic viewer to GET to a new document. Otherwise, the Mosaic viewer
is to attempt to execute an HTTP PUT directive and replace the HTML
document.

Some accessories may not need the full HTML document, but instead
could get by with just a list of parameters. Other accessories might want
both start-up parameters and an HTML document. Parameters could be
passed in the body of the HTML document using SGML comments.
However, I believe the following is a cleaner way to handle parameters: 

   <A AREF=accessory/x-mailer
   ARGS="arg1=34&arg2=test"></A> 

The accessory will then receive the following message-packet:

   Location: http://www.ncsa.uiuc.edu/index.html
   Length: xxxx
   Arguments: arg1=34&arg2=test
   Content-type: text/html

   --- HTML DOCUMENT --- 

where the arguments are URL-encoded.

External Communications

Accessories are free to talk to a variety of external ports. These could
include:

   files, e.g. a news diary, a mail alias file, hotlist file. 
   external TCP-IP ports 
   infrared hand held remote device. 
   links to a pager. 
   Closed caption feed from a TV station. 
   links to a packet cellular or cellular phone (CPDP) link. 

For example, one can create a two-way pager remote accessory (or more
likely a PDA such as the Apple Newton). Then one could in effect, run
Mosaic on one's pager, by having the accessory talk to the pager. The
accessory would be responsible for the translation of pager events and
request into Mosaic requests. Personal information (diaries of news articles
read, signatures, security codes, etc.) could be kept on the PDA and
accessed by the accessory to fill in forms or weed out previously read news
articles.

Another idea for PDA accessories would be a personal database
accessory. This database would contain information about what one had
already browsed on the net, personal filtering agents, information about
expense accounts, personal telephone directories, etc. Then, as one moves
from site to site and workstation to workstation, one could plug this
information into the local Mosaic Viewer in order to personalize it.

In general this is the idea behind these client side accessories. That we
should be able to extend the Mosaic client to provide wider access to
Mosaic and to make it work in better harmony with other tools on our hosts.

Communication Issues

The use of standard input/output for communications is less than perfect.
One problem is that it is a stream protocol. Which requires us to either mark
packet boundaries, or to give packet counts. Also, there is no good channel
for signaling events. However, packet protocols such as TCP, UDP, or RPC
are really not meant for such application protocols. In the long run, I think
we should be moving both the CGI interface and the Accessory interface
to one of the emerging applications protocols. That is, either CORBA, OLE
or perhaps a hybrid multi-protocol interface. In any case, the final choice
should be based on it being widely used across platforms and suitable for
both LAN and WAN applications communications.

I have deliberately skirted the synchronization issue. That is, what happens
when there are multiple accessories all attempting to re-direct the same
Mosaic viewer. For the moment, I think we can allow such race conditions
to exist - since most accessories are meant to be controlled by a single live
user via their user interface. However, I would like to hear the viewpoint of
others.

Security Issues

Accessories are meant solve a security problem. One could implement
accessories today, with no changes in the Mosaic client, by means of client
code viewers. For example, a PERL viewer could be loaded with arbitrary
PERL code from a HTML document, and execute accessory functions on
demand. However, this would open up a door to trojan horse code being
uploaded to the client machine and running without the users knowledge or
awareness.

The Accessory mechanism gives the user the ability to: 

   Choose which accessories to use (those he trusts). 
   De-Virus and verify accessories before use. 
   Supply substitute secure accessories of his own creation. 

Implementation Issues

Implementation of accessories seems to be a fairly straight forward and
simple modification of the current viewer mechanism. There is also the
issue of modifying the Anchor directive. However, it does not seem that it
would take a lot of work. I would certainly have gone ahead and modified
Mosaic myself. However, there are issues here that I think the entire
WWW community should be discussing. Furthermore, I would want
anything that is done to become part of the base-line Mosaic system rather
than just a personal variant Mosaic viewer.

Summary of Advantages

Allow OEM'ing of Mosaic GUI tools. 
   Currently tools such as the Document Viewer, Hotlist Viewer,
   Mailer, and Document Saver are all integral parts of Mosaic. NCSA
   staff has to be devoted to maintain these tools as well as write new
   ones such as the HTML editor. The Accessory mechanism will
   allow third party development of these tools. Leading to wider
   customer choice and faster development time. 
Stop Kvetching! 
   NCSA staff is frequently asked to make certain changes to the
   Hotlist viewer, Mailing tool etc. By providing an easy customer
   modifiable mechanism we can give the end user the ability to create
   his own variants of these tools. Only basic PERL, AWK, or C-shell
   script writing skills will be needed. 
Consolidate functionality 
   The accessory mechanism consolidates current tools such as the
   Document viewer, new tools such as the HTML editor, and old
   mechanisms such as the Remote control feature, into one unified
   simple mechanism. 
Remote communications 
   There is an upcoming explosion of personal mobile physical
   accessories. For example, two-way pagers and PDA's such as the
   Apple Newton. The accessory mechanism is a quick way to give a
   subset of Mosaic/WWW access to these devices. 
Database access 
   Accessories allows one to have personal or external databases. The
   accessory can then use these databases to provide automatic form
   filling out, and filtering of mail or of news. 
Security 
   Currently extending the client involves loading code into a code
   viewer (e.g. a PERL or CSH viewer). There are big security
   problems with this approach. The accessory mechanism gives one a
   more secure means of extending a client. 

Future Directions

Ultimately, one could imagine the Mosaic Viewer itself being
re-implemented as an accessory. Mosaic then would become an
background daemon process. This daemon process would act as a master
conductor, informing its accessories when a new document was switched
to, and arbitrating between the responses of the various accessories. I
believe this is the way that TkWWW now works. In such a scheme we
would have effectively OEM'ed the entire Mosaic interface. 

Document location

The HTML version of this document can be found on:
ftp://ftp.gte.com/pub/circus/accessories.html 

Yechezkal-Shimon Gutfreund
GTE Laboratories
40 Sylvan Road
Waltham, MA 02254
+1-617-466-2933
sgutfreund@gte.com


----- End Included Message -----



----- End Included Message -----




From sanders@BSDI.COM  Wed Jan 26 13:46:58 1994 -0600
Message-Id: <199401261946.NAA01571@austin.BSDI.COM>
Date: Wed, 26 Jan 1994 13:46:58 -0600
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Inlined image format 

> doesn't strike me as so terribly important.  I think that image filters
> can do the job just as well (Mosaic or whatever calls a filter with the
> data and gets back a GIF file, for instance).  The important thing is to
> have some kind of standard way to represent any kind of linear transformations
> that such a filter might induce, so that polygonal regions defined as buttons

I agree with you on this point.  However, I also believe there is value
in HTML defining a limited set of supported data formats for embedded
image delivery.  It is unreasonable to require a user to setup lots of
converters just to read a few documents.  Oh, this server uses TIFF, this
server uses JPEG, this server uses...  By supporing a single format every
browser that supports <IMG> works correctly, that is a big deal.  You start
adding in new types and users will start finding documents they cannot
read.

--sanders



From nikos@cbl.leeds.ac.uk  Wed Jan 26 20:05:02 1994 GMT
Message-Id: <8711.9401262005@cblelca.cbl.leeds.ac.uk>
Date: Wed, 26 Jan 94 20:05:02 GMT
From: nikos@cbl.leeds.ac.uk (nikos@cbl.leeds.ac.uk)
Subject: Announcing: LaTeX2HTML v0.5.3 is available

Hello all,

This is to let you know that version 0.5.3 of LaTeX2HTML is now
available.

Other than many bug fixes the main changes since version 0.3.1 are:

+ the ability to include raw HTML in LaTeX
+ the ability to specify conditional text (i.e. only for the
  online version or only for the DVI file) in LaTeX
+ justified equations (!!!)
+ better numbering and cross-referencing for equations 
+ better facilities for cross references in general
+ handling of LaTeX accents and special characters
+ a mechanism for autoloading code to cope with style files
  (at the moment there is code for german.sty, french.sty 
  and makeidx.sty)
+ a much better looking and configurable navigation panel
  on each generated page
+ some more navigation buttons
+ easier installation and a script that tests your intallation
+ several new command line options

The LaTeX2HTML manual has been extensively revised. Among other changes 
there are new sections on how to use the new features and on general 
troubleshooting. Please try to read them before reporting problems
or before giving up!

You can get the code from the "Getting LaTeX2HTML" section of the 
manual which is at:
http://cbl.leeds.ac.uk/nikos/tex2html/doc/latex2html/latex2html.html

Another section of the manual contains an installation guide.

Some more details on the differences between versions can be found at
http://cbl.leeds.ac.uk/nikos/tex2html/doc/latex2html/section3_10.html
Apologies to those who picked up earlier versions this week.

Thanks to everybody who contributed comments, bug reports, and even 
code. Please keep them coming. If anybody else is willing to test later
versions before they are released then let me know.

Any URL's pointing to successfully converted documents would be very 
much appreciated.

Enjoy,

Nikos.

--
Nikos Drakos			
Computer Based Learning Unit   	nikos@cbl.leeds.ac.uk
University of Leeds		http://cbl.leeds.ac.uk/nikos/personal.html




From nikos@cbl.leeds.ac.uk  Wed Jan 26 20:20:41 1994 GMT
Message-Id: <8731.9401262020@cblelca.cbl.leeds.ac.uk>
Date: Wed, 26 Jan 94 20:20:41 GMT
From: nikos@cbl.leeds.ac.uk (nikos@cbl.leeds.ac.uk)
Subject: Announcing: LaTeX2HTML v0.5.3 is available

Hello all,

This is to let you know that version 0.5.3 of LaTeX2HTML is now
available.

Other than many bug fixes the main changes since version 0.3.1 are:

+ the ability to include raw HTML in LaTeX
+ the ability to specify conditional text (i.e. only for the
  online version or only for the DVI file) in LaTeX
+ justified equations (!!!)
+ better numbering and cross-referencing for equations 
+ better facilities for cross references in general
+ handling of LaTeX accents and special characters
+ a mechanism for autoloading code to cope with style files
  (at the moment there is code for german.sty, french.sty 
  and makeidx.sty)
+ a much better looking and configurable navigation panel
  on each generated page
+ some more navigation buttons
+ easier installation and a script that tests your intallation
+ several new command line options

The LaTeX2HTML manual has been extensively revised. Among other changes 
there are new sections on how to use the new features and on general 
troubleshooting. Please try to read them before reporting problems
or before giving up!

You can get the code from the "Getting LaTeX2HTML" section of the 
manual which is at:
http://cbl.leeds.ac.uk/nikos/tex2html/doc/latex2html/latex2html.html

Another section of the manual contains an installation guide.

Some more details on the differences between versions can be found at
http://cbl.leeds.ac.uk/nikos/tex2html/doc/latex2html/section3_10.html
Apologies to those who picked up earlier versions this week.

Thanks to everybody who contributed comments, bug reports, and even 
code. Please keep them coming. If anybody else is willing to test later
versions before they are released then let me know.

Any URL's pointing to successfully converted documents would be very 
much appreciated.

Enjoy,

Nikos.

--
Nikos Drakos			
Computer Based Learning Unit   	nikos@cbl.leeds.ac.uk
University of Leeds		http://cbl.leeds.ac.uk/nikos/personal.html




From burchard@geom.umn.edu  Wed Jan 26 16:28:58 1994 -0600
Message-Id: <9401262228.AA20862@mobius.geom.umn.edu>
Date: Wed, 26 Jan 94 16:28:58 -0600
From: burchard@geom.umn.edu (burchard@geom.umn.edu)
Subject: Re: hyper-TeX and standards for client-side extensions

> I think a more realistic and useful thing would be to add a
> Hypertext Extension to Postscript (which, of course,
> TeX document can convert into). 


That would indeed be useful (assuming the PostScript files were  
compressed in transmission).

However, what I'm proposing is actually considerably simpler.  I  
think I might have caused some confusion by using the name  
"hyper-TeX", when I'm actually only talking about shipping  
fully-processed DVI files around.  As I was explaining, the amount of  
work that needs to be done beyond what already exists in Mosaic and  
xdvi is quite limited; the main thing is to make sure it's  
implemented in a future-thinking way.

--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------



From janssen@parc.xerox.com  Wed Jan 26 15:54:25 1994 PST
Message-Id: <chFk_l0B0KGWMYkuRP@holmes.parc.xerox.com>
Date: Wed, 26 Jan 1994 15:54:25 PST
From: janssen@parc.xerox.com (Bill Janssen)
Subject: Re: hyper-TeX and standards for client-side extensions

Hyper-DVI, then.  Sure, should work just fine.  I was thinking that
Hyper-Postscript would take about the same amount of work, and probably
provide a bigger payback, in terms of versatility.  In fact, Paul
Kranenburg at Erasmus University in the Netherlands just sent me a
version that only needs two changes:  first, the links it supports are
Sun answerbook links, rather than Internet URL's (trivial change); and
second, it's implemented as an XView widget rather than an Xt widget
(sort of a big change).  The links are just comments in the Postscript
which indicate a document, and a sensitive rectangle on the page.  The
widget ``signals'' the specified document when the user clicks on the
specified region.  Easy.  In fact, dvips could generate this format from
Hyper-DVI, I imagine.

Bill



From stumpf@informatik.tu-muenchen.de  Thu Jan 27 02:54:54 1994 +0100
Message-Id: <94Jan27.025507mesz.311353@hprbg5.informatik.tu-muenchen.de>
Date: Thu, 27 Jan 1994 02:54:54 +0100
From: stumpf@informatik.tu-muenchen.de (Markus Stumpf)
Subject: patch for NCSA httpd-1.1 to allow http-gatewaying

Hello!

I have put the patch on
    ftp://ftp.informatik.tu-muenchen.de/tmp/httpd-1.1gate.tar.gz
In this file is also the below appended README.gate.

I hope everything is ok so far. It's about 3:00 am local time and
I am not sure I didn't miss something. :)

I have tested it with Mosaic for X11 2.0 and daemons running on
HP 9000/715 HP/UX 9.01 but I don't think I have some machine dependend
code in it.

Good night :)

	\Maex

---------8<----------8<----------8<---------8<---------8<-------------

This is the README for the gateway code to NCSA httpd-1.1

This gateway currently ONLY gates http protocol requests!!!
It cannot act as a "standard" httpd AND a gateway httpd.
I haven't found a way to distinguish between a GET from
a client in, and one from a client not in gateway
mode. If you know of one PLEEEEEAAAAASE tell me :)
(This makes it also rather impossible to decide whether the
gate request is for a gopher: http: or ... URL, as the
protocoll information is also stripped. Anyone knows
why this was done? Older versions of libwww sent a protocol:/
with the request).

However: if a gateway-server and a "standard" server are
running on the same machine the gateway-server would
connect the "standard" server. This is really overhead.
I have added code that checks whether the gateway request
is for the same machine the gateway-server runs on.
If so, I take a look at the port numbers. Maybe another
specialised httpd is running on the same machine.
Per default a server port of 80 (SameServerPort keyword,
see below) is assumed for the "standard" httpd.
If both server name and port number match, the gate-httpd
acts like a "standard" httpd and delivers the "local"
document itself.

There are two new keywords for conf/httpd.conf:
o  SameServerPort (default 80)
   To set the port for the "standard" httpd to something other
   than 80. (see above)

o  GateTransferLog (default logs/gate_access_log)
   Like TransferLog. The log of the gateway-server is slightly
   different. It contains one additional field after the [time] field
   indicating the size (in bytes) of the data that has been
   gated.

There are 2 more files in the tar-archive:
o patch
    contains a context diff to the httpd-1.1 source code
o src/httpd_gate.c
    an additional module containing all the code needed for
    gatewaying the http protocol.

The gateway code is enabled with a -DGATE_HTML compile switch.
The code still has some unnecessary comments in it for
debugging purpose and is lacking a few need comments for
documention.

To have a client use the gateway-server do e.g.
    setenv WWW_http_GATEWAY //gateway_host:gate_httpd_port

Please send bugs/comments to the address below.

P.S. I have also modified SERVER_VERSION and SERVER_SUPPORT in hope
     that the ncsa folx don't get blamed for errors I possibly made.

Have fun

	\Maex

---------8<----------8<----------8<---------8<---------8<-------------
-- 
______________________________________________________________________________
 Markus Stumpf                        Markus.Stumpf@Informatik.TU-Muenchen.DE 
                                http://www.informatik.tu-muenchen.de/~stumpf/



From timbl@ptpc00.cern.ch  Thu Jan 27 11:25:56 1994 +0100
Message-Id: <9401271025.AA04268@ptpc00.cern.ch>
Date: Thu, 27 Jan 94 11:25:56 +0100
From: timbl@ptpc00.cern.ch (Tim Berners-Lee)
Subject: Universal network graphics language


Profeessor Sleator brings up a very pertinent question which has also
arisen in the World-Wide Web, and which I feel is a very important next step.  
I will add my own slant on it.

I would like to see a graphics composition language which allows a structured  
display to be composed on the screen from a number of items, which may be
local or remote.  For example, I would like to see a chess screen made up
of a composition of a basic board, and a bunch or pieces, all identified
by their network or local addresses (URIs).  These would be cached in
practice, so to redraw the board would be the very rapid operation of
respecifying the structure.  Another example is a picture of a conference
room composed of a background, an overhead projector screen which is in
fact a whiteboard, or IRC, session, and people sitting around the table
which are GIFs, or videos for those with cameras.  We have the formats
for the basic graphics (though only TIFF a la NeXT has the much-needed
 transparency channel I believe).  We need a composition language.
 

There will be those who suggest adapting PDF, which is basically
 postscript with the ability to embedd other formats.
Maybe the renderman format would have something to give us.
There will be those who suggest augmenting TIFF, using its general
 tagging.
There will be those who say it should be SGML.
There will be those who say that HyTime ought to be used for this,
 as that was what it was indended for more or less.
There will be those who feel like writing it in an afternoon from
 scratch.

 (By the way, I disagree that postscript is too terribly
 _big_ -- with display postscript coming with X, and already doing a
 neat job as the screen interface in this system as I write,  if
 it was what we wanted I would go for it. I feel though that
 we want something very declarative, rather than procedural,
 as a base.  I would like it to support lazy eveluation, so
 that when a bit of embedded video becomes covered by another
 window, the bandwidth can be saved, or if I can't see the
 output of a simulation, the CPU can be saved.  "If you never
 have a dream, then you never have a dream come true".)
  

 Clearly some 3d or fake 3d ability is useful.  Taking the
 object-oriented approach, I would imagine objects suitable
 for composition responing to, according to their sophistication,
 methods
 

 	Renderself();
 	RenderSelf(viewed_from_x,y,z);
	Renderself(viewed_frorm_x,y,z, lighting_conditions);


The results would, with HTTP, be returned in any format the
client could handle, so those who could handle video might get
back a video stream, those which can handle 3d might get back 3d,
defaulting down to 2d with transparency or just plain GIFs.
(I guess we rule out 3d video, but video with transparancy
would be cool -- using color separation overlay to paste
people from their own blue painted conference room into
a common virtual environment).

An object which only had a 2d representation would always
return the same picture, with the result that, for example,
when one rotated the conferenec room, the people would
always be facing one. That is a reasonable compromise,
the sort of thing which makes practical systems really work,
and turn smoothly into ultimate systems with time, bandwidth
and money.

I would like dynamic editing, so that the
chairperson (chairobject? ;-) can mouse drag Prof. Sleator to the
head of the table
to explian his ideas, and he can drop documents into the
overhead projector, or one can drag/drop the chess pieces)
We would have a great basis for consutruction of networked
VR, graphical MUDDS, and cyberspace would never be the same again.
This would be totally in keeping with web tradition in
being a powerful means of communication with an incredibly
intuitive interface, and with the ability to grow in
sophistication limited only be the imagination, and all based
on not very much.

In fact, many of the requirements, such as the format negotiation
and the embedding of graphics by reference go, much of it is there
in differing amounts in different clients, and spec'd out.
The missing thing is the composition language, with its 3d elements.
Which is why Prof. Sleator's message hits the nail on the
head.

I would encourage a discussion of this, an evaluation of what
is there some rapid work toward some prototypes.  I would
strongly encourage a viuew of this as just another sort of
object, with various possible representations (just like
basic images -- in fact this is an image object, embeddable
in a document).  Let's go fo it...

Tim Berners-Lee

_______________________________________________________

Begin forwarded message:
>
>From: Daniel Sleator <sleator@cs.cmu.edu>
>Date: Wed, 26 Jan 94 22:58:16 EST
>
>I wrote most of the internet chess server (which can currently be
>reached with "telnet ics.uoknor.edu 5000").  A number of other
>programmers have written interfaces that run on your local
>PC/MacIntosh/Xterminal/Next.  The interface parses the output from the
>chess server and displays a pretty chess board on your screen.  It
>also allows the user to make moves with the mouse.  These mouse
>strokes are converted into algebraic chess move notation.  Some of the
>interfaces show a dynamically changing clock, indicating the amount of
>time remaining for each player.
>
>Each of these interfaces is a custom-made special purpose-program for
>connecting to this chess server.  A similar set of interfaces has been
>created for use with the go server.
>
>This whole set-up is really cumbersome for a number of reasons.
>(1) It's very difficult to set up other game servers with graphics,
>because a whole set of special purpose interface programs must be
>written, one for each major platform. (2) Any change in the chess
>server must be backward compatible with all the interfaces out there.
>This limits the the way the server can develop.  (3) When a separate
>new platform emerges, a new interface must be written for each of the
>internet games with graphics.
>
>All of these problems could be solved by a standard language for
>expressing the simple types of graphics needed for these games,
>combined with a set of interfaces to interpret this language on a
>variety of platforms.  Postscript suffers from at least two drawbacks.
>(1) It's extremely verbose -- since many users connect through slow
>lines, this a problem. (2) full postscript is a HUGE language that is
>very difficult to implement, and 99% of it is unnecessary for these
>purposes.
>
>I believe that the development of such a set of tools would spur a
>tremendous explosion of new simple graphics games and applications
>that could run through telnet the way the chess server does now.  It
>would be possible for one person to write a new game (such as double
>bughouse chess) without having to write a half dozen graphics
>interfaces.  Many really cool things change from being impossible to
>being quite feasible.  (The PLATO system developed in the 70s at the
>University of Illinois had some of these properties: simple graphics
>available to all users, fast interaction among a large pool of users.
>The result was the development of a number of very popular and
>engrossing interactive games.)
>
>So, my question is: does a language with these properties already
>exist?  If not, how do we go about creating it?  This whole idea seems
>to fit quit well into the philosophy of Mosaic, which is a standard
>interface to the net that runs on all platforms.  If it emerges, would
>this new type of network interaction be built into Mosaic?
>
>Please forward this message to anybody else who you think would have
>some useful insight on this problem.  Thanks.
>
>                         Daniel Sleator
>                         Professor of Computer Science
>                         Carnegie Mellon University
>
>



From bach@sneezy.nosc.mil  Thu Jan 27 08:47:29 1994 -0500
Message-Id: <9401271347.AA03131@sneezy.nosc.mil>
Date: Thu, 27 Jan 94 08:47:29 -0500
From: bach@sneezy.nosc.mil (Eric J. Bach)
Subject: subscribe

subscribe
bach@nosc.mil



From dsr@hplb.hpl.hp.com  Thu Jan 27 15:42:24 1994 GMT
Message-Id: <9401271542.AA09248@manuel.hpl.hp.com>
Date: Thu, 27 Jan 94 15:42:24 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Universal network graphics language

I too have been putting thought into a similar scheme. It seems to me
that the processing power of workstations and high end PCs is now
good enough to support platform independent VR with Web style global
hyperlinks. Like Tim, I believe that the way forward will naturally lead
to non-proprietary VR, and think now is the time to start exploring
how we can do this.

A key to allowing effective platform independence is to use logical
descriptions so that viewers can fill in the details according to their own
rendering capabilities. As an example, you could describe a room in terms of
the polygon defining the floor plan, the height of the walls, and categories
for the textures of floor, walls and ceiling. Hierarchical descriptions of
wall textures could include: raw color and a link to the tiling pattern for
an explicit design of wall paper. Low power systems would use plain walls,
saving the cost of retrieving and patterning the walls. Fractal techniques
offer interesting possibilities too.

Shared models would avoid the need to download detailed models, e.g. for
wall paper, window and door fittings, chairs, tables, carpets etc. These
models, by using well known names can be retrieved over the net and cached
for subsequent use. The models would include hierarchical levels of detail.
This is important for "distancing" and reducing the load on lower power
clients. In addition to appearence, models could include behaviours
defined by scripts, e.g. sound of a clock ticking, the way a door opens
and functional calculators, radios and televisions.

Full VR needs expensive I/O devices, but we could get by with side-ways
movement of the mouse (cursor keys) to turn left or right and up-down
movement of the mouse to move forwards and backwards in the scene. I believe
that allowing a progression from simple to sophistocated I/O devices with the
same VR interchange formats will be critical to broad take up of VR.

So far I have outline a way in which you could click on an HTML link and
appear in a VR museum and wander around at will. Pushing on doors would
correspond clicking on hypertext links in HTML. The next step is to get to
meet other people in these VR environments. The trick here, is to wrap
real-time video images of people's faces onto 3D models of their heads.
This has already been done by a research group at ATR in Japan. Our library
couldn't find any relevant patents, so it looks like there are no problems
in defining non-proprietary protocols/interchange formats for this approach.

The bandwidth needed is minimised by taking advantage of the 3D models
to compress movements. By wrapping the video image of a face onto a 3D model,
you get excellent treatment of facial details, as needed for good non-verbal
communication, while minimizing the number of polygons needed.

The effectiveness of this approach has been demonstrated by Disney who
project video images on onto a rubber sheet deformed by a mask pushing out
of the plane. Needless to say, there remain some research issues here ...

The first steps in achieving this vision is to start work on a lightweight
interchange format for VR enviroments and experimenting with viewers
and http. A starting point is to pool info on available software tools
we could use to get off the ground.

Regards,

Dave Raggett (looking forward to the Web's VR version of the Vatican Exhibit). 

-----------------------------------------------------------------------------
Hewlett Packard Laboratories,           +44 272 228046
Bristol, England                        dsr@hplb.hpl.hp.com



From adrian@ora.com  Thu Jan 27 11:48:04 1994 -0500
Message-Id: <199401271648.AA20530@ora.com>
Date: Thu, 27 Jan 94 11:48:04 -0500
From: adrian@ora.com (Adrian Nye)
Subject: Re: CGI and REMOTE_USER 


> Who out there is already working on "authenticated" Mosaic, ie an http server
> which knows to serve encrypted pages to only a select set of users whose
> clients will know to decrypt them for display & interpretation?

O'Reilly and Associates is doing some work on decryption
in Mosaic (and other browsers), but not the server part.  Our goal, however, is probably
different from some other people's, and may be something to think
about (or it may be irrelevant - I'm not sure).  

We want to license books, 
so we want people to be able to purchase keys to view the books.  But we don't want
unencrypted versions of the books to get out of the browser.
We've worked out some ways to do this, but obviously it's not
the same problem as when you want to encrypt data for tranfer
over the network but you don't mind if the recipient sees the unencrypted
data files.

Adrian Nye



From vinay@eit.COM  Thu Jan 27 11:07:43 1994 PST
Message-Id: <9401271907.AA21441@eit.COM>
Date: Thu, 27 Jan 94 11:07:43 PST
From: vinay@eit.COM (Vinay Kumar)
Subject: Re: Universal network graphics language

Very interesting indeed. Recently i saw a running demo from General Magic's
"MagicCap" UI environment. It seems to do a lot of the stuff mentioned by others
on this list earlier (assuming i understand the emails correctly ofcourse). MagicCap UI 
shows a downtown view on the desktop, using a mouse one could navigate (VR style)
around houses, rooms, hallways, libraries, etc...One could customize wall colors, wall
papers, posters, and other artifacts in and outside the rooms. Drag and drop feature is 
supported. Linking of objects is thru drag and drop. However i am not sure if linking of
objects over distributed networks is supported. They claim everything in their 
environment is an "object" and almost every object could be linked to any other object. 
I will recommend everyone on W3 to atleast take a look at this product. (I apologize if
this sounds like infomercial on General Magic's product, certainly didn't mean that way).
In essence, it makes lot of sense in viewing W3 alternatively in a "non-document" 
centric manner as well. At this point, i am not sure what is the best way to do 
this in W3, certainly W3 is powerful and flexible enough to allow us to accomplish such a thing. Sounds like there is need for a multimedia-scripting-and-synchronization 
language (whatever that means....). Shall get back to you on this more after careful 
thinking.
--
  Vinay Kumar
vinay@eit.com




From les_stockton@wiltel.com  Thu Jan 27 14:17:05 1994 -0600
Message-Id: <9401272017.AA51959@shoe.wiltel.com>
Date: Thu, 27 Jan 1994 14:17:05 -0600
From: les_stockton@wiltel.com (Les Stockton x3123)
Subject: Mosaic

I'd like to be able to get an easy way to  create  documents
for  Mosaic.   I was told that you might be able to help me.
Any help you can give  would  be  greatly  appreciate.   Any
tools  to make things close to WYSIWYG would be appreciated.
Thanx.






From sg04%kesser@gte.com  Thu Jan 27 15:28:14 1994 EST
Message-Id: <9401272028.AA20538@kesser.cisl214>
Date: Thu, 27 Jan 94 15:28:14 EST
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: Re: Universal network graphics language

At the ACM MultiMedia'93 conference (Anaheim CA, USA), Vicki de May and
Simon Gibbs presented a paper called "A MultiMedia Composition Kit".

One of the applications they created was a Virtual Museum.  3D scenes
were rendered by one machine, video was generated on another machine
(and inserted into hanging picture frames generated by the first
machine). There were also 3D sprites being generated by other machines
which were composed and composited in.

The entire network had SGIs, Macs, Sparcs, and Next's. Each machine was
used for what it did best (generation of 3D textured scenes, video, or
sprites).

They showed a video where one could walk through the galleries of the
museum, and when one faced the picture frames on the walls, 2D video
was in the middle of the picture. One could even play multi-user games
with each person controlling a sprite which were composited into
the scene.

The work seemed to be partially sponsered by Kaliada and Apple.
However, the co-authors are at the University of Geneva.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Yechezkal-Shimon Gutfreund		 	   sgutfreund@gte.com [MIME]
GTE Laboratories, Waltham MA        http://www.gte.com/circus/home/home.html
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=




From sg04%kesser@gte.com  Thu Jan 27 16:19:19 1994 EST
Message-Id: <9401272119.AA20615@kesser.cisl214>
Date: Thu, 27 Jan 94 16:19:19 EST
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: Re: Mosaic Accessories

I think the following will briefly clarifies more precisely
what I am getting at with this proposal:


In article HI3@lut.ac.uk, J.P.Knight@lut.ac.uk (Jon P. Knight) writes:
]
]Well I read the proposal and its pretty interesting; I'm very much in
]favour of the modular toolkit approach.  However, if TkWWW already works
]this way and its so easy to modify it using TCL, why isn't everybody
]using it already?  Why fuss about changing the whole way Mosaic works? 
]Why not leave Mosaic as a single monolithic browser for those that want
]that and use TkWWW as the extensible hackers browser?
]

You are correct. Technically, TCL/TK already correspond to this model,
and TkWWW is easier to modify.

The issue is part sociological. NCSA SDL, Marc and Eric should be
complimented many times on how their interface has "defined" the face
for which WWW is known.

While I have been an advocate inside the Telcos for Mosaic, I am truely
amazed at how fast and how high Mosaic is being recognized.  This
Mosaic has become a by-word - moving very rapidly up to the top
executive offices. And these people are not technical, and so they
don't even know what WWW is, they only know Mosaic. The success is
phenomenal, but the name it goes under is the GUI: Mosaic.

But there is more to this than just sociology. The interface that I am
proposing (call it ACI - Accessory Client Interface) can be thought of
as a universal "extension" cord to any info-bot. Thus, I see it as
connecting to: gopher, lotus notes, news, and calender clients. Indeed,
why not to any info-bot?

And if we can hammer out a standard. Then I can have an accessory (on
my PDA, Pager, Notebook or whatever) and that accessory could talk to
any of these info clients (gopher, mosaic, lotus notes).

This would clearly be the dawn of Ubiquitous Computing.  I could walk
around with a portable device. Loaded with accessory programs and plug
into Mosaic and Lotus Notes clients at will.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Yechezkal-Shimon Gutfreund		 	   sgutfreund@gte.com [MIME]
GTE Laboratories, Waltham MA        http://www.gte.com/circus/home/home.html
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=



From rst@ai.mit.edu  Thu Jan 27 16:37:11 1994 EST
Message-Id: <9401272137.AA18201@volterra>
Date: Thu, 27 Jan 94 16:37:11 EST
From: rst@ai.mit.edu (Robert S. Thau)
Subject: Mosaic

   Date: Thu, 27 Jan 1994 14:17:05 -0600
   From: les_stockton@wiltel.com (Les Stockton x3123)

   I'd like to be able to get an easy way to  create  documents
   for  Mosaic.   I was told that you might be able to help me.
   Any help you can give  would  be  greatly  appreciate.   Any
   tools  to make things close to WYSIWYG would be appreciated.
   Thanx.




Well, most of what I know is in http://www.ai.mit.edu/writing-html.html.
The bottom line is that there's no good, free WYSIWYG solution for Unix
boxes.  On the other hand, if you've got Microsoft Word, there are a couple
of things you can try, though I can't vouch for any of them myself (my life
has been complete without Microsoft products)

One thing to look at is rtftohtml, (see ftp.cray.com://src/cjh/RTF), which
converts Rich Text Format (which Word can write out) into HTML.  The Mosaic
FAQ section on "Other software..." contains a converters section which may
also point to tools of use, including another set of macros to help you use
Microsoft Word, and the Latex2HTML converter which converts LaTeX to HTML.
Latex2HTML is particularly noteworthy for being able to do anything at all
with math and tables --- if only by running them through TeX and turning
them into GIFs.

What most people around here use is either LaTeX2HTML, or the emacs mode,
which takes most people around an hour to learn at most.

Good luck.
rst




From rst@ai.mit.edu  Thu Jan 27 16:44:28 1994 EST
Message-Id: <9401272144.AA18217@volterra>
Date: Thu, 27 Jan 94 16:44:28 EST
From: rst@ai.mit.edu (Robert S. Thau)
Subject: Ooops!!!

Apologies for replying to the whole list.  Yikes!

rst



From jbluming@chivalry.eit.COM  Thu Jan 27 12:25:46 1994 -0800
Message-Id: <9401272035.AA22280@eit.COM>
Date: Thu, 27 Jan 1994 12:25:46 -0800
From: jbluming@chivalry.eit.COM (Jason B. Bluming)
Subject: Re: Universal network graphics language 


I'd like to add another angle to this discussion (taking it yet farther
afield), that being the representation of time.  Since TimBL took the 
step from composition to VR/gaming, and thus opened the door, I'd like
to address how to make these non-static elements interact properly
(that being "as we would like").

To create, for example, the virtual conference room spoken of, we need
not only the mechanism for incorporating individual's pictures to our
room, but one to synchronize their motions with sound, to handle the
(possibly) varied sampling rates of distinct forms of audio/video and
perhaps other chronological sequences (Vinay Kumar's "gesture recording"
comes to mind).

For generality, please consider both the linear and cyclic aspects of 
time, those that distinguish "from now til then" from "every 6th Monday
at noon."  Making this distinction, I'd propose a simple schema for
allowing systems to set up synchronizer systems using a "cookie cutter"
view of time -- if any of you recall the child's toy Spirograph in which
a circle with a pencil hole through the rim was rolled through a larger
circle to create interesting looping patterns -- in which these periodic
events can be described and then actively "rolled" to create an active
schedule.  Thereby, if in our conference room, we could create not only
mechanisms for "show this and then this", but we could have clients which
knew the intended rates for each input and would trim/pad the input to 
keep all media synchronous.  

I believe that the generic encoding of time is one of the primary 
issues in moving from a document-centered view of the Web to an
information-centered view.

=============================================================
			Jason Bluming
	     Enterprise Integration Technologies
                      jbluming@eit.com

(415) 617 - 8018 (office)    |    459 Hamilton Avenue
      617 - 8019 (fax)       |    Palo Alto, CA 94301
============================================================= 







From vinay@eit.COM  Thu Jan 27 14:24:48 1994 PST
Message-Id: <9401272224.AA23674@eit.COM>
Date: Thu, 27 Jan 94 14:24:48 PST
From: vinay@eit.COM (Vinay Kumar)
Subject: Re: Mosaic Accessories

Shimon,

I am not sure if i understand your proposal correctly. But here are some quick
thoughts.
As of now, there are couple of ways you can communicate with Mosaic:

1. One using the remote-control option based on Mosaic taking SIGUSR1 signal. This
   may or may not be the right thing, but it exists.
2. I had added a patch to XMosaic-1.2  whereby any number of Mosaic's running over
   Wide Area TCP/IP could communicate with each other via IP-Multicast sockets. It
   supports point to point communication over UDP/IP sockets as well. (Take a
   look at, http://www.eit.com/software/mosaic/sh-mosaic.html ).

However, i agree, there should be a comunication channel for other disparate 
applications to talk to, say Mosaic, at a higher sematic level. As an example, one can 
leave XMosaic running on their machine at work, and use some line mode W3 browser
from home over a slow speed link. The line-mode browser should be able to talk to
Mosaic running at work and get some cool info like "view-source" from mosaic, etc..

It would be nice to see an Interface that will turn Mosaic into a mixer/gateway/
translator for other (non)-W3 info browsers to communicate with. I shall look into your
specific suggestions and respond back soon. Right now, i have to hurry to a meeting...

(Hey, by the way, how about using your CircusTalk stuff....).
--
  Vinay Kumar
vinay@eit.com




From settle@merengue.oit.unc.edu  Thu Jan 27 17:44:12 1994 -0500 (EST)
Message-Id: <Pine.3.89.9401271724.B9978-0100000@merengue.oit.unc.edu>
Date: Thu, 27 Jan 1994 17:44:12 -0500 (EST)
From: settle@merengue.oit.unc.edu (Dykki Settle)
Subject: Re: Mosaic



On Thu, 27 Jan 1994, Robert S. Thau wrote:

>    Date: Thu, 27 Jan 1994 14:17:05 -0600
>    From: les_stockton@wiltel.com (Les Stockton x3123)
> 
>    I'd like to be able to get an easy way to  create  documents
>    for  Mosaic.   I was told that you might be able to help me.
>    Any help you can give  would  be  greatly  appreciate.   Any
>    tools  to make things close to WYSIWYG would be appreciated.
>    Thanx.
> 

My name is Dykki Settle, and I have been a lurker on this list for nearly 
nine months.  This semester I am enrolled in a class on software 
engineering, and the task my group has been assigned under client Paul 
Jones, is an effective Unix GUI HTML editor under Motif, that will 
allow users to convert, edit, and test HTML documents.
	
	I am the director/designer of the project due to my relative long 
experience with Mosaic (in all its forms), and the WWW.

	The proposal and communication of our project can be found at URL:
http://sunsite.unc.edu/dykki/W4/w4home.html

	Please look it over, and if you have questions, comments, or issues we 
are likely to run into, please let me know.


Thanks,

:)>
Dykki_Settle@unc.edu

_____________________________________________________________________
<a href="http://sunsite.unc.edu/dykki/dykki.html">To Dykki's World</a>
Global Hypermedia Developer
Office for Information Technology, Computing Systems, UNC-Chapel Hill
(919)962-9107

Internet Information Specialist
University of North Carolina Press, Chapel Hill
(919)966-3561
______________________________________________________________________




From Jared_Rhine@hmc.edu  Thu Jan 27 17:48:36 1994 -0800
Message-Id: <199401280148.RAA25603@osiris.ac.hmc.edu>
Date: Thu, 27 Jan 1994 17:48:36 -0800
From: Jared_Rhine@hmc.edu (Jared_Rhine@hmc.edu)
Subject: WWW indexing and the resource location service

Koster> Are you aware of ALIWEB? It is currently the only system that is
Koster> doing what you want: it retrieves hand-prepered IAFA templates via
Koster> HTTP, and rolls them into a searcheable database (which in turn is
Koster> included in the W3 Catalog; one of the best WWW catalogs about).
Koster> Currently about 20 sites have deployed index files, and the number
Koster> is growing.

I was not aware that ALIWEB was performing its magic via IAFA files.  I,
too, am developing systems to interface with the IAFA indexing system; it's
good to hear it is catching on elsewhere.  I'm taking a slightly different
approach than w3-catalog, though.

My system, called ResInfo for Resource Info, is a site-local resource
maintenance tool.  That is to say, it is a tool designed to track and
maintain information about the information resources available within a
given administrative domain.

ResInfo is a key/value database utilizing a fairly lightweight protocol for
transactions.  Transactions are handled via TCP sockets, allowing any host
(subject to access restrictions) to query the site for information
pertaining to the resources.  Administration of the database (such as
updates) is handled via the same protocol.

I prefer this kind of system because it seems that manual maintenance of the
IAFA files would be rather tedious and prone to error; all around, not fun.
The IAFA files are fairly flexible and useful, but their implementation does
not lend itself to easy interfacing with other indexing systems and
information protocols.

My plan is to instead have IAFA files generated __automatically__ by
periodically querying the ResInfo database and building the template files
based on the information in the ResInfo database.  This means that changes
to the ResInfo database will be automatically reflected elsewhere.  Other
formats besides IAFA files could be extracted from the database, such as
static gopher and web indexes.

The simplicity of the protocol allows for a variety of clients to be written
to access the information in the database.  For instance, some way of
searching the database is required.  While w3-catalog is an excellent tool
and a boon to the Internet community, its searching capabilities are rather
limited.  To search the ResInfo database, I've developed an interactive
document which utilizes HTML+ forms.  Using a gateway to ResInfo, the
interactive document presents a form, allowing the various fields to specify
parameters to the search, such as keywords, author, abstract, dates,
languages, and so forth.  The interactive document formulates that into a
ResInfo query, asks ResInfo for a search, gets the results, and formats the
results in a pleasing HMTL document, complete with hypertext links.  The
format and level of detail of the document returned is specified by the
user.

ResInfo has been designed with the Interpedia project in mind.  Interpedia
articles are stored on the local server in whatever form you wish, perhaps
as plaintext or HTML or postscript.  Each article has a ResInfo entry, and a
variety of associated information such as SOAPs, authors, abstracts,
languages, and so forth.  The search procedure described above is also used
with Interpedia documents.  When you perform a search, you would get back a
list of documents that hit, displayed with varying levels of detail, such as
toggling the display of the abstract.  A link would be present to take you
to the actual document.

Since ResInfo is a fairly simple protocol, it is easy to have simple clients
interact with it.  For example, if you are familiar with Deutsch's and
Weider Internet draft, "A Vision of an Integrated Internet Information
Service", you may view ResInfo as a part of the Resource Location Service
(RLS).  Resource transponders, entities which travel with a resource and
keep track of information about the resource, would easily be able to
contact ResInfo and announce, "Here I am; here's some information about my
history" and all the systems which query ResInfo about resources would
suddenly know about the new resource.

An important part of ResInfo is that it was designed primarily to be
accessed via other automated systems.  I've already mentioned a variety of
these: resource transponders, other parts of the Resource Location Service,
Interpedia gateways, IAFA file generation programs, and other interactive
documents.  Since ResInfo keeps track of the current location(s) of the
resources (probably on the local LAN, but not necessarily), gateways which
wish to describe an instance of a resource can ask ResInfo where it is
currently located and hopefully be fairly assured of getting a correct
answer.  HTML gateways would translate the location information into a link
on the fly, giving a w3-catalog kind of feel.

Since administration commands are built into the protocol, a variety of
gateways could be built for administrative purposes.  My primary tool for
administration of the database is an interactive HTML document which uses
HTTP authentication.  This is difficult to do right, but I think the system
is generally more useful if you can do all aspects of it from within a
single client, your Web browser.  It is pretty slick and makes maintenance
very simple.

I see the primary advantage of ResInfo as being the consolidation of
infosystem administration into an easy to use, automated package.  It is not
infosystem-specific, and allows much of the work of updating the database to
be done automatically. (How about having ftp mirroring software contact
ResInfo to tell it that a new version of a document just got mirrored?)  It
can also expand to fulfill other parts of the Resource Location System, and
it can return the URN of a document instead of a local URL, if desired.  It
integrates well with current systems such as the IAFA indexing project.
Gateways are trivial to write since there is a (loosely) defined API (at
least for Perl).

I currently do not have plans to turn ResInfo into a full-blown part of a
RLS.  It is still highly experimental and I'm playing around to see what
works and what doesn't.  It is not coded for efficiency or speed, but rather
for flexibility and rapid prototyping.  The current implementation is in
perl which means that searches are generally perl regexps.  The database is
implemented as a shared dbm file.  It could be reimplemented with some
faster algorithms and coding techniques, but for now, perl gives acceptable
performance.

I'm not worrying too much about RLS-specific features, such as inter-ResInfo
communication.  The standards of the RLS will certainly not be decided by
me; if standard protocols are agreed upon for those transactions, it should
be fairly easy to write a gateway between that protocol and the ResInfo
protocol.  Although that looks like a very interesting area of research,
that will require external coordination, so I'll put that off for now.

I'm not sure what the current state of the art is for these kind of things.
I haven't looked at Whois++ since I've started writing these kind of
site-oriented databases (I have another one called UserInfo which keeps
track of account information such as centralized mail forwarding info,
site-wide UIDs, directory information, and so forth.  Lots of good gateways
and interfaces written for that).  At this early stage in the development of
this field, it seems like a good idea to get multiple systems using
different paradigms deployed in order to gain some understanding of the
various strengths and weaknesses of the various approaches.

I'd be interested in hearing about other such resource management tools.
The ResInfo database is currently pretty empty, but as the parts fall into
place and the fundamental format of the database is nailed down, it is
getting easier and easier to add data.  The next step is getting some
programs which automatically scan the local resources and update the
database.

- --
Jared Rhine         Jared_Rhine@hmc.edu
wibstr              Harvey Mudd College
                    http://www.hmc.edu/www/people/jared.html

"Society in every state is a blessing, but Government, even in its
best state, is but a necessary evil; in its worst state, an intolerable one."
                                              -- Thomas Paine



From Jared_Rhine@hmc.edu  Thu Jan 27 07:01:12 1994 -0800
Message-Id: <199401280212.SAA26316@osiris.ac.hmc.edu>
Date: Thu, 27 Jan 1994 07:01:12 -0800
From: Jared_Rhine@hmc.edu (Jared_Rhine@hmc.edu)
Subject: WWW indexing and the resource location service

Koster> Are you aware of ALIWEB? It is currently the only system that is
Koster> doing what you want: it retrieves hand-prepered IAFA templates via
Koster> HTTP, and rolls them into a searcheable database (which in turn is
Koster> included in the W3 Catalog; one of the best WWW catalogs about).
Koster> Currently about 20 sites have deployed index files, and the number
Koster> is growing.

I was not aware that ALIWEB was performing its magic via IAFA files.  I,
too, am developing systems to interface with the IAFA indexing system; it's
good to hear it is catching on elsewhere.  I'm taking a slightly different
approach than w3-catalog, though.

My system, called ResInfo for Resource Info, is a site-local resource
maintenance tool.  That is to say, it is a tool designed to track and
maintain information about the information resources available within a
given administrative domain.

ResInfo is a key/value database utilizing a fairly lightweight protocol for
transactions.  Transactions are handled via TCP sockets, allowing any host
(subject to access restrictions) to query the site for information
pertaining to the resources.  Administration of the database (such as
updates) is handled via the same protocol.

I prefer this kind of system because it seems that manual maintenance of the
IAFA files would be rather tedious and prone to error; all around, not fun.
The IAFA files are fairly flexible and useful, but their implementation does
not lend itself to easy interfacing with other indexing systems and
information protocols.

My plan is to instead have IAFA files generated __automatically__ by
periodically querying the ResInfo database and building the template files
based on the information in the ResInfo database.  This means that changes
to the ResInfo database will be automatically reflected elsewhere.  Other
formats besides IAFA files could be extracted from the database, such as
static gopher and web indexes.

The simplicity of the protocol allows for a variety of clients to be written
to access the information in the database.  For instance, some way of
searching the database is required.  While w3-catalog is an excellent tool
and a boon to the Internet community, its searching capabilities are rather
limited.  To search the ResInfo database, I've developed an interactive
document which utilizes HTML+ forms.  Using a gateway to ResInfo, the
interactive document presents a form, allowing the various fields to specify
parameters to the search, such as keywords, author, abstract, dates,
languages, and so forth.  The interactive document formulates that into a
ResInfo query, asks ResInfo for a search, gets the results, and formats the
results in a pleasing HMTL document, complete with hypertext links.  The
format and level of detail of the document returned is specified by the
user.

ResInfo has been designed with the Interpedia project in mind.  Interpedia
articles are stored on the local server in whatever form you wish, perhaps
as plaintext or HTML or postscript.  Each article has a ResInfo entry, and a
variety of associated information such as SOAPs, authors, abstracts,
languages, and so forth.  The search procedure described above is also used
with Interpedia documents.  When you perform a search, you would get back a
list of documents that hit, displayed with varying levels of detail, such as
toggling the display of the abstract.  A link would be present to take you
to the actual document.

Since ResInfo is a fairly simple protocol, it is easy to have simple clients
interact with it.  For example, if you are familiar with Deutsch's and
Weider Internet draft, "A Vision of an Integrated Internet Information
Service", you may view ResInfo as a part of the Resource Location Service
(RLS).  Resource transponders, entities which travel with a resource and
keep track of information about the resource, would easily be able to
contact ResInfo and announce, "Here I am; here's some information about my
history" and all the systems which query ResInfo about resources would
suddenly know about the new resource.

An important part of ResInfo is that it was designed primarily to be
accessed via other automated systems.  I've already mentioned a variety of
these: resource transponders, other parts of the Resource Location Service,
Interpedia gateways, IAFA file generation programs, and other interactive
documents.  Since ResInfo keeps track of the current location(s) of the
resources (probably on the local LAN, but not necessarily), gateways which
wish to describe an instance of a resource can ask ResInfo where it is
currently located and hopefully be fairly assured of getting a correct
answer.  HTML gateways would translate the location information into a link
on the fly, giving a w3-catalog kind of feel.

Since administration commands are built into the protocol, a variety of
gateways could be built for administrative purposes.  My primary tool for
administration of the database is an interactive HTML document which uses
HTTP authentication.  This is difficult to do right, but I think the system
is generally more useful if you can do all aspects of it from within a
single client, your Web browser.  It is pretty slick and makes maintenance
very simple.

I see the primary advantage of ResInfo as being the consolidation of
infosystem administration into an easy to use, automated package.  It is not
infosystem-specific, and allows much of the work of updating the database to
be done automatically. (How about having ftp mirroring software contact
ResInfo to tell it that a new version of a document just got mirrored?)  It
can also expand to fulfill other parts of the Resource Location System, and
it can return the URN of a document instead of a local URL, if desired.  It
integrates well with current systems such as the IAFA indexing project.
Gateways are trivial to write since there is a (loosely) defined API (at
least for Perl).

I currently do not have plans to turn ResInfo into a full-blown part of a
RLS.  It is still highly experimental and I'm playing around to see what
works and what doesn't.  It is not coded for efficiency or speed, but rather
for flexibility and rapid prototyping.  The current implementation is in
perl which means that searches are generally perl regexps.  The database is
implemented as a shared dbm file.  It could be reimplemented with some
faster algorithms and coding techniques, but for now, perl gives acceptable
performance.

I'm not worrying too much about RLS-specific features, such as inter-ResInfo
communication.  The standards of the RLS will certainly not be decided by
me; if standard protocols are agreed upon for those transactions, it should
be fairly easy to write a gateway between that protocol and the ResInfo
protocol.  Although that looks like a very interesting area of research,
that will require external coordination, so I'll put that off for now.

I'm not sure what the current state of the art is for these kind of things.
I haven't looked at Whois++ since I've started writing these kind of
site-oriented databases (I have another one called UserInfo which keeps
track of account information such as centralized mail forwarding info,
site-wide UIDs, directory information, and so forth.  Lots of good gateways
and interfaces written for that).  At this early stage in the development of
this field, it seems like a good idea to get multiple systems using
different paradigms deployed in order to gain some understanding of the
various strengths and weaknesses of the various approaches.

I'd be interested in hearing about other such resource management tools.
The ResInfo database is currently pretty empty, but as the parts fall into
place and the fundamental format of the database is nailed down, it is
getting easier and easier to add data.  The next step is getting some
programs which automatically scan the local resources and update the
database.

--
Jared Rhine         Jared_Rhine@hmc.edu
wibstr              Harvey Mudd College
                    http://www.hmc.edu/www/people/jared.html

"Society in every state is a blessing, but Government, even in its
best state, is but a necessary evil; in its worst state, an intolerable one."
                                              -- Thomas Paine



From Jared_Rhine@hmc.edu  Thu Jan 27 18:37:19 1994 -0800
Message-Id: <199401280237.SAA28053@osiris.ac.hmc.edu>
Date: Thu, 27 Jan 1994 18:37:19 -0800
From: Jared_Rhine@hmc.edu (Jared_Rhine@hmc.edu)
Subject: WWW indexing and the resource location service

(This message originally bounced to www-talk; the original recipients are
listed below.  My apologies to those who are on multiple lists.)

To: www-talk@www0.cern.ch, interpedia@telerama.lm.com, Peter Deutsch
    <peterd@bunyip.com>, Chris Weider <clw@bunyip.com>, Martijn Koster
    <m.koster@nexor.co.uk>, bajan@bunyip.com, quality@sunsite.unc.edu,
    unite@mailbase.ac.uk

Koster> Are you aware of ALIWEB? It is currently the only system that is
Koster> doing what you want: it retrieves hand-prepered IAFA templates via
Koster> HTTP, and rolls them into a searcheable database (which in turn is
Koster> included in the W3 Catalog; one of the best WWW catalogs about).
Koster> Currently about 20 sites have deployed index files, and the number
Koster> is growing.

I was not aware that ALIWEB was performing its magic via IAFA files.  I,
too, am developing systems to interface with the IAFA indexing system; it's
good to hear it is catching on elsewhere.  I'm taking a slightly different
approach than w3-catalog, though.

My system, called ResInfo for Resource Info, is a site-local resource
maintenance tool.  That is to say, it is a tool designed to track and
maintain information about the information resources available within a
given administrative domain.

ResInfo is a key/value database utilizing a fairly lightweight protocol for
transactions.  Transactions are handled via TCP sockets, allowing any host
(subject to access restrictions) to query the site for information
pertaining to the resources.  Administration of the database (such as
updates) is handled via the same protocol.

I prefer this kind of system because it seems that manual maintenance of the
IAFA files would be rather tedious and prone to error; all around, not fun.
The IAFA files are fairly flexible and useful, but their implementation does
not lend itself to easy interfacing with other indexing systems and
information protocols.

My plan is to instead have IAFA files generated __automatically__ by
periodically querying the ResInfo database and building the template files
based on the information in the ResInfo database.  This means that changes
to the ResInfo database will be automatically reflected elsewhere.  Other
formats besides IAFA files could be extracted from the database, such as
static gopher and web indexes.

The simplicity of the protocol allows for a variety of clients to be written
to access the information in the database.  For instance, some way of
searching the database is required.  While w3-catalog is an excellent tool
and a boon to the Internet community, its searching capabilities are rather
limited.  To search the ResInfo database, I've developed an interactive
document which utilizes HTML+ forms.  Using a gateway to ResInfo, the
interactive document presents a form, allowing the various fields to specify
parameters to the search, such as keywords, author, abstract, dates,
languages, and so forth.  The interactive document formulates that into a
ResInfo query, asks ResInfo for a search, gets the results, and formats the
results in a pleasing HMTL document, complete with hypertext links.  The
format and level of detail of the document returned is specified by the
user.

ResInfo has been designed with the Interpedia project in mind.  Interpedia
articles are stored on the local server in whatever form you wish, perhaps
as plaintext or HTML or postscript.  Each article has a ResInfo entry, and a
variety of associated information such as SOAPs, authors, abstracts,
languages, and so forth.  The search procedure described above is also used
with Interpedia documents.  When you perform a search, you would get back a
list of documents that hit, displayed with varying levels of detail, such as
toggling the display of the abstract.  A link would be present to take you
to the actual document.

Since ResInfo is a fairly simple protocol, it is easy to have simple clients
interact with it.  For example, if you are familiar with Deutsch's and
Weider Internet draft, "A Vision of an Integrated Internet Information
Service", you may view ResInfo as a part of the Resource Location Service
(RLS).  Resource transponders, entities which travel with a resource and
keep track of information about the resource, would easily be able to
contact ResInfo and announce, "Here I am; here's some information about my
history" and all the systems which query ResInfo about resources would
suddenly know about the new resource.

An important part of ResInfo is that it was designed primarily to be
accessed via other automated systems.  I've already mentioned a variety of
these: resource transponders, other parts of the Resource Location Service,
Interpedia gateways, IAFA file generation programs, and other interactive
documents.  Since ResInfo keeps track of the current location(s) of the
resources (probably on the local LAN, but not necessarily), gateways which
wish to describe an instance of a resource can ask ResInfo where it is
currently located and hopefully be fairly assured of getting a correct
answer.  HTML gateways would translate the location information into a link
on the fly, giving a w3-catalog kind of feel.

Since administration commands are built into the protocol, a variety of
gateways could be built for administrative purposes.  My primary tool for
administration of the database is an interactive HTML document which uses
HTTP authentication.  This is difficult to do right, but I think the system
is generally more useful if you can do all aspects of it from within a
single client, your Web browser.  It is pretty slick and makes maintenance
very simple.

I see the primary advantage of ResInfo as being the consolidation of
infosystem administration into an easy to use, automated package.  It is not
infosystem-specific, and allows much of the work of updating the database to
be done automatically. (How about having ftp mirroring software contact
ResInfo to tell it that a new version of a document just got mirrored?)  It
can also expand to fulfill other parts of the Resource Location System, and
it can return the URN of a document instead of a local URL, if desired.  It
integrates well with current systems such as the IAFA indexing project.
Gateways are trivial to write since there is a (loosely) defined API (at
least for Perl).

I currently do not have plans to turn ResInfo into a full-blown part of a
RLS.  It is still highly experimental and I'm playing around to see what
works and what doesn't.  It is not coded for efficiency or speed, but rather
for flexibility and rapid prototyping.  The current implementation is in
perl which means that searches are generally perl regexps.  The database is
implemented as a shared dbm file.  It could be reimplemented with some
faster algorithms and coding techniques, but for now, perl gives acceptable
performance.

I'm not worrying too much about RLS-specific features, such as inter-ResInfo
communication.  The standards of the RLS will certainly not be decided by
me; if standard protocols are agreed upon for those transactions, it should
be fairly easy to write a gateway between that protocol and the ResInfo
protocol.  Although that looks like a very interesting area of research,
that will require external coordination, so I'll put that off for now.

I'm not sure what the current state of the art is for these kind of things.
I haven't looked at Whois++ since I've started writing these kind of
site-oriented databases (I have another one called UserInfo which keeps
track of account information such as centralized mail forwarding info,
site-wide UIDs, directory information, and so forth.  Lots of good gateways
and interfaces written for that).  At this early stage in the development of
this field, it seems like a good idea to get multiple systems using
different paradigms deployed in order to gain some understanding of the
various strengths and weaknesses of the various approaches.

I'd be interested in hearing about other such resource management tools.
The ResInfo database is currently pretty empty, but as the parts fall into
place and the fundamental format of the database is nailed down, it is
getting easier and easier to add data.  The next step is getting some
programs which automatically scan the local resources and update the
database.

- - --
Jared Rhine         Jared_Rhine@hmc.edu
wibstr              Harvey Mudd College
                    http://www.hmc.edu/www/people/jared.html

"Society in every state is a blessing, but Government, even in its
best state, is but a necessary evil; in its worst state, an intolerable one."
                                              -- Thomas Paine



From wtwong@eit.COM  Thu Jan 27 18:46:21 1994 PST
Message-Id: <9401280246.AA27452@eit.COM>
Date: Thu, 27 Jan 94 18:46:21 PST
From: wtwong@eit.COM (William Tao-Yang Wong)
Subject: subscribe wtwong@eit.com

subscribe wtwong@eit.com



From kevinh  Thu Jan 27 19:21:42 1994 PST
Message-Id: <9401280321.AA27786@eit.COM>
Date: Thu, 27 Jan 94 19:21:42 PST
From: kevinh (Kevin 'Kev' Hughes)
Subject: Updating httpd 1.1


	Just so everyone knows, I will be updating the server to 1.1
(with the patch) sometime before the end of next week...there shouldn't
be any compatibility problems.
	Also, I've finished the beta for getstats, my web log analyzer,
if anyone's interested. Docs are at

	http://www.eit.com/software/getstats/getstats.html

	-- Kev



From dsr@hplb.hpl.hp.com  Fri Jan 28 11:54:20 1994 GMT
Message-Id: <9401281154.AA01151@manuel.hpl.hp.com>
Date: Fri, 28 Jan 94 11:54:20 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Universal network graphics language

Vinay Kumar writes:

> Recently i saw a running demo from General Magic's "MagicCap" UI
> environment. It seems to do a lot of the stuff mentioned by others on this
> list earlier (assuming i understand the emails correctly ofcourse). MagicCap
> UI  shows a downtown view on the desktop, using a mouse one could navigate
> (VR style) around houses, rooms, hallways, libraries, etc...One could
> customize wall colors, wall papers, posters, and other artifacts in and
> outside the rooms. Drag and drop feature is  supported. Linking of objects
> is thru drag and drop. However i am not sure if linking of objects over
> distributed networks is supported. They claim everything in their 
> environment is an "object" and almost every object could be linked to any
> other object. 

I haven't seen Magic in person, but have read the blurb about their
programming language and seen demos in a television programme about it.
They have a neat scripting language in which you can "fork" processes
over the network link so that you end up with the child process at
the other end. They also support named classes so that if both ends
agree that they share some classes, then the objects can be compressed
to just the data, with the code stripped out for sending down the wire.

My problem with this approach (apart from Magic being a proprietary
language) is that it is too procedural and subject to security concerns
about downloading general purpose code into your own system. I believe
we can develop am approach similar in spirit to SGML for a more declarative
approach in which behaviours are specified using scripts which are highly
restricted in their API. I am currently following this approach with HTML+
for adding client-side checking of forms.

We were recently shown a demo of a "VR" system running on a PC. It used
realistic 3D rendering with distancing and neat behaviours for objects,
e.g. you could open a drawer in a desk and take out a working calculator
or pen. It shouldn't be too hard to realise this kind of thing in the
WWW context.

Dave Raggett



From broccol@arlut.utexas.edu  Fri Jan 28 09:21:31 1994 -0600
Message-Id: <199401281521.JAA06544@csdsun1.arlut.utexas.edu>
Date: Fri, 28 Jan 1994 09:21:31 -0600
From: broccol@arlut.utexas.edu (Jonathan Abbey)
Subject: Re: Universal network graphics language

I'm really surprised that everyone is jumping off from Dr. Sleator's
initial comment to the land of virtual reality and autonomous agents..

I think that simply having a graphical system (ideally with unicode support)
that you can use to telnet with would go a long way towards giving the
Internet a much fluid and flexible face.

Here are some candidates for the job:

NAPLPS.  I've not yet seen the actual specs for the protocol, but I've read
some of Dave Hughes' writings on the things he has accomplished with it, and
it sounds at least flexible enough to handle foreign character sets (such as
Turkish, and other non left-to-right writing systems) and basic graphics.  It
also is supposed to provide support mathematical writing and music.  No word
here on whether it allows you to define fonts and/or objects to be manipulated.

RIPscrip.  This is a display graphics protocol currently growing in popularity
on the PC BBS platform.  It has the ability to do high-resolution graphics
with drawing instructions rather than bitmaps, and it supports some kind
of mouse interaction.

Other existing candidates: the X protocol, PostScript, Display PostScript. 
I'd rule out the X protocol because there is no defined way to be able to save
an "X document", to preserve the graphics and formatting information across an
entire interaction, and because it requires a high-bandwidth, low-latency
connection to function effectively.  PostScript I'd rule out for not having
the kind of interactive features that we'd need.  This leaves Display
PostScript along with NAPLPS and RIPscrip and suchlike things.

Does anyone out there have any pointers to NAPLPS standards information,
and/or pointers to any NAPLPS-compatible freely distributable UNIX software?

-------------------------------------------------------------------------------
Jonathan Abbey				               broccol@arlut.utexas.edu
Applied Research Laboratories                 The University of Texas at Austin
-------------------------------------------------------------------------------



From perrier@onera.fr  Fri Jan 28 16:33:34 1994 +0100
Message-Id: <9401281532.AA08533@minerve.noname>
Date: Fri, 28 Jan 1994 16:33:34 +0100
From: perrier@onera.fr (Christian Perrier)
Subject: 



--
 Christian Perrier
 Materials Testing Laboratory, Materials Engineering Dpt
 ONERA, BP72, 92322 Chatillon Cedex FRANCE
 Voice : +33-1-4673.4563 Fax : +33-1-4673.4142 
 Home  : +33-1-3069.9222 BBS : +33-1-3051.6504 (Fidonet 2:320/213)
 




From wei@xcf.Berkeley.EDU  Fri Jan 28 08:02:44 1994 -0800
Message-Id: <9401281602.AA21358@xcf.Berkeley.EDU>
Date: Fri, 28 Jan 94 08:02:44 -0800
From: wei@xcf.Berkeley.EDU (Pei Y. Wei)
Subject: Re: Universal network graphics language


In some ways, I think the ViolaWWW browser is developing some 
capabilities that could possibly be useful for the ideas being 
discussed here, particuarily relevant along the lines of "objects" 
and "scripts".

Right now, the ViolaWWW that is under development can embed viola 
objects/applications inside of HTML documents. This is useful in that,
for example, if you needed a hyper-active tree widget in your HTML 
document, and HTML+ doesn't happen to define it, you could build it as
a mini viola application. Same thing with customized input-forms that 
could conceivably do complicated client-side checking. Or, complex 
tables. Or, a chess board.

You'd basically have a simple GUI toolkit and language at your disposal,
along with HTML.

To make this a bit more concrete, here's an simple example of one
viola object being embedded into a HTML doc.

----------------------------- 
<P>Watch Big Bird count from zero to infinity: 
<LINK REL="vobj" HREF="http://sesame.st/count.v">.
-----------------------------

Listed below is what the file "http://sesame.st/count.v" would look 
like, just to give people a feel of the language:

----------------------------- 
\class {txtLabel}
\name {count}
\script {
        /* This object counts and displays 
         */
        switch (arg[0]) {
        case "setup":
                /* arg[1] == parent object
                 *
                 * Convention sez to link to the parent object (the 
                 * document object).
                 */
                set("parent", arg[1]);
                after(50000, self(), "tic"); /* start counting in ~5 secs */
                return;
        break;
        case "tic":
                /* Increment and display. Call self after 10000 micro secs.
                 */
                set("label", n++);
                render();
                after(10000, self(), "tic");
                return;
        break;
        }
        usual();
}
\font {normal_large}
\width {200}
\height {50}
\BGColor {darkGreen}
\FGColor {white}
\BDColor {black}
\
----------------------------- 

To implement the chess board interface, you'd *basically* write a 
mini viola application that consists of: objects for representing 
board-cells, chess pieces; an object doing the talking with the chess 
server; a clock object; text fields for IRC type communications...

The ViolaWWW browser could fetch this chess-board-application 
(embedded in HTML or not) from a WWW server, and renders it on the fly.

Now, what I've just described is, I think, more of GUI than Graphics
of a "network graphics language". But the class hierarchy could be 
expanded, and grown, to include other classes of rendering objects. 
The language syntax could be more abbreviated to make program size 
smaller. The viola objects abstraction could be raised to improve 
cross platform portability. Viola could be ported to other platforms.
Security could be better improved without giving up too much flexibility
(currently every object has a security tagging, and all objects 
instantiated from foreign sources are marked as untrusted to system 
priviledges...). etc.


-Pei
                                        Pei Y. Wei  (wei@ora.com)
                                        O'Reilly & Associates, Inc.


PS. you might want to forward this to the original To:list. My machine
    ran out of machine id spots, it said.



From broccol@arlut.utexas.edu  Fri Jan 28 12:14:54 1994 -0600
Message-Id: <199401281814.MAA27687@csdsun1.arlut.utexas.edu>
Date: Fri, 28 Jan 1994 12:14:54 -0600
From: broccol@arlut.utexas.edu (Jonathan Abbey)
Subject: Encryption and Limited Data Sharing on the Web

> O'Reilly and Associates is doing some work on decryption in Mosaic (and other
> browsers), but not the server part.  Our goal, however, is probably different
> from some other people's, and may be something to think about (or it may be
> irrelevant - I'm not sure).  
>
> We want to license books, so we want people to be able to purchase keys to
> view the books.  But we don't want unencrypted versions of the books to get
> out of the browser.
> [...]
>
> Adrian Nye

This is an interesting development, indeed.  It has been obvious to me for
a good while that this sort of thing is an inevitable development.  As long
as the browser is still able to connect to other sources, and as long as
there is no impediment to connecting to non-encrypted servers (and saving
data gained therby), I welcome this.

A lot of interesting questions arise out of this, though..

Are you going to publish your technique for this kind of authentication?  It
would be unfortunate for each information provider to have to develop their
own standard (and browser) for this kind of thing.  Does your authentication
security depend on secrecy?  Do you believe that such secrecy could be
maintained indefinitely?

Are you going to allow printing of data gained from your encrypted servers?
What about cut and paste via X selections / Mac / Windows cut and paste?
What about emailing documents?  Portions of documents?  Document URL's?

Will receivers of the data be able to keep a local encrypted copy so that
they retain access to the data if and when you cease to make the information
available on the net?

Do your keys expire?  Who is responsible for handling such expiration, the
client or the server?

Would a user be able to republish your data if someone accessed their
server with a key that O'Reilly had verified with a digital signature?

Will you be developing a way whereby I could email a URL along with a key
that I paid for so that a friend can view one of your URL's on a single-time
or multiple-access basis?  Will such single-time loans be part of your
standard license?

Would you pay a royalty to someone who published your URL in one of
their documents?

I think that it is important to try and design things so that you maintain
the kind of transferrability that your paper books have, and that you maintain
the ability to freely transfer information that makes the net worth having.
I am troubled by the prospect of an information economy in which the
recipient has less rights of ownership over a document than he or she
does over a paper copy that he or she bought.

This is a very big and very important step for the Internet, it's important to
set precedents to make this as open as possible, I think..

Adrian, with your permission, I'd like to repost your message to
comp.infosystems, comp.infosystems.www, comp.org.eff.talk and
talk.politics.crypto.  I think these issues need public discussion.

-------------------------------------------------------------------------------
Jonathan Abbey				               broccol@arlut.utexas.edu
Applied Research Laboratories                 The University of Texas at Austin
-------------------------------------------------------------------------------



From Jared_Rhine@hmc.edu  Fri Jan 28 11:30:01 1994 -0800
Message-Id: <199401281930.LAA29948@osiris.ac.hmc.edu>
Date: Fri, 28 Jan 1994 11:30:01 -0800
From: Jared_Rhine@hmc.edu (Jared_Rhine@hmc.edu)
Subject: WWW indexing and the resource location service

(This message originally bounced to www-talk; the original recipients are
listed below.  My apologies to those who are on multiple lists.)

To: www-talk@www0.cern.ch, interpedia@telerama.lm.com, Peter Deutsch
    <peterd@bunyip.com>, Chris Weider <clw@bunyip.com>, Martijn Koster
    <m.koster@nexor.co.uk>, bajan@bunyip.com, quality@sunsite.unc.edu,
    unite@mailbase.ac.uk

Koster> Are you aware of ALIWEB? It is currently the only system that is
Koster> doing what you want: it retrieves hand-prepered IAFA templates via
Koster> HTTP, and rolls them into a searcheable database (which in turn is
Koster> included in the W3 Catalog; one of the best WWW catalogs about).
Koster> Currently about 20 sites have deployed index files, and the number
Koster> is growing.

I was not aware that ALIWEB was performing its magic via IAFA files.  I,
too, am developing systems to interface with the IAFA indexing system; it's
good to hear it is catching on elsewhere.  I'm taking a slightly different
approach than w3-catalog, though.

My system, called ResInfo for Resource Info, is a site-local resource
maintenance tool.  That is to say, it is a tool designed to track and
maintain information about the information resources available within a
given administrative domain.

ResInfo is a key/value database utilizing a fairly lightweight protocol for
transactions.  Transactions are handled via TCP sockets, allowing any host
(subject to access restrictions) to query the site for information
pertaining to the resources.  Administration of the database (such as
updates) is handled via the same protocol.

I prefer this kind of system because it seems that manual maintenance of the
IAFA files would be rather tedious and prone to error; all around, not fun.
The IAFA files are fairly flexible and useful, but their implementation does
not lend itself to easy interfacing with other indexing systems and
information protocols.

My plan is to instead have IAFA files generated __automatically__ by
periodically querying the ResInfo database and building the template files
based on the information in the ResInfo database.  This means that changes
to the ResInfo database will be automatically reflected elsewhere.  Other
formats besides IAFA files could be extracted from the database, such as
static gopher and web indexes.

The simplicity of the protocol allows for a variety of clients to be written
to access the information in the database.  For instance, some way of
searching the database is required.  While w3-catalog is an excellent tool
and a boon to the Internet community, its searching capabilities are rather
limited.  To search the ResInfo database, I've developed an interactive
document which utilizes HTML+ forms.  Using a gateway to ResInfo, the
interactive document presents a form, allowing the various fields to specify
parameters to the search, such as keywords, author, abstract, dates,
languages, and so forth.  The interactive document formulates that into a
ResInfo query, asks ResInfo for a search, gets the results, and formats the
results in a pleasing HMTL document, complete with hypertext links.  The
format and level of detail of the document returned is specified by the
user.

ResInfo has been designed with the Interpedia project in mind.  Interpedia
articles are stored on the local server in whatever form you wish, perhaps
as plaintext or HTML or postscript.  Each article has a ResInfo entry, and a
variety of associated information such as SOAPs, authors, abstracts,
languages, and so forth.  The search procedure described above is also used
with Interpedia documents.  When you perform a search, you would get back a
list of documents that hit, displayed with varying levels of detail, such as
toggling the display of the abstract.  A link would be present to take you
to the actual document.

Since ResInfo is a fairly simple protocol, it is easy to have simple clients
interact with it.  For example, if you are familiar with Deutsch's and
Weider Internet draft, "A Vision of an Integrated Internet Information
Service", you may view ResInfo as a part of the Resource Location Service
(RLS).  Resource transponders, entities which travel with a resource and
keep track of information about the resource, would easily be able to
contact ResInfo and announce, "Here I am; here's some information about my
history" and all the systems which query ResInfo about resources would
suddenly know about the new resource.

An important part of ResInfo is that it was designed primarily to be
accessed via other automated systems.  I've already mentioned a variety of
these: resource transponders, other parts of the Resource Location Service,
Interpedia gateways, IAFA file generation programs, and other interactive
documents.  Since ResInfo keeps track of the current location(s) of the
resources (probably on the local LAN, but not necessarily), gateways which
wish to describe an instance of a resource can ask ResInfo where it is
currently located and hopefully be fairly assured of getting a correct
answer.  HTML gateways would translate the location information into a link
on the fly, giving a w3-catalog kind of feel.

Since administration commands are built into the protocol, a variety of
gateways could be built for administrative purposes.  My primary tool for
administration of the database is an interactive HTML document which uses
HTTP authentication.  This is difficult to do right, but I think the system
is generally more useful if you can do all aspects of it from within a
single client, your Web browser.  It is pretty slick and makes maintenance
very simple.

I see the primary advantage of ResInfo as being the consolidation of
infosystem administration into an easy to use, automated package.  It is not
infosystem-specific, and allows much of the work of updating the database to
be done automatically. (How about having ftp mirroring software contact
ResInfo to tell it that a new version of a document just got mirrored?)  It
can also expand to fulfill other parts of the Resource Location System, and
it can return the URN of a document instead of a local URL, if desired.  It
integrates well with current systems such as the IAFA indexing project.
Gateways are trivial to write since there is a (loosely) defined API (at
least for Perl).

I currently do not have plans to turn ResInfo into a full-blown part of a
RLS.  It is still highly experimental and I'm playing around to see what
works and what doesn't.  It is not coded for efficiency or speed, but rather
for flexibility and rapid prototyping.  The current implementation is in
perl which means that searches are generally perl regexps.  The database is
implemented as a shared dbm file.  It could be reimplemented with some
faster algorithms and coding techniques, but for now, perl gives acceptable
performance.

I'm not worrying too much about RLS-specific features, such as inter-ResInfo
communication.  The standards of the RLS will certainly not be decided by
me; if standard protocols are agreed upon for those transactions, it should
be fairly easy to write a gateway between that protocol and the ResInfo
protocol.  Although that looks like a very interesting area of research,
that will require external coordination, so I'll put that off for now.

I'm not sure what the current state of the art is for these kind of things.
I haven't looked at Whois++ since I've started writing these kind of
site-oriented databases (I have another one called UserInfo which keeps
track of account information such as centralized mail forwarding info,
site-wide UIDs, directory information, and so forth.  Lots of good gateways
and interfaces written for that).  At this early stage in the development of
this field, it seems like a good idea to get multiple systems using
different paradigms deployed in order to gain some understanding of the
various strengths and weaknesses of the various approaches.

I'd be interested in hearing about other such resource management tools.
The ResInfo database is currently pretty empty, but as the parts fall into
place and the fundamental format of the database is nailed down, it is
getting easier and easier to add data.  The next step is getting some
programs which automatically scan the local resources and update the
database.

- - - --
Jared Rhine         Jared_Rhine@hmc.edu
wibstr              Harvey Mudd College
                    http://www.hmc.edu/www/people/jared.html

"Society in every state is a blessing, but Government, even in its
best state, is but a necessary evil; in its worst state, an intolerable one."
                                              -- Thomas Paine
------- end -------



From uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch  Fri Jan 28 13:06:00 1994 PST
Message-Id: <2D497F30@MSMAIL.INDY.TCE.COM>
Date: Fri, 28 Jan 94 13:06:00 PST
From: uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch (Fisher Mark)
Subject: RE: Mosaic Accessories


This type of idea is what is needed to move software into the next 
generation: where various programs can be combined into compound documents 
(to borrow a Microsoft-ism) without respect to the differences between the 
programs.  Accessories plus a universal network graphics language would 
allow a whole new category of programs (can everyone say "killer apps"? :) 
for monitoring and exploring information.  (For example, a live, graphical 
display of traffic congestion between major Internet backbone sites.)  As a 
long time UNIX user (starting with Version 6 in 1978), I am very happy to 
see the idea of multiple cooperating programs extended into the GUI world. 
 The announcement that OLE and CORBA should interoperate may also be cause 
for celebration if it brought off.  Here, here!
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From ZAPANTIS@uvphys.phys.UVic.CA  Fri Jan 28 12:04:01 1994 -0800 (PST)
Message-Id: <940128120402.20400b2f@uvphys.phys.UVic.CA>
Date: Fri, 28 Jan 1994 12:04:01 -0800 (PST)
From: ZAPANTIS@uvphys.phys.UVic.CA (Nik Zapantis, UVic Physics, Victoria BC)
Subject: Need help with FTP URL

I am trying to setup an FTP URL as part of my home page, but I am having no
luck getting it to work.

the setup: VMS(5.5-2) httpd server from CERN, Mosaic 2.0 and MULTINET.

Anonymous FTP works fine interactively, but when used in the URL
	 "ftp://info.phys.uvic.ca/"

Mosaic tells me that either the server is not responding, or the file cannot
be found.

When I run CERN's line-mode WWW browser with Verbose turned on I get the
following error message, _after_ the client successfully connects to the 
anonymous FTP server:
...
Tx: RETR /
 Rx: 550 %RMS-F-SYN, file specification syntax error
  Tx: CWD /
 Rx: 550 %RMS-F-SYN, file specification Can't access `ftp://info.phys.uvic.ca/'
...

I have tried putting a directory name in the URL, but it did not help.

If I change the URL to point to a Document, such as
            "ftp://info.phys.uvic.ca/test.doc"

the line-mode WWW client fetches the file, but MOSAIC still tells me that it 
cannot.

Am I missing something here? Shouldn't I get a directory listing when a
connection is made to the FTP server?

BTW, I am not using an httpd config file, if this is where the problem is.

thanks in advance,

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
| Nik Zapantis                  | zapantis@uvphys     (BITNET)  |
| Dept. of Physics & Astronomy  | 45393::zapantis(HEPnet/SPAN)  |
| University of Victoria        | zapantis@uvphys.phys.UVic.CA  |
| Victoria, BC                  | Phone: (604)721-7729          |
| V8W 3P6                       | FAX:   (604)721-7715          |
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++




From Jared_Rhine@hmc.edu  Fri Jan 28 12:09:52 1994 -0800
Message-Id: <199401282009.MAA01905@osiris.ac.hmc.edu>
Date: Fri, 28 Jan 1994 12:09:52 -0800
From: Jared_Rhine@hmc.edu (Jared_Rhine@hmc.edu)
Subject: Sincerest apologies

I've recently been informed that I've ended up sending multiple copies of a
(rather lengthy) letter to www-talk.  I wish to express my most sincere
apologies.  Although I'm sure I look like an idiot, I must plead innocence;
all of my posts have been returned with the following bounce:

   ----- Transcript of session follows -----
sh: /usr/local/bin/deliver: not found
"|/usr/local/bin/deliver -b /userd/tbl/hypertext/WWW/Archive/www-talk.archive"... unknown mailer error 1
sh: /usr/local/bin/deliver: not found
"|/usr/local/bin/deliver -b /userd/tbl/Mailboxes/WWW_Talk_Unread.mbox/mbox"... unknown mailer error 1
554 "|/usr/lib/sendmail -odi -oi -f www-talk-request@info.cern.ch www-talk-members"... Service unavailable

   ----- Unsent message follows -----
[...]

The last line (sendmail www-talk-members: service unavailable) caused me to
believe the message had not been actually delivered, which I believe was a
reasonable assumption.

My apologies for the worldwide sucking of disk space that I've caused.

Much thanks to Tim Evans for preventing my further embarrassment.

--
Jared Rhine         Jared_Rhine@hmc.edu
wibstr              Harvey Mudd College
                    http://www.hmc.edu/www/people/jared.html

"Fornicate and take drugs against the terrible strain of idiots who govern
 the world."
    -- Albert Szent-Gyorgyi, Nobel Laureate in Medicine and Physiology



From connolly@bullwinkle.atrium.com  Fri Jan 28 14:24:54 1994 CST
Message-Id: <9401282024.AA08304@bullwinkle.atrium.com>
Date: Fri, 28 Jan 94 14:24:54 CST
From: connolly@bullwinkle.atrium.com (Daniel W. Connolly)
Subject: FWD: Status of www_and_frame (aka "WWW Meets FrameMaker")]


I tried to send this to www-announce, but I got:
550 "sendmail..." cannot create output
or some such.

Anyway, sorry for any duplication...

------- Forwarded Message

Message-Id: <9401281658.AA08210@bullwinkle.atrium.com>
To: Axel Boldt <boldt@math.ucsb.edu>, Bob Stayton <bobs@sco.com>,
        Bruce White <bw@ncrtory.torreypinesca.ncr.com>,
        Cindy Baca <baca@luke.atdiv.lanl.gov>,
        Daniel Miles Kehoe <fortuity!kehoe@uu6.psi.com>,
        Daniel Rich <drich@sandman.lerc.nasa.gov>,
        Donald Beaudry <don@vicorp.com>, Dory Meynert <dam@microplex.com>,
        Erin MacNeil <macneri@statcan.ca>, Jiuhong Chen <v3_dd@vxeng.cern.ch>,
        Joe Rehder <joe@vab02.larc.nasa.gov>,
        Jon Prime <prime@rocket.sanders.com>,
        Juan-Manuel Guijarro Plaza <guijarro@sunsoft.cern.ch>,
        "Kelley L. Fugelso" <klfugel@sandia.gov>,
        Koz Tembekjian <koz@acadia.jpl.nasa.gov>,
        Matt Ranney <mjr@syl.dl.nec.com>, Nick Shore <nrs@planet.bt.co.uk>,
        Omy Ronquillo <omy@San-Jose.ate.slb.com>,
        Rob Gabbard <rob.gabbard@sdrc.com>,
        Robert Stabl <stabl@informatik.uni-muenchen.de>,
        Sam Moore <smoore@tx.ncsu.edu>,
        "Song W. Koh" <swk@mlb.semi.harris.com>,
        "Thomas L. Georges" <tlg@pc3q.corp.harris.com>,
        Tim Scheitlin <scheitln@niwot.scd.ucar.edu>,
        hermann@cul-ipn.uni-kiel.de, hmallat@vinkku.hut.fi
Cc: www-announce@www0.cern.ch, harward@convex.com, tonyj@bsdi.com
Subject: Status of www_and_frame (aka "WWW Meets FrameMaker")
Date: Fri, 28 Jan 94 10:58:09 CST
From: "Daniel W. Connolly" <connolly@atrium.com>
Content-Length: 2285


A little over a year ago while I was working at Convex Computer
Corporation in Dallas, I developed and distributed www_and_frame, a
package for developing and maintaing HTML documents using FrameMaker.
It works well with FrameMaker 3.x, and it seems that a few folks are
using it successfully.

In February of 1993, I moved from Dallas to Austin and started working
for Atrium Technologies. In stark contrast from Convex, where I had
essentially a T1 link to the internet and a gigabyte of disk space on
my desk top, Atrium Technologies is a start-up company with few
resources, and my new position allows me little time to contribute to
the WWW project.

Since then, I have received numerous requests for help with
www_and_frame (especially since the release of FrameMaker 4.0). I have
attempted to help some folks, but since I don't even have the source
to www_and_frame handy, I haven't been very helpful. I apologize to
those whose mail I acknowledged slowly or not at all.

If anyone else would like to take up ownership of www_and_frame and
maintain it, I think it would be a valuable service to the WWW
community. I'm willing to work closely with one person with novel
issues if that person can deal with the known problems and
installation issues.

Also, just before I left Convex, I was working on a revision of
www_and_frame (most of the XLisp code has been re-written in C), but
it was not ready for release when I changed jobs. I got permission to
put the source on a tape and take it with me when I left Convex. I'm
willing to distribute it to anyone who would like to do development on
www_and_frame -- I doubt it would be of much use to the general user
community. (Also, I need a tape drive to read the tape!)

Atrium is getting a dial-up connection to the Internet, so I may be a
little better connected in the near future. But in the past year I
have also become a husband and a home owner, and I have little time to
devote to WWW.

Good luck!

Dan Connolly
Atrium Technologies, Austin Texas
(512) 328-6977

"I can see their logic (in that the HP 710 could have a read/write
colormap, and colormaps are black magic, and we didn't kill a chicken
this month, and so on and so on) but I don't think colormaps are the cause."
		-- Ken Harward <harward@convex.com>, 1993

------- End of Forwarded Message




From uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch  Fri Jan 28 16:49:00 1994 PST
Message-Id: <2D49B135@MSMAIL.INDY.TCE.COM>
Date: Fri, 28 Jan 94 16:49:00 PST
From: uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch (Fisher Mark)
Subject: RE: Service unavailable (was: RE: Sincerest apologies)


I have seen this same message whenever I successfully post to this mailing 
list.  Anyone have any idea why?  We have a mail connection to Internet via 
UUNET.
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From Steve_Mullinax@ccm.hf.intel.com  Fri Jan 28 15:40:22 1994 PST
Message-Id: <940128154022_8@ccm.hf.intel.com>
Date: Fri, 28 Jan 94 15:40:22 PST
From: Steve_Mullinax@ccm.hf.intel.com (Steve Mullinax)
Subject: MS Word to HTML Conversion (Ga. Tech)

I picked up a package from Georgia Tech containing Microsoft Word macros to 
allow HTML editing.  Unfortunately, I didn't see anything in the package about 
which version of Word it is (or isn't) compatible with.

Also didn't see any way to contact the developers, either with the package or 
browsing around the Ga. Tech Web.

Can anyone give me leads or help answer the version question?



From omy@San-Jose.ate.slb.com  Fri Jan 28 16:31:18 1994 PST
Message-Id: <9401290031.AA09964@San-Jose.ate.slb.com>
Date: Fri, 28 Jan 94 16:31:18 PST
From: omy@San-Jose.ate.slb.com (Omy Ronquillo)
Subject: Question on addressing.


Hi,

What do I need to do to allow this type of addressing?

http://www.some.domain.com/~username/welcome.html

Thanks.

Omy



From montulli@stat1.cc.ukans.edu  Sun Jan 30 15:11:26 1994 CST
Message-Id: <9401302111.AA46242@stat1.cc.ukans.edu>
Date: Sun, 30 Jan 94 15:11:26 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: How to fix this unsubscribe/subscribe noise!

> 
> 
> Being a www-subscriber for only 5 days I'm amazed and very annoyed by
> everybody sending subscription req. here.
> 
> Couldn't the mail-robot be modified to ignore all e-mail or redirect every
> e-mail that has the word "subscribe" or "unsubscribe" in it to the
> appropriate robot?  Just some thoughts.
> 
ListProcessor 6.0 by Anastasios Kotsikonas will return all mail
to the sender that contains any of several common command words that
should usually be sent to listserv rather than the list itself.

The Lynx-dev list runs this software and only one message in 6
months has slipped through.  Over 100 subscribe and unsubscribe
messages were correctly rejected!

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From joe@MIT.EDU  Sun Jan 30 20:35:40 1994 EST
Message-Id: <9401310135.AA00632@theodore-sturgeon.MIT.EDU>
Date: Sun, 30 Jan 94 20:35:40 EST
From: joe@MIT.EDU (joe@MIT.EDU)
Subject: Announcing tkWWW-0.10

tkWWW Version 0.10 beta (joe@athena.mit.edu)
--------------------------------------------
WHAT IS THIS?
-------------
World Wide Web (WWW) is a hypertext project which seeks to build a
world wide network of hypertext links.  There are several different
browsers for this system including a simple tty interface.

Tk is an interpreted toolkit which allows one to build X11 applications
quickly and easily.

tkWWW is a Tk interface to (WWW).

Since the entire user interface is written in an interpreted language,
it is very easy to make modifications and extensions to the system.
tkWWW is the first X11 browser with the ability to edit HTML!!!!!

To get tkWWW 0.10
---------------------
tkWWW can be anonymous ftped from 

info.cern.ch:/pub/www/src/tkWWW-0.10.tar.Z
ftp.x.org:/contrib/tkWWW-0.10.tar.Z
harbor.ecn.purdue.edu:/pub/tcl/extensions/tkWWW-0.10.tar.Z

To Install
----------
In this directory

1. Type "./configure"
2. Type "make"
3. Type "make install"







From joe@MIT.EDU  Sun Jan 30 20:43:49 1994 EST
Message-Id: <9401310143.AA00662@theodore-sturgeon.MIT.EDU>
Date: Sun, 30 Jan 94 20:43:49 EST
From: joe@MIT.EDU (joe@MIT.EDU)
Subject: Announcing tkWWW-0.10

tkWWW Version 0.10 beta (joe@athena.mit.edu)
--------------------------------------------
WHAT IS THIS?
-------------
World Wide Web (WWW) is a hypertext project which seeks to build a
world wide network of hypertext links.  There are several different
browsers for this system including a simple tty interface.

Tk is an interpreted toolkit which allows one to build X11 applications
quickly and easily.

tkWWW is a Tk interface to (WWW).

Since the entire user interface is written in an interpreted language,
it is very easy to make modifications and extensions to the system.
tkWWW is the first X11 browser with the ability to edit HTML!!!!!

To get tkWWW 0.10
---------------------
tkWWW can be anonymous ftped from 

info.cern.ch:/pub/www/src/tkWWW-0.10.tar.Z
ftp.x.org:/contrib/tkWWW-0.10.tar.Z
harbor.ecn.purdue.edu:/pub/tcl/extensions/tkWWW-0.10.tar.Z

To Install
----------
In this directory

1. Type "./configure"
2. Type "make"
3. Type "make install"







From akgul@bilkent.edu.tr  Mon Jan 31 08:48:41 1994 +0300 (EET)
Message-Id: <199401310548.AA03368@firat.bcc.bilkent.edu.tr>
Date: Mon, 31 Jan 1994 08:48:41 +0300 (EET)
From: akgul@bilkent.edu.tr (Mustafa Akgul)
Subject: Re: How to fix this unsubscribe/subscribe noise!

I am  using the same software that Lou mentioned.
It eliminates all subs, unsubscribe and all bounced mails.

Moreover; it is quite easy to install; and much powerful than
MailRobot.

I think it would be good idea to switch to Tasos's
ListProcessor. Current version is 6.0c+ and available from
cs.bu.edu.

Regards




From alanb@ncsa.uiuc.edu  Mon Jan 31 05:25:25 1994 CST
Message-Id: <9401311125.AA20124@void.ncsa.uiuc.edu>
Date: Mon, 31 Jan 94 05:25:25 CST
From: alanb@ncsa.uiuc.edu (Alan Braverman)
Subject: Question on addressing.

Omy Ronquillo writes:
> 
> Hi,
> 
> What do I need to do to allow this type of addressing?
> 
> http://www.some.domain.com/~username/welcome.html
> 
> Thanks.
> 
> Omy

Check out the "Personal Webs" link at good ol' U of I:

	http://www.cen.uiuc.edu/

--
Alan Braverman
Software Development Group
National Center for Supercomputing Applications
alanb@ncsa.uiuc.edu



From tkevans@barkeep.es.dupont.com  Mon Jan 31 09:44:20 1994 -0500 (EST)
Message-Id: <9401311444.AA12802@barkeep>
Date: Mon, 31 Jan 1994 09:44:20 -0500 (EST)
From: tkevans@barkeep.es.dupont.com (Tim Evans)
Subject: Wais Stops Working with NCSA httpd 1.1

I've just installed NCSA httpd 1.1, using almost totally default
conf files, except for changing DocumentRoot, and am finding that
my WAIS interface no longer works.  This include both local WAIS
clients and the httpd WAIS gateway.  Can someone point me in the 
right direction?  Thanks.

-- 
Tim Evans                     |    E.I. du Pont de Nemours & Co.
tkevans@eplrx7.es.dupont.com  |    Experimental Station
(302) 695-9353/8638 (FAX)     |    P.O. Box 80357
EVANSTK AT A1 AT ESVAX        |    Wilmington, Delaware 19880-0357



From guenther.fischer@hrz.tu-chemnitz.de  Mon Jan 31 16:29:34 1994 +0100 (MET)
Message-Id: <9401311529.AA23938@flash1.hrz.tu-chemnitz.de>
Date: Mon, 31 Jan 1994 16:29:34 +0100 (MET)
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: I've added AddHref to http-1.1

Hi,

try: http://www.tu-chemnitz.de/ftp-home/pub/MS-Windows/demo

I'm happy with the FancyIndexing in http-1.1 and AddIcons and ...

Because I had have done something for the view of our ftp archive 
(simtel, MS-Windows), I want to have the same feature to look in
an archive. So I've added a AddHref derective to the srm.conf
and some small changes in the server to get it.

You can say:
AddHref /cgi-bin/unarch .zip .tar.Z

then the Icon for such files will be a selector. If you select you
will get the list of the archive.

Try it and vote for adding the changes to httpd?

	~Guenther

-- 
Name:      Guenther Fischer / Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361     / mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From sg04%kesser@gte.com  Mon Jan 31 14:01:19 1994 EST
Message-Id: <9401311901.AA24655@kesser.cisl214>
Date: Mon, 31 Jan 94 14:01:19 EST
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: re: Mosaic Accessories

There are a couple of design "principles" that (at least in my
opinion) seem important for the design of the ACI (accssory-client-interface):

1. It might be possible to "hack" an implementation out of
   the current viewer & remote control structures (or modify TkWWW).
   In fact I am already in the process of modifing TkWWW. Still,
   what good is a local version done by one PhD working at a
   telco research lab? It might be good for a paper at the next
   multimedia conference (Indeed, I have just got one accepted at
   the IEEE MultiMedia'93 Conference in Boston). But what good
   does this do? I also had a paper at ACM Siggraph/MultiMedia'93,
   and it didn't seem to do much. The multimedia world is not
   really paying much attention to what gets done at the IEEE or ACM
   conferences.

   Mosaic, by dint of its rapid acceptence in the internet world, and
   concurrent craze in public for exploring the internet, is becoming
   a tremendous vehicle for shaping the "road-scape" of the information 
   highway. Thus, if we can get this feature imbeded into the
   Mosaic software, we can have a significant impact on the nature
   of that infomation highway.

2. The Accessory mechanism is meant to be more than just a way to
   do symbionic editors. It is meant to combine multiple existing
   tools (hotlists, document source viewers, etc.) and provide a
   way for implementing a new class of tools, in addition to the
   symbionic HTML editors

3. The current WWW model is largely (though not entirely) one
   of fetch, then static display of document pages. Lots of
   people have been looking for more interactive modes of interaction.
   For example, game designers would like a more dynamic mode of
   interaction. The accessory mechanism really opens this up.

   One puts all one's dynamic interaction into the accessory. And
   since it has a two-way pipe with the WWW client (e.g. Mosaic)
   one can "piggy-back" on WWW. Things that correspond to static
   documents can remain on the Mosaic side, while all dynamic
   communications can remain on the accessory side.

   Another example. I saw a while back a guy who wanted to link
   live video captions to WWW documents. That is, lets have Mosaic
   documents being cued by the TV feed. One thing my group has toyed
   with is scanning the "hearing impared closed caption" info (you
   can buy a box for this), and use it to key up documents. E.g.
   one could have an accessory always running looking for hot
   news stories. When one came through, one would fire off a WAIS
   query, bundle up the results with the video clip and send it
   off to the person who was interested. (Great product for 
   the Intelligence Analysts at the CIA who spend a lot of their
   time scanning CNN :-)  ).

4. Basically, what I am calling for is a "escape-hatch" were all
   such dynamic modes of interaction could operated in tandem
   with Mosaic. But it must be a standard. So that I can plug
   into all sorts of other things on my machine (from Lotus Notes,
   to Mac HyperCards, to DataBases, etc.) 


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Yechezkal-Shimon Gutfreund		 	   sgutfreund@gte.com [MIME]
GTE Laboratories, Waltham MA        http://www.gte.com/circus/home/home.html
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=


   



From drummond@aristarchus.rutgers.edu  Mon Jan 31 14:11:09 1994 EST
Message-Id: <CMM-RU.1.3.760043469.drummond@aristarchus.rutgers.edu>
Date: Mon, 31 Jan 94 14:11:09 EST
From: drummond@aristarchus.rutgers.edu (Walt Drummond)
Subject: Patches to NCSA httpd 1.1

Hi all,

I've made some changes to the NCSA httpd 1.1 server.  The changes are
documented at http://www-ns.rutgers.edu/httpd and the patch is
available from ftp://rutgers.edu/pub/httpd_1.1/1.1-patch

The changes enhance access control and provide much more control over
the look and feel of directory indexes.  If you intend to use the
patches or have suggestions, additions, or bug reports, please feel
free to contact me.

					Walt

_________________________________________________________
Walt Drummond (drummond@noc.rutgers.edu)
Network Services
Rutgers University Computing Services
 - Lost: One mind. Owner sad. Reward.



From tkevans@barkeep.es.dupont.com  Mon Jan 31 16:53:55 1994 -0500 (EST)
Message-Id: <9401312153.AA01443@barkeep>
Date: Mon, 31 Jan 1994 16:53:55 -0500 (EST)
From: tkevans@barkeep.es.dupont.com (Tim Evans)
Subject: Follow-up:  Wais and NCSA httpd 1.1

Earlier today, I posted to this list a message asking about my
Wais hyperlinks failing since I had installed NCSA httpd 1.1.

I have resolved this problem, and wanted to share the details with
the list.

To begin with, it turns out that the problem was limited *only* to
Solaris 2.3 Web browsers (specifically, NCSA Mosaic 2.1 and lynx
2.1.1) built with the freeWAIS 0.202 libraries.  All my other
systems--Suns running SunOS 4.1.3, RS/6000's running AIX 3.2.5,
and SGI's running IRIX 4.0.5--did not encounter these problems.

Since this was a Solaris-2.3-specific problem, I have rebuilt the
freeWAIS distribution, then rebuilt NCSA Mosaic 2.1.  This resolved
the problem.  I would presume, although I haven't rebuilt it, that
the problem will go away when I re-do lynx as well.

-- 
Tim Evans                     |    E.I. du Pont de Nemours & Co.
tkevans@eplrx7.es.dupont.com  |    Experimental Station
(302) 695-9353/8638 (FAX)     |    P.O. Box 80357
EVANSTK AT A1 AT ESVAX        |    Wilmington, Delaware 19880-0357



From DLCROSS@ucs.indiana.edu  Mon Jan 31 17:08:04 1994 EST
Message-Id: <9401312208.AA29416@dxmint.cern.ch>
Date: Mon, 31 Jan 94 17:08:04 EST
From: DLCROSS@ucs.indiana.edu (DLCROSS@ucs.indiana.edu)
Subject: RE: MS Word to HTML Conversion (Ga. Tech)

I would love to get hold of that package! Is it "freeware"?

I am working on an Object-oriented HTML database for cave exploration and reserach data, and have been writing oodles of MS WOrd for Windows macros. I have
macros which make hypermedia jumps, install anchors automatically, update the
<base ..> tag, and handle the creation of html documents which are either
themselves object representations or collections of them (the file structure
is divorced from the object structure).

Anyone else with Windows utilities for HTML?

Alan Canon
dlcross@ucs.indiana.edu



From marca@eit.COM  Tue Feb  1 00:01:35 1994 GMT
Message-Id: <199402010001.AAA03035@threejane>
Date: Tue, 1 Feb 1994 00:01:35 GMT
From: marca@eit.COM (Marc Andreessen)
Subject: re: Mosaic Accessories

sg04%kesser@gte.com writes:
> 1. It might be possible to "hack" an implementation out of
>    the current viewer & remote control structures (or modify TkWWW).
>    In fact I am already in the process of modifing TkWWW. Still,
>    what good is a local version done by one PhD working at a telco
>    research lab?

Two non-PhD's (one of them an undergrad) working at a government
research lab essentially knocked out Mosaic in 3 months.  In other
words, it could do a lot of good.

Cheers,
Marc



From mcrae@ora.com  Mon Jan 31 16:48:31 1994 -0800
Message-Id: <199402010048.AA29918@rock.west.ora.com>
Date: Mon, 31 Jan 94 16:48:31 -0800
From: mcrae@ora.com (Christopher J. McRae)
Subject: ADDENDUM #2: SIGWEB #4 (Sunnyvale, CA)


  We are pleased to announce that our panel has been expanded to include
John Duhring, Director of Business Development for WAIS, Inc.  The other
five panelists will be
  Dale Dougherty        O'Reilly & Associates
  Barry Leiner          Internet Activities Board and USRA
  Clifford Lynch        UC Division of Library Automation
  Marty Tenenbaum       Enterprise Integration Technologies
  Terry Winograd        Stanford University

The original announcement is included below for those who missed it.

Unfortunately, this meeting will not be multicast - but it will be videotaped
and the recording will be published on the net.  

Chris
-----------------------------------------------------------------------
Christopher McRae			            mcrae@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036


To: sig-web@library.ucsf.edu
Cc: gopher-news@boombox.micro.umn.edu, wais-talk@think.com,
    www-talk@info.cern.ch
Subject: ANNOUNCEMENT: SIGWEB #4 (Sunnyvale, CA)
Date: Fri, 14 Jan 94 14:55:45 -0800
From: "Christopher J. McRae" <mcrae>



				SIGWEB #4
		       Where Are We Going, and Why?


A panel discussion in two parts, featuring...
  Dale Dougherty        O'Reilly & Associates
  Barry Leiner          Internet Activities Board and USRA
  Clifford Lynch        UC Division of Library Automation
  Marty Tenenbaum       Enterprise Integration Technologies
  Terry Winograd        Stanford University

Part 1: The Global/National Internet Community
The first part of the meeeting will be a general discussion about the 
state-of-the-art in (internet-based) information systems and where it 
is/should-be/will-be going.  How do the Internet community and Internet-based 
information tools fit into the development of a National Information 
Infrastructure in the U.S.?  What is the future of Internet-based information 
systems?  What are the biggest obstacles facing us in the next five
years?

Part 2: The S.F. Bay Area Internet Community (i.e. SIGWEB)
The second part of this meeting will focus more narrowly on how the local
community, i.e. SIGWEB, fits into the vision(s) elucidated in the first part.
Our intention is for the meeting to help forge a more solid identity/mission
for the group.  Who are we?  Why do we show up for these meetings?  Where are
we each/all going?  What is the role of SIGWEB?  If there's anything else we
should be doing, who is willing/able to do it and what kind of help do you
need?

Prepared statements will be kept to a minimum, this is an audience
participation free-for-all.  Please come prepared to join in the discussion.

Location: Amdahl Corporation - Sunnyvale, CA
    Date: February 3rd, 1994
    Time: 2:00 - 5:00 PM

Directions to be announced shortly.

-----------------------------------------------------------------------
Christopher McRae			            chrism@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036

To: sig-web@library.ucsf.edu
Cc: gopher-news@boombox.micro.umn.edu, wais-talk@think.com,
    www-talk@info.cern.ch
Subject: ADDENDUM: SIGWEB #4 (Sunnyvale, CA)
Date: Mon, 17 Jan 94 15:04:59 -0800
From: "Christopher J. McRae" <mcrae>


(See below for a copy of the original announcement)

Directions to SIG-WEB #4 at Amdahl Building 7 in Santa Clara:
------------------------------------------------------------------------------

Building 7 is in Amdahl's Santa Clara complex at the intersection of Central
and San Tomas Expressways.  The expressway interchange makes the directions
more complicated than they should be.

The address is 2251 Lawson Lane in Santa Clara.  Lawson Lane is the off-ramp
between northbound San Tomas and westbound Central.  From some directions it
may be more convenient to use the other entrance on Scott Blvd.

Enter at the Building 7 Lobby.  The SIG-WEB meeting will be in the cafeteria.

>From the East Bay:
   * Use I-880 or I-680 to southbound Montague Expressway
   * Get in the left lane before you reach 101. (you won't have time after it.)
   * Montague becomes San Tomas Expressway at Hwy 101
   * Take the next left at the light immediately after 101 at Scott Blvd
   * Turn right at the first driveway (Amdahl Bldg 8)
   * Turn right and drive around the Building 8/7 complex to the front of
     Building 7

>From the peninsula: 
   * Use 101 "southbound" (it's really eastbound).  The San Tomas Expressway
     exit is just after Great America in Santa Clara.  Exit right to
     southbound San Tomas Expressway.  (The other direction is called Montague
     so you can only go southbound on San Tomas.)
   * Unless the road is clear, there isn't time to safely get in the left
     lane for Scott Blvd so STAY IN THE RIGHT LANE
   * Go to the interchange at Central Expressway immediately after Scott
     Blvd.
   * Go underneath Central Expressway and exit right to eastbound Central
   * Exit again to northbound San Tomas.  Go underneath Central again.
   * Exit again... this off-ramp is Lawson Lane.  Amdahl Building 7 is the
     first driveway on the left.

>From San Jose:
   * Use 101 "northbound" to San Tomas Expressway in Santa Clara.  It's past
     the San Jose Airport and Trimble/De La Cruz exit.
   * Unless the road is clear, there isn't time to safely get in the left
     lane for Scott Blvd so stay in the right lane
   * follow instructions as above from the peninsula

>From parts of Santa Clara, Cupertino, etc:
   * Use northbound San Tomas Expressway
   * Go underneath Central Expressway and exit right immediately after it,
     as if you were going to westbound Central Expressway.
   * The off-ramp is Lawson Lane.  Make the first left to the Amdahl Bldg 7.





To: sig-web@library.ucsf.edu
Cc: gopher-news@boombox.micro.umn.edu, wais-talk@think.com,
    www-talk@info.cern.ch
Subject: ANNOUNCEMENT: SIGWEB #4 (Sunnyvale, CA)
Date: Fri, 14 Jan 94 14:55:45 -0800
From: "Christopher J. McRae" <mcrae>



				SIGWEB #4
		       Where Are We Going, and Why?


A panel discussion in two parts, featuring...
  Dale Dougherty        O'Reilly & Associates
  Barry Leiner          Internet Activities Board and USRA
  Clifford Lynch        UC Division of Library Automation
  Marty Tenenbaum       Enterprise Integration Technologies
  Terry Winograd        Stanford University

Part 1: The Global/National Internet Community
The first part of the meeeting will be a general discussion about the 
state-of-the-art in (internet-based) information systems and where it 
is/should-be/will-be going.  How do the Internet community and Internet-based 
information tools fit into the development of a National Information 
Infrastructure in the U.S.?  What is the future of Internet-based information 
systems?  What are the biggest obstacles facing us in the next five
years?

Part 2: The S.F. Bay Area Internet Community (i.e. SIGWEB)
The second part of this meeting will focus more narrowly on how the local
community, i.e. SIGWEB, fits into the vision(s) elucidated in the first part.
Our intention is for the meeting to help forge a more solid identity/mission
for the group.  Who are we?  Why do we show up for these meetings?  Where are
we each/all going?  What is the role of SIGWEB?  If there's anything else we
should be doing, who is willing/able to do it and what kind of help do you
need?

Prepared statements will be kept to a minimum, this is an audience
participation free-for-all.  Please come prepared to join in the discussion.

Location: Amdahl Corporation - Sunnyvale, CA
    Date: February 3rd, 1994
    Time: 2:00 - 5:00 PM

Directions to be announced shortly.

-----------------------------------------------------------------------
Christopher McRae			            chrism@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036



From mcrae@ora.com  Mon Jan 31 17:04:16 1994 -0800
Message-Id: <199402010104.AA00423@rock.west.ora.com>
Date: Mon, 31 Jan 94 17:04:16 -0800
From: mcrae@ora.com (Christopher McRae)
Subject: Re: Mosaic Accessories 

Fisher Mark writes:
> long time UNIX user (starting with Version 6 in 1978), I am very happy to 
> see the idea of multiple cooperating programs extended into the GUI world. 
> The announcement that OLE and CORBA should interoperate may also be cause 
> for celebration if it brought off.  Here, here!

Well, since you brought it up...
  We should coordinate our standardization efforts with the CORBA spec.
Is anyone out there very familiar with the status of the CORBA effort?
Does anyone have any references to relevant documentation?

Chris
-----------------------------------------------------------------------
Christopher McRae			            mcrae@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036



From broccol@arlut.utexas.edu  Mon Jan 31 23:19:40 1994 -0600
Message-Id: <199402010519.XAA17264@csdsun1.arlut.utexas.edu>
Date: Mon, 31 Jan 1994 23:19:40 -0600
From: broccol@arlut.utexas.edu (Jonathan Abbey)
Subject: Re: Comments on HTML+ Request For Comments

George Forman wrote:

| You believe "KEYWORDS is inadequate" and we should have something else.  I
| agree, but KEYWORDS should also be supported.  There's a lot of existing
| literature that has keywords, and it's better to be able to semantically tag
| their keywords than to ignore them.
| 
| I don't know about Interpedia.  If no one posts something, I encourage
| you to post what you know.

Re: KEYWORDS, Agreed.  Ideally, both would be present, with KEYWORDS being
able to be used as a further refinement once you exhaust the possibility of a
hierarchical categorization scheme.

I'll see if I can dig up some information on Interpedia for the list.

-------------------------------------------------------------------------------
Jonathan Abbey				               broccol@arlut.utexas.edu
Applied Research Laboratories                 The University of Texas at Austin
-------------------------------------------------------------------------------



From broccol@arlut.utexas.edu  Mon Jan 31 23:11:54 1994 -0600
Message-Id: <199402010511.XAA17260@csdsun1.arlut.utexas.edu>
Date: Mon, 31 Jan 1994 23:11:54 -0600
From: broccol@arlut.utexas.edu (Jonathan Abbey)
Subject: Re: Comments on HTML+ Request For Comments

Definite agreement on the semantic markings.. this is one of the single
most important things that we should be attending to now.. devising ways
to support things like the Interpedia project from within the WWW framework.

I would actually hope to see a richer set of semantic tags.. 
DOCUMENT_TYPE is essential (and it's good to see it here), but I tend
to think that KEYWORDS is inadequate.  What about some kind of
hierarchical categorization coding, like dewey decimal or library of
congress numbers?

Is anyone in the Internet community working on this?  Is anyone here
familiar with the details of the Interpedia project?

-------------------------------------------------------------------------------
Jonathan Abbey				               broccol@arlut.utexas.edu
Applied Research Laboratories                 The University of Texas at Austin
-------------------------------------------------------------------------------



From forman@cs.washington.edu  Mon Jan 31 20:59:55 1994 -0800
Message-Id: <199402010459.UAA17295@hake.cs.washington.edu>
Date: Mon, 31 Jan 1994 20:59:55 -0800
From: forman@cs.washington.edu (George Forman)
Subject: Comments on HTML+ Request For Comments


	I enjoyed reading the "HTML+ Request For Comments."  I appreciate the
efforts.  Comments were requested to be sent to www-talk.  My 16 comments are
attached.  I'll be interested in the response.

Thanks to those of you who take the time,

-George Forman
graduate student, Computer Science, Univ. of Washington, Seattle


Comments on the HTML+ Request For Comments:
================================================================

1. LINKS:
  The proposed attributes NEXT, PREV, and PARENT seem like a good idea.
  I have a suggestion to make these more flexible.

  Problem: The proposed attributes don't work with page sharing.  If you
  write a linked document and insert meaningful NEXT fields, I can't re-use
  one of your pages (without copying it manually), because I want a different
  NEXT pointer.  This problem also surfaces when authors want to put multiple
  organizations on the pages of a document, e.g. cronological or by subject.

  Solution: To allow page sharing, let the *parent* document specify a *list*
  of links, then NEXT and PREV depend on how you got to that document.  The
  list can simply be formed from all the hyperlinks in the parent document,
  i.e. we don't need a new HTML mechanism.  But we might want a new mechanism
  to 1) encourage browsers to implement this, and 2) let authors have
  incidental links that are not part of the sequence of pages. 
  (Perhaps the parent could even specify a *tree* of links, to override
  deeper levels in the document tree.  Dubious.)

2. INPUT: the SIZE attribute seems to have fuzzy semantics, serving for both
  layout size and input precision.  Attribute SIZE should have just one
  meaning for all tags: the size of the object in the layout.  Perhaps there
  should be a separate attribute PRECISION or FORMAT to control the allowable
  user input formats.

  This whole thing about constraining the allowable input format seems to
  open half a can of worms.  I suggest following some established standard.
  For example, scanf()/printf() formats let you specify all these things and
  more.  And it's well-tested and well-known.  Here's a nearly free
  implementation: after user entry, the browser runs scanf() with the format
  string on the user's input; if scanf() returns an error, make the user
  re-do it.

  Attribute INT: It'd be a lot better & more powerful to specify the range or
  allowable values, rather than the number of digits, e.g. RANGE="1..100" or
  more generally, VALUES="-1,20..30,999"

  It'd be really useful to allow an error *message* with the proposed ERROR
  attribute, e.g. ERROR="Value must be less than 100."


3. UNORDERED LISTS: I believe attributes should have the same
  meaning for different tags.  Applying this principle, attribute COMPACT
  wouldn't suppress bullets for unordered lists; instead, it would do the
  same thing it does for OL--- just use less vertical space.  If you want,
  add a separate attribute NOBULLETS to omit bullets.  I think it's better 
  having the attributes be orthogonal.

  Maybe the attribute NARROW should be renamed MULTICOLUMN, since that's what
  it does.  This attribute could be applicable to other kinds of lists as
  well, in the name of orthogonality.


4. PANELS: It seems like the attribute HREF should be renamed SRC.  The name
  HREF suggests something that is only fetched if the user clicks on it,
  whereas SRC specifies an included file (as in Latex \input or C #include)
  consistent with the use in: <IMG SRC="mosaic.gif">

  
5. OVERLAYS: There's a WIDTH attribute but no HEIGHT attribute.  How about
  allowing an explicit HEIGHT specification.  Or, the 2-dimensional layout
  size could be specified like this: SIZE="0.3, 0.2"

  How about changing the tag name of an overlay from FIGT to OVERLAY?


6. EMBED: this is a great feature.  I'm glad it's here.  2 comments:

  1. Unless the object is displayed as a pop-up, then something needs to tell
  the browser how much space is needed in the layout.  I recommend a SIZE
  attribute to say how much vertical and horizontal space to leave.

  2. It'd be nice to have a way to specify arbitrary 8-bit data that doesn't
  need interpreting for &, < and > entities, e.g. to send a pile of data down
  to the audio device without having to scan through it all substituting
  these characters.  For this type of data, allow one to specify the *length*
  of the data.  This is quick to process.


7. ENTITY DEFINITIONS: Most of these special characters are for foreign
  languages.  I suggest adding entity definitions for special symbols used by
  us technologists, e.g. math symbols from \section 3.3.2 of Latex manual: 
	==>      \Alpha    \alpha     >=    \integral   \infinity   \forall


8. ACTIVE AREAS: Question: what happens when the area clicked is inside
  multiple active areas?  I suggest using the first one in the list.
  Certainly I wouldn't choose non-deterministically nor leave it unspecified.


9. POP-UPS: Frequently the RFC suggests displaying things in pop-ups such as
  footnotes, margin notes, EMBEDs, etc.  This encourages pop-ups, and I think
  pop-ups are mostly a bad thing.  In a large HTML page, you'd get lots of
  pop-ups which are unrelated to the contents displayed on the screen at any
  given moment.  Even for small pages, you end up doing a lot more window
  managing: placing, shuffling and destroying windows.  Therefore, I suggest
  discouraging pop-ups in the RFC rather than encouraging them.


10. ALIGN:   Instead of:	align=left	align=center	align=right
  how about we just use:	left   	      center   	      right  
  Less wordy, easier to type.
  There is no programming advantage to having "align="; internally the
  browser have a single "align" variable that holds the choice.


11. ROLE attributes for EM tags: AUTHOR, COPYRIGHT, etc.
  I see EMPHASIS as separate and independent of ROLES.  I believe ROLE should
  be its own tag, not just an attribute for EM.  An author may or may not
  want to emphasize a citation.


12. MORE SEMANTIC MARKING OF CONTENT:
  For automatic indexing, searching, and abstracting of HTML pages, it'd be
  good to establish important new roles for authors to include.  Here's my
  proposed list of what you commonly want semantically labeled:
	TITLE		-- (the existing TITLE tag)
  	SUBTITLE
	DOCUMENT_TYPE	-- RFC, Internet Draft, manual, tutorial, catalog
	DATE
	VERSION
	ABSTRACT
	KEYWORDS
	AUTHOR 		-- the author(s) of *this* page, not referenced pages
	ORGANIZATION	-- author's affiliation
	POSITION	-- author's title/position/role/qualifications
	COPYRIGHT
	ACKNOWLEDGEMENTS
	CITATION	-- (the proposed CITE role)
	SOURCE		-- location(s) where document can be found, e.g.
			   published in journal X in June 1993, 
			   UCB tech report #89...,
			   article #7251 of comp.infosystems.www, 
			   available by ftp:.....


13. FIGURES: captions & figure descriptions yield another great opportunity
  for automatic indexing.


14. TABLES: Consider what information would need to be added so that browsers
  could treat tables as datasets, i.e. use the values.  The user could then
  plot them, re-plot them log-scale, plot this table against that table,
  manipulate them in a spreadsheet....  Powerful concept.


15. LINKS:
  The link attribute EFFECT specifies whether the fetched document should get
  a new window or replace the current document.  I feel this should be up to
  the user/browser, not the document.


16. FIGURES: It's common to want multiple figures side-by-side, for example,
  to compare 3 graphs.  I suggest allowing some sort of GROUP or BOX
  construct that puts multiple figures (or general HTML objects) within a box
  side by side.



From CJA@ml0.ucs.edinburgh.ac.uk  Tue Feb  1 11:06:06 1994 GMT
Message-Id: <MAILQUEUE-101.940201110606.416@ml0.ucs.ed.ac.uk>
Date: Tue, 1 Feb 1994 11:06:06 GMT
From: CJA@ml0.ucs.edinburgh.ac.uk (Chris Adie)
Subject: World-Wide Web server for Windows NT: beta test version

This message is to announce the availability of a beta-test HTTP server
implementation (called "HTTPS") for Windows NT.  This software allows a
Windows NT machine to serve information using the World-Wide Web distributed
hypermedia system.

HTTPS version 0.2 is an HTTP/1.0 server which runs as a Windows NT
"Service".  Executables for Intel-based systems, and DEC Aplha systems,
are available.

HTTPS is not a World-Wide Web client.  Existing 16-bit Windows 3.1 clients
run satisfactorily on Windows NT.

The server may be FTP'd from  emwac.ed.ac.uk  in the directory  pub/https.
There are two ZIP files - one for Intel-based systems, one for Alpha-based
systems.  Be sure you download the right one for your processor.


FEATURES
========

* HTTP/1.0 server which understands HEAD and GET methods.

* Runs as a Windows NT "Service", so that it keeps going even
  when there's no-one logged in to the machine.

* Available for DEC Alpha and Intel architectures.

* Much faster than the SERWEB server ported from Windows 3.1.

* Multi-threaded!

* Logs errors using the Windows NT Event Logger.

* Configuration through the Control Panel.

* Configuration information stored in Registry.

* Optionally logs HTTP transactions in the Event Logger.

Additional features are planned for future releases (eg searching
capabilities).


This server has been produced as part of the European Microsoft Windows
NT Academic Centre (EMWAC) project.  EMWAC has been set up to support
and act as a focus for Windows NT within European academia.  It is
sponsored by Datalink Computers, Digital, Microsoft, Research Machines,
Sequent and the University of Edinburgh.

A Gopher server for Windows NT will be available for beta test shortly.
A WAIS server is also planned.


Regards,

Chris Adie                                   Phone:  +44 31 650 3363
Edinburgh University Computing Service       Fax:    +44 31 662 4809
University Library, George Square            Email:  C.J.Adie@edinburgh.ac.uk
Edinburgh EH8 9LJ, United Kingdom



From waterbug@epims1.gsfc.nasa.gov  Tue Feb  1 06:02:04 1994 +0500
Message-Id: <9402011102.AA13639@epims1>
Date: Tue, 1 Feb 1994 06:02:04 +0500
From: waterbug@epims1.gsfc.nasa.gov (Steve Waterbury)
Subject: Semantic Tagging in Web Objects (was: Comments on HTML+ Request For Comments)



Jonathan Abbey wrote:
 
> Definite agreement on the semantic markings.. this is one of the single
> most important things that we should be attending to now.. devising ways
> to support things like the Interpedia project from within the WWW framework.
> 
> I would actually hope to see a richer set of semantic tags.. 
> DOCUMENT_TYPE is essential (and it's good to see it here), but I tend
> to think that KEYWORDS is inadequate.  What about some kind of
> hierarchical categorization coding, like dewey decimal or library of
> congress numbers?

I don't think it's a good idea to burden HTML+ with semantic tagging. 
Categorizations, and their cousins, attributes, in general need 
separate support.  I have been in this discussion before (on the other 
side, in fact!) and, IMHO, orthogonality is needed here.  

I think it would be cleaner and more flexible, and would preserve the 
focus of HTML+, to do semantic tagging with SGML tags from outside the 
HTML+ tag set.  These semantic tags would be invisible to an HTML+ 
browser, but would be known to a set of specialized indexing engines, 
SGML editor/parsers, knowbots, etc., whose purpose in life would be to 
record:

1.  the locations {URLs/URNs} of all "objects" that contain certain tags 
2.  the tag contents for those tags in those objects

and to maintain indexes of them on semantic data servers specialized 
to the various semantic domains (granularity TBD).   

This would enable direct querying to find the set of objects on the 
net with a specified tag and with the contents of that tag containing 
a certain string or a value within a certain range of values, etc.  
Of course the objects retrieved can be arbitrary:  documents, binaries, 
images, product catalogs or specific "data sheets", organizational 
directories, technical standards, specifications, whatever.  The 
specialized semantic data servers would be the grandchildren of whois++, 
x.500, WAIS, and SQL servers.  

The BIG project, and there is lots of work being done on this as we 
speak, is to achieve consensus on technically sound information models 
for the various semantic domains.  Of course, the mapping of the 
information models of various sorts into DTD's is non-trivial, but I 
believe it is technically feasible, and I would rate it much easier 
than the original modeling task itself.  

As for categorizations, I think it is a fallacy to believe that 
universal consensus on them is either possible or necessary.  
Categorization schemes become important only for access to otherwise 
uncharacterized objects, but are not nearly as important when query 
access directly to the objects' attributes is available.  

Categories will always be with us, but to be created properly, they 
must derive from a consensual set of attributes (semantic tags) -- 
i.e., they should sit on top of the information models, and will 
probably come in several different flavors for each semantic domain.  
Even within a domain, different groups like to slice things a little 
differently ("around here we call that an _extra_ large!!") 
... but that's no problem -- as long as the basic information models 
and their attribute sets are agreed to, the categorization-du-jour 
can be selected by the end-user.

Steve Waterbury

=====================================================================
Stephen C. Waterbury		Phone:	301-286-7557
NASA Parts Project Office	FAX:	301-286-1695
Code 310.A			email:	waterbug@epims1.gsfc.nasa.gov
NASA/GSFC			"Sometimes you're the windshield;
Greenbelt, MD 20771			sometimes you're the bug."
=====================================================================



From redback!jimmc@eskimo.com  Tue Feb  1 09:47:16 1994 PST
Message-Id: <9402011747.AA16330@redback.>
Date: Tue, 1 Feb 94 09:47:16 PST
From: redback!jimmc@eskimo.com (Jim McBeath)
Subject: 

> 1. LINKS:
>  The proposed attributes NEXT, PREV, and PARENT seem like a good idea.
>  I have a suggestion to make these more flexible.
>
>  Problem: The proposed attributes don't work with page sharing.  If you
>  write a linked document and insert meaningful NEXT fields, I can't re-use
>  one of your pages (without copying it manually), because I want a different
>  NEXT pointer.  This problem also surfaces when authors want to put multiple
>  organizations on the pages of a document, e.g. cronological or by subject.
>
>  Solution: To allow page sharing, let the *parent* document specify a *list*
>  of links, then NEXT and PREV depend on how you got to that document.  The
>  list can simply be formed from all the hyperlinks in the parent document,
>  i.e. we don't need a new HTML mechanism.  But we might want a new mechanism
>  to 1) encourage browsers to implement this, and 2) let authors have
>  incidental links that are not part of the sequence of pages. 
>  (Perhaps the parent could even specify a *tree* of links, to override
>  deeper levels in the document tree.  Dubious.)

Dave Raggett and I came up with a proposal for a mechanism we called PATHs
which deals with this issue in much the way you suggest.  It is based on adding
two new link values to the REL attribute: Node and Path.  This proposal was
posted to www-talk on January 10 (with "Subdocument" used instead of "Node"
in that posting).

You can access that posting from the stanford www-talk archives at

    http://gummo.stanford.edu/html/hypermail/www-talk-1994q1.messages/125.html

There seemed to be little disagreement about the proposal.  I believe Dave
was planning on adding it to the HTML+ spec.

-Jim McBeath
jimmc@eskimo.com



From forman@cs.washington.edu  Tue Feb  1 10:29:38 1994 -0800
Message-Id: <199402011829.KAA12314@june.cs.washington.edu>
Date: Tue, 1 Feb 1994 10:29:38 -0800
From: forman@cs.washington.edu (George Forman - GHF)
Subject: Semantic Tagging in Web Objects (was: Comments on HTML+ Request For Comments)


> I think it would be cleaner and more flexible, and would preserve the 
> focus of HTML+, to do semantic tagging with SGML tags from outside the 
> HTML+ tag set.  These semantic tags would be invisible to an HTML+ 
> browser, but would be known to a set of specialized indexing engines...


Steve,
	You've got a good idea there-- having the set of semantic tags
separate from HTML so that they're effectively invisible.  Then HTML browsers
don't have to be concerned with implementing these tags.  Dave commented that
getting into all the semantic tags becomes a bottomless pit.

	But on the other hand, I think it'd be really useful if the HTML
browser knew at least a few semantic tags.  HTML already has the TITLE tag;
you wouldn't think of taking that out.  I propose adding AUTHOR, ABSTRACT,
DATE, and VERSION.  Why?

	1) The browser uses this info to decide how to display things, just
	   as it does with <TITLE>.

	2) Users may have personal preferences on how AUTHORS/ABSTRACTS are
	   displayed.

	3) A better history list can be constructed if the HTML browser knows
	   these things about the document.  I'd want a list of
	   documents I've read listing: title, author, abstract, and source.

	4) Suppose you want to glance at a document without pulling all of it
	   across the Internet-- HTTP could send just these key elements.
	   This lets you look at the abstract before you decide to fetch the
	   whole thing.


	So, I believe that HTML needs to specify at least a few tags so that
browsers can use the semantic information.  Semantics aren't just for
automatic indexers.

	We agree that orthogonality is important-- for this reason the ROLE
attribute shouldn't be coupled with <EM>phasis.  I believe orthogonality can
be achieved even with certain semantic tags added to HTML+.

-GHF



From luotonen@ptsun00.cern.ch  Tue Feb  1 21:45:20 1994 +0100
Message-Id: <9402012045.AA00523@ptsun03.cern.ch>
Date: Tue, 1 Feb 94 21:45:20 +0100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Configurable log proposal


How about having a fully configurable log files, something
that would understand escapes like this:

Time:
	%a abbreviated weekday name
	%b abbreviated month name
	%d day of month
	%h 12h clock hour
	%H 24h clock hour
	%m month in decimal
	%M minutes
	%S seconds
	%T time as %H:%M:%S
	%w weekday in decimal
	%y year without centuries
	%Y full year
	%Z time-zone
	%% a single %
	%n \n
	%t \t

HTTP:
	%B bytes transmitted
	%U requested URL
	%C client host address
	%E user email address
	%A authenticated username
	%I remote ident
	%D method
	%P protocol version


...and what else?  I would be happy to implement this.  Maybe the %letters
could be selected in a better way and not respect the ones strftime()
uses.

-- Cheers, Ari --



From robm@ncsa.uiuc.edu  Tue Feb  1 14:58:04 1994 -0600
Message-Id: <9402012058.AA22722@void.ncsa.uiuc.edu>
Date: Tue, 1 Feb 1994 14:58:04 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Configurable log proposal

/*
 * Configurable log proposal  by Ari Luotonen
 *    written on Feb  1,  9:45pm.
 *
 * 
 * How about having a fully configurable log files, something
 * that would understand escapes like this:
 * 
 * Time:
 * 	%a abbreviated weekday name
 * 	%b abbreviated month name
 * 	%d day of month
 * 	%h 12h clock hour
 * 	%H 24h clock hour
 * 	%m month in decimal
 * 	%M minutes
 * 	%S seconds
 * 	%T time as %H:%M:%S
 * 	%w weekday in decimal
 * 	%y year without centuries
 * 	%Y full year
 * 	%Z time-zone
 * 	%% a single %
 * 	%n \n
 * 	%t \t
 * 
 * HTTP:
 * 	%B bytes transmitted
 * 	%U requested URL
 * 	%C client host address

I assume this is a host name if known, and an IP address if the name is not
known?

 * 	%E user email address
 
Where is this e-mail address coming from? It will either come from the auth.
or from identd, which are both provided below...
 
 * 	%A authenticated username
 * 	%I remote ident
 * 	%D method
 * 	%P protocol version

HTTP protocol version, I assume?

 * ...and what else?  I would be happy to implement this.  Maybe the %letters
 * could be selected in a better way and not respect the ones strftime()
 * uses.
 */

I actually tossed this idea around last October. I was wondering if it was
overkill. It will make the statistic collectors more complicated...

--Rob



From mcrae@ora.com  Tue Feb  1 13:00:37 1994 -0800
Message-Id: <199402012100.AA21728@rock.west.ora.com>
Date: Tue, 01 Feb 94 13:00:37 -0800
From: mcrae@ora.com (Christopher J. McRae)
Subject: Protocol Benchmarking

  Has anyone done any recent studies of the efficiency of the various
common information sharing protocols (HTTP, Gopher, WAIS, etc)?  I'd
like to know how each compares to the others and also where the majority
of the bandwidth is being consumed (transferring images, I assume).
  Is anyone planning on doing such stuides?  Does anyone have any suggestions
and/or experience regarding measurement techniques?
Chris
-----------------------------------------------------------------------
Christopher McRae			            mcrae@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036



From swb1@cornell.edu  Tue Feb  1 16:02:36 1994 -0500
Message-Id: <199402012102.AA08621@postoffice2.mail.cornell.edu>
Date: Tue, 1 Feb 1994 16:02:36 -0500
From: swb1@cornell.edu (Scott W Brim)
Subject: Re: Configurable log proposal

Ari, first, are you proposing this for a particular server?  Second, if
you do this be sure to make the configuration readable by the utilities
which scan the log and summarize it, so they will be able to decipher
the log.





From luotonen@ptsun00.cern.ch  Tue Feb  1 22:15:45 1994 +0100
Message-Id: <9402012115.AA00577@ptsun03.cern.ch>
Date: Tue, 1 Feb 94 22:15:45 +0100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: Configurable log proposal


>  * 	%C client host address
> 
> I assume this is a host name if known, and an IP address if the name is not
> known?

That's what I had in mind.


>  * 	%E user email address
>  
> Where is this e-mail address coming from? It will either come from the auth.
> or from identd, which are both provided below...

HTTP From: field.


>  * 	%P protocol version
> 
> HTTP protocol version, I assume?

Yes, maybe just the number, like 1.0 (?)


> I actually tossed this idea around last October. I was wondering if it was
> overkill. It will make the statistic collectors more complicated...

I didn't mean any more trouble to statistic collectors.  I prefer having
a good default for this (your proposal is ok), but if someone should
happen to absolutely want a diffent kind of log then it would be possible.

Well, either way is fine.  Less work for me if we just forget it :-)

-- Cheers, Ari --




From relihanl@ul.ie  Tue Feb  1 21:31:20 1994 +0000 (WET)
Message-Id: <Pine.3.05.9402012116.B3486-b100000@itdsrv1.ul.ie>
Date: Tue, 1 Feb 1994 21:31:20 +0000 (WET)
From: relihanl@ul.ie (Liam Relihan)
Subject: Re: Configurable log proposal

On Tue, 1 Feb 1994, Ari Luotonen wrote:

> How about having a fully configurable log files, something
> that would understand escapes like this:
> 
> Time:
> 	%a abbreviated weekday name
> 	%b abbreviated month name
[stuff deleted]
> 
> 
> ...and what else?  I would be happy to implement this.  Maybe the %letters
> could be selected in a better way and not respect the ones strftime()
> uses.

This sounds like a good idea. Currently, I am writing some scripts that
generate monthly performance graphs. These escapes would be more
logical than awk's $1, $2, $3, etc which are unreliable.

It would be a good idea to find out how log files are *likely* to change in
the near future so as to prevent changes in your escape sequences.

Liam

--
 Liam Relihan,               |               Voice: +353-61-333644 ext.5015
 CSIS, Schumann Building,    |/|                        Fax: +353-61-330876
 University Of Limerick,       |                     E-mail: relihanl@ul.ie
 Ireland.                      http://itdsrv1.ul.ie/PERSONNEL/lrelihan.html




From montulli@stat1.cc.ukans.edu  Tue Feb  1 15:48:17 1994 CST
Message-Id: <9402012148.AA21390@stat1.cc.ukans.edu>
Date: Tue, 1 Feb 94 15:48:17 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Configurable log proposal

> 
>  * 	%E user email address
>  
> Where is this e-mail address coming from? It will either come from the auth.
> or from identd, which are both provided below...
>  
>  * 	%A authenticated username
>  * 	%I remote ident
>  * 	%D method
>  * 	%P protocol version
> 

The email address would come from the HTTP/1.0 "From:"  header
wouldn't it?

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From kevinh@eit.COM  Tue Feb  1 13:58:10 1994 PST
Message-Id: <9402012158.AA10992@eit.COM>
Date: Tue, 1 Feb 94 13:58:10 PST
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re:  Configurable log proposal


Ari (luotonen@ptsun00.cern.ch) writes:

> How about having a fully configurable log files, something
> that would understand escapes like this:
> 
> Time:
> 	%a abbreviated weekday name
> 	%b abbreviated month name
> 	%d day of month
>       ...
>
> ...and what else?  I would be happy to implement this.  Maybe the %letters
> could be selected in a better way and not respect the ones strftime()
> uses.

	I really like the idea - in a log analyzing program, the user
can define their log format in some sort of format string, and all
that's needed is some sort of generic sscanf() function. Once something is
agreed upon, I'll certainly (try to) write one for my analyzer.
	How about escapes for a status flag, such as

	Whether a file/script was found
	Unknown failure
	Successful (appears to have sent the request)

	...and breaking down the request pathname (full, url, or partial),
and one generic escape for miscellaneous (unknown) data known only to the
server. This would allow a person to define by escapes logs like:

hymir.ifi.uio.no [Sat Oct 30 00:17:42 1993] GET /help/modem.html HTTP/1.0

	(site, date, method, partial path, protocol)

hymir.ifi.uio.no [Sat Oct 30 00:17:42 1993] GET http://site.com/help/modem.html HTTP/1.0

	(site, date, method, url, protocol)

hymir.ifi.uio.no [Sat Oct 30 00:17:42 1993] HEAD /usr/local/www/help/modem.html HTTP/1.0

	(site, date, method, full path, protocol)

doboj.cca.vu.nl: Sent cache (GET /1/mathdir): Sun Oct 31 11:44:12 1993
doboj.cca.vu.nl: Sent binary (GET /9/newton.gif): Sun Oct 31 12:26:42 1993

	(site, miscellaneous data, method, partial path, date)

	-- Kevin



From kevinh  Tue Feb  1 14:54:29 1994 PST
Message-Id: <9402012254.AA11852@eit.COM>
Date: Tue, 1 Feb 94 14:54:29 PST
From: kevinh (Kevin 'Kev' Hughes)
Subject: Server log stats are up



	For those wanting to know, full and daily statistics for the
server will be at:

	http://www.eit.com/log.stats/daily.html
	http://www.eit.com/log.stats/full.html

	...these URLs are accessible by internal addresses only.
The full stats will be probably be updated every Monday morning.

	-- Kev



From fielding@simplon.ICS.UCI.EDU  Tue Feb  1 14:43:00 1994 -0800
Message-Id: <9402011443.aa24929@paris.ics.uci.edu>
Date: Tue, 01 Feb 1994 14:43:00 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Configurable log proposal 

Ari said:

> How about having a fully configurable log files, something
> that would understand escapes like this: ...

Although on the surface this seems like a good idea, I want to determine
exactly why it is desirable and make it clear where the pitfalls lie.

I can think of three reasons why it is desirable:

1) Make the largest set of known access data available for logging;

2) Allow the individual service provider to choose the exact subset which
   should NOT be logged, thus saving local disk space.

3) Make the log file format consistent across multiple servers/services
   (assuming those servers follow the same log conventions).

Note, however, that 1 and 3 are also accomplished by establishing a
fixed format which contains all the information.

Now, as for the pitfalls:

1) There is no compelling need for log flexibility other than saving
   disk space.  Regardless of how the data is organized, people will still
   want to use some log analyzer to view the data and thus the format should
   be designed primarily for machine readability and for occasional human
   reading or grep search.  The log analyzer will have its own (possibly
   configurable) output format for human consumption.

2) The content of the data is only known at the time the log entry is
   made.  Any information that was not written at that time is lost to
   any later analysis.  Thus, it is usually preferable to log everything
   and let the log analysis program choose what should be ignored.

3) Every time the configuration changes, the old log file must be deleted.
   This is because any log analyzer will only be able to understand one
   log format at a time.

4) Special formatting conventions (like the square brackets surrounding
   the date field in NCSA httpd logs) make it much easier for analyzers
   to parse the data and identify mangled entries -- a condition which
   occurs quite often with NCSA httpd.

5) It makes it slightly harder for people like me to write and test a
   simple log analyzer program.


Having said all that, I still think that it may be a good idea providing
that the above concerns are addressed (i.e. I have faith that the server
authors will go out of their way to make my life easier, providing that
I let them know what will make my life easier).  Although I personally
would prefer a fixed format, I am willing to go with the flow.

In that spirit, let me propose that some generic indicator (such as "-")
be used for any field which is desired by the configuration string (or by
the fixed format) but is unknown or empty for a particular log entry. 
Thus, if the configurable string indicates REMOTE_IDENT should be logged
between FULL YEAR and CLIENT HOST ADDRESS (as in "%Y %I %C"), and
REMOTE_IDENT is empty, then the output should be like:

        "1994 - simplon.ics.uci.edu"

rather than

        "1994  simplon.ics.uci.edu"

for reasons that should be obvious to most hackers.


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From fielding@simplon.ICS.UCI.EDU  Tue Feb  1 15:39:08 1994 -0800
Message-Id: <9402011539.aa28450@paris.ics.uci.edu>
Date: Tue, 01 Feb 1994 15:39:08 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Configurable log proposal 

> 	...and breaking down the request pathname (full, url, or partial),
> and one generic escape for miscellaneous (unknown) data known only to the
> server.

Gack! Not for me.  I want to know what the actual request was, not how
the server interpreted it (other than the server's response code).
Although having the server translate the request makes it easier for
automated log analyzers, it also reduces the utility of logs as means
of finding out "what went wrong."


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From WIGGINS@msu.edu  Tue Feb  1 22:54:21 1994 EST
Message-Id: <9402020420.AA22681@dxmint.cern.ch>
Date: Tue, 01 Feb 94 22:54:21 EST
From: WIGGINS@msu.edu (Rich Wiggins)
Subject: Re: Protocol Benchmarking

It would be interesting to see a formal study of some sort, but your
initial assumption has got to be correct. Just look at a typical HTML
document with embedded images. Small embedded GIFs are easily 10K, 20K,
30K. That's many pages worth of straight text. None of the protocols
themselves add much to the traffic. (Well, HTTP, Gopher0, and Gopher+
have minimal overhead; I assume WAIS does as well, but don't know that
first hand.)

I for one often wish WinMosaic had a much deeper cache for inline
images, especially as I page up and down just a few layers in a server
and go "oops, downloading that lovely but huge mess of icons that I just
saw three pages ago." By contrast, the Gopher user very rarely feels any
pain when cruising menus. Only a few menus out there are tremendously
long.

You could probably do a pretty good simple study by analyzing logs, and
coming up with a statistical profile of the various files delivered --
ie keeping track of bytes of text delivered versus bytes of images,
sounds, etc.

You'd have to define terms if you undertook a more in-depth study. What
do you mean by "efficiency"? By browsing 200 Gopher servers I might find
a document I'm looking for, but to an external observer it appears I'm
harvesting a lot of useful information. But an index tool might've saved
me 199 scans.

This is one reason why taking the NSFnet traffic stats and graphing them
can be misleading -- the value of information delivered is not
proportional to the quantity of bytes sent out.  (I'm guilty of
distribution such charts myself.)   And of course network
bandwidth isn't the only issue -- painting those inline images also
takes real time for the client program.

/Rich Wiggins, CWIS Coordinator, Michigan State U


>  Has anyone done any recent studies of the efficiency of the various
>common information sharing protocols (HTTP, Gopher, WAIS, etc)? I'd
>like to know how each compares to the others and also where the majority
>of the bandwidth is being consumed (transferring images, I assume).
>  Is anyone planning on doing such stuides? Does anyone have any sugge stions
>and/or experience regarding measurement techniques? Chris
>-----------------------------------------------------------------------



From guenther.fischer@hrz.tu-chemnitz.de  Wed Feb  2 08:21:25 1994 +0100 (MET)
Message-Id: <9402020721.AA05918@flash1.hrz.tu-chemnitz.de>
Date: Wed, 2 Feb 1994 08:21:25 +0100 (MET)
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: Re: Patches to NCSA httpd 1.1

> 
> Hi all,
> 
> I've made some changes to the NCSA httpd 1.1 server.  The changes are
> documented at http://www-ns.rutgers.edu/httpd and the patch is
> available from ftp://rutgers.edu/pub/httpd_1.1/1.1-patch
> 
> The changes enhance access control and provide much more control over
> the look and feel of directory indexes.  If you intend to use the
> patches or have suggestions, additions, or bug reports, please feel
> free to contact me.
> 

Hi,
hope you have read and tried about my patch to the same features of
httpd.

ReadmeTop is fine - I also want to have it configurable.
LinkIcons         - As I see the Href from the Icon is the same as from
		    the name after. 
		    My vote is for such like my AddHref, so I can bind
		    a variable view to spesial file types - so I can
		    make the funtionality to look in archives with a
		    cgi script
		    TRY:
		    http://www.tu-chemnitz.de/ftp-home/pub/simtel/msdos/animate

What I would like:
  - we can configure the global view in srm.conf with FancySelections,
    ReadmeTop, AddHref ... and
  - we can put configuration files on directories in the archive area
    as httpd does with .htaccess
    So we could define different views to different parts. There I could
    overwrite the "global view" as in srm.conf.

    What should be there:
      - all the features as in srm.conf for indexing, but I can change
        from defines in srm.conf
      - CGI-View /cgi-bin/<name> to call a script with pathname to
	generate an index outside the server. Very good for good
	managed archives like simtel or windows from cica. So you can
	generate Hypertext Indexes from the indexes on the archives.
        Try http://www.tu-chemnitz.de/cgi-bin/simtel?animate


What do you think about this? I could send you the patches for AddHref
and the unarch CGI, but I think Rob should do the real patch if it is
a good one. 

	~Guenther

PS: Patch as patch can ...
-- 
Name:      Guenther Fischer / Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361     / mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From koblas@netcom.com  Wed Feb  2 00:20:47 1994 -0800 (PST)
Message-Id: <199402020820.AAA21738@mail.netcom.com>
Date: Wed, 2 Feb 1994 00:20:47 -0800 (PST)
From: koblas@netcom.com (David Koblas)
Subject: Re: Protocol Benchmarking

> Has anyone done any recent studies of the efficiency of the various
>common information sharing protocols (HTTP, Gopher, WAIS, etc)?  I'd
>like to know how each compares to the others and also where the majority
>of the bandwidth is being consumed (transferring images, I assume).

Though it would be hard to "see" in a protocol study.  HTTP has a one
probably that I would enjoy seeing fixed in the near future (it is my 
personal beef with HTTP).

HTTP 1.0 example negotiating session:
        
        Connection opened:
                GET /index.html HTTP/1.0
                [ batch of "Accept" and other headers sent,
                  mosaic sends about 1K worth ]
 
                File is sent
        Connection closed:
        Connection opened (to the same host):
                GET /logo.gif HTTP/1.0
                [ batch of "Accept" and other headers sent,
                  mosaic sends about 1K worth ]

                File is sent
        Connection closed:
 
This is awful, since not only is there connection creation/tear down
expenses, but also retrasmission of "client information" to the server.
Also, since (from the survey of my server) most of the HTML documents
are ~1K in size, it means that twice the information is being sent
than necessary...  Not good for a slow link..

This hopefully could get changed (HTTP 1.1?) into a protocol that
doesn't close the connection after one file is trasmitted, but rather
leaves it open for a "short" while.  Where it is closed either through
a client "QUIT" or a server timeout.

--koblas@netcom.com




From Paul.Wain@brunel.ac.uk  Wed Feb  2 08:35:32 1994 +0000 (GMT)
Message-Id: <17411.9402020835@thor.brunel.ac.uk>
Date: Wed, 2 Feb 1994 08:35:32 +0000 (GMT)
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Re: Configurable log proposal

@ HTTP:
@ 	%B bytes transmitted
@ 	%U requested URL
@ 	%C client host address
@ 	%E user email address
@ 	%A authenticated username
@ 	%I remote ident
@ 	%D method
@ 	%P protocol version
@ 
@ 
@ ...and what else?  I would be happy to implement this.  Maybe the %letters
@ could be selected in a better way and not respect the ones strftime()
@ uses.

I havent got to the end of my mailbox so sorry if I repeat anyone, but
how about the error #/message aswell...  This contains a lot of
information in a small amount of data.

P.

.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From waterbug@epims1.gsfc.nasa.gov  Wed Feb  2 04:06:18 1994 +0500
Message-Id: <9402020906.AA14669@epims1>
Date: Wed, 2 Feb 1994 04:06:18 +0500
From: waterbug@epims1.gsfc.nasa.gov (Steve Waterbury)
Subject: Re: Semantic Tagging in Web Objects (was: Comments on HTML+ Request For Comments)



George,

[You wrote:]

> 	You've got a good idea there-- having the set of semantic tags
> separate from HTML so that they're effectively invisible.  Then HTML browsers
> don't have to be concerned with implementing these tags.  Dave commented that
> getting into all the semantic tags becomes a bottomless pit.

Yes.  Thanks ... maybe if I implement some of the stuff I talk about, 
it will be even more interesting.   :-)   

> 	... [stuff omitted] ...
> 	... I believe that HTML needs to specify at least a few tags so that
> browsers can use the semantic information.  Semantics aren't just for
> automatic indexers.

Agree (see below).  

> 	We agree that orthogonality is important-- for this reason the ROLE
> attribute shouldn't be coupled with <EM>phasis.  I believe orthogonality can
> be achieved even with certain semantic tags added to HTML+.

I agree -- there has to be some semantic tagging within HTML (I 
overstated the case :) -- of course there already is some, and 
possibly should be more.  Defining a proper tag set is a subtle 
thing -- for HTML and for lots of other applications!  

Some context is needed (I hope I provided some in my somewhat rambling 
scenario) so it can be decided which semantic tags are appropriate to 
include within HTML and which ones are better left to the domain of a 
companion application.  

- Steve.



From luotonen@ptsun00.cern.ch  Wed Feb  2 10:53:19 1994 +0100
Message-Id: <9402020953.AA01425@ptsun03.cern.ch>
Date: Wed, 2 Feb 94 10:53:19 +0100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: Configurable log proposal


This was meant for the list.

----- Begin Included Message -----

>From koblas@netcom.com Wed Feb  2 09:17:00 1994
From: koblas@netcom.com (David Koblas)
Subject: Re: Configurable log proposal
To: luotonen@ptsun00.cern.ch (Ari Luotonen)
Date: Wed, 2 Feb 1994 00:09:30 -0800 (PST)
X-Mailer: ELM [version 2.4 PL23]
Mime-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 8bit
Content-Length: 1205

>...and what else?  I would be happy to implement this.  Maybe the %letters
>could be selected in a better way and not respect the ones strftime()
>uses.

%letters is 80's style programming, this is the nineties.  After all most of
us probably read enough cryptic files every day.  It would be nice if
it was more like:

HTTP:
        TIME            time
        BYTES           bytes transmitted
        FILE            requested FILE
        URL             requested URL
        HOSTNAME        client host address
        EMAIL           user email address
        AUTH_USER       authenticated username
        REMOTE_USER     remote ident
        METHOD          method
        HTTP_VERSION    protocol version

Where a formatting like would be something like (eg httpd 1.1):

        $HOST [$TIME] $METHOD $FILE $HTTP_VERSION

Standard shell formatting rules apply.  The only "unanswered" problem is
how to handle $TIME formatting since are two ways you could do it:
        $TIME{"%a %b %d %T %Y"}
or
        $TIME_WK $TIME_MON $TIME_TIME $TIME_YEAR

[general rule for the above]
        TIME_WK         abbreviated weekday
        TIME_WEEK       weekday

Food for thought,
--koblas@netcom.com



----- End Included Message -----




From hoesel@chem.rug.nl  Wed Feb  2 12:05:52 1994 +0100 (MET)
Message-Id: <9402021105.AA00633@Xtreme>
Date: Wed, 2 Feb 1994 12:05:52 +0100 (MET)
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Re: Protocol Benchmarking (HTTP protocol)

My problem with http too!!

but the solution I would suggest is a mutliple GET request for
multiple files (perhaps named MGET ?) which has the big
advantages:

o  closes the connection as soon as possible
o  it doesn't break anything: a browsers could send the MGET
   to any HTTP deamon, if it finds that it doesn't understand the MGET
   it can go ahead with the old method.
   Eventually all HTTP deamons would understand it and everybody would
   be happy
o  It's much faster! not only the transmission time is saved, but also
   a lot of time is saved by not having to wait till the connection
   starts over again. Allthough I am on a slip line I can see from the
   LEDs on the modem that much time is wasted just in the phase where
   the connection have to be made. the modem is just idle, so I'm
   definitly not limited by transmission speed during that phase.

it has one disadvantage:

o for a given document it still needs two accesses: one to get the document
  and one to get the images using MGET.

  however there is an advantage hiding here: during the first GET, the
  httpd could actualy tell the browser it understands MGET requests,
  so any MGET request need never to fail on the system were the document
  is coming from.


another possibility is that during Accept headers phase the browser tells
the server it want all images too, and the server would itself see what
files are needed from the local system and send them too (as if the
mget has been send). This might look as taking a lot of cpu cycles on the
server, but it could actually save a lot, because the images will be
requested anyway.
In all cases the browser would act as usual to get the needed images from
systems other than the one the document is coming from (using MGET)

- frans

> Though it would be hard to "see" in a protocol study.  HTTP has a one
> probably that I would enjoy seeing fixed in the near future (it is my 
> personal beef with HTTP).
> 
> HTTP 1.0 example negotiating session:
>         
>         Connection opened:
>                 GET /index.html HTTP/1.0
>                 [ batch of "Accept" and other headers sent,
>                   mosaic sends about 1K worth ]
>  
>                 File is sent
>         Connection closed:
>         Connection opened (to the same host):
>                 GET /logo.gif HTTP/1.0
>                 [ batch of "Accept" and other headers sent,
>                   mosaic sends about 1K worth ]
> 
>                 File is sent
>         Connection closed:
>  
> This is awful, since not only is there connection creation/tear down
> expenses, but also retrasmission of "client information" to the server.
> Also, since (from the survey of my server) most of the HTML documents
> are ~1K in size, it means that twice the information is being sent
> than necessary...  Not good for a slow link..
> 
> This hopefully could get changed (HTTP 1.1?) into a protocol that
> doesn't close the connection after one file is trasmitted, but rather
> leaves it open for a "short" while.  Where it is closed either through
> a client "QUIT" or a server timeout.
> 
> --koblas@netcom.com
> 






From hoesel@chem.rug.nl  Wed Feb  2 12:20:23 1994 +0100 (MET)
Message-Id: <9402021120.AA00653@Xtreme>
Date: Wed, 2 Feb 1994 12:20:23 +0100 (MET)
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Re: Configurable log proposal

Roy said:
> 
> Now, as for the pitfalls:
> 
> 1) There is no compelling need for log flexibility other than saving
>    disk space.  Regardless of how the data is organized, people will still
>    want to use some log analyzer to view the data and thus the format should
>    be designed primarily for machine readability and for occasional human
>    reading or grep search.  The log analyzer will have its own (possibly
>    configurable) output format for human consumption.

which is not completely true. I never use a log analyzer. I look at the
file, and when I want some particular information I use grep. that works
great for me and I don't need to learn how to use a log analyzer, just to
generate the info Ari wants to put in the logfile. But I'm only
a small user. On the other hand large sites may very well be interested
in saving diskspace (except for the very very large sites :-))
> 
> 2) The content of the data is only known at the time the log entry is
>    made.  Any information that was not written at that time is lost to
>    any later analysis.  Thus, it is usually preferable to log everything
>    and let the log analysis program choose what should be ignored.
> 
why? If I choose to write down only a limited set into the LOG than I know
I cannot fetch the other information, but it's my choise and my responsibility.

> 3) Every time the configuration changes, the old log file must be deleted.
>    This is because any log analyzer will only be able to understand one
>    log format at a time.
> 
> 4) Special formatting conventions (like the square brackets surrounding
>    the date field in NCSA httpd logs) make it much easier for analyzers
>    to parse the data and identify mangled entries -- a condition which
>    occurs quite often with NCSA httpd.
> 
> 5) It makes it slightly harder for people like me to write and test a
>    simple log analyzer program.
> 
3) 4) 5) are irrelevant to a guy like me who simply want to broswe the
logfile and has only a very small local disk.
3) can be solved by writing down the configuration change in the logfile
  which is also handy for 5) so you will know at all times what the
  format is and which fields are written in the logfile.
  ie. the same string that does the formatting in ari's program should
  be present in the logfile; or if you are willing to forget about 3)
  should be made avaliable for the log analyzer. That makes your '-'
  suggested below unnesecairy,

- frans

> 
> Having said all that, I still think that it may be a good idea providing
> that the above concerns are addressed (i.e. I have faith that the server
> authors will go out of their way to make my life easier, providing that
> I let them know what will make my life easier).  Although I personally
> would prefer a fixed format, I am willing to go with the flow.
> 
> In that spirit, let me propose that some generic indicator (such as "-")
> be used for any field which is desired by the configuration string (or by
> the fixed format) but is unknown or empty for a particular log entry. 
> Thus, if the configurable string indicates REMOTE_IDENT should be logged
> between FULL YEAR and CLIENT HOST ADDRESS (as in "%Y %I %C"), and
> REMOTE_IDENT is empty, then the output should be like:
> 
>         "1994 - simplon.ics.uci.edu"
> 
> rather than
> 
>         "1994  simplon.ics.uci.edu"
> 
> for reasons that should be obvious to most hackers.
> 
> 
> ....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
>                    (fielding@ics.uci.edu)
>     <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>
> 






From M.J.Cox@bradford.ac.uk  Wed Feb  2 11:25:33 1994 GMT
Message-Id: <29960.9402021125@compute.brad.ac.uk>
Date: Wed, 2 Feb 94 11:25:33 GMT
From: M.J.Cox@bradford.ac.uk (M.J.Cox@bradford.ac.uk)
Subject: Re: Configurable log proposal

| 	%B bytes transmitted
|       ....
|  	%P protocol version

What would be a nice addition would be a "transfer time" like you get
with ftp logs.  You can then find out easily if your inline images are
too big from the time it takes most people to get them.   This is 
something I miss with the NCSA httpd logs.

Mark
Mark J Cox ---------------------- <URL:http://www.eia.brad.ac.uk/mark.html>
Industrial Technology, Bradford University, UK.   +44 274 384024/fax 391333




From neuss@igd.fhg.de  Wed Feb  2 13:42:49 1994 +0100
Message-Id: <9402021242.AA13093@wildturkey.igd.fhg.de>
Date: Wed, 2 Feb 94 13:42:49 +0100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: Re: Configurable log proposal

Mark J Cox (M.J.Cox@bradford.ac.uk) wrote:

> What would be a nice addition would be a "transfer time" like you get
> with ftp logs.  You can then find out easily if your inline images are
> too big from the time it takes most people to get them.   This is 

> something I miss with the NCSA httpd logs.
Yes.. that'd be a very useful feature! It'd also give document writers
a "feel" for the size of their files, and the consequences this has
for downloading. I've seen people write files more then a megabyte big,
which take far to long to transfer over a medium speed connection.
Not everybody has a two megabit feed.

Chris  




From hgs@research.att.com  Wed Feb  2 07:52:22 1994 EST
Message-Id: <9402021252.AA23525@dxmint.cern.ch>
Date: Wed, 2 Feb 94 07:52:22 EST
From: hgs@research.att.com (Henning G. Schulzrinne)
Subject: Re: Protocol Benchmarking (HTTP protocol)

"Chained"/multiple GETs would also really help with TCP-level firewalls,
where connection set-up cost are substantial, as well as with international
links where the three-way TCP handshake can easily add seconds to the
retrieval of a short document with lots of small images/icons.

The MGET method seems preferable; timeouts are always tricky to get
right. One possible difficulty is error reporting, unless an all-or-
nothing error indication is sufficient.

---
Henning Schulzrinne (hgs@research.att.com)
AT&T Bell Laboratories  (MH 2A-244)
600 Mountain Ave; Murray Hill, NJ 07974
phone: +1 908 582-2262; fax: +1 908 582-5809



From sg04%kesser@gte.com  Wed Feb  2 10:22:24 1994 EST
Message-Id: <9402021522.AA26550@kesser.cisl214>
Date: Wed, 2 Feb 94 10:22:24 EST
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: re: Mosaic Accessories

marca@eit.COM writes:

] Two non-PhD's (one of them an undergrad) working at a government
] research lab essentially knocked out Mosaic in 3 months.  In other
] words, it could do a lot of good.

This reminds me of the "two schools" of design that frequently
become flash points at international conferences:

The American School:

Successful examples first -> then standards

The European School:

Standards first -> then implementations

Obviously this is a bit of an oversimplification - but as many
stereotypes, there sometimes is a grain of truth there.

I come from Marc's school. Literally. I was an undergrad at the Univ of
Ill. from 1974-78. I spent literally thousands of hours on PLATO. And
so did Ray Ozzie, Len Kawell, and Tim Halvorsen, all classmates of mine
and who went on to create DECnotes and later Lotus Notes. It's
definitely a schooling in the "American School" of software. Good
training. Knock out good products and you will become a standard.

But there is something to be said for the "European school".  A
standard interface, a standard API, is a platform on which many good
things can grow. For example, once HTTP was fairly stable, people felt
comfortable going off and building WWW clients. Once the CGI interface
is stable, one will see people invest in building scripts.

Few people want to stick things directly into the Mosaic or TkWWW
client. Mainly because they continue to evolve so fast. That is why
there are interface "standards". The solidfy a fixed interface, and
allow good "accessories" to flourish. A booming OEM market, so to
speak.

In my Accesories proposal I pointed out the shortcomings of the VCI
(Viewer Client Interface). I proposed how a small change could result
in an ACI (Accessories Client Interface).  In such a scheme many
current Mosaic tools (document viewer, hotlist viewer and eventual HTML
editor) could be OEM'ed.  A big win in maintenance for the NCSA staff.
Not only that, but since the interface would be a bi-directional
continous stream of MIME objects a lot of nomadic applications could be
spawned (and I have mentioned several for PDA, automatic form fillers,
etc).

Since I don't "own" a client, I can't directly imbed the interface into
any client. Unless I want to get saddled with a continous maintenance
function (which my management will not allow).  Besides what I want to
work on are the accessories themselves, but once a well developed
protocol is in place.

Luckily, there are plenty of people in www land who can beat this
proposal into shape - and I have no particular attachment to my ideas
or the chutzpah to think that I have the only solution.  (Indeed, as I
pointed out, I think both the CGI and ACI will have to migrate
eventually to a more CORBA or OLE like mechanism).

All we need is agreement from the client builders that this is a "good
thing" and that it's worth the effort to create an Internet Draft.

BTW: I do come from the U of I school, so I have already started
work on a "hack" to the viewer mechanism that gives two-way communicaiton
by embedding TCL-DP sends into HTML documents.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Yechezkal-Shimon Gutfreund		 	   sgutfreund@gte.com [MIME]
GTE Laboratories, Waltham MA        http://www.gte.com/circus/home/home.html
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=





From drummond@aristarchus.rutgers.edu  Wed Feb  2 10:52:31 1994 EST
Message-Id: <CMM-RU.1.3.760204351.drummond@aristarchus.rutgers.edu>
Date: Wed, 2 Feb 94 10:52:31 EST
From: drummond@aristarchus.rutgers.edu (Walt Drummond)
Subject: Re: Patches to NCSA httpd 1.1

Hi there,

>
> hope you have read and tried about my patch to the same features of
> httpd.
> 

I saw the announcement as I was working on my addition, but I couldn't
find the patch itself...

> ReadmeTop is fine - I also want to have it configurable.
> LinkIcons         - As I see the Href from the Icon is the same as from
> 		    the name after. 
> 		    My vote is for such like my AddHref, so I can bind
> 		    a variable view to spesial file types - so I can
> 		    make the funtionality to look in archives with a
> 		    cgi script
> 		    TRY:
>		   http://www.tu-chemnitz.de/ftp-home/pub/simtel/msdos/animate

All of the changes I've made are applicable in srm.conf, access.conf
and .htaccess.  You're right, however.  I think we want to see some
way of extending the directory index to do special things with a
subset of files; ie: an archive list. 

> 
> What I would like:
>   - we can configure the global view in srm.conf with FancySelections,
>     ReadmeTop, AddHref ... and
>   - we can put configuration files on directories in the archive area
>     as httpd does with .htaccess
>     So we could define different views to different parts. There I could
>     overwrite the "global view" as in srm.conf.
> 

Right.  That was the part of intent of my changes. Check out
http://www-ns.rutgers.edu/httpd/demo.html.

>     What should be there:
>       - all the features as in srm.conf for indexing, but I can change
>         from defines in srm.conf

If I understand you correctly, its done.  The global Directory Index
defines in srm.conf are superseded by those in access.conf and
htindex.

>       - CGI-View /cgi-bin/<name> to call a script with pathname to
> 	generate an index outside the server. Very good for good
> 	managed archives like simtel or windows from cica. So you can
> 	generate Hypertext Indexes from the indexes on the archives.
>         Try http://www.tu-chemnitz.de/cgi-bin/simtel?animate
> 
> 
> What do you think about this? I could send you the patches for AddHref
> and the unarch CGI, but I think Rob should do the real patch if it is
> a good one. 
> 

Actually, I was about to try and get this patch from you, if just to
use here at Rutgers.  I'd really us like to integrate our patches and
jointly submit them to Rob.  Let's do this: send me a copy of your
patch, I'll integrate it into my changes, and send the new patch back
to you.  If we agree on the implementation, we'll send it to Rob to
see if we can get it included into the distribution.  If not, we'll
reimplement where needed.  Sound good?


> 	~Guenther
> 


				Walt



_________________________________________________________
Walt Drummond (drummond@noc.rutgers.edu)
Network Services
Rutgers University Computing Services
 - Lost: One mind. Owner sad. Reward.



From relihanl@ul.ie  Wed Feb  2 15:48:30 1994 +0000 (WET)
Message-Id: <Pine.3.05.9402021527.B16371-c100000@itdsrv1.ul.ie>
Date: Wed, 2 Feb 1994 15:48:30 +0000 (WET)
From: relihanl@ul.ie (Liam Relihan)
Subject: re: Mosaic Accessories

On Wed, 2 Feb 1994 sg04%kesser@gte.com wrote:
[some cute observations on differences between European and US
researchers deleted]

> But there is something to be said for the "European school".  A
> standard interface, a standard API, is a platform on which many good
> things can grow. For example, once HTTP was fairly stable, people felt
> comfortable going off and building WWW clients. Once the CGI interface
> is stable, one will see people invest in building scripts.

Mmmmmmm...I'd wonder if HTTP could be said to be stable. I have done a lot
of searching and I have still not found a definitive definition of the
HTTP "standard" as it currently stands or as it ever stood.
There are various half completed, half maintained specifications, I'll admit.

I really think that we need a little more of the "European" approach to
development. If we have not specified what has been done already, it will be
rather difficult to specify future additions.

This is a pet gripe of mine. I have written an abstract which argues for
a more formal treatment. It can be found at
http://itdsrv1.ul.ie/Research/WWW/need_for_formal_treatments.html

In short, we must be very wary of adding functionality. We really must try
to document all changes properly and also document the rationale behind them.
Otherwise, we could program ourselves into a hole from which it will be
difficult to emerge. If a new piece of functionality is really a "good thing"
then its developer should be able to convince the rest of us of the fact. 

Will there be changes in the future that will mean that browsers/clients
will need to be scrapped ? With the current state of documention and
development it is impossible to predict. We cannot expect users/authors to
put up with the need for regular changes in software just because something is
scrapped or something new is added. 

For instance, CGI *appears* to be a good thing. However, who has documented
the rationale behind it ? (forgive me it this has been done sufficiently
already). While CGI *might* be a "good thing", it is still necessary for server
admins to rewrite many of their scripts. Are more changes like this to be
expected ?

With the huge quantity of documents now on the Web, it is safe to say that
we are now dealing with a real system --- not just a testbed. If changes
are made to the standards and protocols of W3, the people who make the
changes should remember that they have a responsibility to all those who
browse the Web and write for it --- ie. developers should document their
changes and explain in detail *why* they were necessary.


[more stuff deleted]
> All we need is agreement from the client builders that this is a "good
> thing" and that it's worth the effort to create an Internet Draft.

Excellent modus operandi ;-)


Liam


--
 Liam Relihan,               |               Voice: +353-61-333644 ext.5015
 CSIS, Schumann Building,    |/|                        Fax: +353-61-330876
 University Of Limerick,       |                     E-mail: relihanl@ul.ie
 Ireland.                      http://itdsrv1.ul.ie/PERSONNEL/lrelihan.html




From robm@ncsa.uiuc.edu  Wed Feb  2 11:27:16 1994 -0600
Message-Id: <9402021727.AA08683@void.ncsa.uiuc.edu>
Date: Wed, 2 Feb 1994 11:27:16 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Patches to NCSA httpd 1.1



Now that you guys mention it, I'd like to see the directory indexing more
configurable as well. But I don't want to use four million globals and
config directives to do it...

What do you think of having something like DirectoryOptions? So your
configuration would look something like this:

DirectoryOptions ReadmeTop FancyIndexes IconLinks
IconLinkPrefix /cgi-bin/munge-me

/cgi-bin/munge-me would be a CGI script called with the name of the current
file as extra path information, and munge-me could use PATH_TRANSLATED to
get at the real filename.

For compatibility, I would still support FancyIndexes in srm.conf but it
could be replaced with DirectoryOptions.

--Rob



From drummond@aristarchus.rutgers.edu  Wed Feb  2 12:44:23 1994 EST
Message-Id: <CMM-RU.1.3.760211063.drummond@aristarchus.rutgers.edu>
Date: Wed, 2 Feb 94 12:44:23 EST
From: drummond@aristarchus.rutgers.edu (Walt Drummond)
Subject: Re: Patches to NCSA httpd 1.1

> Now that you guys mention it, I'd like to see the directory indexing more
> configurable as well. But I don't want to use four million globals and
> config directives to do it...
> 

Right, 4 million options would slow down the parsing just a bit ... :)

> What do you think of having something like DirectoryOptions? So your
> configuration would look something like this:
> 
> DirectoryOptions ReadmeTop FancyIndexes IconLinks
> IconLinkPrefix /cgi-bin/munge-me
> 
> /cgi-bin/munge-me would be a CGI script called with the name of the current
> file as extra path information, and munge-me could use PATH_TRANSLATED to
> get at the real filename.
> 
> For compatibility, I would still support FancyIndexes in srm.conf but it
> could be replaced with DirectoryOptions.
> 

That sounds like a good plan.  That way, Guenther and I can go have a
field day without stomping on you too much.  Why don't I do this: I'll
modify my changes to use the DirectoryOptions construct.  Then, when
Guenther sends me his patch, he and I can add options ad nausium.
Guenther? Sound ok?

> --Rob
> 

					Walt

_________________________________________________________
Walt Drummond (drummond@noc.rutgers.edu)
Network Services
Rutgers University Computing Services
 - Lost: One mind. Owner sad. Reward.



From dsr@hplb.hpl.hp.com  Wed Feb  2 18:15:43 1994 GMT
Message-Id: <9402021815.AA11749@manuel.hpl.hp.com>
Date: Wed, 2 Feb 94 18:15:43 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Semantic Tagging in Web Objects

George Forman writes:

>        But on the other hand, I think it'd be really useful if the HTML
> browser knew at least a few semantic tags.  HTML already has the TITLE tag;
> you wouldn't think of taking that out.  I propose adding AUTHOR, ABSTRACT,
> DATE, and VERSION.

The DTD already includes ABSTRACT to allow the overview of a document to
be shown in a distinct manner from the rest of the text. Authors often
use headers for this now, but this leads to unpredictable results.
Document bylines are handled with the familiar ADDRESS element.

Version and ownership etc. can be handled in a generic way using the
META element which appears in the document's head:

    <!--
     Servers should read the document head to generate HTTP headers
     corresponding to META elements, e.g. if the document contains:

        <meta name="Expires" value="Tue, 04 Dec 1993 21:29:02 GMT">

     The server should include the HTTP date format header field:

        Expires: Tue, 04 Dec 1993 21:29:02 GMT

     Other likely names are "Created", "Owner" (a name) and
     "Reply-To" (an email address)
    -->

    <!ELEMENT META - O EMPTY>
    <!ATTLIST META
            id      ID      #IMPLIED -- to allow meta info  --
            name    CDATA   #IMPLIED -- HTTP header e.g. "Expires" --
            value   CDATA   #IMPLIED -- associated value -->

Another powerful approach is to extend the HTML+ DTD on a per document
basis and use the RENDER element to tell the browser how to render new
elements in terms of older ones. In fact, I really want to throw out
most of the existing semantic emphasis tags!

Dave Raggett



From michael.shiplett@umich.edu  Wed Feb  2 11:43:30 1994 -0500
Message-Id: <199402021643.LAA04515@totalrecall.rs.itd.umich.edu>
Date: Wed, 02 Feb 1994 11:43:30 -0500
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: Configurable log proposal 


"rtf" == Roy T. Fielding <fielding@ics.uci.edu> writes:

rtf> In that spirit, let me propose that some generic indicator (such
rtf> as "-") be used for any field which is desired by the
rtf> configuration string (or by the fixed format) but is unknown or
rtf> empty for a particular log entry.  Thus, if the configurable
rtf> string indicates REMOTE_IDENT should be logged between FULL YEAR
rtf> and CLIENT HOST ADDRESS (as in "%Y %I %C"), and REMOTE_IDENT is
rtf> empty, then the output should be like:

rtf>         "1994 - simplon.ics.uci.edu"

rtf> rather than

rtf>         "1994  simplon.ics.uci.edu"

rtf> for reasons that should be obvious to most hackers.

  While I think preserving the number of fields is a good idea, I
suggest something along the lines of '--' or 'N/A' be used for empty
or "not applicable/available" fields. The meaning of either,
especially the latter, is more obvious to me than a single hyphen.

michael








From waterbug@epims1.gsfc.nasa.gov  Wed Feb  2 14:10:02 1994 +0500
Message-Id: <9402021910.AA15306@epims1>
Date: Wed, 2 Feb 1994 14:10:02 +0500
From: waterbug@epims1.gsfc.nasa.gov (Steve Waterbury)
Subject: Re: Semantic Tagging in Web Objects


Dave Raggett writes:

> The DTD already includes ABSTRACT to allow the overview of a document to
> be shown in a distinct manner from the rest of the text....
>    [... stuff omitted ....]
> Version and ownership etc. can be handled in a generic way using the
> META element which appears in the document's head ....
> 

I vote for this.

> 
> Another powerful approach is to extend the HTML+ DTD on a per document
> basis and use the RENDER element to tell the browser how to render new
> elements in terms of older ones. In fact, I really want to throw out
> most of the existing semantic emphasis tags!

Good.  


Steve Waterbury



From fielding@simplon.ICS.UCI.EDU  Wed Feb  2 13:52:45 1994 -0800
Message-Id: <9402021352.aa09432@paris.ics.uci.edu>
Date: Wed, 02 Feb 1994 13:52:45 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Configurable log proposal 

frans said:

> Roy said:
>> 
>> Now, as for the pitfalls:
>> 
>> 1) There is no compelling need for log flexibility other than saving
>>    disk space.  Regardless of how the data is organized, people will still
>>    want to use some log analyzer to view the data and thus the format should
>>    be designed primarily for machine readability and for occasional human
>>    reading or grep search.  The log analyzer will have its own (possibly
>>    configurable) output format for human consumption.
> 
> which is not completely true. I never use a log analyzer. I look at the
> file, and when I want some particular information I use grep. that works
> great for me and I don't need to learn how to use a log analyzer, just to
> generate the info Ari wants to put in the logfile. But I'm only
> a small user. On the other hand large sites may very well be interested
> in saving diskspace (except for the very very large sites :-))

Isn't that exactly what I said?  Please note that these are pitfalls,
not barriers.  Also, some analyzers (wwwstat in particular) are easier to
use than grep.

>> 2) The content of the data is only known at the time the log entry is
>>    made.  Any information that was not written at that time is lost to
>>    any later analysis.  Thus, it is usually preferable to log everything
>>    and let the log analysis program choose what should be ignored.
>
> why? If I choose to write down only a limited set into the LOG than I know
> I cannot fetch the other information, but it's my choise and my
> responsibility.

Exactly.  However, you are assuming that the person setting up the
configuration is the same as the one reading the log.  My point was
that you can't decide what you need after-the-fact and to make this
clear to everyone.

>> 3) Every time the configuration changes, the old log file must be deleted.
>>    This is because any log analyzer will only be able to understand one
>>    log format at a time.
>> 
>> 4) Special formatting conventions (like the square brackets surrounding
>>    the date field in NCSA httpd logs) make it much easier for analyzers
>>    to parse the data and identify mangled entries -- a condition which
>>    occurs quite often with NCSA httpd.
>> 
>> 5) It makes it slightly harder for people like me to write and test a
>>    simple log analyzer program.
>> 
> 3) 4) 5) are irrelevant to a guy like me who simply want to broswe the
> logfile and has only a very small local disk.

They are design concerns regardless of whether they are specifically
relevant to you.

> 3) can be solved by writing down the configuration change in the logfile
>   which is also handy for 5) so you will know at all times what the
>   format is and which fields are written in the logfile.
>   ie. the same string that does the formatting in ari's program should
>   be present in the logfile; or if you are willing to forget about 3)
>   should be made avaliable for the log analyzer. ...

Unless, of course, your log analyzer happens to be a simple spreadsheet
or database program that cannot handle dynamically reconfigurable record
structures.  And beyond that, how is the analyzer supposed to summarize
data that changes content in mid-stride?  This kind of feature is what
I call a living hell.

>   ... That makes your '-' suggested below unnesecairy,

Nope.  It has no effect on the question of what should be logged when a
particular requested field is empty.  My point was that it MUST NOT
be logged as an empty string because that will throw off the field
count which is important for simple analyzers and garbage recovery.


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From vinay@eit.COM  Wed Feb  2 14:46:07 1994 PST
Message-Id: <9402022246.AA23878@eit.COM>
Date: Wed, 2 Feb 94 14:46:07 PST
From: vinay@eit.COM (Vinay Kumar)
Subject: Re: Protocol Benchmarking (HTTP protocol)

If i understand this correctly, then another possible solution could 
lie in upgrading the current Browsers/Servers to handle "MIME multipart" 
messages within the same GET request. You could now easily transport 
multiple files with one request. Each part of the "mutipart" msg could 
easily be a "Content-type: text/html", "Content-type: image/gif", 
"Content-type: audio/basic", etc....
--
  Vinay Kumar
vinay@eit.com





From altis@ibeam.jf.intel.com  Wed Feb  2 14:52:54 1994 -0800
Message-Id: <m0pRqP4-00041AC@ibeam.intel.com>
Date: Wed, 2 Feb 1994 14:52:54 -0800
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: Protocol Benchmarking (with Accept examples - long)

At 12:20 AM 2/2/94 -0800, David Koblas wrote:
>HTTP 1.0 example negotiating session:
>
>        Connection opened:
>                GET /index.html HTTP/1.0
>                [ batch of "Accept" and other headers sent,
>                  mosaic sends about 1K worth ]
>
>                File is sent
>        Connection closed:
>
>This is awful, since not only is there connection creation/tear down
>expenses, but also retrasmission of "client information" to the server.
>Also, since (from the survey of my server) most of the HTML documents
>are ~1K in size, it means that twice the information is being sent
>than necessary...  Not good for a slow link..

I definitely would like to see some way of doing multiple GETs with a
single connection. However, the connection costs right now aren't that
great except for the Accept. Perhaps current Accept usage should be
rethought? Several issues to consider are:
1. What are clients currently sending to servers? Lynx only sends a few
Accepts since binary information isn't displayed or manipulated, but saved
to disk. On the other hand, Mosaic for X sends approximately 1K worth.
Here's everything sent out at the start of a typical Lynx transaction.

GET /index.html HTTP/1.0
Accept: www/source
Accept: text/html
User-Agent: Lynx/2.2  libwww/2.14
From: altis@ibeam.intel.com

Other Accept examples are at the end of the message.
2. Since only gif and xbm are used for inline images shouldn't GUI clients
simply send
Accept: image/gif
Accept: image/x-xbm
when fetching <IMG SRC> inlined images? Some GUI clients actually accept
other image types for inlined images, but it is non-standard.
3. Should the transaction be reworked so that the server says I've got this
MIME type, then the client says YES or NO?
4. The number of binary image formats and binary document formats is very
large in the PC and Mac arena. Is it reasonable to assume in most cases
that the document will simply be handed off to an external browser so that:
Accept: application/*
Accept: audio/*
Accept: image/*
is the way to go rather than sending a hundred plus Accept types? The MIME
mapping application/x-excel or whatever would still get sent with the reply
so that the client can still map between the MIME type and an external
application to launch. This assumes we continue to have a central
repository for MIME types so all our x- types match across clients and
servers.

ka
---
Cello doesn't currently send Accept.

X Mosaic 2.1
Accept: text/plain
Accept: application/x-html
Accept: application/html
Accept: text/x-html
Accept: text/html
Accept: application/x-hdf
Accept: application/x-netcdf
Accept: application/hdf
Accept: application/netcdf
Accept: audio/basic
Accept: audio/x-aiff
Accept: image/gif
Accept: image/jpeg
Accept: image/tiff
Accept: image/x-portable-anymap
Accept: image/x-portable-bitmap
Accept: image/x-portable-graymap
Accept: image/x-portable-pixmap
Accept: image/x-rgb
Accept: image/rgb
Accept: image/x-xbitmap
Accept: image/x-xpixmap
Accept: image/xwd
Accept: image/x-xwd
Accept: image/x-xwindowdump
Accept: video/mpeg
Accept: application/postscript
Accept: application/x-dvi
Accept: message/rfc822
Accept: application/x-latex
Accept: application/x-tex
Accept: application/x-texinfo
Accept: application/x-troff
Accept: application/x-troff-man
Accept: application/x-troff-me
Accept: application/x-troff-ms
Accept: text/richtext
Accept: text/tab-separated-values
Accept: text/x-setext
Accept: */*

Windows Mosaic 1.0
Accept: application/pdf
Accept: application/x-frame
Accept: audio/x-midi
Accept: application/x-rtf
Accept: video/msvideo
Accept: video/qtime
Accept: video/mpeg
Accept: image/jpeg
Accept: image/gif
Accept: application/postscript
Accept: audio/wav
Accept: text/plain
Accept: text/html
Accept: audio/x-aiff
Accept: audio/basic

Mac Mosaic 1.0.3
Accept: text/plain
Accept: application/x-html
Accept: application/html
Accept: text/x-html
Accept: text/html
Accept: text/richtext
Accept: application/octet-stream
Accept: application/postscript
Accept: application/mac-binhex40
Accept: application/zip
Accept: application/macwriteii
Accept: application/msword
Accept: image/gif
Accept: image/jpeg
Accept: image/x-pict
Accept: image/tiff
Accept: image/x-xbm
Accept: audio/x-aiff
Accept: audio/basic
Accept: video/mpeg
Accept: video/quicktime
Accept: application/macbinary
Accept: application/pdf
Accept: */*





From scohen@Accurate.COM  Wed Feb  2 16:28:18 1994 -0500
Message-Id: <9402022128.AA11061@Accurate.COM>
Date: Wed, 02 Feb 94 16:28:18 -0500
From: scohen@Accurate.COM (Stephen P.Cohen)
Subject: DTD for HTML+

I would like to obtain a copy of the DTD for HTML+ which is supported by Mosaic
version 2.0 for X and httpd version 1.0.5a from NCSA.  Is there a definitive one
available for general use and, if so, where?  Thanks,
Stephen Cohen
scohen@accurate.com



From atotic@ncsa.uiuc.edu  Wed Feb  2 17:54:19 1994 -0600 (CST)
Message-Id: <9402022354.AA15398@void.ncsa.uiuc.edu>
Date: Wed, 2 Feb 1994 17:54:19 -0600 (CST)
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: Protocol Benchmarking (with Accept examples - long)

> I definitely would like to see some way of doing multiple GETs with a
> single connection. However, the connection costs right now aren't that
> great except for the Accept. Perhaps current Accept usage should be
> rethought? Several issues to consider are:

I am not sure how great are the costs for Accept:. Since you need to send at
least one packet over the network, What is the difference between 50 bytes
and 1K worth of data sent?. Any networking people out there that can give
us better cost analysis?

Multiple accepts is a good idea, but I am not sure how feasible would it be
to implement. Right now, client could combine all the outstanding requests to
a one big one. I am not sure how well this will work in the future, with
possible proxy caching and URNs. Few other problems to think of: What about
order in which images are received and parsed? Some clients depend upon
some images arriving before the others. Whose responsibility would it be to
eliminate multiple requests for the same image?

> 3. Should the transaction be reworked so that the server says I've got this
> MIME type, then the client says YES or NO?

Not a bad idea. Maybe a two-stage process. The first request sends a list
of types supported internally by the client (similar to Lynx), and if the
choosen document is not determined to be one of these types, server requests
a confirmation from the client that a non-standard type can be handled. Client
could look up its master list, and if that fails ask the user if he really 
wants to download an unusual data type.

> 4. The number of binary image formats and binary document formats is very
> large in the PC and Mac arena. Is it reasonable to assume in most cases
> that the document will simply be handed off to an external browser so that:
> Accept: application/*
> Accept: audio/*
> Accept: image/*

This defeats the purpose of type negotiation, which is to enable clients and 
servers to cooperate in determining what type is sent to the client. So far,
we have not seen any use of this capability. I would love to see a server that
would serve PICTs to Mac clients, smaller GIFs to clients with a small screen,
automatically decompress the files if the client does not accept compressed
data, etc. I think that all this functionality can be accomplished with the
protocol as is. I am not sure how well would simplified versions work for these
cases.

Aleks




From montulli@stat1.cc.ukans.edu  Wed Feb  2 18:37:06 1994 CST
Message-Id: <9402030037.AA23076@stat1.cc.ukans.edu>
Date: Wed, 2 Feb 94 18:37:06 CST
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Protocol Benchmarking (with Accept examples - long)

> 
> > 3. Should the transaction be reworked so that the server says I've got this
> > MIME type, then the client says YES or NO?
> 
> Not a bad idea. Maybe a two-stage process. The first request sends a list
> of types supported internally by the client (similar to Lynx), and if the
> choosen document is not determined to be one of these types, server requests
> a confirmation from the client that a non-standard type can be handled. Client
> could look up its master list, and if that fails ask the user if he really 
> wants to download an unusual data type.
> 
Since Kevin and I were discussing this about half an hour ago, I
might as well put in my 4 or 5 cents worth. (inflation :)

If the host sent back the type immediately after the client sends
the get, the client could check it's list of accepts to see if
it's an acceptable type, for instance "image/jpeg".  
If the type is acceptable, the client responds, "OK
send me the data", otherwise the client says "I don't understand
image/jpeg" but I do understand "image/gif and image/x-xbm".
If the server can deliver those types then he sends the data,
if not then the server may attempt a different sub-group, for
instance "application/x-mac-draw".  In each case the client
would only send the accept header of the specified sub-group,
thereby saving the broadcast of a very large group of accepts.

This form of negotiation would also be useful for Authorization,
charge back, etc., since each of them would benifit by not
having to open the connection twice.  (currently Authorization
must first be rejected by an authorized server and then retry
with a password)

:lou



From omy@San-Jose.ate.slb.com  Wed Feb  2 16:45:45 1994 PST
Message-Id: <9402030045.AA15963@San-Jose.ate.slb.com>
Date: Wed, 2 Feb 94 16:45:45 PST
From: omy@San-Jose.ate.slb.com (Omy Ronquillo)
Subject: access_log


Hi!

Can I as easily mv access_log from the logs directory in httpd
and create a new one without having to re-fire up httpd? I want
to archive the old access_log and create a new one.

Or you can suggest what is normally done in this case.

Thanks.
Omy



From hoesel@chem.rug.nl  Thu Feb  3 02:29:10 1994 +0100 (MET)
Message-Id: <9402030129.AA01817@Xtreme>
Date: Thu, 3 Feb 1994 02:29:10 +0100 (MET)
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Re: Protocol Benchmarking (shortened :-)

 lou wrote:

> Since Kevin and I were discussing this about half an hour ago, I
> might as well put in my 4 or 5 cents worth. (inflation :)
> 
> If the host sent back the type immediately after the client sends
> the get, the client could check it's list of accepts to see if
> it's an acceptable type, for instance "image/jpeg".  
> If the type is acceptable, the client responds, "OK
> send me the data", otherwise the client says "I don't understand
> image/jpeg" but I do understand "image/gif and image/x-xbm".
> If the server can deliver those types then he sends the data,
> if not then the server may attempt a different sub-group, for
> instance "application/x-mac-draw".  In each case the client
> would only send the accept header of the specified sub-group,
> thereby saving the broadcast of a very large group of accepts.
> 
> This form of negotiation would also be useful for Authorization,
> charge back, etc., since each of them would benifit by not
> having to open the connection twice.  (currently Authorization
> must first be rejected by an authorized server and then retry
> with a password)
> 
but isn't it a 'nice' feature of http that it doesn't need to keep connections
to the client open, so it cannot wait for the client to respond with "OK".
If it could, then it could also wait a bit longer so the MGET
suggetion (or the multipart request or whatever) isn't needed at all!

- frans

(just my $2 (If I were paid by the hour :-))





From hgs@research.att.com  Wed Feb  2 20:39:21 1994 EST
Message-Id: <9402030140.AA13393@dxmint.cern.ch>
Date: Wed, 2 Feb 94 20:39:21 EST
From: hgs@research.att.com (Henning G. Schulzrinne)
Subject: Re: Protocol Benchmarking (with Accept examples - long)

To add some very basic performance numbers:
- each cross-country round-trip will take a delay of 60 ms or more
  (example: ping times from here on the East coast to the West coast
  from and to well-connected sites are on the order of 100 ms).

- a 1 KByte packet will take 8 ms of transmission time on each
  1 Mb/s (T1) hop, without any queueing. Take an example:

  ping (round-trip) time from portal.research.att.com to beta.xerox.com:
  56 bytes:   about 100 ms
  1000 bytes: about 190 ms

The arithmetic is thus fairly straightforward: if you need more than
two even tiny packets (on average) to do the job of one monster
packet, you lose. The calculation gets worse with high-latency
firewalls or transatlantic connections in the middle. For slow
links, clearly, things look somewhat different; there is no
universally best strategy. 

Henning Schulzrinne




From hotsand!ellson  Wed Feb  2 20:49:40 1994 EST
Message-Id: <9402030149.AA10338@hotsand.dacsand>
Date: Wed, 2 Feb 94 20:49:40 EST
From: hotsand!ellson (John Ellson)
Subject: Motif 1.2.2 lock up problems.

I'm not sure that this is specifically a Mosaic problem as I've seen
similar symptoms in other applications, but I'm hoping that
someone here might know how to fix it in Mosaic.

This problem occurs under Openwindows 3, SunOS4.1.3, and only with
Motif 1.2.2, not with Motif 1.1.  X11R4 or X11R5 makes no difference.

The problem occurs if you open a menu item, e.g. File -> Open Local,
and quickly click on the "File" item in the menu bar before the
sub window has opened.  (I think other menu related sequences cause
the lockup too but I can do this one repeatably).  The result is that 
Openwindows locks up.  The mouse is still live but you can't select 
anything, and the mouse pointer points NE instead of its usual NW.

Does anybody know how to fix this?

John



From mcrae@ora.com  Wed Feb  2 17:42:25 1994 -0800
Message-Id: <199402030142.AA04574@rock.west.ora.com>
Date: Wed, 02 Feb 94 17:42:25 -0800
From: mcrae@ora.com (Christopher McRae)
Subject: MIME Transfer Format (was Re: Protocol Benchmarking...)

Vinay Kumar writes:
> If i understand this correctly, then another possible solution could 
> lie in upgrading the current Browsers/Servers to handle "MIME multipart" 
> messages within the same GET request. You could now easily transport 
> multiple files with one request. Each part of the "mutipart" msg could 
> easily be a "Content-type: text/html", "Content-type: image/gif", 
> "Content-type: audio/basic", etc....

  This is the right way to do it, I believe.  We've kicked this idea around
before (see background messages included below), and it's clear that using
multipart messages is a big win in other ways, too.  For instance, one could
use multipart messages to deliver pages containing sections which would be
displayed only if certain conditions were met.  The rules specifying how and
when the various sections should be expanded/collapsed would be delivered
along with the other parts, perhaps as the first part of the multipart.  Note
that this technique is also useful for delivering encrypted information, and
for including the DTD along with a document instance (when we abandon HTML for
arbitrary DTD's :-)
  In fact, I have been thinking about how one would go about defining standard
ways of putting together such multipart packages.  To use the techniques
mentioned above, we would need to define a control scripting language for
describing the relationships between the different message parts and for
orchestrating their interaction.  In my thinking, this control language shares
some characteristics of the 'Universal network graphics language' which Tim
brought up on this list last week.  Although I think 'Universal network graphics
language' is a misnomer for the kind of thing we wrote about - graphical objects
are just one of the object types we're all interested in squirting around the net.

Example:

    MIME-Version: 1.0
    Content-Type: multipart/related;
    boundary=unique-boundary-1

    --unique-boundary-1
    Content-Type: application/html-control

    [
      here is where one would put a simple control script instructing the browser
      which part to show first, second, etc.
    ]

    --unique-boundary-1
    Content-Type: text/html
    Part-Name: doc1

    [ html formatted document here ]

    --unique-boundary-1
    Content-Type: image/gif
    Part-Name: image1

    [ image here ]

    --unique-boundary-1


Chris
-----------------------------------------------------------------------
Christopher McRae			            mcrae@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036


Background messages - more context can be found at
  http://gummo.stanford.edu/html/hypermail/www-talk-1993q4.index.html
-----------------------------------------------------------------------

Replied: Mon, 18 Oct 1993 11:03:14 PDT
Replied: Christopher.McRae
>From www-talk-request@dxcern.cern.ch Sun Oct 17 12:36:16 1993
Received: from dxmint.cern.ch by library.ucsf.edu with SMTP id AA17872
  (5.67a8/IDA-1.5 for <www-talk-local@ckm.ucsf.edu>); Sun, 17 Oct 1993 12:36:14 -0700
Received: from dxcern.cern.ch by dxmint.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA05510; Sun, 17 Oct 1993 20:33:56 +0100
Received: by dxcern.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA03196; Sun, 17 Oct 1993 20:28:50 +0100
Received: from dxmint.cern.ch by dxcern.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA03192; Sun, 17 Oct 1993 20:28:48 +0100
Received: from nxoc01.cern.ch by dxmint.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA03233; Sun, 17 Oct 1993 20:28:46 +0100
Received: from dxmint.cern.ch by  nxoc01.cern.ch  (NeXT-1.0 (From Sendmail 5.52)/NeXT-2.0)
	id AA04035; Sun, 17 Oct 93 19:59:56 MET
Received: from athena.library.ucsf.edu by dxmint.cern.ch (5.65/DEC-Ultrix/4.3)
	id AA03229; Sun, 17 Oct 1993 20:28:42 +0100
Received: from sphinx.library.ucsf.edu by library.ucsf.edu with SMTP id AA17807
  (5.67a8/IDA-1.5 for <www-talk>); Sun, 17 Oct 1993 12:27:30 -0700
Message-Id: <199310171927.AA17807@library.ucsf.edu>
From: Christopher.McRae (Christopher McRae)
Organization: UCSF Center for Knowledge Management
Email: mcrae@ckm.ucsf.edu or uunet!mcrae
Phone: 415/476-3577
Fax: 415/476-4653
To: www-talk@library.ucsf.edu
Cc: Dave_Raggett <dsr@hplb.hpl.hp.com>
In-Reply-To: Your message of Tue, 12 Oct 1993 13:18:16 -0000
	<9310121218.AA27649@manuel.hpl.hp.com> 
Subject: Re: request for new forms submission consensus 
Date: Sun, 17 Oct 1993 12:34:04 PDT
Sender: Christopher.McRae

Dave Raggett writes:
> Frans van Hoesel has pointed out the value in being able to
> use a checkbox or an icon or whatever to submit forms. In 
> a tax return there might be a load of questions that become
> irrelevant when you click the checkbox to indicate that you
> are "single". In this case the form would be submitted and
> the server would then return it with the irrelevant questions
> greyed-out:
> 
>	 Single? <INPUT NAME="single" TYPE=checkbox SUBMIT>
> 
> The idea here is to make SUBMIT an independent attribute
> that can be used with arbitrary field types. You could
> have multiple "submit buttons" in the same form. This way
> authors can choose whether the form contents gets checked
> after each field on a per field basis.

  Why not allow/define the use of MIME multipart messages for such applications?
That is, rather than using the SUBMIT attribute as above to retreive a
customized version of the form, why not include named sections of a document
section and define EXPAND/COLLAPSE attributes to control display?  For instance,

    MIME-Version: 1.0
    Content-Type: multipart/mixed;
        boundary=unique-boundary-1

    --unique-boundary-1
    Content-Type: application/html-form

    [some stuff here]
    <INPUT NAME="single" TYPE=checkbox EXPAND=single_form COLLAPSE=couple_form>
    [more stuff here]

    --unique-boundary-1
    Content-Type: application/html-form
    Part-Name: single_form

    [ single-specific stuff goes here ]

    --unique-boundary-1--
    Content-Type: application/html-form
    Part-Name: couple_form

    [ couple-specific stuff goes here ]

    --unique-boundary-1--

  Note that one could also use only one of EXPAND or COLLAPSE to toggle display
of a document section as opposed to selecting between alternate sections.

Another example:
<SELECT SEVERAL NAME="what-to-do">
<LI EXPAND=book_selection_form> Read A Book
<LI EXPAND=walking_trail_map> Take a Walk
<LI EXPAND=bagelry_menu> Buy a Bagel
<LI EXPAND=tv_guide> Watch TV
</SELECT>

  This model would work well for those applications where the overhead of
transferring possibly irrelevant sections of a document was low relative to
the overhead of performng multiple GETs.

Chris
--------------------------------------------------------------------------------
Christopher McRae			mail: mcrae@ckm.ucsf.edu
UCSF Center for Knowledge Management	at&t: 415/476-3577
530 Parnassus Avenue, Box 0840	 	fax: 415/476-4653
San Francisco, California 94143

Replied: Mon, 18 Oct 1993 10:39:34 PDT
Replied: Dave_Raggett <dsr@hplb.hpl.hp.com>
>From dsr@hplb.hpl.hp.com Mon Oct 18 04:02:08 1993
Received: from hplb.hpl.hp.com by library.ucsf.edu with SMTP id AA22789
  (5.67a8/IDA-1.5 for <Christopher.McRae@library.ucsf.edu>); Mon, 18 Oct 1993 04:02:06 -0700
Received: from dragget.hpl.hp.com by hplb.hpl.hp.com; Mon, 18 Oct 93 11:51:41 +0100
Received: by manuel.hpl.hp.com
	(16.6/15.6+ISC) id AA26007; Mon, 18 Oct 93 12:00:49 +0100
From: Dave_Raggett <dsr@hplb.hpl.hp.com>
Message-Id: <9310181100.AA26007@manuel.hpl.hp.com>
Subject: Re: request for new forms submission consensus 
To: Christopher.McRae@library.ucsf.edu
Date: Mon, 18 Oct 93 12:00:47 BST
Mailer: Elm [revision: 66.36.1.1]

  Why not allow/define the use of MIME multipart messages for such applications?
> That is, rather than using the SUBMIT attribute as above to retreive a
> customized version of the form, why not include named sections of a document
> section and define EXPAND/COLLAPSE attributes to control display?  For
> instance,

An interesting idea! Unfortunately, I don't see much chance of it getting
supported by the browser writers for some time yet. Sorry to be downbeat but
right now I am struggling to get the discussion document finished and to
find a balance between what is possible and what browser writers will
agree to implementing quickly. There is considerable pressure to get the
basic spec pinned down ASAP now that Mosaic and other browsers are starting
to implement forms in the absence of a solid spec.

I see ideas like this finding their worth once we have had basic forms
support for a while, and people are prepared to take on new ideas.

Many thanks,

Dave Raggett

From Christopher.McRae Mon Oct 18 10:56:37 1993
Received: from sphinx.library.ucsf.edu by library.ucsf.edu with SMTP id AA26274
  (5.67a8/IDA-1.5 for <Christopher.McRae>); Mon, 18 Oct 1993 10:56:36 -0700
Message-Id: <199310181756.AA26274@library.ucsf.edu>
From: Christopher.McRae (Christopher McRae)
Organization: UCSF Center for Knowledge Management
Email: mcrae@ckm.ucsf.edu or uunet!mcrae
Phone: 415/476-3577
Fax: 415/476-4653
To: Christopher.McRae
In-Reply-To: Your message of Sun, 17 Oct 1993 12:34:04 -0700
	<199310171927.AA17807@library.ucsf.edu> 
Subject: Re: request for new forms submission consensus 
Date: Mon, 18 Oct 1993 11:03:12 PDT
Sender: Christopher.McRae

I hope I'm not about to provoke Marc's kitchen sink speech again. ;)

On Sun, 17 Oct I wrote:
> Why not allow/define the use of MIME multipart messages for such applications?
> That is, rather than using the SUBMIT attribute as above to retreive a
> customized version of the form, why not include named sections of a document
> section and define EXPAND/COLLAPSE attributes to control display?  For instance
[ rest of message not included ]

  I realized that we could use the same technique to store and retrieve
encrypted documents as well.  For instance,

MIME-Version: 1.0
Content-Type: multipart/related;
boundary=unique-boundary-1

--unique-boundary-1
Content-Type: application/html-form

[some stuff here]
<INPUT NAME="key" TYPE=text ENCRYPT=@secret SUBMIT>
[ possibly more stuff here ]

--unique-boundary-1
[ whatever MIME headers are appropriate for an encrypted message ]
Content-Id: secret

[ encrypted text of message goes here ]

--unique-boundary-1--

One could hit a button on the client to pull in the above template, then type
in a key and the secret text.  The client would encrypt the secret portion
using the given key, and then submit it to the server.  
  Later, when the document is downloaded, the secret portion will be unreadable
until the appropriate key is typed in.
  This method is nice since the key never even needs to leave the client
machine.  The server doesn't even know how to decrypt the secret part(s).
Of course, you could share the key with others in separate transmissions and
thus allow them to view the secret message.

Chris
--------------------------------------------------------------------------------
Christopher McRae			mail: mcrae@ckm.ucsf.edu
UCSF Center for Knowledge Management	at&t: 415/476-3577
530 Parnassus Avenue, Box 0840	 	fax: 415/476-4653
San Francisco, California 94143




From masinter@parc.xerox.com  Wed Feb  2 18:06:36 1994 PST
Message-Id: <94Feb2.180638pst.2732@golden.parc.xerox.com>
Date: Wed, 2 Feb 1994 18:06:36 PST
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Protocol Benchmarking (with Accept examples - long)


While we're optimizing things, how about looking at the exchanges
necessary for an authenticated call. Right now, access control adds a
lot of exchanges.  Couldn't you somehow send your credentials (oh, who
you are, with the time and the host ID of yourself and the destination
host signed with your private key) to the remote host in the initial
GET?



From fielding@simplon.ICS.UCI.EDU  Wed Feb  2 18:41:56 1994 -0800
Message-Id: <9402021842.aa26798@paris.ics.uci.edu>
Date: Wed, 02 Feb 1994 18:41:56 -0800
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: access_log 

> Can I as easily mv access_log from the logs directory in httpd
> and create a new one without having to re-fire up httpd? I want
> to archive the old access_log and create a new one.

Assuming you are talking about NCSA httpd, just move it out of
the directory (or rename the file).  The daemon will create a
new one when it gets the next request.


....Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From robm@ncsa.uiuc.edu  Wed Feb  2 23:02:54 1994 -0600
Message-Id: <9402030502.AA18851@void.ncsa.uiuc.edu>
Date: Wed, 2 Feb 1994 23:02:54 -0600
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: access_log

/*
 * Re: access_log  by "Roy T. Fielding"
 *    written on Feb  2,  6:41pm.
 *
 * > Can I as easily mv access_log from the logs directory in httpd
 * > and create a new one without having to re-fire up httpd? I want
 * > to archive the old access_log and create a new one.
 * 
 * Assuming you are talking about NCSA httpd, just move it out of
 * the directory (or rename the file).  The daemon will create a
 * new one when it gets the next request.
 * 
 */

Only if you're running from inetd. If you're going standalone, you have to
restart the server with a SIGHUP.

--Rob



From guenther.fischer@hrz.tu-chemnitz.de  Thu Feb  3 07:12:02 1994 +0100 (MET)
Message-Id: <9402030612.AA01140@flash1.hrz.tu-chemnitz.de>
Date: Thu, 3 Feb 1994 07:12:02 +0100 (MET)
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: Re: Patches to NCSA httpd 1.1

> > 
> 
> That sounds like a good plan.  That way, Guenther and I can go have a
> field day without stomping on you too much.  Why don't I do this: I'll
> modify my changes to use the DirectoryOptions construct.  Then, when
> Guenther sends me his patch, he and I can add options ad nausium.
> Guenther? Sound ok?
> 
OK - I'll send you the patches Walt and I hope to find the functionality
in the next official patch from Rob - I don't like unofficial patches
...
I'll put the patches and some CGI's around that at:
http://www.tu-chemnitz.de/~fischer/fancyindex/

Hope it's there at 12 MET.

	~Guenther


-- 
Name:      Guenther Fischer / Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361     / mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From J.P.Knight@lut.ac.uk  Thu Feb  3 09:04:37 1994 +0000 (GMT)
Message-Id: <Pine.3.05.9402030935.A27151-b100000@suna>
Date: Thu, 3 Feb 1994 09:04:37 +0000 (GMT)
From: J.P.Knight@lut.ac.uk (Jon P. Knight)
Subject: Re: Protocol Benchmarking (with Accept examples - long)

On Wed, 2 Feb 1994, Lou Montulli wrote:
> 
> If the host sent back the type immediately after the client sends
> the get, the client could check it's list of accepts to see if
> it's an acceptable type, for instance "image/jpeg".  
> If the type is acceptable, the client responds, "OK
> send me the data", otherwise the client says "I don't understand
> image/jpeg" but I do understand "image/gif and image/x-xbm".
> If the server can deliver those types then he sends the data,
> if not then the server may attempt a different sub-group, for
> instance "application/x-mac-draw".  In each case the client
> would only send the accept header of the specified sub-group,
> thereby saving the broadcast of a very large group of accepts.
> 

Correct me if I'm wrong but currently we send Accepts in a single outward
call and get some form of result (either the document or an error)
returned from the server.  With your scheme we send a request, receive a
list of possible types, send back a request for types the client can
handle and then get the reponse from the server.  Therefore we've gone
from one round trip delay to two.  Thus to save transmitting a few bytes
we take an increased latency hit.  Right?  When some of the network links
have RTDs in the thousands of ms from where I'm sitting and yet we have a
nice fat pipe to the Internet, I think we'd rather waste a few bytes on
small documents.  Latency is Your Enemy(tm) in WAN based distributed systems.

Just IMHO.

Jon

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
Jon Knight, Research Student in High Performance Networking and Distributed
Systems in the Department of _Computer_Studies_ at Loughborough University.
* Its not how big your share is, its how much you share that's important. *





From koblas@netcom.com  Thu Feb  3 01:38:20 1994 -0800 (PST)
Message-Id: <199402030938.BAA17954@mail.netcom.com>
Date: Thu, 3 Feb 1994 01:38:20 -0800 (PST)
From: koblas@netcom.com (David Koblas)
Subject: MIME Transfer Format (is Re: Protocol Benchmarking...)

Christopher McRae writes:
>In fact, I have been thinking about how one would go about defining
>standard ways of putting together such multipart packages.  To use the
>techniques mentioned above, we would need to define a control scripting
>language for describing the relationships between the different message
>parts and for orchestrating their interaction.  In my thinking, this
>control language shares some characteristics of the 'Universal network
>graphics language' which Tim brought up on this list last week.
>Although I think 'Universal network graphics language' is a misnomer
>for the kind of thing we wrote about - graphical objectsare just one of
>the object types we're all interested in squirting around the net.
>
>Example:
>
>    MIME-Version: 1.0
>    Content-Type: multipart/related;
>    boundary=unique-boundary-1
>
>    --unique-boundary-1
>    Content-Type: application/html-control
>
>    [
>      here is where one would put a simple control script instructing
>      the browser which part to show first, second, etc.
>    ]

This is a nifty idea, though it could be simplified...

Instead of "Content-Type: application/html-control", it might be better
to make it "applicaton/url-index" and then instead of scripting commands
it could be just a list of URL's for the message.  Thus boundry-2 would
refer to the first URL (the one requested), boundry-3 the second URL (an
inlined image), etc.  This way a browser would load a short-term-cache of
URL's that it already had just been sent.  There is ONE problem, which is
that still a server might say 'Accept: image/gif' but, is really a text
based browser and thus can't do inlined images... thus the images shouldn't
be sent.  [if somebody has a solution to this (inlined images), I might be
tempted to generate an example server...]

However this still stuffer from one problem, it means that the server
has to parse every document (or have a document dependancy list built).
Part of the "advantage" of having persistant HTTP connections where
mulitple gets could be issued, is that the inlined icon problem isn't
such a problem.  Also, for URL's that refere to connection orieted
services (aka FTP) then the connection wouldn't be torn down every time.

--koblas@netcom.com

ps.  You still could have the second boundry message be
     'applicaton/www-control', which could do all of the complicated
     things you would like.




From luotonen@ptsun00.cern.ch  Thu Feb  3 11:10:20 1994 +0100
Message-Id: <9402031010.AA01724@ptsun03.cern.ch>
Date: Thu, 3 Feb 94 11:10:20 +0100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: Protocol Benchmarking (with Accept examples - long)


> While we're optimizing things, how about looking at the exchanges
> necessary for an authenticated call. Right now, access control adds a
> lot of exchanges.

In a long run it doesn't add much -- first time you have one
extra connection "wasted", but from then on, if the documents
are organized in a sensible way, everything goes so smoothly
you don't notice anything extra happening.

-- Cheers, Ari --




From dsr@hplb.hpl.hp.com  Thu Feb  3 10:09:23 1994 GMT
Message-Id: <9402031009.AA14202@manuel.hpl.hp.com>
Date: Thu, 3 Feb 94 10:09:23 GMT
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Protocol Benchmarking (HTTP protocol)

Henning Schulzrinne writes:

> "Chained"/multiple GETs would also really help with TCP-level firewalls,
> where connection set-up cost are substantial, as well as with international
> links where the three-way TCP handshake can easily add seconds to the
> retrieval of a short document with lots of small images/icons.

We use a simple tcp packet relay mechanism invoked by inetd and find the
setup costs to be minimal. Contact me for further info.

Before we set about support for multiple GETs, browsers should try taking
advantage of "Accept: multipart/related" to request a document and its inlined
images with a single GET. The HTTP standard specifies how to do this now!

Best wishes,

Dave Raggett,

-----------------------------------------------------------------------------
Hewlett Packard Laboratories,           +44 272 228046
Bristol, England                        dsr@hplb.hpl.hp.com



From secret@hpwww.cern.ch  Thu Feb  3 15:14:12 1994 --100
Message-Id: <9402031412.AA13415@dxmint.cern.ch>
Date: Thu, 3 Feb 1994 15:14:12 --100
From: secret@hpwww.cern.ch (Arthur Secret)
Subject: Test with ListProcessor

www-talk is now maintained by ListProcessor. 
This is a test. A help file will follow

Thank you

Arthur
--
       Arthur Secret,       phone:(41-22) 767-37-55     
Technical Student at CERN,   e-mail: secret@dxcern.cern.ch



From MONICA@cepes.ro  Thu Feb  3 15:52:46 1994 --100
Message-Id: <01H8G6R2CABO0001S3@CEPES.RO>
Date: Thu, 3 Feb 1994 15:52:46 --100
From: MONICA@cepes.ro (Monica Cucoanes)
Subject: Re: CEPES Publications

                MULTILINGUAL LEXICON OF HIGHER EDUCATION
                *****************************************
                *****************************************

UNESCO/CEPES, European Centre for Higher Education, Editor-in-
Chief: Adrian Nicolescu. Vol. 1: Western Europe and North America.
Munchen, New Providence, London, Paris: K. G. SAUR, 1993, 346 pp.
HB. DM 148.00. ISBN 3-598-11058-8. Vol. 2: Central and Eastern
Europe. 1995, HB, c.DM 148.00. ISBN 3-598-11059-6. Subscription
price until 31.12.1994 for the complete set DM 268.00. ISBN 3-598-
10883-4.

In 1993 CEPES, the UNESCO European Centre for Higher Education,
located in Bucharest (Romania), published the first volume of its
large-scale multilingual lexicon of higher education. The volume
defines over 4,100 terms relating to higher education in the
following 20 countries: Austria, Belgium, Canada, Denmark, Finland,
France, Germany, Iceland, Ireland, Italy, Luxembourg, Malta, The
Netherlands, Norway, Portugal, Spain, Sweden, Switzerland, the UK,
and the USA. The terms are classified under 9 rubrics, each rubric
covering a major component of a higher education system (i.e. the
system; institutions and their structures; governing and
administrative bodies; academic staff; students; access and
admissions; teaching and learning; examining and evaluation;
degrees and diplomas). Within each rubric, the entries are listed
in the alphabetical order of the respective national and/or
official language or languages, followed by short definitions in
English.

An alphabetical list of all higher education abbreviations used in
the body of the work, as well as a general country index complete
this first volume of Lexicon.

Volume 2, due to be published in 1995, will cover the higher
education terminology used in the Central and Eastern European
countries, plus Greece, Israel, and Turkey.

The Lexicon is a reliable source of information for individuals
whose lines of study, teaching, or research require them to study
or work abroad. At the same time, it proves to be an invaluable
guide for employers needing to know the level of education of
prospective employees who have studied educational systems
unfamiliar to them. The Multilingual Lexicon of Higher Education
should not be missing from the shelves of university libraries,
departments of education, and other bodies engaged in the field of
higher education.

Volume 1 can be ordered directly from: K. G. SAUR Verlag, A Reed
Reference Publishing Company, Postfach 701620, D-81316 Munchen,
Germany; tel.: 49-89-76902-0.

For more information on the Multilingual Lexicon of Higher
Education, please contact: Ms Mariana Patru, Editor, CEPES-UNESCO,
39, Stirbei Voda St., 70732 Bucharest, Romania; e-mail:
mariana@cepes.ro.


Monica Cucoanes
System Manager
E-mail: Monica@cepes.ro



From atotic@ncsa.uiuc.edu  Thu Feb  3 16:26:34 1994 --100
Message-Id: <9402031524.AA23715@void.ncsa.uiuc.edu>
Date: Thu, 3 Feb 1994 16:26:34 --100
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: Protocol Benchmarking (with Accept examples - long)

> Correct me if I'm wrong but currently we send Accepts in a single outward
> call and get some form of result (either the document or an error)
> returned from the server.  With your scheme we send a request, receive a
> list of possible types, send back a request for types the client can
> handle and then get the reponse from the server.  Therefore we've gone
> from one round trip delay to two.  Thus to save transmitting a few bytes
> we take an increased latency hit.  Right?

I was thinking of sending only types that client can handle internally (GIF,
HTML, XBM), in the first request. Only if the requested file is an "unusual"
type, handled by external apps, would the second trip be necessary. This scheme
assumes that for a while, clients will to handle a few types internally, and 
a large number of external types.

Aleks



From ags@informatics.jax.org  Thu Feb  3 14:48:35 1994 --100
Message-Id: <9402031345.AA02095@zappa.informatics.jax.org>
Date: Thu, 3 Feb 1994 14:48:35 --100
From: ags@informatics.jax.org (Alexander G. Smith)
Subject: Re: Motif 1.2.2 lock up problems.

> From www-talk-request@www0.cern.ch Wed Feb  2 21:00 EST 1994
> From: ellson@hotsand.att.com
> Date: Wed, 2 Feb 94 20:49:40 EST
> Original-From: hotsand!ellson (John Ellson)
> To: www-talk@www0.cern.ch
> Subject: Motif 1.2.2 lock up problems.
> 
> I'm not sure that this is specifically a Mosaic problem as I've seen
> similar symptoms in other applications, but I'm hoping that
> someone here might know how to fix it in Mosaic.
> 
> This problem occurs under Openwindows 3, SunOS4.1.3, and only with
> Motif 1.2.2, not with Motif 1.1.  X11R4 or X11R5 makes no difference.
> 
> The problem occurs if you open a menu item, e.g. File -> Open Local,
> and quickly click on the "File" item in the menu bar before the
> sub window has opened.  (I think other menu related sequences cause
> the lockup too but I can do this one repeatably).  The result is that 
> Openwindows locks up.  The mouse is still live but you can't select 
> anything, and the mouse pointer points NE instead of its usual NW.
> 
> Does anybody know how to fix this?
>

Just press the Return key.

We have noticed this problem in various guises on any number of Motif 1.2
applications (including those we develop here).

Anyone know where the cause lies?

> John
> 




From montulli@stat1.cc.ukans.edu  Thu Feb  3 17:18:32 1994 --100
Message-Id: <9402031612.AA30075@stat1.cc.ukans.edu>
Date: Thu, 3 Feb 1994 17:18:32 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Protocol Benchmarking (with Accept examples - long)

> 
> On Wed, 2 Feb 1994, Lou Montulli wrote:
> > 
> > If the host sent back the type immediately after the client sends
> > the get, the client could check it's list of accepts to see if
> > it's an acceptable type, for instance "image/jpeg".  
> > If the type is acceptable, the client responds, "OK
> > send me the data", otherwise the client says "I don't understand
> > image/jpeg" but I do understand "image/gif and image/x-xbm".
> > If the server can deliver those types then he sends the data,
> > if not then the server may attempt a different sub-group, for
> > instance "application/x-mac-draw".  In each case the client
> > would only send the accept header of the specified sub-group,
> > thereby saving the broadcast of a very large group of accepts.
> > 
> 
> Correct me if I'm wrong but currently we send Accepts in a single outward
> call and get some form of result (either the document or an error)
> returned from the server.  With your scheme we send a request, receive a
> list of possible types, send back a request for types the client can
> handle and then get the reponse from the server.  Therefore we've gone
> from one round trip delay to two.  Thus to save transmitting a few bytes
> we take an increased latency hit.  Right?  When some of the network links
> have RTDs in the thousands of ms from where I'm sitting and yet we have a
> nice fat pipe to the Internet, I think we'd rather waste a few bytes on
> small documents.  Latency is Your Enemy(tm) in WAN based distributed systems.
> 
True, but 99% of the time the client will simply accept the first
type given by the server.  The negotiation will only take place
less than 1% of the time, and in the case of negotiation the server
will most likely have to make some on the fly conversion that will
probably be slower than the latency.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************




From secret@hpwww.cern.ch  Thu Feb  3 17:28:57 1994 --100
Message-Id: <9402031627.AA12651@dxmint.cern.ch>
Date: Thu, 3 Feb 1994 17:28:57 --100
From: secret@hpwww.cern.ch (Arthur Secret)
Subject: Our new listserver - Please read this


We have now switched to ListProcessor, thanks to the suggestions
of Richard  Wiggins (six months ago ;-), Lou Montulli and Mustafa Akgul.

Any problems with this software or suggestions on the configuration
should be mailed to me (secret@info.cern.ch)

Your initial password is WTALK. Please change it as soon as you can
by issuing the following request to listproc@info.cern.ch:

                SET WWW-TALK PASSWORD WTALK new-password

WARNING: Do not use your login password; you will be breaching security at your
site.

You may change the address you are subscribed with (currently secret@hpwww.cern.
ch)
with the following request:

                SET WWW-TALK ADDRESS WTALK new-address

assuming that you keep the same password.

For information on this service and how to use it, send the following
request in the body of a mail message to listproc@info.cern.ch:

                        HELP

Below is an excerpt of the HELP file
*********************************************************************

                          ListProcessor 6.0

Here is a brief description of the set of requests recognized by ListProcessor.
Everything appearing in [] below is optional; everything
appearing in <> is mandatory; all arguments are case insensitive. The vertical
bar ("|") is used as a logical OR operator between the arguments. Requests may
be abbreviated, but you must specify at least the first three characters.

Keep in mind that when referring to a <list>, that list may be of two kinds:
local or remote, unless otherwise noted. When referring to a local list, your
request will be immediately processed; when referring to a remote list (a list
served by another ListProcessor which this system knows about), your request
will be appropriately forwarded. Issue a 'lists' request to get a listing of all
local and known remote lists to this ListProcessor.

Recognized requests are:


help [topic]
------------
Without arguments, this file. Otherwise get specific information on the
selected topic. Topics may also refer to requests. To learn more about this
system issue a 'help listproc' request. To get a listing of all available
topics, generate an error message by sending a bogus request like 'help me'.


set <list> [<option> <arg[s]>]
------------------------------
Without the optional arguments, get a list of all current settings for
the specified list. Otherwise change the option to a new value for that
list. Issue a 'help set' request for more information.


subscribe <list> <your name>
----------------------------
The only way to subscribe to a list.


unsubscribe <list>
signoff <list>
------------------
Two ways of removing yourself from the specified list.


recipients <list>
review <list>
-----------------
Get a listing of all non-concealed people subscribed to the specified list.


information <list>
------------------
Get information about the specified list.


statistics <list> {[subscriber email address(es)] | [-all]}
-----------------------------------------------------------
Get a listing of non-concealed subscribers along with the number of messages
each one of them has sent to the specified list. If the optional email addresses
are given, then statistics will be collected for these users only. For example:
                stat foo user1@domain user2@domain
will generate statistics about these two subscribers. "-all" lists statistics
for all users that have posted on the list (whether currently subscribed ot
not).

run <list> [<password> <cmd [args]>]
------------------------------------
Run the specified command with the optional arguments and receive the output
from stdout and/or stderr. To get a listing of all available commands to run,
omit the arguments, i.e. issue a 'run <list>' request. You have to belong to
the specified list, and must have obtained the password from the list's owner;
the owner's address may be found in the Errors-To: header line of each
delivered message. <list> may be local only.


lists
-----
Get a list of all local mailing lists that are served by this server, as well
as of all known remote lists.


index [archive | path-to-archive] [/password] [-all]
----------------------------------------------------
Get a list of files in the selected archive, or the master archive if no
archive was specified. If an archive is private, you have to provide its
password as well.


get <archive | path-to-archive> <file> [/password] [parts]
----------------------------------------------------------
Get the requested file from the specified archive. Files are usually split in
parts locally, and in such a case you will receive the file in multiple email
messages -- an 'index' request tells you how many parts the file has been split
into, and their sizes; if you need to obtain certain parts, specify them as
optional arguments. If an archive is private, you have to provide its password
as well.

view <archive | path-to-archive>] [/password] [parts]
-----------------------------------------------------
Same as "get" but in interactive mode justs catenates the file on the
screen.

search <archive | path-to-archive>] [/password] [-all] <pattern>
----------------------------------------------------------------
Search all files of the specified archive (and all of its subarchives if -all
is specified) for lines that match the pattern. The pattern can be an
egrep(1)-style regular expression with support for the following additional
operators: '~' (negation), '|' and '&' (logical OR and AND), '<' '>' (group
regular expressions). The pattern may be enclosed in single or double quotes.
Note: . matches any character including new line.


release
-------
Get information about the current release of this ListProcessor system.


which
-----
Get a listing of local mailing lists to which you have subscribed.


Arthur
--
       Arthur Secret,       phone:(41-22) 767-37-55     
Technical Student at CERN,   e-mail: secret@dxcern.cern.ch



From uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch  Thu Feb  3 17:43:41 1994 --100
Message-Id: <2D505DA9@MSMAIL.INDY.TCE.COM>
Date: Thu, 3 Feb 1994 17:43:41 --100
From: uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch (Fisher Mark)
Subject: Re: Mosaic Accessories


Steve Vinoski with the Distributed Computing Program of HP is working on a 
distributed object management facility based on CORBA (he wrote the 
July-August 1993 article, "Distributed Object Computing with CORBA", for 
"The C++ Report", which included his email address :).  I have sent him 
email asking how to obtain copies of the CORBA spec, for which I will post 
his reply.

>....
>Well, since you brought it up...
>  We should coordinate our standardization efforts with the CORBA spec.
>Is anyone out there very familiar with the status of the CORBA effort?
>Does anyone have any references to relevant documentation?
>
>Chris
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From MONICA@cepes.ro  Thu Feb  3 17:54:24 1994 --100
Message-Id: <01H8G7OA6HFC0001SR@CEPES.RO>
Date: Thu, 3 Feb 1994 17:54:24 --100
From: MONICA@cepes.ro (Monica Cucoanes)
Subject: Re: CEPES Review


     CEPES review "Higher Education in Europe" is published with 4
issues per year. Nr. 4:1993 will be ready for distribution in a
week, which will complete the 1993 volume (Volume XVIII). This
volume comprises:

No. 1, 1993


CONFLICT OF INTEREST IN HIGHER EDUCATION

IN THE "EDUCATION BUSINESS", WHO OWNS WHAT?
Chris de WINTER HEBRON

ACADEMIC VALUES AND SERVICE CONFLICTS
Robert B. YOUNG

HIGHER EDUCATION AND CONFLICT OF INTEREST: CANADIAN PERSPECTIVES
Fred WILSON

PROBLEMS IN PARTNERSHIP
Joseph A. WATSON

CONFLICT OF INTEREST BETWEEN CENTRALIZED AND UNIVERSITY-LEVEL
GOVERNING BODIES: THE TURKISH CASE
Ihsan DOGRAMACI

OLD AND NEW TYPES OF CONFLICT IN HIGHER EDUCATION: THE LABOUR UNION
VIEWPOINT
Daniel MONTEUX

THE CONFLICTING REQUIREMENTS OF TEACHING, RESEARCH, AND MANAGEMENT
IN UNIVERSITIES
Miquel Angel CUEVAS DIARTE

WIDENING ACCESS TO UNIVERSITIES WHILE IMPROVING THE QUALITY OF
TEACHING?
Brigitte BERENDT

UNIVERSITY AUTONOMY AND ACADEMIC FREEDOM IN ITALY
Corrado BIGHI

ON THE DIMENSION OF POWER IN EDUCATION
Vilia IVANOVA VELIKOVA

Tribune

UNIVERSITY MANAGEMENT IN EASTERN EUROPE
David RANDALL

Information
CEPES News
Denmark, Germany, Hungary, Poland, Romania, Russian Federation,
Slovak Republic, United Kingdom, USA; Miscellaneous
Bibliographical References
New Publications Received by CEPES

Calendar of Events

Notes on Contributors

CEPES Publications


No. 2, 1993


HIGHER EDUCATION AND THE LABOUR MARKET


HIGHER EDUCATION AND THE WORLD OF WORK: AN OVERVIEW
Maurice KOGAN and John BRENNAN

THE EDUCATION MARKET, QUALIFICATIONS, AND EUROPEAN INTEGRATION
Osmo KIVINEN and Risto RINNE

DOES THE PROGRAMME MATTER? APPROACH AND MAJOR FINDINGS OF THE
KASSEL GRADUATE SURVEY
Harald SCHOMBURG and Ulrich TEICHLER

CAREER PATTERNS OF FINNISH ACADEMICS AND INTELLECTUALS: A
LONGITUDINAL STUDY OF THE 1965 STUDENT GENERATIONS
Yrjo Paavo HAYRYNEN with Liisa HAYRYNEN

ACADEMIC DRIFT AND BLURRING OF BOUNDARIES IN SYSTEMS OF HIGHER
EDUCATION
Claudius GELLERT

THE EXPANSION OF EDUCATION IN EASTERN EUROPE: A REGIONAL VIEW
Tamas KOZMA

EDUCATIONAL POLICY AND ECONOMIC POLICY: THE CASE OF SOUTHERN ITALY
Roberto MOSCATI and Enrico PUGLIESE

THE LOST PROFESSION
Ronald BARNETT and Robin MIDDLEHURST

COMPARING QUALITY IN EUROPE
John BRENNAN, Leo C.J. GOEDEGEBUURE, Tarla SHAH, Don F.
WESTERHEIJDEN, and Peter J.M. WEUSTHOF

Information
CEPES News
Miscellaneous

Calendar of Events

Notes on Contributors
CEPES Publications


No. 3, 1993

POLICY ISSUES OF QUALITY ASSESSMENT AND INSTITUTIONAL ACCREDITATION

OVERVIEW OF THE INTERNATIONAL HIGH-LEVEL CONSULTATION ON POLICY
ISSUES OF QUALITY ASSESSMENT AND INSTITUTIONAL ACCREDITATION IN
HIGHER EDUCATION

UNIVERSITY AUTONOMY AND QUALITY ASSURANCE
Carin BERG

QUALITY ASSURANCE: ISSUES AND POLICY IMPLICATIONS
Lazar VLASCEANU

QUALITY ASSESSMENT IN HIGHER EDUCATION: ISSUES AND CONCERNS
Ian WHITMAN

INSTITUTIONAL SELF-EVALUATION IN ENGLISH HIGHER EDUCATION
Alun THOMAS

SOME QUESTIONS AND ANSWERS WITH REGARD TO EXTERNAL QUALITY
ASSESSMENT
A. I. VROEIJENSTIJN

THE ACADEMIC CO-OPERATION ASSOCIATION (ACA)
Alan SMITH

QUALITY ASSURANCE IN HIGHER EDUCATION: A GLOBAL TOUR OF PRACTICE
AND RESOURCES
Marjorie PEACE-LENN

PROLEGOMENA TO ACCREDITATION IN CENTRAL AND EASTERN EUROPE
Liam RYAN

THE ACCREDITATION OF HIGHER EDUCATION INSTITUTIONS IN THE UNITED
STATES
Ralph A. WOLFF

ACCREDITATION IN HUNGARY AND THE HUNGARIAN ACCREDITATION COMMITTEE
Andras RONA-TAS

QUALITY ASSESSMENT AND INSTITUTIONAL ACCREDITATION IN THE ROMANIAN
HIGHER EDUCATION SYSTEM
Iulian BEJU

THE FUTURE OF HIGHER EDUCATION IN BELARUS
T. E. GALKO

LAW OF THE RUSSIAN FEDERATION ON EDUCATION
Supreme Soviet

THE CONTROL AND EVALUATION OF HIGHER EDUCATION INSTITUTIONS
Alexander PROKOPCHUK

NON-STATE HIGHER EDUCATION INSTITUTIONS IN THE RUSSIAN FEDERATION
A. A. KUSHEL

THE ORADEA STATEMENT

Calendar of Events

Notes on Contributors

CEPES Publications


No. 4, 1993

CAREERS FOR WOMEN AT EUROPEAN UNIVERSITIES: OBSTACLES AND
OPPORTUNITIES

POLICY STRATEGIES FOR THE CAREER DEVELOPMENT OF WOMEN AT EUROPEAN
UNIVERSITIES
Esther K. HICKS

STRUCTURAL CONSTRAINTS ON GENDER EQUALITIES WITH REGARD TO
UNIVERSITY CAREERS IN FINLAND
Veronica STOLTE-HEISKANEN

CAREERS FOR MEN AND WOMEN IN BRITISH UNIVERSITIES: EVOLUTION AND
REVOLUTION
Margaret B. SUTHERLAND

EQUAL OPPORTUNITY OF INTEGRATION FOR WOMEN IN THE DUTCH ACADEMIC
WORLD
Wouter VAN ROSSUM and Esther K. HICKS

WOMEN AND UNIVERSITY EDUCATION IN TURKEY
Feride ACAR

ACADEMIC WOMEN IN SPAIN: AN ELITE SUBJECT TO DISCRIMINATION
Maria Antonia GARCIA DE LEON

Tribune

FEMALE ACCESS TO SCIENCE AND TECHNOLOGICAL EDUCATION IN NIGERIA
Francisca Isi OMORODION

A GLIMPSE INTO THE FUTURE: PROFESSIONAL DEVELOPMENT IN THE 21ST
CENTURY
Peter James MURPHY and Mary NIXON

Information
UNESCO News
CEPES News
Czech Republic, France, Hungary, Russian Federation, USA

Calendar of Events

Bibliographical References
Book-Notes
New Publications Received by CEPES

Notes on Contributors

CEPES Publications


     The price of subscription is for:

     one year:   50.00 $ - Costs of dispatch included
     two years:  80.00 $ - Costs of dispatch included

     Please send your order to

     CEPES UNESCO, 39 Stirbei Voda Street, Bucharest,
     Romania R-70732

     If you accompany your order by a cheque, the proceedings will
be speeded up. Otherwise we will send you an invoice.



From tkevans@barkeep.es.dupont.com  Thu Feb  3 18:05:11 1994 --100
Message-Id: <9402031700.AA00793@barkeep>
Date: Thu, 3 Feb 1994 18:05:11 --100
From: tkevans@barkeep.es.dupont.com (Tim Evans)
Subject: Re: Our new listserver - Please read this

Sez Arthur Secret (for which I'm grateful):
>
>
>We have now switched to ListProcessor, thanks to the suggestions
>of Richard  Wiggins (six months ago ;-), Lou Montulli and Mustafa Akgul.
>
>Any problems with this software or suggestions on the configuration
>should be mailed to me (secret@info.cern.ch)
>
>Your initial password is WTALK. Please change it as soon as you can
>by issuing the following request to listproc@info.cern.ch:
>
>                SET WWW-TALK PASSWORD WTALK new-password
>
>WARNING: Do not use your login password; you will be breaching security at your
>site.
>
>You may change the address you are subscribed with (currently secret@hpwww.cern.
>ch)
>with the following request:
>
>                SET WWW-TALK ADDRESS WTALK new-address
>
>assuming that you keep the same password.
>

It appears that you have included too much information in this message.
Specifically, there is stuff here about passwords which seems to relate
only to the admininstration of the list itself.  As a result, many 
people are going to be confused.

-- 
Tim Evans                     |    E.I. du Pont de Nemours & Co.
tkevans@eplrx7.es.dupont.com  |    Experimental Station
(302) 695-9353/8638 (FAX)     |    P.O. Box 80357
EVANSTK AT A1 AT ESVAX        |    Wilmington, Delaware 19880-0357



From secret@hpwww.cern.ch  Thu Feb  3 18:12:04 1994 --100
Message-Id: <9402031705.AA22483@dxmint.cern.ch>
Date: Thu, 3 Feb 1994 18:12:04 --100
From: secret@hpwww.cern.ch (Arthur Secret)
Subject: Re: Our new listserver - Please read this

Sorry, due to my mistake, the last five messages (all messages sent after
my first message of today, announcing testings on listprocessor)
were only sent to half of the list.

Here was my last one...

> From www-talk@www0.cern.ch Thu Feb  3 17:47 MET 1994
> Received: from dxmint.cern.ch by hpwww.cern.ch with SMTP
> 	(1.37.109.4/16.2) id AA03476; Thu, 3 Feb 94 17:47:07 +0100
> Return-Path: <www-talk@www0.cern.ch>
> Received: from www0.cern.ch by dxmint.cern.ch (5.65/DEC-Ultrix/4.3)
> 	id AA12889; Thu, 3 Feb 1994 17:28:51 +0100
> Received: from  (localhost) by www0.cern.ch (5.0/SMI-4.0)
> 	id AA03091; Thu, 3 Feb 1994 17:27:37 --100
> Date: Thu, 3 Feb 1994 17:27:36 --100
> Message-Id: <9402031627.AA12651@dxmint.cern.ch>
> Errors-To: secret@www0.cern.ch
> Reply-To: www-talk@www0.cern.ch
> Originator: www-talk@info.cern.ch
> Sender: www-talk@www0.cern.ch
> Precedence: bulk
> From: Arthur Secret <secret@hpwww.cern.ch>
> To: Multiple recipients of list <www-talk@www0.cern.ch>
> Subject: Our new listserver - Please read this
> X-Listprocessor-Version: 6.0c -- ListProcessor by Anastasios Kotsikonas
> Content-Length: 5936
> Status: RO
> 
> 
> We have now switched to ListProcessor, thanks to the suggestions
> of Richard  Wiggins (six months ago ;-), Lou Montulli and Mustafa Akgul.
> 
> Any problems with this software or suggestions on the configuration
> should be mailed to me (secret@info.cern.ch)
> 
> Your initial password is WTALK. Please change it as soon as you can
> by issuing the following request to listproc@info.cern.ch:
> 
>                 SET WWW-TALK PASSWORD WTALK new-password
> 
> WARNING: Do not use your login password; you will be breaching security at your
> site.
> 
> You may change the address you are subscribed with (currently secret@hpwww.cern.
> ch)
> with the following request:
> 
>                 SET WWW-TALK ADDRESS WTALK new-address
> 
> assuming that you keep the same password.
> 
> For information on this service and how to use it, send the following
> request in the body of a mail message to listproc@info.cern.ch:
> 
>                         HELP
> 
> Below is an excerpt of the HELP file
> *********************************************************************
> 
>                           ListProcessor 6.0
> 
> Here is a brief description of the set of requests recognized by ListProcessor.
> Everything appearing in [] below is optional; everything
> appearing in <> is mandatory; all arguments are case insensitive. The vertical
> bar ("|") is used as a logical OR operator between the arguments. Requests may
> be abbreviated, but you must specify at least the first three characters.
> 
> Keep in mind that when referring to a <list>, that list may be of two kinds:
> local or remote, unless otherwise noted. When referring to a local list, your
> request will be immediately processed; when referring to a remote list (a list
> served by another ListProcessor which this system knows about), your request
> will be appropriately forwarded. Issue a 'lists' request to get a listing of all
> local and known remote lists to this ListProcessor.
> 
> Recognized requests are:
> 
> 
> help [topic]
> ------------
> Without arguments, this file. Otherwise get specific information on the
> selected topic. Topics may also refer to requests. To learn more about this
> system issue a 'help listproc' request. To get a listing of all available
> topics, generate an error message by sending a bogus request like 'help me'.
> 
> 
> set <list> [<option> <arg[s]>]
> ------------------------------
> Without the optional arguments, get a list of all current settings for
> the specified list. Otherwise change the option to a new value for that
> list. Issue a 'help set' request for more information.
> 
> 
> subscribe <list> <your name>
> ----------------------------
> The only way to subscribe to a list.
> 
> 
> unsubscribe <list>
> signoff <list>
> ------------------
> Two ways of removing yourself from the specified list.
> 
> 
> recipients <list>
> review <list>
> -----------------
> Get a listing of all non-concealed people subscribed to the specified list.
> 
> 
> information <list>
> ------------------
> Get information about the specified list.
> 
> 
> statistics <list> {[subscriber email address(es)] | [-all]}
> -----------------------------------------------------------
> Get a listing of non-concealed subscribers along with the number of messages
> each one of them has sent to the specified list. If the optional email addresses
> are given, then statistics will be collected for these users only. For example:
>                 stat foo user1@domain user2@domain
> will generate statistics about these two subscribers. "-all" lists statistics
> for all users that have posted on the list (whether currently subscribed ot
> not).
> 
> run <list> [<password> <cmd [args]>]
> ------------------------------------
> Run the specified command with the optional arguments and receive the output
> from stdout and/or stderr. To get a listing of all available commands to run,
> omit the arguments, i.e. issue a 'run <list>' request. You have to belong to
> the specified list, and must have obtained the password from the list's owner;
> the owner's address may be found in the Errors-To: header line of each
> delivered message. <list> may be local only.
> 
> 
> lists
> -----
> Get a list of all local mailing lists that are served by this server, as well
> as of all known remote lists.
> 
> 
> index [archive | path-to-archive] [/password] [-all]
> ----------------------------------------------------
> Get a list of files in the selected archive, or the master archive if no
> archive was specified. If an archive is private, you have to provide its
> password as well.
> 
> 
> get <archive | path-to-archive> <file> [/password] [parts]
> ----------------------------------------------------------
> Get the requested file from the specified archive. Files are usually split in
> parts locally, and in such a case you will receive the file in multiple email
> messages -- an 'index' request tells you how many parts the file has been split
> into, and their sizes; if you need to obtain certain parts, specify them as
> optional arguments. If an archive is private, you have to provide its password
> as well.
> 
> view <archive | path-to-archive>] [/password] [parts]
> -----------------------------------------------------
> Same as "get" but in interactive mode justs catenates the file on the
> screen.
> 
> search <archive | path-to-archive>] [/password] [-all] <pattern>
> ----------------------------------------------------------------
> Search all files of the specified archive (and all of its subarchives if -all
> is specified) for lines that match the pattern. The pattern can be an
> egrep(1)-style regular expression with support for the following additional
> operators: '~' (negation), '|' and '&' (logical OR and AND), '<' '>' (group
> regular expressions). The pattern may be enclosed in single or double quotes.
> Note: . matches any character including new line.
> 
> 
> release
> -------
> Get information about the current release of this ListProcessor system.
> 
> 
> which
> -----
> Get a listing of local mailing lists to which you have subscribed.
> 
> 


Sorry about that

Arthur
--
       Arthur Secret,       phone:(41-22) 767-37-55     
Technical Student at CERN,   e-mail: secret@dxcern.cern.ch



From secret@hpwww.cern.ch  Thu Feb  3 18:24:50 1994 --100
Message-Id: <9402031722.AA25630@dxmint.cern.ch>
Date: Thu, 3 Feb 1994 18:24:50 --100
From: secret@hpwww.cern.ch (Arthur Secret)
Subject: Re: Our new listserver - Please read this

:> 
:> It appears that you have included too much information in this message.
:> Specifically, there is stuff here about passwords which seems to relate
:> only to the admininstration of the list itself.  As a result, many 
:> people are going to be confused.
:> 
:> -- 
:> Tim Evans                     |    E.I. du Pont de Nemours & Co.
:> tkevans@eplrx7.es.dupont.com  |    Experimental Station
:> (302) 695-9353/8638 (FAX)     |    P.O. Box 80357
:> EVANSTK AT A1 AT ESVAX        |    Wilmington, Delaware 19880-0357
:> 
Well, I just took the help file sent to any new subscriber.

I understand the password is used, for example, when you want to change 
the address where you get the mail

Arthur
--
       Arthur Secret,       phone:(41-22) 767-37-55     
Technical Student at CERN,   e-mail: secret@dxcern.cern.ch



From MONICA@cepes.ro  Thu Feb  3 18:18:17 1994 --100
Message-Id: <01H8G6P8HQTC0001S3@CEPES.RO>
Date: Thu, 3 Feb 1994 18:18:17 --100
From: MONICA@cepes.ro (Monica Cucoanes)
Subject: Re: CEPES  >>> First EGAA Conference

                        CALL FOR PARTICIPATION


                                13 January 1994


     Sir/Madam,

     Eight months have already passed since CEPES organized in
Oradea (Romania) a high level consultation focussed on policy
issues of quality assurance and institutional accreditation in the
higher education systems of the central and eastern European
countries. The final statement adopted by the participants urged
CEPES to launch, within the framework of the European Group for
Academic Assessment (EGAA), various operational projects addressing
the needs of the central and eastern European systems of higher
education in the domains of quality assurance and accreditation.
The purpose of this letter is to inform you of the stages of our
*****************************************************************
work, to consult you with regard to further proposals to be made
*****************************************************************
and actions to be taken, and to invite you to attend the first EGAA
********************************************************************
Conference.
***********

     1.   The proceedings of the high level consultation were
published in our review Higher Education in Europe (Vol. XVIII, No.
3, 1993), which is sent to you separately. Meanwhile, we have been
constantly gathering information on developments in all the
countries of the Europe Region. This information can be made
accessible to you upon request. We are currently building up a
database on the recognized institutions of higher education which
will soon be accessible via E-mail and in hard copy (print outs).

     2.   We are involved, together with the World Bank, in an
extensive project focussed on the process of reforming the Romanian
higher education system. Particular attention is being paid to the
procedures and mechanisms of quality assurance and accreditation.

In this context, we have contributed to the drafting of a law on
accreditation which has already been adopted by the Romanian
Parliament, and we are now monitoring pilot projects intended to
prepare the ground for its wide implementation. The emphasis is
being placed on ways to develop institutional quality assurance
mechanisms and on the procedures for licensing and accrediting
newly established higher education institutions. CEPES will present
the conclusions derived from this experimental stage in a report.
It will also formulate various suggestions for the implementation
of the provisions of the law on accreditation both at the national
and the institutional levels. The Romanian authorities are
establishing a national body of a buffer type to be in charge of
quality assessment and accreditation. It will be linked to both
institutional activities and ministerial responsibilities.

     3.   We are co-operating with the Romanian universities, the
Ministry of Education, and the national TEMPUS office for the
establishment of both the National Council on Academic Evaluation
and Accreditation and of an appropriate division in the Ministry of
Education to be given responsibility for quality control,
accreditation matters, and diploma recognition. Indeed, the issues
related to the recognition of diplomas are closely linked to those
of academic quality assurance and accreditation.

     We would like to stress that CEPES is co-operating on these
projects with various international agencies, such as the World
Bank, the Council of Europe, and the TEMPUS Office. As CEPES
intends to strengthen its co-operation also in other domains with
these agencies, we wish to include your country in this framework.

     As you can see, we are particularly concerned that national
systems of higher education and the institutions which compose them
develop appropriate operational projects. As it is from this
perspective that we are addressing you this letter, we would very
much like to receive your suggestions and comments for the eventual
development of similar projects in your country.

     Under the auspices of EGAA, we are planning to organize a
conference focussed on the procedures and operational aspects of
quality assurance and accreditation in higher education. We propose
that this conference be held during the first half of May 1994, in
Bucharest, at CEPES. The objectives of the conference are
threefold:

     i)   to take stock of the latest experiences and developments
          in the fields of academic quality assurance and
          accreditation;

     ii)  to make a detailed presentation of the results of the
          pilot projects carried out in the Romanian system of
          higher education;



     iii) to further plan the activities envisaged under the
          auspices of the European Group for Academic Assessment
          (EGAA).

     If these proposals are of interest to you, please inform us
about your participation in the above-mentioned conference as soon
as possible, but not later than the 15th of February 1994, in order
for us to have sufficient time to prepare for the Conference.

     You can write to us via e-mail:    anda@cepes.ro
                         via fax:       40-1-641 50 25 (CEPES-UNESCO)


                                   Sincerely yours,
                                   Carin Berg
                                   Director of CEPES




From lile@netcom.com  Thu Feb  3 18:30:42 1994 --100
Message-Id: <199402031728.JAA09546@mail.netcom.com>
Date: Thu, 3 Feb 1994 18:30:42 --100
From: lile@netcom.com (Lile Elam)
Subject: Re: Our new listserver - Please read this

I was wondering what the passwd stuff was all about... :)

-lile


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Lile Elam	    |  "Remember... No matter where you go, there you are."
lile@netcom.com     |		
Un*x Admin / Artist |			 Buckaroo Banzai
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




From tkevans@barkeep.es.dupont.com  Thu Feb  3 18:35:54 1994 --100
Message-Id: <9402031729.AA00856@barkeep>
Date: Thu, 3 Feb 1994 18:35:54 --100
From: tkevans@barkeep.es.dupont.com (Tim Evans)
Subject: Re: Our new listserver - Please read this

Sez Arthur Secret (for which I'm grateful):
>
>:> 
>:> It appears that you have included too much information in this message.
>:> Specifically, there is stuff here about passwords which seems to relate
>:> only to the admininstration of the list itself.  As a result, many 
>:> people are going to be confused.
>:> 
>:> -- 
>:> Tim Evans                     |    E.I. du Pont de Nemours & Co.
>:> tkevans@eplrx7.es.dupont.com  |    Experimental Station
>:> (302) 695-9353/8638 (FAX)     |    P.O. Box 80357
>:> EVANSTK AT A1 AT ESVAX        |    Wilmington, Delaware 19880-0357
>:> 
>Well, I just took the help file sent to any new subscriber.
>
>I understand the password is used, for example, when you want to change 
>the address where you get the mail
>
You *understand*?  Don't you think that you should *know* what the 
features of this is before you go announcing it to the world, so
you make correct announcements.  I tried the command to change my
password and it was denied.

-- 
Tim Evans                     |    E.I. du Pont de Nemours & Co.
tkevans@eplrx7.es.dupont.com  |    Experimental Station
(302) 695-9353/8638 (FAX)     |    P.O. Box 80357
EVANSTK AT A1 AT ESVAX        |    Wilmington, Delaware 19880-0357



From montulli@stat1.cc.ukans.edu  Thu Feb  3 18:49:47 1994 --100
Message-Id: <9402031746.AA14049@stat1.cc.ukans.edu>
Date: Thu, 3 Feb 1994 18:49:47 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Our new listserver - Please read this

> >
> >I understand the password is used, for example, when you want to change 
> >the address where you get the mail
> >
> You *understand*?  Don't you think that you should *know* what the 
> features of this is before you go announcing it to the world, so
> you make correct announcements.  I tried the command to change my
> password and it was denied.
> 
> -- 
> Tim Evans                     |    E.I. du Pont de Nemours & Co.

Give it a rest Tim.  He was correct in sending out that message.
Listproc allows the use of a password for user options as
well as for interactive use: yes you can telnet and
use listproc interactively.  Do you really know everything about
all the software packages you use, do you even need or want to
know that much about them?

:lou



From heffron@falstaff.css.beckman.com  Thu Feb  3 19:08:31 1994 --100
Message-Id: <9402031805.AA04238@dxmint.cern.ch>
Date: Thu, 3 Feb 1994 19:08:31 --100
From: heffron@falstaff.css.beckman.com (Matt Heffron)
Subject: Re: Our new listserver - Please read this 

> 
> You may change the address you are subscribed with (currently secret@hpwww.cern.ch)
> with the following request:
> 
>                 SET WWW-TALK ADDRESS WTALK new-address
> 
> assuming that you keep the same password.
> 

I'm confused about one point.
I'm still getting email from www-talk@www0.cern.ch.
So do I need to "subscribe" or "SET WWW_TALK ADDRESS ..." ?
--
Matt Heffron                      heffron@falstaff.css.beckman.com
Beckman Instruments, Inc.         voice: (714) 961-3128
2500 N. Harbor Blvd. MS X-10, Fullerton, CA 92634-3100
I don't speak for Beckman Instruments unless they say so.



From secret@hpwww.cern.ch  Thu Feb  3 19:16:08 1994 --100
Message-Id: <9402031813.AA06729@dxmint.cern.ch>
Date: Thu, 3 Feb 1994 19:16:08 --100
From: secret@hpwww.cern.ch (Arthur Secret)
Subject: Re: Our new listserver - Please read this

> 
> > >
> > >I understand the password is used, for example, when you want to change 
> > >the address where you get the mail
> > >
> > You *understand*?  Don't you think that you should *know* what the 
> > features of this is before you go announcing it to the world, so
> > you make correct announcements.  I tried the command to change my
> > password and it was denied.
> > 
> > -- 
> > Tim Evans                     |    E.I. du Pont de Nemours & Co.
> 
> Give it a rest Tim.  He was correct in sending out that message.
> Listproc allows the use of a password for user options as
> well as for interactive use: yes you can telnet and
> use listproc interactively.  Do you really know everything about
> all the software packages you use, do you even need or want to
> know that much about them?
> 
> :lou
> 
Thanks Lou ;-)

Tim, this must be because you're sending your mail from a different email
than the one you're refered to in the list, which is: 
tkevans@eplrx7.es.dupont.com

So the list can't identify you. 
I can change your email in the list by hand if you want


Such problems may occur for a lot of people. Please mail me
so that I change your entry.

I regret the annoyance,

Arthur
--
       Arthur Secret,       phone:(41-22) 767-37-55     
Technical Student at CERN,   e-mail: secret@dxcern.cern.ch



From MONICA@cepes.ro  Thu Feb  3 19:21:56 1994 --100
Message-Id: <01H8G9R6R64G0001SR@CEPES.RO>
Date: Thu, 3 Feb 1994 19:21:56 --100
From: MONICA@cepes.ro (Monica Cucoanes)
Subject: Re: CEPES UNESCO questionnaire


                                                        CEPES UNESCO
                                                        Feb. 3, 1994


                        QUESTIONNAIRE
                        =============

                            on

                Institutional Evaluation and Accreditation System(s)
                in Higher Education
                ===============================================

        Following the recent approval of the Low on Accreditation on Academic
Evaluation and Accreditation in Romania, CEPES-UNESCO has launched, under the
auspices of the World Bank, and with the agreement of the Romanian Ministry of
Education, two pilot-projects in the field.
        The main objectives are to assess and test various evaluation
mechanisms on different types of higher education institutions, as well as to
improve the methodology of the evaluation procedure(a problem all countries in
Central and Eastern Europe are facing), including the methodology for granting
a national diploma.

        We would very much appreciate if you could assist us in the
accomplishment of this extremely difficult task by accepting an exchange of
information on the following questions:

        1.  Have you implemented an/more evaluation system(s) of higher
            education in your country?
            .........................................................

        2.  If Yes, what kind of system is it: governmental/nongovernmental?
            .........................................................
            .........................................................

        3.  Does the evaluation/accreditation of an institution take place at
            the latter's request? Who supports its cost?
            .........................................................
            .........................................................

        4.  Are performance indicators used in this procedure? Which factors
            are involved in the elaboration of these indicators?
            .........................................................
            .........................................................

        5.  In the evaluation process, do you distinguish between quality
            assurance and quality control? Who is in charge of accomplishing
            each of them?
            .........................................................
            .........................................................

        6.  Does your Evaluation Board/Comittee have any permanent members?
            If yes, how many? Which is their term of office?
            .........................................................
            .........................................................

        7.  Is it compulsory to resort to foreign evaluators in this process?
            .........................................................
            .........................................................
            .........................................................

        8.  To whom is the evaluation report submitted by the Board/Comittee?
            .........................................................
            .........................................................

        9.  Is the report made public?
            .........................................................
            .........................................................

        10. To what extent could the minister of education make use of the
            results of this report in the implementation of appropiate
            policies and strategies of action, particularly in the financing
            of higher education?
            .........................................................
            .........................................................
            .........................................................


------------------------------------------------------------------------------


                                                        CEPES UNESCO
                                                        Jan. 12 1993

        Suite ` la ricente adoptation de la Loi sur l'Evaluation de
l'Habilitation acadimique en Roumanie, le CEPES-UNESCO, sous les auspices de la
Banque Mondiale et avec l'accord du Ministhre de l'Enseignement , a lanci deux
projects-pilot dans le domain concerni.

        Le but principal en est de tester les micamismes d'ivaluation sur les
diffirents types d'itablissement d'enseignement supirieur, d'amiliorer la
mithodologie d'un tel processus (problhme auquel se confronte d'ailleurs tous
les pays de l'Europe centrale et Orientale, de mjme que la mithodologie d'un
examen national de licence).

        Pour nous aider dans cette tbche extremement difficile, nous vous
saurions de bien vouloir  recepter un echange d'informations sur les questions
suivantes:



                        QUESTIONNAIRE
                        =============

                            sur

                Les systhmes d'ivaluation et d'habilitation
                institutionnelle dans l'enseignement supirieur
                ===============================================

        1.  Avez-vous mis en place dans votre pays un, des systhmes
            d'ivaluation de l'enseignement supirieur?
            .........................................................

        2.  Si oui, de quel type ? Gouvernemental ? Non-gouvernemental?
            .........................................................

        3.  L'ivaluation l'habilitation d'un itablissement se  fait-ell sur la
            demande de l'institution respective? Qui en supporte le co{t?
            ...............................................................
            ...............................................................
            ...............................................................
            ...............................................................
            ...............................................................

        4.  Utilisez-vous des indicateurs de performance dans ce processus?
            Quels sont les facteurs impliques dans l'ilaboration de ceux-ci?
            ...............................................................
            ...............................................................

        5.  L'ivaluation permet-elle de distinquer entre l'assurance de la
            qualiti et le contrtle de celle-ci?.............................
            ................................................................
            ................................................................
            Qui est responsable de l'une et de l'autre?.....................
            ................................................................

        6.  Le Comiti d'Evaluation a-t-il des membres permanents? Combien? Pour
            quelle durie?....................................................
            .................................................................
            .................................................................

        7.  Y a-t-il des spicialistes itrangers obligativement impliquis dans ce
            processus?......................................................
            ................................................................
            ................................................................
        8.  A qui s'adresse le rapport d'ivaluation du comiti?..............
            ................................................................
            ................................................................
            ................................................................
        9.  Est-il rendu public?............................................

        10. Dans quelle messure le ministre de l'enseignement peut s'en servir
            pour l'itablissement des politiques d'action pour une stratigie qui
            puisse utiliser d'une manihre adiquate les risultats de celui-ci
            surtout pour le financement? .....................................
            ..................................................................
            ..................................................................
            ..................................................................
            ..................................................................
            ..................................................................







From jay@eit.COM  Thu Feb  3 19:29:58 1994 --100
Message-Id: <199402031836.SAA15518@hnear>
Date: Thu, 3 Feb 1994 19:29:58 --100
From: jay@eit.COM (Jay Glicksman)
Subject: Re: MIME Transfer Format (was Re: Protocol Benchmarking...) 

We (at EIT) have experimented with several types of MIME extensions
for multipart messages (using Content-type and Content-id) to control
placement of document components (both 1d and 2d). A strawman
"standard" would be a very useful starting point--but it needs to be
extendible.  A list of URLs is too limited for some of our needs.

	Jay Glicksman




From hoesel@chem.rug.nl  Thu Feb  3 21:09:45 1994 --100
Message-Id: <9402032006.AA01385@Xtreme>
Date: Thu, 3 Feb 1994 21:09:45 --100
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Re: Protocol Benchmarking (with Accept examples - long)

> > 
> True, but 99% of the time the client will simply accept the first
> type given by the server.  The negotiation will only take place
> less than 1% of the time, and in the case of negotiation the server
> will most likely have to make some on the fly conversion that will
> probably be slower than the latency.

but the client has to tell the server it accepts that type isn't it
(at least that is what learned from your text)
now the cpu time is irrelevant when it comes to saying yes I do accept
that type, give me the file... but I can tell from my modem ligths that any
roundtrip is very bad and surely much more then 1%. Often its true that once
the request is answered and the data is coming the connection is pretty fast
(compared to my 19k2 connection) but in the initial phase the modem is just waiting,

- frans
> 
> :lou
> -- 
>   **************************************************************************
>   *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
>   *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
>   *                         Kuhub.cc.ukans.edu      ACS Computing Services *
>   *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
>   *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
>   **************************************************************************
> 






From drummond@aristarchus.rutgers.edu  Thu Feb  3 21:15:23 1994 --100
Message-Id: <CMM-RU.1.3.760305952.drummond@aristarchus.rutgers.edu>
Date: Thu, 3 Feb 1994 21:15:23 --100
From: drummond@aristarchus.rutgers.edu (Walt Drummond)
Subject: Re: Patches to NCSA httpd 1.1

> OK - I'll send you the patches Walt and I hope to find the functionality
> in the next official patch from Rob - I don't like unofficial patches
> ...
> I'll put the patches and some CGI's around that at:
> http://www.tu-chemnitz.de/~fischer/fancyindex/
> 
> Hope it's there at 12 MET.
> 
> 	~Guenther

OK, 
	I got Guenther's patch, and integrated it into my changes. The
syntax of AddHref is:

AddHref url <list of extensions>

I made two change to Guenther's stuff: if the AddHref clause includes
a ? in the url, the path to the file is added; and the complete,
unaliased pathname is used, not the path relative to DocumentRoot or
the alias.

It's running on http://aristarchus.rutgers.edu/www-icons.  Let me know
what you think.  Note that I haven't changed the way Directory Index
options are parsed.  Also, I haven't made AddHref available from
access.conf and .htaccess.  Do you guys think we need to do that?

I have one question: When LinkIcons is on, how do I make the
AddHref'ed icons stand out??  Currently, the only difference is a
rather cheezy icon I drew... :)

				Walt

_________________________________________________________
Walt Drummond (drummond@noc.rutgers.edu)
Network Services
Rutgers University Computing Services
 - Lost: One mind. Owner sad. Reward.



From vinay@eit.COM  Thu Feb  3 21:33:04 1994 --100
Message-Id: <9402032029.AA04492@eit.COM>
Date: Thu, 3 Feb 1994 21:33:04 --100
From: vinay@eit.COM (Vinay Kumar)
Subject: Re: MIME Transfer Format (was Re: Protocol Benchmarking...)

Few months ago, I experimented with this idea by writing a WYSIWYG MIME 
multipart player client called "StoryBoard".
	 (see http://www.eit.com/software/storyboard/sb.html).

This client program lets users generate/view MIME multipart/parallel msg's.
The Multipart consists of:

--foo
Content-type: image/x-xwd
Content-Description: original MIME document

This is where the original document consisting of images ("bitmaps") and 
text goes.

--foo
Content-Type: image/x-xwd
Content-Description: transparent annotation overlay

This is where all the personal annotations for above document go, it's
almost like laying a celluloid transparency on a document and making
annotations on it.

--foo
Content-Type: audio/basic

This is where recorded audio annotations go.

--foo
Content-Type: application/x-gesture

This is a cute animation feature that lets users use mouse to point to 
different parts of above mentioned documents, and then play back the mouse
gestures synchronously with recorded voice. ("sort of like a multimedia slide
presentation").

--foo--

This client has been integrated as an external viewer into Mosaic using
CGI and ".mailcap". For those of you who would like to take a peek at some
of the StoryBoard generated msg's using metamail and your Sun-Mosaic client, 
add the following line to your ".mailcap":

application/x-sb; metamail %s

and access http://www.eit.com/cgi-bin/sbform/software/storyboard

Again, this is just a small proof of concept. Incorporation of MIME multipart into 
W3 servers/clients will require deeper investigation into special Content-Type's for 
spatial (1d/2d/3d layout) and time-based layouts for differnt MIME multipart objects.

In StoryBoard, the two Content-type's for handling the original MIME document, and the
transparent annotation layer go on top of each other inside the client. That is, the 
spatial layout (x,y) for both MIME Content-Type's inside the viewer is (0, 0), by 
default ofcourse. 

A Content-Type like  "application/x-gesture" that describes time-based relationships
(say, for playing animations) between the different MIME Content-type's would be 
desirable as well.

So much for now on this www-stratosphere-talk...Rest later !
--
  Vinay Kumar
vinay@eit.com




From sg04%kesser@gte.com  Thu Feb  3 22:26:52 1994 --100
Message-Id: <9402032122.AA03256@kesser.cisl214>
Date: Thu, 3 Feb 1994 22:26:52 --100
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: Re: Protocol Benchmarking (shortened)

There is another way to tackle this sort of problem.

CDPD (the Radio Data packet protocol that runs over unused cellular
phone lines and uses IP packets - which the telcos are rolling out
this year) uses something called IP address compression.

Basically you give a token for an IP address. The token is good
for as long as you are in a cell.

Applied to our case, one would have a token to indicate all the
types acceptable to your WinMosaic v1.02.

This scheme works when there are only a few different clients,
when compared to all possible acceptable MIME types.

It's just my 2-cent input in this conversation.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Yechezkal-Shimon Gutfreund		 	   sgutfreund@gte.com [MIME]
GTE Laboratories, Waltham MA        http://www.gte.com/circus/home/home.html
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=



From sg04%kesser@gte.com  Thu Feb  3 22:32:08 1994 --100
Message-Id: <9402032125.AA03259@kesser.cisl214>
Date: Thu, 3 Feb 1994 22:32:08 --100
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: Re: request for new forms submission consensus

Well, this is not a feature for the next release:

But as forms get longer, we are going to all get tired of
filling them out. Especially, when I have databases on
my machine that could automatically fill out some of the
fields (name, address, phone, etc.)

I have been proposing an accessory attachment that could
be used to automatically fill out forms. But we would
have to rethink the forms request process - to
make this more reasonable.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Yechezkal-Shimon Gutfreund		 	   sgutfreund@gte.com [MIME]
GTE Laboratories, Waltham MA        http://www.gte.com/circus/home/home.html
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=



From MONICA@cepes.ro  Thu Feb  3 23:52:02 1994 --100
Message-Id: <01H8G86J31PC0001SR@CEPES.RO>
Date: Thu, 3 Feb 1994 23:52:02 --100
From: MONICA@cepes.ro (Monica Cucoanes)
Subject: Re: CEPES UNESCO DRAFT QUESTIONNAIRE ON WOMEN


                                                February 3, 1994

                        CEPES UNESCO DRAFT QUESTIONNAIRE

                        LEGISLATION CONCERNING WOMEN

CEPES plans to set up a data base for women's activities in Europe (EWA).
The first step will be a feasibility study. For the moment we will concentrate
on collecting information concerning legislation issues relevant for women
(relevant changes, daily practises).
When answering to our questionnaire please don't forget *it is a draft*, so any
comments for improving it are welcomed.
Please also keep us inform about individual experts/national organizations
dealing with this matter.

Thanks for help,

Laura Grunberg (Laura@cepes.ro) or Monica Cucoanes (monica@cepes.ro)



I.   General Information

Country ............      City..........
=========================================================
Please indicate persons/institutions (other than you/
your institution) dealing with women and legislation

     1.1  Name of Institution ..................................

     Type:  __   (NA National, ST State, PV Private, NG Non governmental)


     Address:
     P.O.Box.................Street.............................
     City....................State/Province.....................
     Postal Code............ Tel...........Telex................
     Fax.....................Cable........E-mail................

     1.2 Name of contact person
           Name...................First Name....................
           Title/Occupation:....................................
           Department/Sector Organization Name..................

           Address:(If other than the address of the Parent inst.
                                                  listed before)
           P.O.Box............ Street...........................
           City............... State/Province...................
           Postal Code........ Tel ........... Telex ...........
           Fax.................Cable.......... E-mail ..........

     If more, please add them.

II. Main laws/decrees focussed on Women Legal Issues

2.1  CONSTITUTION Law     (CON)

     2.1.1  General Data

     Name of Law ............................
     Year of last update ....

     Are there any translations available ?                       __ (Y/N)
         if yes please indicate language                         .........
         if possible send a copy
     How do you appreciate the content of the legislation in force
     focussed on women problems__
     (very good VG, good GO, Satisfactory SA,
      non--satisfactory NS)
     How do you appreciate the application in practice of
     the Law/decree__
     (very good VG, good GO, Satisfactory SA,
     non--satisfactory NS)

     2.1.2  Specific Data

     Guarantee of nondiscrimination on the basis of sex           __ (Y/N)
     If Yes  Decree No  ...... / Year : ....
     If No   Modifications foreseen for the future?               __ (Y/N)

     Equal opportunities for women                                __ (Y/N)
     If Yes  Decree No ...... / Year : ....
     If No   Modifications foreseen for the future?               __ (Y/N)

     Right to vote                                                __ (Y/N)
     If Yes  Decree No ....... / Year : ...
     If No   Modifications foreseen for the future?               __ (Y/N)

     Provisions concerning citizenship(relevant for women)
                                                                  __ (Y/N)
     If Yes  Decree No  ...... / Year : ...
     If No   Modifications foreseen for the future?               __ (Y/N)

     Comments: ................................................
     ..........................................................
     ..........................................................

2.2  FAMILY Law     (FAM)

     2.2.1  General Data

     Name of Law ...........................

     Year of last update .....

     Are there any translations available ?                       __ (Y/N)
         if yes please indicate language                         .........
         if possible send a copy
     How do you appreciate the content of the legislation in force
     focussed on women problems? __
     (very good VG, good GO, Satisfactory SA,
     non--satisfactory NS)
     How do you appreciate the application in practice of
     the Law/decree__
     (very good VG, good GO, Satisfactory SA,
     non--satisfactory NS)



     2.2.2  Specific Data

     2.2.2.1   MARRIAGE

               / Age (minimum)  for women ...  for men ...
                 Decree No ...... / Year : ....
                 Modifications foreseen for the future?           __ (Y/N)

               / Parents or others's approval .................
                 ..............................................
                 Decree No ...... / Year : ....
                 Modifications foreseen for the future?           __ (Y/N)

               / Obligation of changing the name ..............
                 ..............................................
                                Decree No ...... / Year : ....
                 Modifications foreseen for the future?           __ (Y/N)

                          / Property issues linked with the marriage
                          ...............................................
                                Decree No ...... / Year : ....
                                Modifications foreseen for the future-- (Y/N)

                          / Parental authority.............................
                          ...............................................
                                Decree No ...... / Year : ....
                                Modifications foreseen for the future-- (Y/N)

               Others ..........................................
                      ..........................................
                      ..........................................

                    / Do you appreciate that the marriage provisions in force
                 protect the woman                                __(Y/N)

                          Comments  .......................................
                                    .......................................

    2.2.2.2    DIVORCE


               Legal conditions for divorce:

               / Main reasons accepted :
                 * condition 1 .................................
                   .............................................
                    Decree No ...... / Year : ....
                 * condition 2 .................................
                    ............................................
                    Decree No ...... / Year : ....

               / Limits of time:  Decree No ..... / Year:....
                 * Single couple (Time Min/Max months )
                 * Couple with children (Time Min/Max months)

               / Modifications foreseen for the future            __ (Y/N)

                / Taxes .........................................
                       Decree No ...... / Year : ....
                       Modifications foreseen for the future--(Y/N)
                       Legal conditions after divorce

               / Parental rights concerning child entrust
                 ...............................................
                 ...............................................
                    Decree No ...... / Year : ....

               / Alimony for wife/for children .................
                 ...............................................
                 ...............................................
                    Decree No ...... / Year : ....

               / Wife name .....................................
                 ...............................................
                    Decree No ...... / Year : ....

                         / Property issues ...............................
                           ...............................................
                                   Decree No ...... / Year : ....

               / Others ........................................
                 . .............................................

               / Modifications foreseen for the future            __ (Y/N)
                      If yes, briefly mention them .................
             Do you appreciate that the divorce provisions in force
             protect the woman (Y/N) ..... the mother (Y/N).......

              Comments .........................................
                       .........................................
                       .........................................

     2.2.2.3    CONCUBINAGE
                / Regulated by law                                __ (Y/N)
                        Decree No ...... / Year : ....
                / Illegal                                         __ (Y/N)
                        Decree No ...... / Year : ....
                                  Sanctions : ...............................
                                      ...............................
                                      ...............................
                / Ignored by law                                 __ (Y/N)

                / Modifications foreseen for the future           __ (Y/N)
                               Comments .....................................
                                        .....................................


     2.3    Labor Law    LAB

     2.3.1  General Data

     Name of Law........................... Code of the law  XXX

     Date of last update  (Year) ....

     Are there any translations available ?                       __ (Y/N)
         if yes please indicate language                         .........
               if possible send a copy
     How do you appreciate the content of the legislation in force
     focussed on women problems?__
     (very good VG, good GO, Satisfactory SA,
     non--satisfactory NS)
     How do you appreciate the application in practice of
     the Law/decree                                               __
     (very good VG, good GO, Satisfactory SA,
     non--satisfactory NS)

     2.3.2  Specific Data

           / Employment Conditions (specific for women)

                Is there any special legislation concerning mothers'
                maternal leave ?                                  __ (Y/N)
                If yes:
                - total possible length while keeping employment (with or
                without salary) ...........................................
                     Decree No ...... / Year : ....
                - legal possibility to take care of sick children ...........
                     Decree No ...... / Year : ....
                - others ............................................
                         ............................................
                Is there any legislation concerning fathers ?     __ (Y/N)
                If yes briefly mention the main points ..............
                ...........................................................
                Modifications foreseen for the future             __ (Y/N)
                Are there any  positive discriminatory conditions for women in
                this area ?                                    __ (Y/N)
                - if yes briefly mention the regulations that could be included
                  in this area
                              / for mothers ...................................
                                            ...................................
                         Decree ...... / Year : ....
                              / for pregnant women ............................
                                            ...................................
                                                  Decree ...... / Year : ....

                Modifications foreseen for the future             __ (Y/N)
                Comments ............................................
                         ............................................

           Are there any negative discriminatory conditions for women in
                general?                                           __(Y/N)
              If Yes briefly mention them:
                   -  differences in minimum age for employment   __ (Y/N)
                                   if yes minimum age for women ....  men ...
                                       Decree No ..... /Year.....
            Any modifications foreseen for the future             __ (Y/N)

              - differences in salary                            __ (Y/N)
                                   Decree No ...... / Year : ....
            Any modifications foreseen for the future             __ (Y/N)
              -  Others .....................................
                 Comments ...........................................
                       ...........................................


                  Are there any negative discriminatory conditions
             for pregnant women?                                  __ (Y/N)
                   If Yes: Briefly mention them .....................
                      ..........................................
                Decree No ...... / Year : ....
                   Any modifications foreseen for the future      __ (Y/N)
              Comments: ........................................
              Are there any negative discriminatory conditions
              for mothers ?                                       __ (Y/N)
                   If Yes: Briefly mention them (for example the interdiction
                   of night jobs, extra jobs) ..........................
                                              ..........................
                                Decree No ...... / Year : ....
                   Any modifications foreseen for the future      __ (Y/N)
                  Comments: ....................................

                   Are there any written documents,statistics proving a
                 discrimination in practice                       __ (Y/N)
                   - if yes are there any translation available   __ (Y/N)
                   - if yes please send a copy

            / Unemployment conditions

              Are there any specific laws to protect:
              * Women                                             __ (Y/N)
                               If Yes mention them: ...........................
                                                  ...........................
                       Decree No ...... / Year : ....

                   * Pregnant women                               __ (Y/N)
                               If yes mention them: .........................
                                                  ...........................
                                                  ...........................
                   Decree No ...... / Year : ....

                   * Mothers                                      __ (Y/N)
                               If Yes mention them : ..........................
                                                   ..........................
                        Decree No ...... / Year : ....

                 Any modifications foreseen for the future        __ (Y/N)
                 Comments ......................................
                               ......................................

                            / Retirement

               Are there any discriminatory
               provisions regarding women?                        __ (Y/N)
               - if Yes briefly mention them and specify if you consider it
                 as a negative (ND) or a positive discrimination (PD)

                 Age              Decree No ...... / Year : ....
                 ND/PD .........................................
                       .........................................

                 Pension limits   Decree No ...... / Year : ....
                                ND/PD .........................................
                       .........................................

               Others ...........................................
               Any modifications foreseen for the future          __ (Y/N)
               Comments ........................................
                        ........................................


     2.4    CRIMINAL Law                     (CRI)

     2.4.1  General Data

     Name of Law ........................... Code of the law  XXX
     Year of last update ....

     Are there any translations available ?                       __ (Y/N)
         if yes please indicate language                         .........
         if possible please send a copy
     How do you appreciate the content of the legislation in force
     focussed on women problems?                                  __
     (very good VG, good GO, Satisfactory SA, non--satisfactory NS)

     How do you appreciate the application in practice of
     the Law/decree                                               __
     (very good VG, good GO, Satisfactory SA, non--satisfactory NS)

     2.4.2  Specific Data

            / Pornography

            Is it a legislation concerning pornography             __(Y/N)
            If Yes :
               * Decree No ...... / Year : .....
               * What is legally defined as pornography ........
                      ...............................................
               * What are the dominant social connotations .....
                 ...............................................
               * Has the attitude changed since 1990?             __ (Y/N)
            Comments (e.g relation pornography-right to expression)
            ....................................................
                 ....................................................
               * What are the aspects of pornography that are
                                  forbitten? .................................
                                             .................................
                 * Sanctions for each ..........................
                                                ..............................
                       conf. Decree No ...... / Year : ....
                    Modifications foreseen for the future          __(Y/N)

            / Prostitution
            Is it legally defined                                  __(Y/N)
            If Yes :

               * Decree No ...... / Year : ....
               * What is legally defined as prostitution .......
                 ...............................................
               * What are the dominant social connotations ......
                 ................................................
               * Has the attitude changed since 1990?             __ (Y/N)
            Comments  ..........................................
            ....................................................
            ....................................................
               * Specific items:

                 y Brothels:           Legal                   __ (Y/N)
                   If Yes :
                   legal conditions    <.....>, <.....>, etc.
                                       Decree No ...... / Year : ....
                Comments .......................................
                         .......................................
                legal punishments <.........>, <.........>, etc.
                                       Decree No ...... / Year : ....
                Comments .......................................
                         .......................................

                 y Traffic of women     Legal                     __ (Y/N)
                   If Yes :
                   legal conditions     <.....>, <......>, etc.
                                        Decree No ...... / Year : ....
                 Comments ......................................
                          ......................................
                 legal punishments <........>, <.........>, etc.
                                        Decree No ...... / Year : ....
                  Comments .....................................
                           .....................................

                  y Others .....................................

               * Modifications foreseen for the future            __ (Y/N)
                 If yes briefly describe them ..................
                                Comments ......................................
                                ......................................


          / Abortion
            Legal                                                 __ (Y/N)
                 since when (year) ... under Conditions: <......>, <......>,
           etc
                              Decree No ...... / Year : ....
                  If illegal mention the punishments: <......>, <......>, etc
               Decree No ...... / Year : ....
             Comments ..........................................
                             ..........................................

             Family planning : legal framework                     __ (Y/N
                  Modifications foreseen for the future           __ (Y/N)
                              If yes, briefly describe them : .................
                              Is it available for minors           __(Y/N)

            / Rape and other violence against women

              * Do you consider that the legislation tolerates too much
                violence in general ?                             __ (Y/N)
                         against women?                           __ (Y/N)

              * Do you appreciate that the code of procedures or the
                criminal code treats rape in a discriminatory
                manner for women?                                 __ (Y/N)
                If Yes comments ................................
               .................................................
                 Legal sanctions for:
                 - Rape ........................................
                     Decree No ...... / Year: ....
                 - Physical aggressions (e.g. wife battering)
                                .............................................
                                .............................................
                      Decree No ...... / YEar : ....
                   - Sexual harassment ...........................
                                  .............................................
                      Decree No ...... / Year : ....
                 - Incest .....................................
                      Decree No ...... / Year : ....
                 -  Others .....................................

                                 Modifications foreseen for the future?__(Y/N)
                                 If Yes, briefly mention them .................
                                         ......................................
                                 Comments ....................................
                                       ....................................

            / Lesbianism

              Is it legal   __ (Y/N) since when .... under Conditions:
              <........>, <........>, Etc
                      Decree No ...... / Year : ...
              If Illegal mention punishments: <.......>, <.......>, etc
                 Decree No ...... / Year : ...
                 Comments ......................................
                 ......................................
                 Are any modifications foreseen for the future__ (Y/N)
                              If yes, briefly mention them ....................

            / Women in Jail
              Are there any special legal provisions for women?
                                                                  __ (Y/N)
              if Yes mention them: <........>, <.........>, Etc
                  Decree No ...... / Year : ....
              Are there any modifications foreseen for the future?
                                                                  __ (Y/N)
                   If yes briefly mention them ......................

     Any other comments  .......................................
     ...........................................................
     ...........................................................
     ...........................................................
     ...........................................................


III.  Form completed by:

Name...................First Name.....................

Title/Occupation:.....................................
Organization Name.....................................
Address:
P.O.Box.................Street........................
City....................State/Province................
Postal Code............ Tel...........Telex...........
Fax.....................Cable........E-mail...........

Date..................  XX XX XXXX (Day, Month & Year)







From forman@cs.washington.edu  Fri Feb  4 01:37:48 1994 --100
Message-Id: <199402040035.QAA22673@june.cs.washington.edu>
Date: Fri, 4 Feb 1994 01:37:48 --100
From: forman@cs.washington.edu (George Forman)
Subject: Comments on HTML Internet Draft

Tim,
	I've just been reading your HTML Internet Draft.  Thanks for all
your good work on HTML.  I have 4 comments below.

-George Forman
graduate student, Computer Science, Univ. of Washington, Seattle

======================================================================


1. I propose a new Anchor attribute SIZE that says approximately how large
the remote object is: SIZE=8KB, SIZE=5MB, SIZE=1GB.  This information is
useful feedback for readers: if an object is large, people want to know
*before* they fetch it.

This attribute is similar in flavor to the attributes TITLE and METHODS.
	1a. they can become inconsistent with the remote object, but that
	   doesn't mean they shouldn't be supported.  The size of most
	   objects is relatively stable, and also, objects typically grow
	   instead of shrink, so the SIZE gives an estimate of the lower
	   bound.
	1b. yes, this information *can* be obtained from the remote 
	   HTTP server, but no browser is going to bother to do that
	   because there is a lot of overhead involved.    Much better to
	   have it specified in the document.


This attribute should also be supported for LINKs and IMGs.  
Users could then tell their browsers to fetch all IMGs with SIZE < 10KB,
but don't fetch the larger ones, for example.


Perhaps it should be called DSIZE or WEIGHT so as not to be confused with
layout "SIZE" attributes such as used by HTML+ INPUT fields.





2. Link Relationship Value:   Precedes

For consistency, I feel the word PRECEDES should be replaced with
FOLLOWS, as in "B FOLLOWS".  As it is now, it reads as though it had the
opposite semantics: "B Precedes". (This change is consistent with the
rest of the relationship values; they apply to the thing being pointed
at, as in "B is a REPLY", "B is a SUBDOCUMENT".)

Also, many people misspell PRECEDE as PRECEED.

Note: I'm not changing the semantics, just the syntax.  You've got
the right semantics; I think REL=Follows will be more commonly used 
that REV=Follows.  






3. REL and REV look so similar, you can bet there will be lots 
of mistakes made, and it will be difficult to notice the bug.  
What about changing REV to REVERSE?





4. Link Relationship Value: Supersedes

Similar to #2 above, I think the word SUPERSEDES should be replaced with
OBSOLETE or SUPERSEDED, i.e. "B is OBSOLETE" or "B is SUPERSEDED".  As it
is now, it reads as though it had the opposite semantics: "B Supersedes".






From creilly@maths.tcd.ie  Fri Feb  4 08:14:58 1994 --100
Message-Id: <9402040712.aa03074@salmon.maths.tcd.ie>
Date: Fri, 4 Feb 1994 08:14:58 --100
From: creilly@maths.tcd.ie (Colman Reilly)
Subject: Re: CEPES UNESCO DRAFT QUESTIONNAIRE ON WOMEN 



     
                                                     February 3, 1994
     
                             CEPES UNESCO DRAFT QUESTIONNAIRE
     
                             LEGISLATION CONCERNING WOMEN
     
     CEPES plans to set up a data base for women's activities in Europe (EWA).
     The first step will be a feasibility study. For the moment we will concent

Why is this relevant to www-talk?

Colman



From guenther.fischer@hrz.tu-chemnitz.de  Fri Feb  4 08:29:01 1994 --100
Message-Id: <9402040724.AA03331@flash1.hrz.tu-chemnitz.de>
Date: Fri, 4 Feb 1994 08:29:01 --100
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: Re: Patches to NCSA httpd 1.1

> 	I got Guenther's patch, and integrated it into my changes. The
> syntax of AddHref is:
> 
> AddHref url <list of extensions>
> 
> I made two change to Guenther's stuff: if the AddHref clause includes
> a ? in the url, the path to the file is added; and the complete,
> unaliased pathname is used, not the path relative to DocumentRoot or
> the alias.
How exact? As /cgi-bin/unarch? its fine. ->
  /cgi-bin/unarch?<real filename>
Do you see other things too ?

> 
> It's running on http://aristarchus.rutgers.edu/www-icons.  Let me know
> what you think.  Note that I haven't changed the way Directory Index
I was looking there, but I don't understand:
How is the AddHref for .Z (is it AddHref / .Z) 

> options are parsed.  Also, I haven't made AddHref available from
> access.conf and .htaccess.  Do you guys think we need to do that?
I don't know, but it would be fine if you have parts of the archives
where you want to interpret files with the same suffix in a different
manner. I don't see it at the moment but, if isn't a great think it
gives you more flexibility.
> 
> I have one question: When LinkIcons is on, how do I make the
> AddHref'ed icons stand out??  Currently, the only difference is a
> rather cheezy icon I drew... :)

Perhaps I don't understand right LinkIcons - I see only the same
reference on the icon as on the filename. If it is so I could miss
this feature. I would reserve the Icon reference for a different
reference as in AddHref.

	~Guenther
-- 
Name:      Guenther Fischer / Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361     / mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From secret@hpwww.cern.ch  Fri Feb  4 10:56:03 1994 --100
Message-Id: <9402040952.AA20248@dxmint.cern.ch>
Date: Fri, 4 Feb 1994 10:56:03 --100
From: secret@hpwww.cern.ch (Arthur Secret)
Subject: Re: CEPES UNESCO DRAFT QUESTIONNAIRE ON WOMEN 

:>      
:>      CEPES plans to set up a data base for women's activities in Europe (EWA).
:>      The first step will be a feasibility study. For the moment we will concent
:> 
:> Why is this relevant to www-talk?
:> 
:> Colman
:> 

I have the deepest pleasure to introduce one feature of listproc:

Laura@cepes.ro
monica@cepes.ro

are both our first entries in the .ignored file

Arthur
--
       Arthur Secret,       phone:(41-22) 767-37-55     
Technical Student at CERN,   e-mail: secret@dxcern.cern.ch



From neuss@igd.fhg.de  Fri Feb  4 11:29:48 1994 --100
Message-Id: <9402041003.AA07770@wildturkey.igd.fhg.de>
Date: Fri, 4 Feb 1994 11:29:48 --100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: CEPES Bull

Arthur Secret (secret@dxcern.cern.ch) wrote:

> I have the deepest pleasure to introduce one feature of listproc:
> Laura@cepes.ro
> monica@cepes.ro 

> are both our first entries in the .ignored file

Congrats, Arthur.. :-) :-) :-)

Methinks this feature more then makes up for the little trouble
some of us had with trying to set a password, doesn't it ;-)

Keep up the good work!

Chris



From jgrass@CNRI.Reston.VA.US  Fri Feb  4 16:33:36 1994 --100
Message-Id: <9402040951.aa08037@CNRI.Reston.VA.US>
Date: Fri, 4 Feb 1994 16:33:36 --100
From: jgrass@CNRI.Reston.VA.US (Judith E. Grass)
Subject: Re: CEPES UNESCO DRAFT QUESTIONNAIRE ON WOMEN 


>>	     
>>                              February 3, 1994
>>                    CEPES UNESCO DRAFT QUESTIONNAIRE
>>	     
>>                      LEGISLATION CONCERNING WOMEN
>>	     
>>   CEPES plans to set up a data base for women's activities in Europe (EWA).
>>   The first step will be a feasibility study. For the moment we will concent

You know, I wouldn't mind seeing ONE of these, but I am getting about
30 messages a day (more?) related to this CEPES stuff.  Couldn't someone
get Monica Cucoanes her own mailing list?

		-- Judy Grass



From drummond@aristarchus.rutgers.edu  Fri Feb  4 17:22:48 1994 --100
Message-Id: <CMM-RU.1.3.760378662.drummond@aristarchus.rutgers.edu>
Date: Fri, 4 Feb 1994 17:22:48 --100
From: drummond@aristarchus.rutgers.edu (Walt Drummond)
Subject: Re: Patches to NCSA httpd 1.1

> > I made two change to Guenther's stuff: if the AddHref clause includes
> > a ? in the url, the path to the file is added; and the complete,
> > unaliased pathname is used, not the path relative to DocumentRoot or
> > the alias.
>
> How exact? As /cgi-bin/unarch? its fine. ->
>   /cgi-bin/unarch?<real filename>
> Do you see other things too ?
>

The AddHref line I'm currently using to test is:

AddHref http://www-ns.rutgers.edu/ .tar.Z .gz .zip .zoo

I've also used

AddHref /htbin/unarch? .tar.Z .gz .zip .zoo
 and
AddHref /htbin/ls .tar.Z .gz .zip .zoo

 
> > 
> > It's running on http://aristarchus.rutgers.edu/www-icons.  Let me know
> > what you think.  Note that I haven't changed the way Directory Index
>
> I was looking there, but I don't understand:
> How is the AddHref for .Z (is it AddHref / .Z) 
> 

Umm, the AddHref line just makes the icon point to the top of my test
server.  Nothing fancy. (AddHref http://aristarchus.rutgers.edu/ .tar.Z)

> > options are parsed.  Also, I haven't made AddHref available from
> > access.conf and .htaccess.  Do you guys think we need to do that?
> 
> I don't know, but it would be fine if you have parts of the archives
> where you want to interpret files with the same suffix in a different
> manner. I don't see it at the moment but, if isn't a great think it
> gives you more flexibility.

OK, I'll take a look at providing AddHref support in access.conf and
.htaccess.  

> > 
> > I have one question: When LinkIcons is on, how do I make the
> > AddHref'ed icons stand out??  Currently, the only difference is a
> > rather cheezy icon I drew... :)
> 
> Perhaps I don't understand right LinkIcons - I see only the same
> reference on the icon as on the filename. If it is so I could miss
> this feature. I would reserve the Icon reference for a different
> reference as in AddHref.
> 

Right, the problem is that if LinkIcons is ON, you can miss the
special reference (ie: AddHref). My question is do you guys think
there's a way to keep LinkIcons _AND_ special references on the same
page while making special references stand out in some non-annoying
manner?  I think the answer is going to be no, but we'll provide
LinkIcons anyway (with AddHref support in access.conf and .htaccess)
and let the service maintainer deal with it.

I'll finish up the code changes today, and hopefully have the patch
off to Guenther late today.  


						Walt


_________________________________________________________
Walt Drummond (drummond@noc.rutgers.edu)
Network Services
Rutgers University Computing Services
 - Lost: One mind. Owner sad. Reward.



From john@math.nwu.edu  Fri Feb  4 18:00:29 1994 --100
Message-Id: <9402041657.AA09784@hopf.math.nwu.edu>
Date: Fri, 4 Feb 1994 18:00:29 --100
From: john@math.nwu.edu (John Franks)
Subject: Clients indicating Content-encodings they accept



Are there any plans for clients to indicate what Content-encodings they
can accept.  This could be done through the header Accept: lines or in
some other way.  My server now has the capability to automatically 
uncompress files as they are served.  I would like to have it not do
the uncompression for those clients which can do it themselves.  There
seems to be no mechanism to detect this however.


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu




From dsr@hplb.hpl.hp.com  Fri Feb  4 19:28:13 1994 --100
Message-Id: <9402041822.AA17370@manuel.hpl.hp.com>
Date: Fri, 4 Feb 1994 19:28:13 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Protocol Benchmarking (HTTP protocol)

Chris,

>> Before we set about support for multiple GETs, browsers should try taking
>> advantage of "Accept: multipart/related" to request a document and its
>> inlined images with a single GET. The HTTP standard specifies how to do
>> this now!

> Where?  The version of the standard I'm using is the Internet Draft.  Where
> can I get the version with the multipart/related spec?  I'd like to
> implement it in the Windows NT server.

Have a look at

  http://info.cern.ch/hypertext/WWW/Protocols/HTTP/Object_Headers.html

The idea is as follows:

If the client can wants to receive the HTML document and the inlined images
as a single MIME message, it includes multipart/related in the Accept: field
as part of the header. You will probably want other Accept: fields to cover
the content types of constituent body parts. Refer to the above for details.

When the server receives such a request, it builds the requested HTML
document into a MIME multipart/related message. Note that the last paragraph
of section 7.2.1 of RFC 1341 states that the notion of stuctured, related
body parts is missing from the core spec, and that standard MIME mail handlers
will treat Content-type: multipart/related as equivalent to multipart/mixed
and will thus still be able to show the user the separate parts.

The MIME message will look like:

    MIME-Version: 1.0
    Content-type: multipart/related; boundary="simple boundary"

    --simple boundary
    Content-type: text/html
    URI: <this documents URI>

    <title>the html document</title>
    etc.
    --simple boundary
    Content-type: image/gif
    URI: <uri for 1st inlined image>

    ...The gif image data ...
    --simple boundary
    Content-type: image/gif
    URI: <uri for 2nd inlined image>

    ...The gif image data ...
    --simple boundary--
    
Notice the URI header is used to identify the URI for each part.
This makes it easy to stick each part into your local cache and
there after use the URI to fetch the images from the cache with
no extra hassle. I have used MIME's boundary mechanism above, and
the final deliminator has a terminating "--" while preceding ones
don't. You could also include the Content-length: field to specify
the length of each part and of the message overall, e.g.

    MIME-Version: 1.0
    Content-type: multipart/related
    Content-length: <combined lengths of parts and their headers>

    Content-type: text/html
    Content-length: <length of this body part>
    URI: <this documents URI>

    <title>the html document</title>
    etc.
    Content-type: image/gif
    Content-length: <length of this body part>
    URI: <uri for 1st inlined image>

    ...The gif image data ...
    Content-type: image/gif
    Content-length: <length of this body part>
    URI: <uri for 2nd inlined image>

    ...The gif image data ...

Here, I have omitted the boundary deliminators which is perhaps a
mistake as it would make it impossible for standard mailers to
find the body parts. On the other hand, such mailers wouldn't
normally be fed the output of an HTTP server ...


Regards,

Dave Raggett



From uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch  Mon Feb  7 10:55:20 1994 --100
Message-Id: <2D52CBCB@MSMAIL.INDY.TCE.COM>
Date: Mon, 7 Feb 1994 10:55:20 --100
From: uunet!tcemail!is3.indy.tce.com!FisherM@dxmint.cern.ch (Fisher Mark)
Subject: FW: Directions for Obtaining the CORBA Spec


As promised, directions for obtaining CORBA specs:
 ----------
From: vinoski
To: fisherm
Subject: Re: Directions for Obtaining the CORBA Spec
Date: Friday, February 04, 1994 1:56PM

Return-Path: <uunet!apollo.hp.com!vinoski@tcemail.indy.tce.com>
Reply-To: uunet!apollo.hp.com!vinoski@tcemail.indy.tce.com
Message-Id: <9402041856.AA07744@relay.hp.com>
To: fisherm
Subject: Re: Directions for Obtaining the CORBA Spec
In-Reply-To: Your message of "Wed, 02 Feb 94 18:06:00 PST."
    <2D505DA3@MSMAIL.INDY.TCE.COM>
Date: Fri, 04 Feb 94 13:56:23 -0500
From: uunet!apollo.hp.com!vinoski@tcemail.indy.tce.com
 ----------------------------------------------------------------------------  
 --
Fisher Mark <is3.indy.tce.com!FisherM%tcemail.UUCP@uunet.UU.NET> writes:
>The World-Wide Web project (distributed hypertext/hypermedia via TCP/IP) is 

>discussing an idea called Accessories, which would be objects that that
>display data in a program's child window when the data is normally foreign
>to the program (for example, inline display of an MPEG movie in a program
>that otherwise would only know how to display a static image).  The proper
>way to do this looks like implementing Accessories as CORBA objects.  Since 

>you wrote an nice article for "The C++ Report" about CORBA, would you 
please
>tell me how people can obtain copies of the CORBA spec?  Thank you very
>much.

Send email to server@omg.org with the single word "help" in the body
of the message.  The OMG server will send you instructions on
retrieving OMG documents via email.

The CORBA 1.1 specification is not available via this service, however
(though you can currently get a draft of the CORBA 1.2 spec this way).
You can find the CORBA 1.1 spec in book form at your local technical
bookstore, or you can get it from the OMG for $50:

Object Management Group, Inc.   Telephone: +1-508-820 4300
492 Old Connecticut Path        Facsimile: +1-508-820 4303
Framingham, MA  01701   U.S.A.  Internet: documents@omg.org

I recommend pulling down the CORBA 1.2 spec (if you can handle
Postscript) before dropping $50 for the book form.

Hope this helps.  Any other questions, please let me know.

 --steve

Steve Vinoski  vinoski@apollo.hp.com    (508)436-5904
Distributed Object Computing Program    fax: (508)436-5122
Hewlett-Packard, Chelmsford, MA 01824



From burchard@geom.umn.edu  Mon Feb  7 11:00:19 1994 --100
Message-Id: <9402050008.AA03868@mobius.geom.umn.edu>
Date: Mon, 7 Feb 1994 11:00:19 --100
From: burchard@geom.umn.edu (burchard@geom.umn.edu)
Subject: Patch for NCSA httpd 1.1 seg fault problem

NCSA httpd 1.1 assumes getenv("PATH") returns non-NULL.
This fails for example on our SGI Irix 4.0.5 server
running from inetd.  Here is a patch to correct the
situation...sorry if this has already been mentioned.


*** httpd.h~	Fri Feb  4 17:48:47 1994
--- httpd.h	Fri Feb  4 17:48:50 1994
***************
*** 213,218 ****
--- 213,221 ----
  /* The name of the access file */
  #define ACCESS_CONFIG_FILE "conf/access.conf"
  

+ /* Default PATH to be inherited by server scripts */
+ #define DEFAULT_PATH "/bin:/usr/bin:/usr/ucb:/usr/bsd:/usr/local/bin"
+ 

  /* Whether we should enable rfc931 identity checking */
  #define DEFAULT_RFC931 0
  /* The default directory in user's home dir */
*** http_script.c~	Fri Feb  4 17:46:43 1994
--- http_script.c	Fri Feb  4 17:57:30 1994
***************
*** 73,79 ****
      int content, nph;
      char cl[MAX_STRING_LEN],t[MAX_STRING_LEN],t2[MAX_STRING_LEN];
      char path_args[MAX_STRING_LEN];
!     char *argv0,**env;
      FILE *psin;
      struct stat finfo;
      register int n,x;
--- 73,79 ----
      int content, nph;
      char cl[MAX_STRING_LEN],t[MAX_STRING_LEN],t2[MAX_STRING_LEN];
      char path_args[MAX_STRING_LEN];
!     char *argv0,**env,*env_path;
      FILE *psin;
      struct stat finfo;
      register int n,x;
***************
*** 97,103 ****
      if(!(env = (char **)malloc((MAX_CGI_VARS + 2) * sizeof(char *))))
          die(NO_MEMORY,"exec_cgi_script",out);
      n = 0;
!     env[n++] = make_env_str("PATH",getenv("PATH"),out);
      env[n++] = make_env_str("SERVER_SOFTWARE",SERVER_VERSION,out);
      env[n++] = make_env_str("SERVER_NAME",server_hostname,out);
      env[n++] = make_env_str("GATEWAY_INTERFACE","CGI/1.0",out);
--- 97,104 ----
      if(!(env = (char **)malloc((MAX_CGI_VARS + 2) * sizeof(char *))))
          die(NO_MEMORY,"exec_cgi_script",out);
      n = 0;
!     if(!(env_path = getenv("PATH"))) env_path = DEFAULT_PATH;
!     env[n++] = make_env_str("PATH",env_path,out);
      env[n++] = make_env_str("SERVER_SOFTWARE",SERVER_VERSION,out);
      env[n++] = make_env_str("SERVER_NAME",server_hostname,out);
      env[n++] = make_env_str("GATEWAY_INTERFACE","CGI/1.0",out);

--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------




From robm@ncsa.uiuc.edu  Mon Feb  7 11:30:06 1994 --100
Message-Id: <9402071016.AA21039@void.ncsa.uiuc.edu>
Date: Mon, 7 Feb 1994 11:30:06 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Patch for NCSA httpd 1.1 seg fault problem

/*
 * Patch for NCSA httpd 1.1 seg fault problem  by burchard@geom.umn.edu
 *    written on Feb  7, 11:01am.
 *
 * NCSA httpd 1.1 assumes getenv("PATH") returns non-NULL.
 * This fails for example on our SGI Irix 4.0.5 server
 * running from inetd.  Here is a patch to correct the
 * situation...sorry if this has already been mentioned.
 * 
 */

Actually, it hasn't. Thanks.

--Rob



From sanders@BSDI.COM  Mon Feb  7 11:07:11 1994 --100
Message-Id: <199402051946.NAA04041@austin.BSDI.COM>
Date: Mon, 7 Feb 1994 11:07:11 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: WWW stats (www.bsdi.com)

For stats gathered over the past 110 days I have logged requests
from 13694 unique hosts.

Anybody have any numbers to compare?

--sanders



From robm@ncsa.uiuc.edu  Mon Feb  7 11:03:34 1994 --100
Message-Id: <9402051101.AA03039@void.ncsa.uiuc.edu>
Date: Mon, 7 Feb 1994 11:03:34 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: proposals for log file format changes

/*
 * Re: proposals for log file format changes  by Tony Sanders
 *    written on Feb  1,  5:39pm.
 *
 * Hmm, this isn't what RFC931 returns though (for example, one useful thing
 * to do is return encrypted usernames via RFC931).  I think this should just
 * be the From: info if provided.  There should be a seperate field for
 * RFC931.

Okay.

 * The ddd and bbbb stuff is a bit of a pain.  Doing it right would require
 * the server to parse the output from gateways.  So long as we define that
 * it is optional it doesn't bother me.
 */

Good point. I don't want to touch the output of nph CGI scripts on my end.

So, how about this revised version:

Here's my proposal for a new logfile format:

host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss] ``request'' ddd bbbb

host: Either the DNS name or the IP number of the remote client
rfc931: Any information returned by identd for this person, - otherwise.
authuser: If user sent a userid for authentication, the user name,
otherwise, -.
Mon: Month (calendar name)
DD: Day
YYYY: Year
hh: hour (24-hour format, the machine's timezone)
mm: minute
ss: seconds
request: The first line of the HTTP request as sent by the client.
ddd: the status code returned by the server, - if not available.
bbbb: the total number of bytes sent, *not including the HTTP/1.0 header*, -
if not available.

The only remaining thing I haven't changed is to make the timezone GMT.
Comments? 

--Rob



From fielding@simplon.ICS.UCI.EDU  Mon Feb  7 12:58:28 1994 --100
Message-Id: <9402070356.aa25146@paris.ics.uci.edu>
Date: Mon, 7 Feb 1994 12:58:28 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: proposals for log file format changes 

Rob said:

> Here's my proposal for a new logfile format:
> 
> host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss] ``request'' ddd bbbb

Are those supposed to be separate single quotes around request or
are they supposed to be doublequote characters (e.g. "request") and
were changed by emacs latex mode?  I prefer doublequote characters.

The rest looks good to me.

> The only remaining thing I haven't changed is to make the timezone GMT.
> Comments? 

I prefer local time, myself, as its extremely rare that someone would
need to compare stats across time zones and what I usually look for in
my stats is what local time is best for system changes.
To put it another way, if it's GMT then every analyzer will either have
to stick with GMT format (not user-friendly) or convert each log entry's
timestamp to localtime (a non-portable bit of code).


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From m.koster@nexor.co.uk  Mon Feb  7 12:35:21 1994 --100
Message-Id: <9402071133.AA16269@dxmint.cern.ch>
Date: Mon, 7 Feb 1994 12:35:21 --100
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: WWW stats (www.bsdi.com) 


Tony Sanders writes:

> For stats gathered over the past 110 days I have logged requests
> from 13694 unique hosts.
>
> Anybody have any numbers to compare?

I've got 24960 unique hosts listed here, but I can't remember quite
when I started to log these, half a year ago? Make of it what you
will. Since 20th of September I've had an average of 156 new hosts per
day.

A graph of the stats since 20th September can be found in
http://web.nexor.co.uk/mak/misc/chart.gif

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From guenther.fischer@hrz.tu-chemnitz.de  Mon Feb  7 16:34:13 1994 --100
Message-Id: <9402071530.AA07991@flash1.hrz.tu-chemnitz.de>
Date: Mon, 7 Feb 1994 16:34:13 --100
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: Cache Server on base of httpd-1.1

Hi,
some days ago I've changed my www server to NCSA's httpd-1.1 and I got
the patch to use it as gateway from Maex (Markus Stumpf). 
On the top of this I've reimplemented my caching to this server.
Here it is:
http://www.tu-chemnitz.de/~ftpadm/httpd/src/cache.html

I hope for feedback.

	~Guenther

-- 
Name:      Guenther Fischer / Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361     / mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From davis@DRI.cornell.edu  Mon Feb  7 16:44:27 1994 --100
Message-Id: <199402071541.AA01243@willow.tc.cornell.edu>
Date: Mon, 7 Feb 1994 16:44:27 --100
From: davis@DRI.cornell.edu (Jim Davis)
Subject: Re: WWW stats (www.bsdi.com)


My server (cs-tr.cs.cornell.edu) has seen 1029 unique hosts
in one month.



From junga@informatik.tu-muenchen.de  Mon Feb  7 17:29:40 1994 --100
Message-Id: <1994Feb7.162534.19538@Informatik.TU-Muenchen.DE>
Date: Mon, 7 Feb 1994 17:29:40 --100
From: junga@informatik.tu-muenchen.de (Achim Jung)
Subject: WWW-FAQ


Hi!

Where can I find all the FAQs for WWW, httpd, XMosaic and Lynx?

Ciao, Achim
-------------------------------------------------------------------
Achim Jung        IRC: Flops        junga@informatik.tu-muenchen.de
WWW: http://www.informatik.tu-muenchen.de/personen/junga/junga.html




From altis@ibeam.jf.intel.com  Mon Feb  7 23:34:08 1994 --100
Message-Id: <m0pTeTt-00042NC@ibeam.intel.com>
Date: Mon, 7 Feb 1994 23:34:08 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: proposals for log file format changes

>Rob said:
>> The only remaining thing I haven't changed is to make the timezone GMT.
>> Comments?

I also prefer local time instead of GMT as do various system administrators
around here that I've talked to.

ka





From kevinh@eit.COM  Tue Feb  8 09:11:27 1994 --100
Message-Id: <9402080808.AA12897@eit.COM>
Date: Tue, 8 Feb 1994 09:11:27 --100
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re: proposals for log file format changes

> Roy said:
>
> Rob said:
> 
> > Here's my proposal for a new logfile format:
> > 
> > host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss] ``request'' ddd bbbb
> 
> > The only remaining thing I haven't changed is to make the timezone GMT.
> > Comments? 
> 
> I prefer local time, myself, as its extremely rare that someone would
> need to compare stats across time zones and what I usually look for in
> my stats is what local time is best for system changes.
> To put it another way, if it's GMT then every analyzer will either have
> to stick with GMT format (not user-friendly) or convert each log entry's
> timestamp to localtime (a non-portable bit of code).

	Good point - it's rather problematic to make portable code that
does that. As for me, I'll leave it up to the user to determine the
GMT time difference, should they wish to report times in GMT. For
backwards compatibility, the user also has to specify whether the
log times are in GMT or local time now.
	So now the format would be

host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss TTT] ``request'' ddd bbbb

	...where TTT is the local timezone?

	-- Kevin



From kevinh@eit.COM  Tue Feb  8 10:30:12 1994 --100
Message-Id: <9402080927.AA13226@eit.COM>
Date: Tue, 8 Feb 1994 10:30:12 --100
From: kevinh@eit.COM (Kevin Hughes)
Subject: Re: WWW stats


Tony Sanders writes:
 
> For stats gathered over the past 110 days I have logged requests
> from 13694 unique hosts.
>
> Anybody have any numbers to compare?

	I dusted off the logs from Honolulu Community College, and
since November 24th (over 78 days), they've had 23,310 unique hosts
and averaged about 400 new hosts per day over the last 7 days.
	Overall, they had about 4,600 requests per day, which I would
say puts them near the middle of intermediate to heavy server load,
heavy load starting at about 10,000 requests per day. In contrast,
EIT gets about 2,200 requests per day these days.
	The funny thing is, the whole Honolulu site's been on autopilot. It
hasn't changed a single byte since I left it over two months ago. :(

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://www.eit.com/)
Hypermedia Industrial Designer * Duty now for the future!



From robm@ncsa.uiuc.edu  Tue Feb  8 11:15:15 1994 --100
Message-Id: <9402081011.AA14822@void.ncsa.uiuc.edu>
Date: Tue, 8 Feb 1994 11:15:15 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: proposals for log file format changes

/*
 * Re: proposals for log file format changes  by Kevin 'Kev' Hughes
 *    written on Feb  8,  9:12am.
 *
 * 	So now the format would be
 * 
 * host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss TTT] ``request'' ddd bbbb
 * 
 * 	...where TTT is the local timezone?
 */

The quotes around request should be double quotes, not two single quotes. My
editor changed it when I cut-and-pasted.

host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss TTT] "request" ddd bbbb


--Rob



From rik@rdt.monash.edu.au  Tue Feb  8 11:38:10 1994 --100
Message-Id: <199402081036.VAA13209@daneel.rdt.monash.edu.au>
Date: Tue, 8 Feb 1994 11:38:10 --100
From: rik@rdt.monash.edu.au (Rik Harris)
Subject: Re: proposals for log file format changes 

Kevin wrote:
> Roy said:
> > I prefer local time, myself, as its extremely rare that someone would
> > need to compare stats across time zones and what I usually look for in
> > my stats is what local time is best for system changes.
> > To put it another way, if it's GMT then every analyzer will either have
> > to stick with GMT format (not user-friendly) or convert each log entry's
> > timestamp to localtime (a non-portable bit of code).
> 
> 	Good point - it's rather problematic to make portable code that
> does that. As for me, I'll leave it up to the user to determine the
> GMT time difference, should they wish to report times in GMT. For
> backwards compatibility, the user also has to specify whether the
> log times are in GMT or local time now.
> 	So now the format would be
> 
> host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss TTT] ``request'' ddd bbbb
> 
> 	...where TTT is the local timezone?

There is no standard for three letter timezones.  The safest way to
specify timezones is [GMT][+/-]nnnn.  ie. an offset from GMT.

rik.
--
Rik Harris - rik.harris@vifp.monash.edu.au              || Systems Programmer
+61 3 560-3265 (AH) +61 3 90-53227 (BH)                 || and Administrator
Fac. of Computing & Info.Tech., Monash Uni, Australia   || Vic. Institute of
http://www.vifp.monash.edu.au/people/rik.html           || Forensic Pathology



From michael.shiplett@umich.edu  Tue Feb  8 12:08:16 1994 --100
Message-Id: <199402081105.GAA14196@totalrecall.rs.itd.umich.edu>
Date: Tue, 8 Feb 1994 12:08:16 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: proposals for log file format changes 

"kh" = Kevin 'Kev' Hughes <kevinh@eit.COM> writes:

kh>	So now the format would be

kh> host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss TTT] ``request'' ddd bbbb

kh>	...where TTT is the local timezone?

  I hope TTT is the offset from UTC, e.g. -0400, and not a
three-letter abbreviation for the timezone name, e.g. EST. Timezone
abbreviations do not transfer well from country to country (if at
all).

michael



From hgs@research.att.com  Tue Feb  8 14:25:24 1994 --100
Message-Id: <199402081322.OAA08782@eunet.EU.net>
Date: Tue, 8 Feb 1994 14:25:24 --100
From: hgs@research.att.com (Henning G. Schulzrinne)
Subject: Re: proposals for log file format changes

May I suggest that the date format explicity conforms to whatever
date format is used with HTTP? (RFC 822, I believe). In particular,
time zones preferably should be specified as +/-0700 or some such.
There seems to be no good reason to have seventeen different time
formats within the same protocol realm.

Henning Schulzrinne
 



From stumpf@informatik.tu-muenchen.de  Tue Feb  8 16:50:44 1994 --100
Message-Id: <94Feb8.164556mesz.311358@hprbg5.informatik.tu-muenchen.de>
Date: Tue, 8 Feb 1994 16:50:44 --100
From: stumpf@informatik.tu-muenchen.de (Markus Stumpf)
Subject: SECURITY LEAK in ncsa httpd - PLEASE READ!!!!

Hoi folx,

there is IMHO a serious security leak in the ncsa httpd.

We run httpd from inetd and I always thought (but never checked)
that User and Group (from the conf oder httpd.h files) applies
in that case, too.
This is NOT true! (and should be stated clearly in the conf files
IMHO).

You could now argue to use the "user" entry in the inetd.conf file,
BUT:
-  I can't set the gid there
-  some older systems don't support this (yet)

Rob, could you please add the code from the standalon section to
the inetd section?!

This all doesn't solve a more serious problem with the <INC>
instruction!
Having user-directories configured, any user is able to execute ANY
command out of this document, and this command is run under
server privileges.
This should IMHO be changed to only allow starting of programs
out of .../cgi-bin/ for example.

	\Maex
-- 
______________________________________________________________________________
 Markus Stumpf                        Markus.Stumpf@Informatik.TU-Muenchen.DE 
                                http://www.informatik.tu-muenchen.de/~stumpf/



From sanders@BSDI.COM  Tue Feb  8 17:35:00 1994 --100
Message-Id: <199402081631.KAA13749@austin.BSDI.COM>
Date: Tue, 8 Feb 1994 17:35:00 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: proposals for log file format changes 

> 	Good point - it's rather problematic to make portable code that
> does that. As for me, I'll leave it up to the user to determine the
> GMT time difference, should they wish to report times in GMT. For
> backwards compatibility, the user also has to specify whether the
> log times are in GMT or local time now.
> 	So now the format would be
> 
> host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss TTT] ``request'' ddd bbbb
> 
> 	...where TTT is the local timezone?

I think you have it backwards, it's well known how to go from GMT to a
given timezone (and the software to do so is freely available), however,
given just a TTT it is *impossible* to get back to GMT.  If you insist on
localtime (which I think is a bad idea personally, however, this is up to
the server) then at least use a numeric timezone (e.g., -0600).

--sanders



From reverman@ka.reg.uci.edu  Tue Feb  8 17:41:08 1994 --100
Message-Id: <9402081635.AA11619@ka.reg.uci.edu>
Date: Tue, 8 Feb 1994 17:41:08 --100
From: reverman@ka.reg.uci.edu (Richard Everman)
Subject: Will this be true tomorrow?

Statement: (hopefully not out of context)

> Rob said:
> I prefer local time, myself, as its extremely rare that someone would
> need to compare stats across time zones 

Question:

Will this be true tomorrow? next year? 5 years from now?  At the rate 
the use of the Internet is growing, will we always (at least in the 
foreseeable future) be interested ONLY in local stats?  If there is a 
good change that as institutuions, companies, etc. become more 
global (e.g., the USA-based software company that went to a 24 hour 
help line by switching after hours calls to their AU-based site), 
shouldn't we also take a global view?  How many microseconds does it 
take to convert GMT to local time? :-}

LLAP

Richard
reverman@uci.edu

**************************************************************

>From: kevinh@eit.COM (Kevin 'Kev' Hughes)
> Rob said:
> 
> > Here's my proposal for a new logfile format:
> > 
> > host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss] ``request'' ddd bbbb
> 
> > The only remaining thing I haven't changed is to make the timezone GMT.
> > Comments? 
> 
> I prefer local time, myself, as its extremely rare that someone would
> need to compare stats across time zones and what I usually look for in
> my stats is what local time is best for system changes.
> To put it another way, if it's GMT then every analyzer will either have
> to stick with GMT format (not user-friendly) or convert each log entry's
> timestamp to localtime (a non-portable bit of code).

	




From robm@ncsa.uiuc.edu  Tue Feb  8 17:50:54 1994 --100
Message-Id: <9402081647.AA18047@void.ncsa.uiuc.edu>
Date: Tue, 8 Feb 1994 17:50:54 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: SECURITY LEAK in ncsa httpd - PLEASE READ THE DOCUMENTATION

/*
 * SECURITY LEAK in ncsa httpd - PLEASE READ!!!!  by Markus Stumpf
 *    written on Feb  8,  4:52pm.
 *
 * We run httpd from inetd and I always thought (but never checked)
 * that User and Group (from the conf oder httpd.h files) applies
 * in that case, too.
 * This is NOT true! (and should be stated clearly in the conf files
 * IMHO).

The first line of the documentation for User and Group says they only apply
to standalone mode. In addition, httpd.conf-dist says ``If you are running
from inetd, go to ServerAdmin'' which skips over User and Group.

 * You could now argue to use the "user" entry in the inetd.conf file,
 * BUT:
 * -  I can't set the gid there
 * -  some older systems don't support this (yet)
 * 
 * Rob, could you please add the code from the standalon section to
 * the inetd section?!

I'll consider it. You should consider running standalone.

 * This all doesn't solve a more serious problem with the <INC>
 * instruction!
 * Having user-directories configured, any user is able to execute ANY
 * command out of this document, and this command is run under
 * server privileges.
 * This should IMHO be changed to only allow starting of programs
 * out of .../cgi-bin/ for example.
 */

PLEASE READ THE DOCUMENTATION.

The Options directive in access.conf can be used to stop this. If you've
done your homework and you know that Directory applies to physical
directories instead of logical directories, you can use:

<Directory />
AllowOverride None
Options Indexes FollowSymLinks
</Directory>

Similarly, if all of your users' directories come out of one or two
directories (like /user1 or /home), you can change the / in Directory / to
that directory.

--Rob



From sg04%kesser@gte.com  Tue Feb  8 18:39:16 1994 --100
Message-Id: <9402081734.AA10532@kesser.cisl214>
Date: Tue, 8 Feb 1994 18:39:16 --100
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: Mathematica as a WWW client

Ok, I got you laughing, right?

Well, actually there are a lot of advantages to notebooks:

1. Embedded Video or live movies widgets in the viewer (I have already
   converted a JPEG movie and stuck it into a Math Notebook)
2. Full Text layout (almost as good as the latest version of Word)
   Since one has sections, tabs, mixed font, headers, footers, etc.
3. Embedded Audio
4. Full-feature postscript can be embedded into the document
5. One can even wrap text around graphics (with a little bit of 
   Postscript hacking).
6. Already runs on PCs and Macs and Unix boxes.

Well, if I had some way faking the GUI inputs for the X Front End,
I think I would have it (coupled with a www-library deamon program
that I can create pretty easily).

What I need is help with one of the following:

1. Some way to talk from the math kernel to the FrontEnd (e.g. a mathlink
   connection). However, I do not know the protocol the FrontEnd uses
   over Mathlink, and if it is possible to send mathlink commands to
   make the Front End do such actions as load notebook from file.

2. Alternatively, (and even a worse hack), I could send X-events that
   fake keypresses to open and close notebook files.

3. I am also looking for some way to associate a graphic cell with a
   mathematica script. That is, I want something similiar to a button
   press running a script (possibly I could remap the evaluate-selection
   function to be a mouse button 2 press, or something like that).


---
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Yechezkal-Shimon Gutfreund		 	   sgutfreund@gte.com [MIME]
GTE Laboratories, Waltham MA        http://www.gte.com/circus/home/home.html
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=



From kevinh@eit.COM  Tue Feb  8 19:35:27 1994 --100
Message-Id: <9402081832.AA17016@eit.COM>
Date: Tue, 8 Feb 1994 19:35:27 --100
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re: proposals for log file format changes

> Tony Sanders <sanders@BSDI.COM> writes:
> 
> I write:
>
> > 	So now the format would be
> > 
> > host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss TTT] ``request'' ddd bbbb
> > 
> > 	...where TTT is the local timezone?
> 
> I think you have it backwards, it's well known how to go from GMT to a
> given timezone (and the software to do so is freely available), however,
> given just a TTT it is *impossible* to get back to GMT.  If you insist on
> localtime (which I think is a bad idea personally, however, this is up to
> the server) then at least use a numeric timezone (e.g., -0600).

	BTW, does anyone have pointers to generic C code I can use to do this?
Without using time.h functions or calling shell commands?
	I agree that GMT time all over the place is a good thing, while
it is easier for webmasters to look at things in local time. As long as
web walkers, log analyzers, and other beasts that may depend on log entries
know how to recognize and convert the times if needed, that's fine with me.
	More or less following RFC 822, then:

host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss UT[+/-]HHMM] "request" ddd bbbb

	How's that?

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://www.eit.com/)
Hypermedia Industrial Designer * Duty now for the future!



From marca@eit.COM  Tue Feb  8 19:56:50 1994 --100
Message-Id: <199402081903.TAA07489@threejane>
Date: Tue, 8 Feb 1994 19:56:50 --100
From: marca@eit.COM (Marc Andreessen)
Subject: Re: SECURITY LEAK in ncsa httpd - PLEASE READ THE DOCUMENTATION

>  * SECURITY LEAK in ncsa httpd - PLEASE READ!!!!  by Markus Stumpf
>  *    written on Feb  8,  4:52pm.
>  *
>  * We run httpd from inetd and I always thought (but never checked)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>  * that User and Group (from the conf oder httpd.h files) applies
>  * in that case, too.
>  * This is NOT true! (and should be stated clearly in the conf files
>  * IMHO).

I've never been able to figure out why someone would advertise his own
lack of understanding of a situation to a large group of people in
screaming capital letters.

In any case, the docs for User -- for example -- have always stated:

"This directive is only applicable if you are using a ServerType of
standalone."

An erroneous assumption does not a SECURITY LEAK make, when the docs
clearly state the facts.

Cheers,
Marc



From b75894@ii.uni.wroc.pl  Tue Feb  8 20:02:39 1994 --100
Message-Id: <9402081852.AA08065@swiatowit.ii.uni.wroc.pl>
Date: Tue, 8 Feb 1994 20:02:39 --100
From: b75894@ii.uni.wroc.pl (Miroslaw Budzanowski)
Subject: ilp

I don't know weather I should write to you, but I do not know whom else I could
write to. I tried to compile ilp on Sun SparcStation 2 SunOS 4.1.1. As a result I
got:
ilp.c: 101: Can't find include file sys/select.h
ilp.c: 104: Can't find include file ulimit.h
When I had unset options -DHAVE_SELECT_H -DHAVE_ULIMIT_H I got a mass of warnings and
errors. I was compiling ilp using CC (C++ compiler), normal cc had not liked Ansi.
Could you, please, help me or direct to a person that will?



Cha chall a gheibh caraid,

===============================================================================
Mirek Budzanowski
Student of Computer Science at University of Wroclaw
===============================================================================
          Krakowska 3/3           Internet: b75894@ii.uni.wroc.pl     
   47-100 Strzelce Opolskie                 cu105@fim.uni-erlangen.de
          Poland                    Bitnet: b75894@plwruw11
===============================================================================





From ajcole@cbl.leeds.ac.uk  Tue Feb  8 20:56:22 1994 --100
Message-Id: <218.9402081953@cblsica.cbl.leeds.ac.uk>
Date: Tue, 8 Feb 1994 20:56:22 --100
From: ajcole@cbl.leeds.ac.uk (ajcole@cbl.leeds.ac.uk)
Subject: Re: SECURITY LEAK in ncsa httpd - PLEASE READ THE DOCUMENTATION

Marc,

>An erroneous assumption does not a SECURITY LEAK make, when the docs
>clearly state the facts.

Strictly thats not true if its easy to leave a loophole through not
reading the docs (in detail) that mistake is going to be made by many
people.  Some 'damage' is bound to have been done just because
its all too easy to setup like this and cause a security leak (potential
or actual).  I wouldnt be suprised if some managemnts immediately
pulled some plugs.  Equally I wouldnt be suprised if on some sites
people are exploiting this hole and keeping quite.

Dont get me wrong you know whose side I am on....

Andrew



From fielding@simplon.ICS.UCI.EDU  Tue Feb  8 21:10:57 1994 --100
Message-Id: <9402081208.aa26191@paris.ics.uci.edu>
Date: Tue, 8 Feb 1994 21:10:57 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Will this be true tomorrow? 

Richard said:
> Statement: (hopefully not out of context)
> 
>> Rob said:
>> I prefer local time, myself, as its extremely rare that someone would
>> need to compare stats across time zones 

Actually, I said that, so nobody should blame Rob.

> Question:
> 
> Will this be true tomorrow? next year? 5 years from now?  At the rate 
> the use of the Internet is growing, will we always (at least in the 
> foreseeable future) be interested ONLY in local stats?  If there is a 
> good change that as institutions, companies, etc. become more 
> global (e.g., the USA-based software company that went to a 24 hour 
> help line by switching after hours calls to their AU-based site), 
> shouldn't we also take a global view?  How many microseconds does it 
> take to convert GMT to local time? :-}

Quite a few when you consider how many entries are in a log.  However,
speed was not my primary concern.

Server stats are and will remain a local issue no matter how fast the
Internet grows.  There are several reasons for this:

1. The log file is not directly accessible to the web -- some local processing
   must be performed before it is made available.

2. Machine usage is generally dependent on local users (and their work hours)
   and its generally considered "nicer" to perform large remote accesses
   during that site's off-peak hours.  Thus, as a webmaster, I need to
   evaluate usage at my site in terms of how it effects local usage during
   local peak hours.

3. The log file is intended to assist humans trying to maintain their
   own server, and (except in the UK), all humans think in local time.
   This is particularly notable at the begin/end of each month when the
   log file is truncated by date.

4. Local events (such as power failures, lightning storms, earthquakes,
   due dates for final projects, etc.) occur in local time and, if they
   have some effect on the server, it's easier to match the effect with
   the event if the log reflects local time.

Having said that, it's certainly possible that some people will want to
publish their log in GMT.  However, since I anticipate that to be an
extremely small percentage of web sites, it makes more sense to have
those sites convert their log to GMT (using any number of analyzers)
rather than to require all other sites to convert from GMT to local time.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From hoesel@chem.rug.nl  Tue Feb  8 21:35:18 1994 --100
Message-Id: <9402082032.AA03777@Xtreme>
Date: Tue, 8 Feb 1994 21:35:18 --100
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Re: Will this be true tomorrow?


[...] 

> Server stats are and will remain a local issue no matter how fast the
> Internet grows.  There are several reasons for this:
> 
> 1. The log file is not directly accessible to the web -- some local processing
>    must be performed before it is made available.
> 
> 2. Machine usage is generally dependent on local users (and their work hours)
>    and its generally considered "nicer" to perform large remote accesses
>    during that site's off-peak hours.  Thus, as a webmaster, I need to
>    evaluate usage at my site in terms of how it effects local usage during
>    local peak hours.
> 
> 3. The log file is intended to assist humans trying to maintain their
>    own server, and (except in the UK), all humans think in local time.
>    This is particularly notable at the begin/end of each month when the
>    log file is truncated by date.
> 
> 4. Local events (such as power failures, lightning storms, earthquakes,
>    due dates for final projects, etc.) occur in local time and, if they
>    have some effect on the server, it's easier to match the effect with
>    the event if the log reflects local time.
> 
> Having said that, it's certainly possible that some people will want to
> publish their log in GMT.  However, since I anticipate that to be an
> extremely small percentage of web sites, it makes more sense to have
> those sites convert their log to GMT (using any number of analyzers)
> rather than to require all other sites to convert from GMT to local time.
> 
> 
> ...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
>                    (fielding@ics.uci.edu)
> 

well, I short time ago there was some discussion about what the logfile
was good for. I myself modified the deamon so it writes only as little
info as possible, and exclude even the logs from my local host.
Others claimed that the log file should contain as much information
as possible, so one could always decide afterwards what do save or
examine from the log. In order to do that you should not store the
local time in your log file. Use GMT. most computer brands now
have some kind of internationalization subroutines. one of the standard
routines is presumably the routine that converts from GMT to local time!
you analyzer program needs only to use such a routine and you are
ready. besides you probably now how far away GMT is from your local time

I admit that all humans think in local time (including those in the
Uk btw) but thinking in GMT is not too difficult eighter. besides
that were you have your analyzer for. the cpu times spend are itrrelevant
because if your analyzer doesn't do the conversion, then the logger has to!

I don't see why 1) must be done in local time
2) your analysis would be done by using an analyzer (at least if your
  interest in the log is that big, and the log is large too
3) your analyzer would report times in local time (or in gmt at
   your choise)
your point 4) could be of interest to site outside yours (when you
publice your logs, and therefore should be in GMT)

I cannot easely know what eastern standard time is, or arizona summer
time or whatever local time you have. I do know what GMT -9.00 is!

- frans





From reverman@ka.reg.uci.edu  Tue Feb  8 22:27:48 1994 --100
Message-Id: <9402082124.AA22983@ka.reg.uci.edu>
Date: Tue, 8 Feb 1994 22:27:48 --100
From: reverman@ka.reg.uci.edu (Richard Everman)
Subject: Will this be true tomorrow? 

Roy (Sorry Rob)

Is there a basic assumption being made here that the Webmaster will 
always be located at a "local" site; that the Webmaster will never 
telecommute?  What happens if the Webmaster is responsible for UCI, 
UC's Washington DC center, and UC's Education Abroad Program sites 
(which are located in 72 countries)?

Cheers

Richard
reverman@uci.edu

*********************************************************************

From: "Roy T. Fielding" <fielding@simplon.ICS.UCI.EDU>
Date: Tue, 8 Feb 1994 21:11:53 --100
To: Multiple recipients of list <www-talk@www0.cern.ch>
Subject: Re: Will this be true tomorrow? 

Richard said:
> Statement: (hopefully not out of context)
> 
>> Rob said:
>> I prefer local time, myself, as its extremely rare that someone would
>> need to compare stats across time zones 

Actually, I said that, so nobody should blame Rob.

> Question:
> 
> Will this be true tomorrow? next year? 5 years from now?  At the rate 
> the use of the Internet is growing, will we always (at least in the 
> foreseeable future) be interested ONLY in local stats?  If there is a 
> good change that as institutions, companies, etc. become more 
> global (e.g., the USA-based software company that went to a 24 hour 
> help line by switching after hours calls to their AU-based site), 
> shouldn't we also take a global view?  How many microseconds does it 
> take to convert GMT to local time? :-}

Quite a few when you consider how many entries are in a log.  However,
speed was not my primary concern.

Server stats are and will remain a local issue no matter how fast the
Internet grows.  There are several reasons for this:

1. The log file is not directly accessible to the web -- some local processing
   must be performed before it is made available.

2. Machine usage is generally dependent on local users (and their work hours)
   and its generally considered "nicer" to perform large remote accesses
   during that site's off-peak hours.  Thus, as a webmaster, I need to
   evaluate usage at my site in terms of how it effects local usage during
   local peak hours.

3. The log file is intended to assist humans trying to maintain their
   own server, and (except in the UK), all humans think in local time.
   This is particularly notable at the begin/end of each month when the
   log file is truncated by date.

4. Local events (such as power failures, lightning storms, earthquakes,
   due dates for final projects, etc.) occur in local time and, if they
   have some effect on the server, it's easier to match the effect with
   the event if the log reflects local time.

Having said that, it's certainly possible that some people will want to
publish their log in GMT.  However, since I anticipate that to be an
extremely small percentage of web sites, it makes more sense to have
those sites convert their log to GMT (using any number of analyzers)
rather than to require all other sites to convert from GMT to local time.


..Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)




From ebina@ncsa.uiuc.edu  Wed Feb  9 00:05:41 1994 --100
Message-Id: <9402082302.AA25581@void.ncsa.uiuc.edu>
Date: Wed, 9 Feb 1994 00:05:41 --100
From: ebina@ncsa.uiuc.edu (ebina@ncsa.uiuc.edu)
Subject: NCSA Mosaic for X 2.2 available


NCSA Mosaic for X 2.2 is now available.

..ftp.ncsa.uiuc.edu in /Mosaic:

  o Source in /Mosaic/Mosaic-source.

  o Binaries for SunOS 4.1.3, AIX 3.2.4 with X11R5, IRIX 4.x, IRIX 5.1.x,
    DEC Alpha (OSF/1), DEC Ultrix, and HP/UX 9.x (700-series) in
    /Mosaic/Mosaic-binaries.

As always, thanks for all the feedback!

If you have any comments, questions, or problems with Mosaic 2.2,
please send mail to mosaic-x@ncsa.uiuc.edu.  Also please drop us a
note if you enjoy using Mosaic or if you are using it in any
interesting projects or applications -- we love to hear from our
users!

Changes from version 2.1 to version 2.2 include:

 o Fixed bug in unrecognized URLs that have whitespace at the beginning.
 o Fixed bug with transparent color GIF89 images.
 o Fixed more inlined image parsing code dumps.
 o Fixed odd extra flashing in documents accessed after form documents.
 o Fixed socket leak in interrupted I/O.
 o Fixed bug in whitespace terminated entity '&' escapes.
 o Added Frans Van Hoesel's latest postscript changes.
 o Added Frans Van Hoesel's extra font support.
 o Speedups in GIF decoding, thanks to David Koblas.
 o Improved handling of monochrome displays. Detect mono without needing
   the -mono option, and dither inlined images.
 o Added <OPTION VALUE=val> support so you can have the value returned
   different than the string displayed.
 o Added <INPUT TYPE=hidden> to allow invisible constant name/value pairs
   to be added to submitted forms.
 o Made resource verticalScrollOnRight user settable.
 o Added hooks for PEM and PGP decoding by external application(s).
 o Added support for proxy gateways, specifiable on a per access method basis.


Cheers,
Marc (in absentia) & Eric

--
Marc Andreessen & Eric Bina
Software Development Group
National Center for Supercomputing Applications
marca@ncsa.uiuc.edu & ebina@ncsa.uiuc.edu





From fielding@simplon.ICS.UCI.EDU  Wed Feb  9 02:08:21 1994 --100
Message-Id: <9402081706.aa16843@paris.ics.uci.edu>
Date: Wed, 9 Feb 1994 02:08:21 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Will this be true tomorrow? 

Richard said:

> Is there a basic assumption being made here that the Webmaster will 
> always be located at a "local" site; that the Webmaster will never 
> telecommute? 

Not always -- almost always.  The reason for not telecommuting is
that the problems which may effect a local site are often the same
problems that effect telecommuting (network connections down, etc.),
and thus webmasters are usually local to the server.  However, I can
see the case when file-management-by-distance is desirable.

I still believe, though, that a webmaster will prefer seeing the local time
at the site so that problem investigation can proceed within the context
of that site.  Furthermore, in the case of US timezones, it is easier for
most people to translate EDT to PDT than it is to translate GMT to PDT
for the simple reason that they are used to it.  And, most importantly,
since at many sites (like ours) 95% or more of the web traffic is from
users within the local timezone, no translation is required for 95% of
the users if time is represented as local time.

Naturally, the decision of how time should be displayed on your web
should depend on how that web is being used.  However, the default
behavior of the software should correspond to how MOST sites will want
it to behave -- automated translators (or config options) can support
the rest.

> What happens if the Webmaster is responsible for UCI, 
> UC's Washington DC center, and UC's Education Abroad Program sites 
> (which are located in 72 countries)?

 [quickie answer -- the webmaster would probably go insane ;-)]

Is it necessary for each site to have their own server?  If there is only
one webmaster, why not have one web server (with world-wide access)?
I can think of several reasons (e.g. network latency, security, distributed
authors, etc.) why separate servers are better, but most of those reasons
are also arguments against a centralized webmaster.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From fielding@simplon.ICS.UCI.EDU  Wed Feb  9 03:04:01 1994 --100
Message-Id: <9402081802.aa29359@q2.ics.uci.edu>
Date: Wed, 9 Feb 1994 03:04:01 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Will this be true tomorrow? 

frans said:

> well, I short time ago there was some discussion about what the logfile
> was good for. I myself modified the deamon so it writes only as little
> info as possible, and exclude even the logs from my local host.

Yes, hacking the server works where configuration options don't.

> Others claimed that the log file should contain as much information
> as possible, so one could always decide afterwards what do save or
> examine from the log.

Yep, I said that.

> In order to do that you should not store the
> local time in your log file.

Why?  No information is lost when local time is stored -- it's not
as if the site is changing timezones (except for daylight <-> standard).

> Use GMT. most computer brands now
> have some kind of internationalization subroutines. one of the standard
> routines is presumably the routine that converts from GMT to local time!
> you analyzer program needs only to use such a routine and you are
> ready. besides you probably now how far away GMT is from your local time

I wish that were true.  It's not.  There are almost always routines to convert
from machine time to local time or GMT time.  There are almost always routines
to convert local time (in ASCII) to machine time.  Sometimes, there are
routines to convert GMT time to machine time (usually provided for NNTP).
The problem comes in determining what is the local timezone, for which
no portable solution exists.

FYI, I have no idea how far away PST is from GMT, but I'm sure I could
look it up somewhere.  But why should I when my world (and all my cares)
revolve around PST (and occasionally NZST).  I know that sounds rather
provincial, but it is nevertheless a fact of life.  I use GMT when interfacing
with programs, not with people. 

> I admit that all humans think in local time (including those in the
> Uk btw) but thinking in GMT is not too difficult eighter. besides
> that were you have your analyzer for. the cpu times spend are itrrelevant
> because if your analyzer doesn't do the conversion, then the logger has to!

It's a lot easier to think in GMT when your timezone is near GMT.
Personally, the only time I ever think in GMT is when I'm testing
HTTP server protocols via telnet.  If local time is stored in the log
and you want your web usage statistics printed in local time (as I do),
then no conversion is needed at all.  Even when you do want GMT, it's
much easier to go from local -> machine -> GMT than the other way around.

[I know, I should have put a smiley on that UK joke]

> ...
> 
> I cannot easely know what eastern standard time is, or arizona summer
> time or whatever local time you have. I do know what GMT -9.00 is!

Actually, I don't know what time that is (I could look it up in the 
rfc822 spec, but who wants to do that?).  The problem I always have is
remembering whether you add/subtract from the listed time to get GMT or
the listed time is GMT and you add/subtract the modifier to get the local
time (I suspect it's the latter).  Things get worse when the modifier
crosses over a date line.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From rogers@ISI.EDU  Wed Feb  9 03:17:26 1994 --100
Message-Id: <25519.760760106@drax.isi.edu>
Date: Wed, 9 Feb 1994 03:17:26 --100
From: rogers@ISI.EDU (Craig Milo Rogers)
Subject: Re: Will this be true tomorrow? 

>> In order to do that you should not store the
>> local time in your log file.
>
>Why?  No information is lost when local time is stored -- it's not
>as if the site is changing timezones (except for daylight <-> standard).

	Unless you're using a mobile host.  In my experience, though,
mobile hosts use GMT as "local" time, so the log should work out OK.

					Craig Milo Rogers



From rogers@ISI.EDU  Wed Feb  9 03:24:11 1994 --100
Message-Id: <25655.760760510@drax.isi.edu>
Date: Wed, 9 Feb 1994 03:24:11 --100
From: rogers@ISI.EDU (Craig Milo Rogers)
Subject: Re: Will this be true tomorrow? 

>3. The log file is intended to assist humans trying to maintain their
>   own server, and (except in the UK), all humans think in local time.
>   This is particularly notable at the begin/end of each month when the
>   log file is truncated by date.

1)	The log file will probably be used to debug problems between
	clients and servers.  Given the distributed nature of the Web,
	the clients and servers may be in different time zones.  Using
	GMT might help.

2)	Big computer centers that were part of a distributed network
	tended to run in GMT.  Of course, big computer centers are
	now mainly passe.

3)	Military personnel are often taught to work in GMT.

4)	Isn't Iceland also on GMT?  :-)

					Craig Milo Rogers



From courtaud@limeil.cea.fr  Wed Feb  9 09:53:38 1994 --100
Message-Id: <9402090741.AA00267@limeil.cea.fr>
Date: Wed, 9 Feb 1994 09:53:38 --100
From: courtaud@limeil.cea.fr (Didier.Courtaud)
Subject: Re: NCSA Mosaic for X 2.2 available

> From www-talk@www0.cern.ch Wed Feb  9 08:05:22 1994
> X400-Received: by /PRMD=cea/ADMD=atlas/C=FR/;
>  Relayed; 09 Feb 94 00:09:11+0100
> X400-Received: by /PRMD=inria/ADMD=atlas/C=FR/;
>  Relayed; 09 Feb 94 00:09:11+0100
> X400-Received: by /PRMD=CERN/ADMD=ARCOM/C=CH/;
>  Relayed; 09 Feb 94 00:05:41-0100
> Date: 09 Feb 94 00:05:41-0100
> From: ebina@ncsa.uiuc.edu
> Sender: www-talk@www0.cern.ch
> Reply-To: www-talk@www0.cern.ch
> To: Multiple recipients of list <www-talk@www0.cern.ch>
> Subject: NCSA Mosaic for X 2.2 available
> Originator: www-talk@info.cern.ch
> X-Listprocessor-Version: 6.0c -- ListProcessor by Anastasios Kotsikonas
> Content-Length: 1868
> 
> NCSA Mosaic for X 2.2 is now available.
> 
> ..ftp.ncsa.uiuc.edu in /Mosaic:
> 
>   o Source in /Mosaic/Mosaic-source.
> 
>   o Binaries for SunOS 4.1.3, AIX 3.2.4 with X11R5, IRIX 4.x, IRIX 5.1.x,
>     DEC Alpha (OSF/1), DEC Ultrix, and HP/UX 9.x (700-series) in
>     /Mosaic/Mosaic-binaries.
> 

Why is there no binaries for SunOS 5.x ( Solaris 2.x ) 

Do you have yet problems with this operating system ?

------------------------------------------------------------------------------
                            Didier  COURTAUD

                     Graphical Applications Group Leader
                      Commissariat a l'Energie Atomique
                   Direction des Applications Militaires
                  Departement de Mathematiques Appliquees
              Service Architectures Informatiques et Methodes
                  94195 Villeneuve Saint Georges Cedex 
                               France

 Phone : (33 - 1) 45 95 67 07
 Fax   : (33 - 1) 45 95 95 55
 Email : courtaud@limeil.cea.fr
-----------------------------------------------------------------------------





From FisherM@is3.indy.tce.com  Wed Feb  9 09:58:33 1994 --100
Message-Id: <2D583D27@MSMAIL.INDY.TCE.COM>
Date: Wed, 9 Feb 1994 09:58:33 --100
From: FisherM@is3.indy.tce.com (Fisher Mark)
Subject: RE: MIME Transfer Format


(Sorry this is late...  Strangely enough, the listserver requestor wouldn't 
forward my message to the group :))
 ------------------------------------
In <199402030142.AA04574@rock.west.ora.com>, Christopher McRae wrote:

>  This is the right way to do it, I believe.  We've kicked this idea around
>before (see background messages included below), and it's clear that using
>multipart messages is a big win in other ways, too.  For instance, one 
could
>use multipart messages to deliver pages containing sections which would be
>displayed only if certain conditions were met.  The rules specifying how 
and
>when the various sections should be expanded/collapsed would be delivered
>along with the other parts, perhaps as the first part of the multipart. 
 Note
>that this technique is also useful for delivering encrypted information, 
and
>for including the DTD along with a document instance (when we abandon HTML 
for
>arbitrary DTD's :-)
>  In fact, I have been thinking about how one would go about defining 
standard
>ways of putting together such multipart packages.  To use the techniques
>mentioned above, we would need to define a control scripting language for
>describing the relationships between the different message parts and for
>orchestrating their interaction.  In my thinking, this control language 
shares
>some characteristics of the 'Universal network graphics language' which Tim
>brought up on this list last week.  Although I think 'Universal network
>graphics
>language' is a misnomer for the kind of thing we wrote about - graphical
>objects
>are just one of the object types we're all interested in squirting around 
the
>net.

I think that we have two interlocking needs here.  One is the ability to 
send multimedia non-interactive objects, which MIME was developed to handle. 
 What I got from Tim's message (which may not be what he meant :( ) was the 
ability to dynamically and interactively send multimedia around, so that you 
could have events like SIGWEB #4002, "The First On-Line SIGWEB!", or more 
prosaically, videoconference your company's East and West Coast engineering 
staffs in a brainstorming meeting with shared whiteboards (rather than 
erase, just spawn an additional whiteboard...).  Some kind of scripting 
language seems appropriate, as defining the primitives needed for such a 
language is probably at least an order of magnitude easier than anticipating 
every potential use of these capabilities.  Although we may not need to 
bound by the same constraints as MIME (most notably, handling mailers with 
broken line-length and character-set handlers), the MIME nested multipart 
structure looks like a big win to me also.  Any language designers care to 
respond?
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From hoesel@chem.rug.nl  Wed Feb  9 11:45:07 1994 --100
Message-Id: <9402091040.AA00466@Xtreme>
Date: Wed, 9 Feb 1994 11:45:07 --100
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Re: Will this be true tomorrow?

Roy wrote:
> 
> > In order to do that you should not store the
> > local time in your log file.
> 
> Why?  No information is lost when local time is stored -- it's not
> as if the site is changing timezones (except for daylight <-> standard).

there is information lost! I have no idea what NZST is, I you had written
that time in your log file, then that logfile would have no meaning to me,
nor to any analyzer (except your local one). I I were to have a look at
your log with an analyzer then I need an extremely clever analyzer that
knows all the local time habbits correctly. You would also need to state
in your log file what NZST is.

> 
> > Use GMT. most computer brands now
> > have some kind of internationalization subroutines. one of the standard
> > routines is presumably the routine that converts from GMT to local time!
> > you analyzer program needs only to use such a routine and you are
> > ready. besides you probably now how far away GMT is from your local time
> 
> I wish that were true.  It's not.  There are almost always routines to convert
> from machine time to local time or GMT time.  There are almost always routines
> to convert local time (in ASCII) to machine time.  Sometimes, there are
> routines to convert GMT time to machine time (usually provided for NNTP).
> The problem comes in determining what is the local timezone, for which
> no portable solution exists.

it is true! at least on all the unix boxes around. every unix machine
stores its time in GMT (not of ant interest to you as a human :-), and
when it displays the time for you, it does the conversion from GMT to your
local time. No big deal at all. You have a TIMEZONE variable once set on
your system to tell it how to convert from GMT to your local time.

[...] 
> 
> It's a lot easier to think in GMT when your timezone is near GMT.
> Personally, the only time I ever think in GMT is when I'm testing
> HTTP server protocols via telnet.  If local time is stored in the log
> and you want your web usage statistics printed in local time (as I do),
> then no conversion is needed at all.  Even when you do want GMT, it's
> much easier to go from local -> machine -> GMT than the other way around.
> 
as I said that conversion *is* needed eighter your deamon does it for you,
or your analyzer program. Why not keep think flexible and global and do
the conversion in the analyse phase?

> > I cannot easely know what eastern standard time is, or arizona summer
> > time or whatever local time you have. I do know what GMT -9.00 is!
> 
> Actually, I don't know what time that is (I could look it up in the 
> rfc822 spec, but who wants to do that?).  The problem I always have is
> remembering whether you add/subtract from the listed time to get GMT or
> the listed time is GMT and you add/subtract the modifier to get the local
> time (I suspect it's the latter).  Things get worse when the modifier
> crosses over a date line.
> 
[fyi: the minus sign is there to subtract 

- frans





From timbl@ptpc00.cern.ch  Wed Feb  9 12:20:49 1994 --100
Message-Id: <9402091118.AA03829@ptpc00.cern.ch>
Date: Wed, 9 Feb 1994 12:20:49 --100
From: timbl@ptpc00.cern.ch (Tim Berners-Lee)
Subject: Re: proxy and Next browser



I'm distributing this reply to a private mail from Kevin Altis to the
list because lower down there are some nitty gritty points about
assumptions about access rights in HTTP which affect gateways, and
I will change the spec to reflect that. So ignore this message 

unless you're into that sort of thing. :-)

> we're pretty close to announcing the new proxy support that Ari, Lou, and I
> have been working on. Much of the credit should go to you though since I
> got the idea last year based on your earlier GATEWAY code. Any comments you
> have, especially any gaping holes in the method would be greatly
> appreciated;

I understand from Ari that it works just like the WWW_xxxx_GATEWAY
method, except the environment variable is different.  Is that
right?  What are the differences?

>            the only thing that comes to mind right now are protocols
> (Z39.50 ???, maybe DCE stuff?) that can't be handled by HTTP transactions
> today which would mean that the proxy can't handle that kind of request.
> Should HTTP/2.0 cover those cases?

Yes -- in  fact, we should have a well-defined method of defining
how any arbitrary method maps onto HTTP.  The idea was that HTTP should
be self-extending: the Allowed: header comming back would give a list
of operations, and the client could then query the server to get
a description of new operations in some (NETGOTIATED) language.
See teh SHOWMETHOD method.
That negotiation is a key.  Suppose you have a z39.50 gateway to
a server doing a boolean search of some special variety. The
"Z2345678SEARCH" is mentioned in the "ALLOWED:" header
and the client checks it out with the the server.  Current person-oriented
clients canoinly handle HTML+ and get back a form. Fine -- if the
it contains fields and instructions, and links to explanation of what the  
parameters mean.  Future smart clients can get back a semantic
description of the operation (pre- and post-conditions) as well as
a more formal description of the parameters.  I see this extensibility
as being the direction for not only gateways but also arbitrary OO
systems out there.


So we just need to
say how any arbitrary parameter set would be sent by HTTP.
Obvioulsy a MIME/multipart would be a way when the values are big
objects, but it is ugly(-ier even) when the parameters are small --
like integers.  A halfway house would be to allow both like

Param:  <parametername> <type> <value>

which can include

Param: <parametername> SEE <URL>

where URL could be a cid:<content-type> referring to an enclosure.
Make sense?  Use SGML Instead?  ASN/1? Mapping should be
bidirectional. Shouldn't matter whether DCE or HTTP is underneath.


> I think this will make a huge difference in the usage of the Internet and
> the Web in particular.

Fortunately, it won't clobber the nackbones -- in fact will reduce
the long distance traffic.  Because HTTP is now a significant share of
the traffic, so we think twice before introducing fetaures which
may up the trafic overnight.  Caching may even cut the figures right down.
We will need good monitors on the caches to be able to estimate the
effective traffic which would be generated were it not for them.
That will be interesting to plot vs total line capacity!

> Suddenly Mac and PC based users behind firewalls
> that have never been able to use the Internet will be able to reach out. Of
> course, Unix and VMS users will be helped as well, but at least they've
> usually had some grungy ways of getting out to the rest of the world. On
> the other hand, this solution should go over real well with administrators
> since the level of logging and restrictions on the type of things the user
> can do are much greater than SOCKS or other halfway solutions without
> really censoring a user (maybe PUTs).

Yes.

> It was also intentional on my part to do the proxy this way to elevate the
> importance of HTTP on the Internet (HTTP is the vehicle for proxying) and
> make it possible to put much of the smarts in the caching server rather
> than the client. For example, if we have to support gopher+, it can
> probably be done on the proxy server, not on every client.

This decision really ends up being made separately for each case.
Some people want ta totally capable client.  They have a good net
connection, lots of local cache, and no sysadmin support.
They want fast response and software which works out of the box.
For them, the totally-equipped client. For others, the firewall
and a good sysadmin and maybe a slow connection from the firewall
out make use of the proxy a must.  So from the code point of
view, I am happy that the same code can be used in server and
client.  We should stick with this model with any new protocol
additions, I think.

A cool thing might be to use dns to find the nearest proxy. Just
as I hope new clients to look for a local www.dom.ain server for
a default home page, maybe someone needing a wais gateway could
look for www-wais.dom.ain and then www-wais.ain in an attempt to
find one lying around. This would make things work better out of the box
and reduce configuration bugs.

> You can also
> make a lightweight client that only speaks HTTP if you want, but since
> clients will probably speak native ftp, wais, etc. within an organization
> rather than always going through a proxy that might not be important.
> Clients don't have to speak TCP/IP to a proxy either, which opens up some
> interesting options.

We ran HTPP over Decnet through a proxy.  Then the folks who asked
about it ended up getting TCP/IP on their vaxes!  We didn't want to
supprt it.  But the Novell guys might take that on. (Or maybe we should
not encourage it too much if we believe IP on everything is the best
for the world)

> I might be able to get Web clients onto handhelds yet.

How come my phone has no wires but my notebook pc does? Crazy.
I'd be happy with portable phone technology -- don't really
need cellular. Much cheaper too. Don't understand why I can't buy it...

> Doing HEAD, expires, etc. is suddenly going to get important. Might put
> some pressure on the URNs issue as well.

Yes.  Also, the Public: is important.  We must get the default understanding  
completely clear.  At the moment in HTTP is seems as though Public: is
just informational, as in fact if anyone really wants to test access then
they can just try it.  With caching, the Public: allows the caching server
to return it directly.  If we specify the current assumption that
if nothing is specified then the document is public, This is 

NOT fail-safe.  Would it be better to make that assuption ONLY if no
Authorization: header was sent?

IE
    If Public: present, it is definitive.
    Else	if authorization was needed, then assume NO public access
    		else	if Allowed: is present, assume public access is same
			else assume public access is GET only.

I'll put that in the spec -- if anyone has any troubles with this
say now.

> Ari mentioned you're working on a Next browser again. I would like to see
> that sometime. I'm going to be running Nextstep 3.2 on my Pro/GX as soon as
> the software arrives.

It is a "spare time" activity, and right now the thing craches. A number
of people have expressed interest in helping, but not come up to
speed yet.  The state is embarassing right now, but soon I will
be able to give source out but only to people who cancommit to
a particular aspect of improvement.

> ka

Tim BL

> 

> 




From neuss@igd.fhg.de  Wed Feb  9 12:31:59 1994 --100
Message-Id: <9402091131.AA09219@wildturkey.igd.fhg.de>
Date: Wed, 9 Feb 1994 12:31:59 --100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: Re: Will this be true tomorrow?

Hiyall,

frans van hoesel (hoesel@chem.rug.nl) wrote:

> Roy wrote:
> > 

> > > In order to do that you should not store the
> > > local time in your log file.
> > 

> > Why?  No information is lost when local time is stored -- it's not
> > as if the site is changing timezones (except for daylight <-> standard).
> 

> there is information lost! I have no idea what NZST is, I you had written
> that time in your log file, then that logfile would have no meaning to me,
> nor to any analyzer (except your local one). I I were to have a look at
> your log with an analyzer then I need an extremely clever analyzer that
> knows all the local time habbits correctly. You would also need to state
> in your log file what NZST is.
Hmm.. there's a point there, I guess. How about a compromise that doesn't
cost too much: Adding the UNIX system time will not require any conversion,
and it can easily be converted to all other file formats by remote analyzers.
Local site users could have the convenient, readable local time, while
tools would simply make use of the system time field.

System time is a long integer, so it would cost another 8 bytes if printed
in hex format. Yeah, this is an ugly format for humans, but easy to use
by computers. ;-)

Cheers,
Chris
---
"I ride a tandem with the random.." 

Christian Neuss   # Fraunhofer Institute for Computer Graphics
Wilhelminenstr.7  #  64283 Darmstadt # Germany
e-mail: neuss@igd.fhg.de  finger: neuss@wildturkey.igd.fhg.de



From hoesel@chem.rug.nl  Wed Feb  9 12:48:23 1994 --100
Message-Id: <9402091145.AA00564@Xtreme>
Date: Wed, 9 Feb 1994 12:48:23 --100
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Re: Will this be true tomorrow?

Chris told us:

[...] 
> Hmm.. there's a point there, I guess. How about a compromise that doesn't
> cost too much: Adding the UNIX system time will not require any conversion,
> and it can easily be converted to all other file formats by remote analyzers.
> Local site users could have the convenient, readable local time, while
> tools would simply make use of the system time field.
> 
> System time is a long integer, so it would cost another 8 bytes if printed
> in hex format. Yeah, this is an ugly format for humans, but easy to use
> by computers. ;-)
> 
I would be happy if we had the choice, just as if you httpd would give
you the configurable output options that allow me to decide what to
log and what not (Hi roy:-), it would be ok with me is there were an
option te record the time in local or in gmt. everyadmin that decides
not log in GMT will no that the analyzer might run into problems, just
as if he decided not to log the name of the remote user, or whatever.
Tha's were the configurable output options are for, isn't it... 
you can use them and create human readable output that contains all
the things you are interested in (and nothing more) (you can use
grep as you analyzer tool) or you don't use them and have some general
purpose analyzer read the log for you and produce out put in human
readable form (which might be local time!)

- frans





From vesely@iiasa.ac.at  Wed Feb  9 13:47:55 1994 --100
Message-Id: <199402091246.AA18778@iiasa.ac.at>
Date: Wed, 9 Feb 1994 13:47:55 --100
From: vesely@iiasa.ac.at (Pavel  VESELY)
Subject: add www-talk

add www-talk



From hgs@research.att.com  Wed Feb  9 13:54:11 1994 --100
Message-Id: <9402091244.AA26219@dxmint.cern.ch>
Date: Wed, 9 Feb 1994 13:54:11 --100
From: hgs@research.att.com (Henning G. Schulzrinne)
Subject: Re: Will this be true tomorrow?

Despite all the discussions about whether or not to use GMT, nobody
has addressed the question why the date format is different than
the one used in HTTP (MM/DD/YY instead of 
    DD-Mon-YY according to RFC850
or  [Wkday,] DD Mon YYYY according to RFC822

Economy of specification, as well as implementation, would suggest
a uniform date format in all WWW protocols and applications.

Henning




From michael.shiplett@umich.edu  Wed Feb  9 14:02:33 1994 --100
Message-Id: <199402091259.HAA09367@totalrecall.rs.itd.umich.edu>
Date: Wed, 9 Feb 1994 14:02:33 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: Will this be true tomorrow? 

"fvh" == frans van hoesel <hoesel@chem.rug.nl> writes:

fvh> I would be happy if we had the choice, just as if you httpd would give
fvh> you the configurable output options that allow me to decide what to
fvh> log and what not (Hi roy:-), it would be ok with me is there were an
fvh> option te record the time in local or in gmt. everyadmin that decides
fvh> not log in GMT will no that the analyzer might run into problems, just
fvh> as if he decided not to log the name of the remote user, or whatever.
fvh> Tha's were the configurable output options are for, isn't it... 
fvh> you can use them and create human readable output that contains all
fvh> the things you are interested in (and nothing more) (you can use
fvh> grep as you analyzer tool) or you don't use them and have some general
fvh> purpose analyzer read the log for you and produce out put in human
fvh> readable form (which might be local time!)

  I agree with the need for choice. I've dealt with monitoring system
logs and sometimes these were read by humans and sometimes by
analyzers. The former prefer to see information in string form in
local time, whereas statistics were better handled by using an epoch
timestamp (especially around the daylight savings switchovers). The
solution I used was to have the first field be the Unix timestamp in
UTC, and to record the pretty human-readable stuff after it.

  Sure it's possible to go from the string format back to a timestamp,
but it's not a pretty conversion and most Unixes don't have routines
to do this. There's no need to do the conversion if the administrator
can choose to have a time stamp (for whatever zone) as well as any
additionally formatting information.

  If the analyzers require a fixed format, I would hope it's one which
is easily created via preprocessing (awk, sed) of the log file.

michael



From M.J.Cox@bradford.ac.uk  Wed Feb  9 14:40:17 1994 --100
Message-Id: <10130.9402091336@compute.brad.ac.uk>
Date: Wed, 9 Feb 1994 14:40:17 --100
From: M.J.Cox@bradford.ac.uk (M.J.Cox@bradford.ac.uk)
Subject: Re: Will this be true tomorrow?

Henning G. Schulzrinne wrote:
| the one used in HTTP (MM/DD/YY instead of ...

Well as long as it isn't MM/DD/YY!!  It gets very confusing in the UK
as to us 01/02/94 is the 1st of February not the 2nd of January.  Makes
it kind of tricky to work out dates at-a-glance.

Mark
Mark J Cox ---------------------- <URL:http://www.eia.brad.ac.uk/mark.html>
Industrial Technology, Bradford University, UK.   +44 274 384024/fax 391333




From luotonen@ptsun00.cern.ch  Wed Feb  9 14:58:59 1994 --100
Message-Id: <9402091356.AA14343@ptsun03.cern.ch>
Date: Wed, 9 Feb 1994 14:58:59 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Timezones


Clearly we cannot make a decision of weather everybody wants
localtime or GMT.  I'll make localtime default, and provide a
-gmt option to allow times in GMT.  Everyone should be happy
with that.  If a given statistics collecter requires a certain
timezone then use that one.

But I do need to know exactly how the timezone info should appear
in log file (if it is necessary):

	1) not at all -- WebAdmin knows it anyway
	2) as GMT +/-NN   or just  +/-NN since GMT always appears in
	   there. Is ftime() portable?
	3) a 3-letter abbreviation for timezone -- I understood that
	   they are not standardised;  how would I get that in C
	   in a portable way.  Is strftime() portable?

My personal comments:

Giving an option is better than restricting to one fixed format
which doesn't satisfy nearly everyone.  This was also what I had
in mind in my original message but I was obviously misunderstood.

If I am only interested in how many times a document X is accessed,
I do _not_ want to have megabytes of diskspace wasted in access
times (whatever timezone), hostnames, remote idents etc.  Since I
want my own kind of special logging then I don't need nor do I want
to run a statistics collector, I just say

	sort log | uniq -q | sort -r > my_stats

and I am very happy.  If I want to use a standard statistics collector
fine, then I just use the standard format.  I thought having fully
configurable log files was a good idea, but some people just fail to
see that the options do not have to be used if they happen cause
nightmares.

IF you want to use them fine, you can have whatever logfile format.
BUT it may then not be understood by statistics collector, which you
probably won't be running in this case anyway.

[I'm inspired by the 31MB log file on info.cern.ch from these 8 days.]

-- Cheers, Ari --




From appel@cih.hcuge.ch  Wed Feb  9 18:33:57 1994 --100
Message-Id: <1583*/S=appel/OU=cih/O=hcuge/PRMD=switch/ADMD=arcom/C=ch/@MHS>
Date: Wed, 9 Feb 1994 18:33:57 --100
From: appel@cih.hcuge.ch (Ron D. Appel)
Subject: Help with fm2html

I am having (big) trouble installing fm2html on our SPARStations. Is there
anybody who did it and could send me the binaries?

Thanks.

-------------------------------------------------------------------------
| Ron D. Appel                             | Tel.:   (+41 22) 372 6264  |
| Hopital Cantonal Universitaire de Geneve | Fax.:   (+41 22) 372 6198  |
| Centre d'Informatique Hospitaliere       | e-mail: appel@cih.hcuge.ch |
| 24, rue Micheli-du-Crest                 |   (S=appel;OU=cih;O=hcuge; |
| CH-1211 Geneve 14                        |    P=switch;A=arcom;C=ch)  |
| Switzerland                              |                            |
|                                                                       |
|  Try the ExPASy Molecular Biology Server at http://expasy.hcuge.ch/   |
-------------------------------------------------------------------------



From sanders@BSDI.COM  Wed Feb  9 19:50:15 1994 --100
Message-Id: <199402091844.MAA19221@austin.BSDI.COM>
Date: Wed, 9 Feb 1994 19:50:15 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Timezones 

> Clearly we cannot make a decision of weather everybody wants
> localtime or GMT.  I'll make localtime default, and provide a
> -gmt option to allow times in GMT.  Everyone should be happy
> with that.  If a given statistics collecter requires a certain
> timezone then use that one.
That's reasonable.

> But I do need to know exactly how the timezone info should appear
> in log file (if it is necessary):
I believe it must be: +/-NNNN   (e.g., -0600).
This is the only hope for being able to make sense of the timezone info.

--sanders



From (no email)  Wed Feb  9 20:32:50 1994 --100
Message-Id: <9402091929.AA08494@void.ncsa.uiuc.edu>
Date: Wed, 9 Feb 1994 20:32:50 --100
From: (no email) ((no name))
Subject: 


The other day I got a curious message from someone asking why CGI didn't
make the value of the From: HTTP header available to scripts. The immediate
answer is "Because Rob didn't know about it at the time". However, the
person went on to describe what they wanted to use it for: an insecure
method of access control.

I was wondering just where the heck someone would get such an idea, until I
looked at the HTTP spec:

-- Excerpt: http://info.cern.ch/hypertext/WWW/Protocols/HTTP/HTRQ_Headers.html

From:

In Internet mail format, this gives the name of the requesting user. This
field may be used for logging purposes and an insecure form of access
protection. The interpretation of this field is that the request is being
performed on behalf of the person given, who accepts responsability for the
method performed.

-- End of excerpt

BTW the word responsability is a typo, probably should be fixed.

Is this really something we want to encourage? "The request is being
performed on behalf of the person given, who accepts responsibility for the
                                         ********************************** 
method performed."
****************

Calling this unsecure is an understatement. I sure don't want to take
responsibility for a method which I didn't necessarily perform. The only
case in which such a setup should be considered is one in which your
protected documents are not accessible by *anyone* outside a group of people
whom you trust not to use telnet and abuse this service. Physical network
barriers would be enough, except that you then have to place a great deal of
trust in every person who has access to the specified subnet or list hosts.

I think we need to change this section to read that From: is to be used for
logging purposes only, and strike the mention of insecure form of access
protection and the section on the person given accepting responsibility for
the method performed. The only access protection this would provide is
applicable in such a limited context that the information in From: is not
useful for more than logging information anyway.

--Rob



From rogers@ISI.EDU  Wed Feb  9 20:40:02 1994 --100
Message-Id: <12830.760822629@drax.isi.edu>
Date: Wed, 9 Feb 1994 20:40:02 --100
From: rogers@ISI.EDU (Craig Milo Rogers)
Subject: Re: Timezones 

>> But I do need to know exactly how the timezone info should appear
>> in log file (if it is necessary):
>I believe it must be: +/-NNNN   (e.g., -0600).
>This is the only hope for being able to make sense of the timezone info.

	Yes, there are regions for which local time is offset by n
hours and 30 minutes, or n hours and 15 minutes.

	There have been *extensive* discussions about representing
time in the header-people mailing list over the years.  There is an
international standard!  For example, RFC1505 specifies:

 DD Mon YYYY HH:MM:SS.FFFFFF [+-]HHMMSS

	I don't like this for log files, because it's harder to sample
and sort than something simple like:

YYYYMMDD.HHMMSS.FFFFFF+-HHMMSS		(with appropriate fields optional,
					 etc.)

					Craig Milo Rogers





From bianco@giant.larc.nasa.gov  Wed Feb  9 20:57:58 1994 --100
Message-Id: <199402091954.TAA14394@MiSTy.larc.nasa.gov>
Date: Wed, 9 Feb 1994 20:57:58 --100
From: bianco@giant.larc.nasa.gov (David Bianco)
Subject: NCSA Mosaic/HTTPD & PGP... Anyone got it working?


Ok, so I'm impatient... 8-)

I got Mosaic 2.2 and NCSA HTTPD 1.1 running on my system (SPARCStation
LX under Solaris 2.3) and tried to set up PGP based authentication for
a specific document tree.  In this directory, I have the following .htaccess
file:

	AuthType PGP
	
	<Limit GET>
	order allow,deny
	require user d.j.bianco@larc.nasa.gov
	</Limit>

I've edited the pgp-{dec,enc} scripts, and set up PGP keyrings for 
the webmaster.  However, I get only authorization errors when I try to
use it:

|| Escape character is '^]'.
|| GET / HTTP/1.0
|| Authorized: PGP entity="d.j.bianco@larc.nasa.gov"
|| Content-type: application/x-www-pgp-request
|| 
|| HTTP/1.0 401 Unauthorized
|| Date: Wednesday, 09-Feb-94 19:51:53 GMT
|| Server: NCSA/1.1
|| MIME-version: 1.0
|| WWW-Authenticate: PGP entity="webmaster@ice-www.larc.nasa.gov"
|| 
|| <HEAD><TITLE>Authorization Required</TITLE></HEAD>
|| <BODY><H1>Authorization Required</H1>
|| Browser not authentication-capable or 
|| authentication failed.
|| </BODY>
|| Connection closed by foreign host.

I don't even get a chance to input my PGP-encrypted request...

I've verified that the webmaster and I can exchange encrypted messages
properly via email, so I know we have our PGP keys exchanged properly.
Does anyone have any ideas on this (or is it still too early? 8-)

	David



From philg@martigny.ai.mit.edu  Wed Feb  9 21:06:39 1994 --100
Message-Id: <9402092003.AA12062@dxmint.cern.ch>
Date: Wed, 9 Feb 1994 21:06:39 --100
From: philg@martigny.ai.mit.edu (Philip Greenspun)
Subject: one author's perspective on HTML (a modest proposal)


Most books and magazines are presented as paragraphs stuck together
(no blank line) with each new paragraph indicated with an indented
first line, e.g.,

-----------------------------example of standard book
   We all laughed when we remembered the two saleswomen
from an advertising agency who came by our plush new
Cambridge offices.  They were showing us their book when
George, who was lying near one woman, started to make
whooping noises.
   "What's that?"  asked the woman.
   "He's going to throw up," I responded while quickly
marshaling Wall Street Journals to place underneath his
mouth.
   The women shrieked and closed themselves into a
windowless, unlit, 2'x2' closet, refusing to emerge for
several minutes.

   Trying to numb myself with fatigue, I ran six miles
through the woods near my house, up and down hills that
overlook the city and ocean
-----------------------------

Note that the blank line here conveys something significant, i.e., the
end of one story and the beginning of another, without forcing the
author to explicitly start a new chapter.

Here is how Mosaic would display it:


-----------------------------example of same text from Mosaic
We all laughed when we remembered the two saleswomen
from an advertising agency who came by our plush new
Cambridge offices.  They were showing us their book when
George, who was lying near one woman, started to make
whooping noises.

"What's that?"  asked the woman.

"He's going to throw up," I responded while quickly
marshaling Wall Street Journals to place underneath his
mouth.

The women shrieked and closed themselves into a
windowless, unlit, 2'x2' closet, refusing to emerge for
several minutes.

Trying to numb myself with fatigue, I ran six miles
through the woods near my house, up and down hills that
-----------------------------

This is very different and much of the author's intent has been
lost.

It isn't enough to say "you have <PRE>, what are you complaining
about?" because that precludes the reader from choosing his own Mosaic
window size, i.e., line length.

Given the prevalence of the first style of formatting (look at any
novel), HTML should support this expression.

I propose the command <NOVEL> that will tell the displayer to break
paragraphs with indentations rather than blank lines.

      -- Philip Greenspun

-------------------------------------------------------------
MIT Department of Electrical Engineering and Computer Science
545 Technology Square, Rm 433, Cambridge, MA 02139, (617) 253-8574
Personal Web URL:  http://martigny.ai.mit.edu/~philg/philg.html




From robm@ncsa.uiuc.edu  Wed Feb  9 21:36:45 1994 --100
Message-Id: <9402092033.AA09461@void.ncsa.uiuc.edu>
Date: Wed, 9 Feb 1994 21:36:45 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: NCSA Mosaic/HTTPD & PGP... Anyone got it working?

/*
 * NCSA Mosaic/HTTPD & PGP... Anyone got it working?  by David Bianco
 *    written on Feb  9,  8:59pm.
 *
 * I've edited the pgp-{dec,enc} scripts, and set up PGP keyrings for 
 * the webmaster.  However, I get only authorization errors when I try to
 * use it:
 * 
 * || Escape character is '^]'.
 * || GET / HTTP/1.0
 * || Authorized: PGP entity="d.j.bianco@larc.nasa.gov"
 
That should be Authorization...
 
 * || Content-type: application/x-www-pgp-request
 * || 
 * || HTTP/1.0 401 Unauthorized
 * || Date: Wednesday, 09-Feb-94 19:51:53 GMT
 * || Server: NCSA/1.1
 * || MIME-version: 1.0
 * || WWW-Authenticate: PGP entity="webmaster@ice-www.larc.nasa.gov"
 * || 
 * || <HEAD><TITLE>Authorization Required</TITLE></HEAD>
 * || <BODY><H1>Authorization Required</H1>
 * || Browser not authentication-capable or 
 * || authentication failed.
 * || </BODY>
 * || Connection closed by foreign host.
 * 
 * I don't even get a chance to input my PGP-encrypted request...
 * 
 * I've verified that the webmaster and I can exchange encrypted messages
 * properly via email, so I know we have our PGP keys exchanged properly.
 * Does anyone have any ideas on this (or is it still too early? 8-)
 */

Have you tried from Mosaic? Did you get any errors either from Mosaic or
from httpd (in error_log)?

Something else you might want to try while troubleshooting is to take a
file, and pgp-enc from the command line, it into another file. If this
works, take that file over to your server, and pgp-dec it, and see if it
works. Both pgp-enc and pgp-dec work as filters, i.e. file on stdin is
transformed to stdout, and they take the remote entity name as argv[1].

--Rob



From (no email)  Wed Feb  9 21:43:16 1994 --100
Message-Id: <9402092028.AA04408@Hypatia.gsfc.nasa.gov>
Date: Wed, 9 Feb 1994 21:43:16 --100
From: (no email) ((no name))
Subject: 

So Rob McCool sez to me:
> I think we need to change this section to read that From: is to be used for
> logging purposes only, and strike the mention of insecure form of access
> protection and the section on the person given accepting responsibility for
> the method performed. The only access protection this would provide is
> applicable in such a limited context that the information in From: is not
> useful for more than logging information anyway.

I agree.  I'm much more interested in clients that can (eventually)
encrypt a paassword field in a document and send it to the server for
validation than in ever suggesting that the From: field could be used
for some sort of access control.  OTOH, I'd just love to have the server
log that information - there are a number of cases where we could make
use of user name information in our summary stats.
_______________________________________________________________________
-- Archie Warnock              Internet:  Archie.Warnock@gsfc.nasa.gov
-- Hughes STX                  "WAIS is the engine, WWW is the track"
-- NASA/GSFC




From philg@martigny.ai.mit.edu  Wed Feb  9 21:49:11 1994 --100
Message-Id: <9402092044.AA16757@dxmint.cern.ch>
Date: Wed, 9 Feb 1994 21:49:11 --100
From: philg@martigny.ai.mit.edu (Philip Greenspun)
Subject: one author's perspective on HTML (a modest proposal)


     I have been thinking the same thing for some time (I would be surprised 
   if this hasn't come up before) and it's really needed.  I have to 
   disagree with the <NOVEL> tag though; the usefulness of no spaces between 
   paragraphs is much more general.  I was thinking of an expansion of the 
   <P> tag, something like

    <PI> for Paragraph (just indent) and
    <PS> for Paragrah (followed by a space) 

   and naturally <P> would be interpreted as <PS> for backward compatability.

   What do you think?

Your mechanism is more general and therefore pretty winning.  However,
we'd also need <PSI> (blank line PLUS indent the first text line) to
accomplish the "break in story" idea you saw in my first msg.

I think your idea is great, although I frankly think that the whole
HTML concept needs to import some ideas from (gasp) Microshaft Word,
et al.

Philip



From (no email)  Wed Feb  9 21:54:41 1994 --100
Message-Id: <9402092051.AA33100@stat1.cc.ukans.edu>
Date: Wed, 9 Feb 1994 21:54:41 --100
From: (no email) ((no name))
Subject: 

> 
> So Rob McCool sez to me:
> > I think we need to change this section to read that From: is to be used for
> > logging purposes only, and strike the mention of insecure form of access
> > protection and the section on the person given accepting responsibility for
> > the method performed. The only access protection this would provide is
> > applicable in such a limited context that the information in From: is not
> > useful for more than logging information anyway.
> 
> I agree.  I'm much more interested in clients that can (eventually)
> encrypt a paassword field in a document and send it to the server for
> validation than in ever suggesting that the From: field could be used
> for some sort of access control.  OTOH, I'd just love to have the server
> log that information - there are a number of cases where we could make
> use of user name information in our summary stats.

While we are on that subject.  I would love to see the 
Within? field logged.  There is some field that is supposed to 
be the URI of the document that contained the requested URI.
If we had that logged then we could tell which documents 
had pointers into our data, and we might be able to inform
people who maintain these documents when we move/destroy 
our own docs.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From (no email)  Wed Feb  9 22:00:48 1994 --100
Message-Id: <9402092052.AA17990@hopf.math.nwu.edu>
Date: Wed, 9 Feb 1994 22:00:48 --100
From: (no email) ((no name))
Subject: 

According to Archie Warnock:
> 
> So Rob McCool sez to me:
> > I think we need to change this section to read that From: is to be used for
> > logging purposes only, and strike the mention of insecure form of access
> > protection and the section on the person given accepting responsibility for
> > the method performed. The only access protection this would provide is
> > applicable in such a limited context that the information in From: is not
> > useful for more than logging information anyway.
> 
> I agree.  I'm much more interested in clients that can (eventually)
> encrypt a paassword field in a document and send it to the server for
> validation than in ever suggesting that the From: field could be used
> for some sort of access control.  OTOH, I'd just love to have the server
> log that information - there are a number of cases where we could make
> use of user name information in our summary stats.

Which, if any, clients currently support the From: header?


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu




From (no email)  Wed Feb  9 22:10:21 1994 --100
Message-Id: <9402092057.AA03752@rossi.astro.nwu.edu>
Date: Wed, 9 Feb 1994 22:10:21 --100
From: (no email) ((no name))
Subject: 

> While we are on that subject.  I would love to see the 
> Within? field logged.  There is some field that is supposed to 
> be the URI of the document that contained the requested URI.
> If we had that logged then we could tell which documents 
> had pointers into our data, and we might be able to inform
> people who maintain these documents when we move/destroy 
> our own docs.

Or when they have the link to our files wrong too, as my logs seem to show
today that somebody has a link wrong somewhere...

We definitely need this.

-Robert Lentz
-- 
lentz@rossi.astro.nwu.edu            http://www.astro.nwu.edu/lentz/plan.html
	"You have to push as hard as the age that pushes against you."
					-Flannery O'Connor



From (no email)  Wed Feb  9 22:16:06 1994 --100
Message-Id: <9402092109.AA21728@dxmint.cern.ch>
Date: Wed, 9 Feb 1994 22:16:06 --100
From: (no email) ((no name))
Subject: 

????, then rob mcool, then lou montulli sez>> 
>> So Rob McCool sez to me:
>> > I think we need to change this section to read that From: is to be
>> > used for logging purposes only, and strike the mention of insecure
>> > form of access protection and the section on the person given
>> > accepting responsibility for the method performed. The only access
>> > protection this would provide is applicable in such a limited context
>> > that the information in From: is not useful for more than logging
>> > information anyway.
>> 
>> I agree.  I'm much more interested in clients that can (eventually)
>> encrypt a paassword field in a document and send it to the server for
>> validation than in ever suggesting that the From: field could be used
>> for some sort of access control.  OTOH, I'd just love to have the server
>> log that information - there are a number of cases where we could make
>> use of user name information in our summary stats.

   The encryption is nicely handled now by the emacs browser ad mosaic2.2 +
httpd1.1, I think this should solve most of the security problems (at least
in the USA... %!#@!ing patents/export restrictions on encryption
algorithms... blah).

>While we are on that subject.  I would love to see the Within? field
>logged.  There is some field that is supposed to be the URI of the
>document that contained the requested URI.  If we had that logged then we
>could tell which documents had pointers into our data, and we might be
>able to inform people who maintain these documents when we move/destroy
>our own docs.

   I think you are thinkinf of the 'Referer:' field.  I send this when
possible, but do any others? I think lynx does, but I don't recall seeing
it in a request from Mosaic.  It could be extremely useful in the case of
failed requests.

-Bill P.



From sanders@BSDI.COM  Wed Feb  9 22:20:53 1994 --100
Message-Id: <199402092110.PAA00664@austin.BSDI.COM>
Date: Wed, 9 Feb 1994 22:20:53 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: one author's perspective on HTML (a modest proposal) 

>    disagree with the <NOVEL> tag though; the usefulness of no spaces between 
..
>     <PS> for Paragrah (followed by a space) 
>     <PI> for Paragraph (just indent) and
..
> I think your idea is great, although I frankly think that the whole
> HTML concept needs to import some ideas from (gasp) Microshaft Word,

Yuck, yuck, yuck.  You don't want anything of the sort.  The correct
solution to the problem is to design an SGML layout language (been done,
aka a style guide) that describes externally to HTML how things should
look.  This is a solved problem waiting for an implementation.

This lets authors do presentation and lets users override them (because
it's ugly, or you just feel like playing around, or your work
environment imposes other presentation requirements).

If you want Microsoft Word then distribute your documents in Word format
not HTML.

Consider this case, you are trying to import a document written at site
B for use at your site, however your site has different style guidelines.
With MS Word you might spend 100's of hours reformatting the document.
With HTML + style-guide you just tweak the style guide and boom, you are
done.  This is why HTML sticks more with SGML concepts than going pure
presentational.  The persentational elements of HTML are for converting
*EXISTING* documents to HTML (e.g., man pages) where you have no
semantic information (just bold/italic).

--sanders



From (no email)  Wed Feb  9 22:25:34 1994 --100
Message-Id: <9402092112.AA21910@dxmint.cern.ch>
Date: Wed, 9 Feb 1994 22:25:34 --100
From: (no email) ((no name))
Subject: 

>According to Archie Warnock:
>> So Rob McCool sez to me:
>> > I think we need to change this section to read that From: is to be
>> > used for logging purposes only, and strike the mention of insecure
>> > form of access protection and the section on the person given
>> > accepting responsibility for the method performed. The only access
>> > protection this would provide is applicable in such a limited context
>> > that the information in From: is not useful for more than logging
>> > information anyway.
>> 
>> I agree.  I'm much more interested in clients that can (eventually)
>> encrypt a paassword field in a document and send it to the server for
>> validation than in ever suggesting that the From: field could be used
>> for some sort of access control.  OTOH, I'd just love to have the server
>> log that information - there are a number of cases where we could make
>> use of user name information in our summary stats.
>
>Which, if any, clients currently support the From: header?

   Lynx and the emacs browser both support it, and I think violaWWW does -
not 100% sure on that though.  Unless things have changed in the 2.2
release, Mosaic/X does not send it, and I don't think the Mac or Windows
versions do either.

-Bill Perry



From (no email)  Wed Feb  9 22:57:34 1994 --100
Message-Id: <9402092154.AA35963@stat1.cc.ukans.edu>
Date: Wed, 9 Feb 1994 22:57:34 --100
From: (no email) ((no name))
Subject: 

> >While we are on that subject.  I would love to see the Within? field
> >logged.  There is some field that is supposed to be the URI of the
> >document that contained the requested URI.  If we had that logged then we
> >could tell which documents had pointers into our data, and we might be
> >able to inform people who maintain these documents when we move/destroy
> >our own docs.
> 
>    I think you are thinkinf of the 'Referer:' field.  I send this when
> possible, but do any others? I think lynx does, but I don't recall seeing
> it in a request from Mosaic.  It could be extremely useful in the case of
> failed requests.
> 
> -Bill P.
> 
Lynx doesn't send the field yet, (or I would probably have known
it's name :) but I will add the support immediately if I can
get at least one server writer to support it.

One issue I'm not sure of is.  What should I send if the user
types the URL in on the command line or with a "goto" command?
Probably not sending the field would work, but maybe
we should be exacting and send something like "User entered"

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From altis@ibeam.jf.intel.com  Wed Feb  9 23:55:52 1994 --100
Message-Id: <m0pUNiN-00042kC@ibeam.intel.com>
Date: Wed, 9 Feb 1994 23:55:52 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: proxy gateway service announcement/testing

I'm pleased to announce the availability of a new application level proxy
gateway service for the WWW. This is an adaptation of Tim Berners-Lee's
gateway code used in libWWW. Lou Montulli, Ari Luotonen, and I (Kevin
Altis) have been working together for the last month or so on this problem
and we now have something for you to try; the NCSA folks are working the
changes into their code as well. The proxy supports http, gopher, ftp,
wais, and news. Full HTTP 1.0 methods are supported including POST, PUT and
authentication.

The proxy service is based on the HTTP protocol, and for clients and
servers based on libWWW only requires a small number of changes. Eric Bina
has already made the changes for X Mosaic 2.2. Mosaic for the Mac and
Windows will soon also have support for the proxy service. Client and
server writers interested in supporting the proxy service should contact me
for more information until we get a full description up on the Web.

More messages will follow on this list for discussion of TESTING this proxy
gateway service, problems you might encounter, etc. This is not a general
release announcement for comp.infosystems.www.

CLIENTS CURRENTLY SUPPORTING PROXY: Lynx2-2, X Mosaic 2.2
The prerelease Lynx 2-2 source files are available at
<ftp://stat1.cc.ukans.edu/pub/lynx/lynx2-2.tar.Z> along with a lynx.cfg in
the same directory.

X Mosaic 2.2 is available at <ftp://ftp.ncsa.uiuc.edu/Mosaic>

Lynx 2-2 and X Mosaic 2.2 support the proxy service through environment
variables. The environment variables are of the form protocol_proxy and
expect a full URL. Within my company we use the following shell script to
launch lynx, which automatically sets the environment variables to the
appropriate proxy machine. Note, that we aren't proxying news since that is
always accessed from a local machine. No other setup is required on the
client side! NOTE: substitute your real gateway server and port number for
"somehost.intel.com:911" below.

#!/bin/sh
http_proxy=http://somehost.intel.com:911/
ftp_proxy=http://somehost.intel.com:911/
wais_proxy=http://somehost.intel.com:911/
gopher_proxy=http://somehost.intel.com:911/
export http_proxy
export ftp_proxy
export wais_proxy
export gopher_proxy
/usr/local/bin/lynx2-2


There is also a prerelease debug version of Win Mosaic with proxy support
at <ftp://ftp.ncsa.uiuc.edu/outgoing/jonm/proxy.zip>; a general release
version should be available sometime next week. Setup in Win Mosaic simply
requires the addition of the following lines in the mosaic.ini file:
[proxy information]
http_proxy=http://somehost.intel.com:911/
ftp_proxy=http://somehost.intel.com:911/
wais_proxy=http://somehost.intel.com:911/
gopher_proxy=http://somehost.intel.com:911/

PROXY SERVERS:
There is a test proxy server running at <http://www1.cern.ch:911/>. You can
run your own by getting the prerelease SUN4 binary of cern_httpd 2.15
available at
<ftp://info.cern.ch/pub/www/bin/sun4/httpd_2.15pre3-gcc-static-lresolv.Z>
supports the new proxy method. Ari will make the cern_httpd sources
available later this month for compilation on other Unix platforms.

You can start your copy of the cern_httpd server to run as a standalone
proxy server with a command line of:
        cern_httpd -p 911 -r httpd.conf -l access_log

The httpd.conf should look like:
pass    http:*
pass    ftp:*
pass    wais:*
pass    gopher:*
fail    news:*
fail    *

The cern_httpd can be used as a regular server at the same time, if
needed.  Just add normal mappings and passes to the end (before
last fail):

        Pass  /*  file:/document/root/*

However, for testing purposes, I would suggest that you run the cern_httpd
only as a proxy. The cern_httpd will automatically do setuid-nobody if it
runs as root. For more information on the cern_httpd server, see the
documentation at <http://info.cern.ch/hypertext/WWW/Daemon/Status.html>.

All requests using a proxy server such as the cern_httpd are logged exactly
the same as normal HTTP requests, except that full URLs appear in the log
file rather than partial URLs. The uniform log format currently being
discussed on www-talk will be used in the cern_httpd server when the format
is finalized.

ka

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Kevin Altis                             2111 N.E. 25th.
Internet Program Architect              Hillsboro, OR 97124
Media Delivery Laboratory               Email: altis@ibeam.intel.com
Intel Corporation JF2-58                Phone: 503-696-8788
                                        Fax:   503-696-6067





From guenther.fischer@hrz.tu-chemnitz.de  Thu Feb 10 07:18:54 1994 --100
Message-Id: <9402100618.AA12286@flash1.hrz.tu-chemnitz.de>
Date: Thu, 10 Feb 1994 07:18:54 --100
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: Re: proxy gateway service announcement/testing

> 
> #!/bin/sh
> http_proxy=http://somehost.intel.com:911/
> ftp_proxy=http://somehost.intel.com:911/
> wais_proxy=http://somehost.intel.com:911/
> gopher_proxy=http://somehost.intel.com:911/
> export http_proxy
> export ftp_proxy
> export wais_proxy
> export gopher_proxy
> /usr/local/bin/lynx2-2
> 

Where can I find something about welldefined environment names for
gateway definition. Till now I use WWW_http_GATEWAY to define one for
http gateway and we have implemented a gateway/cache server on base
of NCSAs http-1.1.
Is http_proxy another name, a new name and has it the same funtionality?

What does the client send to the proxy or gateway. For WWW_http_GATEWAY
it was such like
GET /server:port/path
for a URL: http://server:port/path.

	~guenther

PS: We use http-caching for nearly 8 weeks and users won't miss it. I
would be happy if all clients know about a gateway definition.
Till know we use mosaic2.1 and lynx who know that and I miss it in
WinMosaic.
http://www.tu-chemnitz.de/~ftpadm/httpd/src/cache.htm
-- 
Name:      Guenther Fischer / Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361     / mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From Jon.Tetzchner@nta.no  Thu Feb 10 11:05:49 1994 --100
Message-Id: <199402101001.AA02808@bang.nta.no>
Date: Thu, 10 Feb 1994 11:05:49 --100
From: Jon.Tetzchner@nta.no (Jon.Tetzchner@nta.no)
Subject: Re: Help with fm2html] Help with fm2html


	From ivars Thu Feb 10 07:45:56 1994
	Received: from haydn.nta.no by hal.nta.no with SMTP id AA08699
	  (5.65c/IDA-1.4.4 for <jons>); Thu, 10 Feb 1994 07:45:55 +0100
	Message-Id: <199402100645.AA08699@hal.nta.no>
	Received: by haydn.nta.no (4.1/SMI-4.1)
		id AA03347; Thu, 10 Feb 94 07:45:55 +0100
	To: jons
	Subject: [appel@cih.hcuge.ch: Help with fm2html]
	Date: Wed, 9 Feb 1994 18:33:57 --100
	Reply-To: www-talk@www0.cern.ch
	Originator: www-talk@info.cern.ch
	Sender: www-talk@www0.cern.ch
	Precedence: bulk
	From: "Ron D. Appel" <appel@cih.hcuge.ch>
	To: Multiple recipients of list <www-talk@www0.cern.ch>
	Subject: Help with fm2html
	X-Listprocessor-Version: 6.0c -- ListProcessor by Anastasios Kotsikonas
	Content-Length: 876
	Status: RO

	I am having (big) trouble installing fm2html on our SPARStations. Is there
	anybody who did it and could send me the binaries?

	Thanks.

	-------------------------------------------------------------------------
	| Ron D. Appel                             | Tel.:   (+41 22) 372 6264  |
	| Hopital Cantonal Universitaire de Geneve | Fax.:   (+41 22) 372 6198  |
	| Centre d'Informatique Hospitaliere       | e-mail: appel@cih.hcuge.ch |
	| 24, rue Micheli-du-Crest                 |   (S=appel;OU=cih;O=hcuge; |
	| CH-1211 Geneve 14                        |    P=switch;A=arcom;C=ch)  |
	| Switzerland                              |                            |
	|                                                                       |
	|  Try the ExPASy Molecular Biology Server at http://expasy.hcuge.ch/   |
	-------------------------------------------------------------------------


Please describe your problems in more detail. Have you edited the make
file and done 'make' and 'make install'? 

It would be no use to send you any binaries. The code consists of a 
number of separate scripts (and one main parser). The scripts also
use code made by others (fmbatch, gs, pbmplus, etc.), which must be
installed on your system.

Jon.



From fielding@simplon.ICS.UCI.EDU  Thu Feb 10 11:40:28 1994 --100
Message-Id: <9402100236.aa15415@paris.ics.uci.edu>
Date: Thu, 10 Feb 1994 11:40:28 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: proposals for log file format changes 

Kevin Hughes said:

> 	More or less following RFC 822, then:
> 
> host rfc931 authuser [DD/Mon/YYYY:hh:mm:ss UT[+/-]HHMM] "request" ddd bbbb
> 
> 	How's that?

RFC 822 expects date fields to be separated by spaces.

Now that people are talking about including the Referer: field
(a great idea but a lot of text per log entry), I think the original idea
of a configurable log is now preferable in order to save some people's disks.
However, I would recommend a limited set of options rather than the
fully formattable sscanf codes that Ari first mentioned.
[I think Kevin suggested option names as well, but I didn't save that message].

How about:

     host     = machine.sub.dom.ain
     rfc931   = whatever_it_returns
     fromuser = whatever_From:_gives          (stripped of comments)
     authuser = whatever_Authorization:_gives (stripped of password)
     authpass = whatever_Authorization:_gives (stripped of user - IS THIS SAFE?)
     charge   = whatever_ChargeTo:_gives
     locdate  = [DD Mon YYYY hh:mm:ss]
     gmtdate  = [DD Mon YYYY hh:mm:ss GMT]
     tzdate   = [DD Mon YYYY hh:mm:ss +HHMM]
     request  = "first line from HTTP request"
     response = ddd   (3 digit HTTP response code)
     bytes    = bbbbb (free-formatted number of bytes transmitted)
     referer  = the_referer's_URI

As specified in HTTP2, the From: field looks like an e-mail address.
Should the entire address be logged or just the username?  If only username,
how does the server parse it given the wide variety of address formats?

The next question is: should the order be configurable as well?
If not, then the format can be specified by simple boolean options.
However, I'll bet people will want it configurable.  In that case,
how should it be specified?  A list is probably best, placed in a
server config file (e.g. NCSA's srm.conf).  E.g.:

(host,rfc931,fromuser,authuser,charge,gmtdate,request,response,bytes,referer)

Any field which is requested but is not defined for a particular log entry
should be logged as a single dash "-".

Another question is how should the fields be separated in the log?
Current practice uses a space, but perhaps a comma is better.  Any field
which could possibly include the delimiter would have to be surrounded
by some form of brackets (as is the date and request fields above).

    Some examples:

(host,locdate,request,response,bytes)

    would log something like:

simplon.ics.uci.edu [10 Feb 1994 01:18:51] "GET /ICShome.html HTTP/1.0" 200 4262


(gmtdate,response,host,fromuser,request,bytes,referer)

    would log something like:

[10 Feb 1994 09:18:51 GMT] 200 simplon.ics.uci.edu fielding@ics.uci.edu "GET /ICShome.html HTTP/1.0" 4262 http://www.ncsa.uiuc.edu/SDG/Software/Mosaic/Docs/whats-new.html


    My primary concern about this is the extra work it will require of
the server authors.  Provided that the fields are well defined and can
be parsed unambiguously, there should be no problem for log analyzers.
However, I think it would be much easier on the server authors if the field
order is fixed and simple options defined, e.g.:

LogDate LOCAL       (or GMT or TIMEZONE)
LogReferer NO       (or YES)

I think that decision should be left up to the server authors.

Comments?

...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From germuska@antioch.acns.nwu.edu  Thu Feb 10 16:25:13 1994 --100
Message-Id: <9402101521.AA09436@antioch.acns.nwu.edu>
Date: Thu, 10 Feb 1994 16:25:13 --100
From: germuska@antioch.acns.nwu.edu (Joe Germuska)
Subject: Re: Authoring tools/knowledge

Bill Hefley wrote:
> 
> I'm trying to put together a list of what documents/tools someone who is going
> to be preparing HTML-coded content should be aware of. Sort of a minimalist
> manual for content developers.
> 

I have a very rough start towards that in the following two docs, left
in HTML hopefully for your convenience...
-----http://www.acns.nwu.edu/www-resources.html------
<TITLE>Index of Information about WWW/HTML</TITLE>

This is just a real quick start... It will no doubt grow, and pointers to other good resources are welcomed! <P>

<UL>
<LI> <A HREF="http://www.acns.nwu.edu/www-faq.html">The WWW FAQ</A>
<LI> <A HREF="http://info.cern.ch/hypertext/WWW/TheProject.html">CERN's Intro the the WWW Project</A>
<LI> <A HREF="http://www.ncsa.uiuc.edu/demoweb/html-primer.html">Marc Andreesen's Primer to composing HTML</A> (<EM>very</EM> useful!)
<LI> <A HREF="www-authoring.html">An index of authoring tools and filters for HTML</A>
<LI> <A NAME=10 HREF="http://info.cern.ch/hypertext/WWW/MarkUp/HTML.html">The Actual Hypertext Markup Language (HTML) RFC</A>
<LI> <A HREF="http://hoohoo.ncsa.uiuc.edu/docs/Overview.html">The NCSA httpd home page</A>, for information about the server running on <code>www.acns.nwu.edu</code>
<LI> The <A HREF="http://info.cern.ch/hypertext/WWW/MarkUp/ISOlat1.html">List of ISO Latin 1 character entities</A>, because I can never find it... 
</UL>
 <P>
<A HREF="http://www.acns.nwu.edu/index.html">Back to the NU Home Page</A>

----- begin http://www.acns.nwu.edu/www-authoring.html -------
<TITLE>HTML Authoring</TITLE>
<H1>HTML Authoring</H1>
Here's my shot at presenting a thorough list of tools for the composition of HTML; the information on filters is largely derived from a list developed by Rich Brandwein (rhb@hotsand.att.com) and Mike Sendall (sendall@cernvm.cern.ch).  If you are new to writing HTML, I strongly recommend beginning by reading Marc Andreesen's <A HREF="http://www.ncsa.uiuc.edu/demoweb/html-primer.html">"A Beginners Guide to HTML"</A>.

<H2>Tools for Composing HTML</H2>
<H3>Unix:</H3>
<DL>
<DT> <A HREF="ftp://ftp.ncsa.uiuc.edu/outgoing/marca/">html-mode.el</A>
<DD> Written by XMosaic developer Marc Andreesen, this EMACS library provides basic automation of HTML tagging.  It is not glamorous, but it is functional (this document was composed using html-mode.el) 
</DL>
<H3>Macintosh:</H3>
<DL>
<DT> <A HREF="ftp://world.std.com/pub/bbedit">BBEdit</A> and <A HREF="ftp://ftp.uji.es/pub/mac/util/">HTML Extensions</A>
<DD> The Web Project at Universitat Jaume I in Spain has developed HTML extensions to the popular Mac editor "BBEdit".  The extensions are stored at ftp.uji.es (along with BBEdit).  The canonical location for the original BBEdit is world.std.com.
</DL>
<H3>Windows:</H3>
<DL>
<DT> <A HREF="ftp://ftp.gatech.edu/pub/www/">MS Word for Windows Macros</A>
<DD> Also known as <TT>gw_html.zip</TT>, this is an untested (by me) package of Macros to bring HTML editing to MS Word for Windows.  <A HREF="http://www.gatech.edu/word_html/release.htm">More details</A> can be found at <A HREF="http://www.gatech.edu/">Georgia Tech's Web Server</A>
</DL>

<H2>Tools for Converting Other Document Formats to HTML</H2>
As noted above, much of this information came from a list at info.cern.ch.  Until I'm comfortable with the stability and accuracy of this list, I'll include a <A HREF="http://info.cern.ch/hypertext/WWW/Tools/Filters.html">pointer to the original.</A>
<H3>Word Processor Filters</H3>
<DL>
<DT> <A HREF="ftp://ftp.ebt.com/pub/outgoing/rainbow/">General Translator Archive (Rainbow)</A>
<DD> Electronic Book Technologies is providing a set of translators to move from proprietary word processor formats into various SGML formats, including HTML.
<DT> MS Word RTF (Rich Text Format)
<DD> <DL>
<DT> <A HREF="ftp://ftp.cray.com/src/cjh/RTF/rtftohtml_overview.html">rtftohtml</A>
<DD> This site includes source and Macintosh and Sun binaries to convert Microsoft's RTF format to HTML, and is customizable in several ways.
<DT> <A HREF="ftp://oac.hsc.uth.tmc.edu/public/unix/WWW/">rtf2html</A>
<DD> An older utility written by <A HREF="http://oac.hsc.uth.tmc.edu/bios/chuck.html">Chuck Shotton</A> (author of MacHTTP).  He admits it's an <A HREF="http://info.cern.ch/hypertext/WWW/Tools/HTMLGeneration/rtf2html.html">"Ugly Hack"</A>
</DL>
<DT> <A HREF="file://journal.biology.carleton.ca/pub/software/">WordPerfect 5.1: wp2X</A>
<DD> "WP 5.1 to anything," including a configuration file for WP2HTML.  Written by Michael Richardson of Carleton University

<DT> FrameMaker: 
<DD> <DL>
<DT> <A HREF="file://bang.nta.no/pub/">fm2html</A>
<DD> <A HREF="http://info.cern.ch/hypertext/WWW/Tools/fm2html.html">Developed</A> at Norwegian Telecom Research by Jon von Tetzchner Stephenson (jon.tetchner@nta.no).  
<DT> mif2html
<DD> Developed by Bertrand Rousseau at CERN, this converter is used to present the <A HREF="http://www1.cern.ch/Adamo/guide/Document.html">on-line documentation of the ADAMO programming system</A>  However, the source is not available anywhere I've found!
</DL>

<DT> <A HREF="ftp://ftp.cranfield.ac.uk/source/info-tools/WWW/decw2html">DECWrite: decw2html</A>
<DD> By Peter Lister, Cranfield University, Bedfordshire, UK (p.lister@cranfield.ac.uk)
<DT> <A HREF="http://cbl.leeds.ac.uk/nikos/tex2html/doc/latex2html/latex2html.html">LaTeX2html</A>
<DD> Untested by me...
</DL>

<BLOCKQUOTE>
The manner in which I have chosen to present these materials may not be
the manner in which you will decide to construct the internet. Yet you
will find it useful to glance at. It will help you think about what you
are doing. I have for five years been constantly surprising myself, as I
adventured in this task, and iterated and iterated the materials I am
collecting. Surprised myself, because under my hands the materials have
been resistive, and have required me to follow a shape which I would never
have anticipated. It is not a shape which I have imposed, it is a data
shape which is reluctant to be "created," but which is in the process of
allowing itself to be "discovered."
</BLOCKQUOTE>
<ADDRESS>Austin Meredith<BR>
Creator, Stack of the Artist of Kouroo</ADDRESS>	
-- 
joe germuska * j-germuska@nwu.edu * www * res hall net * instruct tech
      academic computing & network services * northwestern univ
"Don't step on my funk..."



From weh@SEI.CMU.EDU  Thu Feb 10 15:10:26 1994 --100
Message-Id: <9402101401.AA14137@ts5a.sei.cmu.edu>
Date: Thu, 10 Feb 1994 15:10:26 --100
From: weh@SEI.CMU.EDU (Bill Hefley)
Subject: Authoring tools/knowledge

I'm trying to put together a list of what documents/tools someone who is going
to be preparing HTML-coded content should be aware of. Sort of a minimalist
manual for content developers.

I'd like to get your suggestions.  E-mail pointers (URLs, citations, what have
you) to me or post to the list. I'll be collating them, and will forward a
collated list back, once the responses seem to have settled down.

Regards,
	bill hefley
	software engineering institute
	weh@sei.cmu.edu




From sanders@BSDI.COM  Thu Feb 10 17:09:03 1994 --100
Message-Id: <199402101605.KAA03018@austin.BSDI.COM>
Date: Thu, 10 Feb 1994 17:09:03 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: proposals for log file format changes 

"Roy T. Fielding" writes:
> Now that people are talking about including the Referer: field
> (a great idea but a lot of text per log entry), I think the original idea
I plan on hashing the data and just including a reference in the log file.
Considering that the number of documents that point to your server should
be smaller than the number of requests this scheme should save you a lot
of space.

--sanders



From nholtz@civeng.carleton.ca  Fri Feb 11 08:32:54 1994 --100
Message-Id: <9402102119.AA26915@zonker.cascade.carleton.ca>
Date: Fri, 11 Feb 1994 08:32:54 --100
From: nholtz@civeng.carleton.ca (Neal Holtz)
Subject: Identifying scripts by file extension?

I think this was beat to death a month or two ago, but ...

Currently, the only accepted way to designate a script gateway in a
URL is to use some magic prefix such as "/cgi-bin/myscript/" to the
the path.  Unfortunately, this co-exists very poorly with the rules
for forming full URL's from relative ones (see below).  What is the
general opinion on using the file extension to identify gateways or
scripts?

I have a small patch to NCSA HTTPD 1.1 which allows srm.conf entries
like:

	ScriptExtension .xyz ZZZ

Then, path names that end in ".xyz" will be handled as though they
had "ZZZ" prepended, provided:
	a) the path hasn't been aliased, and
	b) the path doesn't already start with a script alias, and
	c) The path, after prepending "ZZZ", starts with a script alias.

Therefore, with an entry like

	ScriptExtension .qml /htbin/tutorial

a path like

	/~nholtz/91.111/q1.qml

would be handled as though it were

	/htbin/tutorial/~nholtz/91.111/q1.qml

Anybody else think this is desireable?

--------------------------

BACKGOUND (long) (for those who aren't convinced)

I am starting to develop a large document (set of lecture notes +
study guide) that has quite a mixture of node types.  Many of the
HTML files require some filtering before being delivered to the
client: one gateway filter handles delivery of tutorial style
questions and the responses to them; another is sort of an "outline
processor" that reveals more of the text, in line, when certain links
are selected.

The basic problem is in the relative URL's that I would like to use.
Having the script identifier at the front of the path name means that
I have to use absolute URL's a lot, and that is going to be a real
pain in a few weeks when I want to move things around.  Identifying
the script in the last component of a path means that I can use
relative URL's everywhere, which makes life a lot easier.

That script identifier is a type specification of a node (actually,
it is both a type specification and instructions on how to convert
the type of the node into one of the standard types).  It has been
decided, long ago, that type information will not be carried by the
underlying file system.  It also seems to be undesireable to have the
file contents themselves reveal the type information.  Therefore that
type specification has to be part of the name (URL).

If the type information is encoded in the front portions of path
names, and if the rules for generating full URL's from relative ones
call for replacement of the rear portions of the path name, then it 
is impossible to use a relative URL to link to a node of a different
type (because the replacement rules wouldn't allow replacement of the
type information).

But of course, the rear portion (extension) is already used to convey
some type information (".gif" for certain types of images, etc).  It
is only gateway scripts that are currently forbidden to use this
mechanism.

What I am proposing is simply to allow the identification of scripts
to be consistant with the identification of other types of data - to
be able to use the file extension as that mechanism.

---
Prof. Neal Holtz,  Dept. of Civil Eng.,  Carleton University,  Ottawa,  Canada
Internet: nholtz@civeng.carleton.ca  Ph: (613)788-2600x5797 Fax: (613)788-3951



From nholtz@civeng.carleton.ca  Fri Feb 11 08:36:36 1994 --100
Message-Id: <9402102135.AA26947@zonker.cascade.carleton.ca>
Date: Fri, 11 Feb 1994 08:36:36 --100
From: nholtz@civeng.carleton.ca (Neal Holtz)
Subject: Server control over history?

Is anyone else interested in having the server have some control over
a clients history list?

I am starting to get a collection of HTML files that require some
filtering before they get sent to the client.  One good example is
a filter that handles tutorial-style questions.  Sometimes a
question will have hints that get revealed, in-line, only when a
reader requests them.  In these cases, you probably do not want to
see these nodes put on the history list.

In cases like these, its the server that could have something to say 
about it, advising the client not to keep a history for certain
nodes (or variations, really).
 

-- 
Prof. Neal Holtz,  Dept. of Civil Eng.,  Carleton University,  Ottawa,  Canada
Internet: nholtz@civeng.carleton.ca  Ph: (613)788-2600x5797 Fax: (613)788-3951



From DLCROSS@ucs.indiana.edu  Fri Feb 11 08:39:01 1994 --100
Message-Id: <9402102257.AA27824@dxmint.cern.ch>
Date: Fri, 11 Feb 1994 08:39:01 --100
From: DLCROSS@ucs.indiana.edu (DLCROSS@ucs.indiana.edu)
Subject: URL Redirection mentioned in WinMosaic 1.0...What is it?

The release notes for WinMosaic say, "now supports URL redirection..."
Can  anyone define the convention for me in genera, and what WInMosaic does in particular?

Is there any more complete documentation for this program than the bug fixes (i.e., sources or a manual?)
Alan
Canon

dlcross@ucs.indiana.edu



From (no email)  Fri Feb 11 08:41:07 1994 --100
Message-Id: <9402102308.AA01395@eies.njit.edu>
Date: Fri, 11 Feb 1994 08:41:07 --100
From: (no email) ((no name))
Subject: 

The 'Referer:' field provides valuable cross reference information-- for
many purposes--   Let's provide it!



From omy@San-Jose.ate.slb.com  Fri Feb 11 08:44:15 1994 --100
Message-Id: <9402102300.AA11924@San-Jose.ate.slb.com>
Date: Fri, 11 Feb 1994 08:44:15 --100
From: omy@San-Jose.ate.slb.com (Omy Ronquillo)
Subject: MAC HTTP software


	Hi!

	Where can I find a MAC HTTP server software?

		Thanks. Omy



From altis@ibeam.jf.intel.com  Fri Feb 11 08:46:52 1994 --100
Message-Id: <m0pUl6D-00042lC@ibeam.intel.com>
Date: Fri, 11 Feb 1994 08:46:52 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: libWWW 2.14 diffs for proxy support

Below is the message Lou Montulli put together to describe the proxy
support for Lynx 2-2. These changes were rolled into the various flavors of
Mosaic (X Mosaic 2.2 has already been released) and libWWW 2.15 which the
CERN folks (Ari and TimBL) are currently working on. Clients based on
libWWW should be able to utilize the changes described below in order to
support a proxy gateway. I'm currently working on actual HTTP transaction
examples to further describe the proxy service.

The main thing to understand about the proxy gateway is that instead of a
partial URL being sent to the HTTP server, which is what we do today when a
client talks directly to an HTTP server, a client must send a full URL
(http://..., gopher://..., ftp://...) to the proxy gateway server, the rest
of the HTTP message is the same. For gopher and ftp, the proxy gateway
server will return the data encapsulated as a MIME content type to the
client like a normal HTTP message. HTTP MIME content types are returned for
all URL requests, regardless of the protocol type of the URL. FTP
directories, Gopher directories, etc. are returned as text/html.

ka
---

From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Firewall gateway solution, exodus
Date: Tue, 1 Feb 94 19:05:06 CST

Per our earlier discussions about using <protocal>_proxy as the
environment variables I have some new diffs:


 This solution will only work with HTTP gateways. These diffs apply to
libWWW 2.14

falcon.cc.ukans.edu> diff ../WWW2.14/Lib*/I*/HTAccess.c WWW/L*/I*/HTAccess.c
54a55
> PUBLIC BOOL using_proxy = NO; /* are we using a proxy gateway? */
155c156,158
<       char * gateway_parameter, *gateway;
---
>       char * gateway_parameter, *gateway, *proxy;
>
>       /* search for gateways */
161a165,169
>
>       /* search for proxy servers */
>       strcpy(gateway_parameter, access);
>         strcat(gateway_parameter, "_proxy");
>       proxy = (char *)getenv(gateway_parameter);
162a171,175
>
>       if(TRACE && gateway)
>           fprintf(stderr,"Gateway found: %s\n",gateway);
>       if(TRACE && proxy)
>           fprintf(stderr,"proxy server found: %s\n",proxy);
169c182,199
<       if (gateway) {
---
>
>       /* make sure the using_proxy variable is false */
>       using_proxy = NO;
>
>       /* proxy servers have precedence over gateway servers */
>       if(proxy) {
>           char * gatewayed=0;
>             StrAllocCopy(gatewayed,proxy);
>             StrAllocCat(gatewayed,addr);
>             using_proxy = YES;
>             HTAnchor_setPhysical(anchor, gatewayed);
>           free(gatewayed);
>           free(access);
>
>           access =  HTParse(HTAnchor_physical(anchor),
>               "http:", PARSE_ACCESS);
>
>       } else if (gateway) {


 I replaced the libWWW version of HTTP.c with
 Xmosaic's version and started changing, so rather than giving
 diffs I'll just describe the changes. :)

 add this to define the global variable:
 extern BOOL using_proxy;    /* are we using an HTTP proxy gateway? */

 add this to remove the first character of the resulting path.
 This is done because the resulting URL of the gateway change
 in HTAccess causes something like these to appear:
 http://gateway.host/gopher://gopher.host/path
 http://gateway.host/wais://wais.host/path
 http://gateway.host/http://http.host/path

 The protocol and host is stripped by the time we reach
 the variable p1 below, by the statement:
 char * p1 = HTParse(arg, "", PARSE_PATH|PARSE_PUNCTUATION);
 so we end up with:
 /gopher://gopher.host/path
 /wais://wais.host/path
 /http://http.host/path

 and we want to send:
 gopher://gopher.host/path
 wais://wais.host/path
 http://http.host/path

     /* if we are using a proxy gateway don't copy in the first slash
      * of say: /gopher://a;lkdjfl;ajdf;lkj/;aldk/adflj
      * so that just gohper://.... is sent.
      */
     if(using_proxy)
         strcat(command, p1+1);
     else
         strcat(command, p1);


That should do it.  Now when a gateway definition is used that
gateway should be sent the entire URL of the requested document.

:lou
--
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************





From masinter@parc.xerox.com  Fri Feb 11 08:49:19 1994 --100
Message-Id: <94Feb10.160855pst.2732@golden.parc.xerox.com>
Date: Fri, 11 Feb 1994 08:49:19 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: proxy gateway service announcement/testing

I sent Kevin Altis this message (about a discussion of Mosaic proxy
gateways):

>Inside Xerox, we'd like to have a different kind of proxy service,
>where the decision of whether to go direct or try the proxy server
>might depend on the host address as well as the protocol type.
>
>Any suggestions? So far, I've been hacking the sources instead.

and he replied:

> Actually, Lou, NCSA guys, Ari, etc., and I have discussed this. The general
> idea is to do a string compare against a host.domain list that you don't
> need to proxy. This kind of feature would be added in the next revision of
> the clients if we can work out all the other issues. This is a good
> discussion topic for www-talk since it would be nice for the clients to do
> this in a standard way.

> ka

So, in response, I'm sending this out to www-talk:

At least for us, string compare against host.domain list isn't as good
as a binary compare against an include/exclude list of partial address
masks.

The procedure we've been using is to have a table of "no proxy partial
addresses" and "exceptions". Most internal hosts can route directly to
our class A (net 13) net, as well as a few hosts that are `inside the
firewall' but are on a separate class C net.

Right now, the tables are wired in, but it would be great if they were
available as a loadable resource or configuration file.



From daeron@geom.umn.edu  Fri Feb 11 09:52:59 1994 --100
Message-Id: <9402110850.AA25141@freeabel.geom.umn.edu>
Date: Fri, 11 Feb 1994 09:52:59 --100
From: daeron@geom.umn.edu (daeron@geom.umn.edu)
Subject: ANNOUNCING: The Geometry Center's Interactive On-Line Gallery

  The Geometry Center proudly presents a gallery of five new
interactive World Wide Web applications, designed for visualizing
and experimenting with geometric ideas in 2, 3, and even higher
dimensions.

These applications demonstrate the innovative use of fill-out forms
to maximize interactivity with virtual objects. We find this
technology exciting because it allows the distribution of powerful
visualization tools to a much wider audience of remote users.

So, if you're looking for something exciting and new to explore,
just web right over to the following URL and check us out!

   http://www.geom.umn.edu/apps/gallery.html

Here is a quick overview of the contents of the exhibit:

QuasiTiler: by Eugenio Durand

  Try your hand at creating beautiful nonperiodic tilings of the plane,
  including the famous Penrose tilings.  QuasiTiler also helps you
  visualize the multidimensional lattices used to generate these tilings.

Cyberview: by Paul Burchard

  An interactive 3D object viewer for the World Wide Web. You can pick
  an object out of our predefined library, or learn about the OOGL
  format and define your own objects.

Lafite: by Adam Deaton

  Learn about the symmetry groups of the hyperbolic plane through
  Escher-like patterns, which are created by replicating a motif
  according to the symmetry group you choose.

Teichmuller Navigator: by Deva van der Werf

  Explore the space of all different angle geometries on a surface
  of genus two.  You can navigate through the space by moving
  vertices of a tiling of the hyperbolic plane by octagons.

Unifweb: by Carlos O'Ryan

  Discover and visualize Riemann surfaces having a specified group of
  symmetries.  Different families of surfaces can be constructed for
  the same symmetry group by choosing different generators and relations.

If you have any questions about this interactive exhibit, please send mail
to webmaster@geom.umn.edu, burchard@geom.umn.edu or daeron@geom.umn.edu.

Enjoy,

Daeron Meyer
daeron@geom.umn.edu






From rik@rdt.monash.edu.au  Fri Feb 11 08:54:40 1994 --100
Message-Id: <199402110144.MAA04516@alquist.rdt.monash.edu.au>
Date: Fri, 11 Feb 1994 08:54:40 --100
From: rik@rdt.monash.edu.au (Rik Harris)
Subject: Re: proxy gateway service announcement/testing 

> I'm pleased to announce the availability of a new application level proxy
> gateway service for the WWW. This is an adaptation of Tim Berners-Lee's
> gateway code used in libWWW. Lou Montulli, Ari Luotonen, and I (Kevin
> Altis) have been working together for the last month or so on this problem
> and we now have something for you to try; the NCSA folks are working the
> changes into their code as well. The proxy supports http, gopher, ftp,
> wais, and news. Full HTTP 1.0 methods are supported including POST, PUT and
> authentication.

Has this project involved any discussion wrt caching?  A proxy server
would be the ideal place to cache documents that don't require
authentication.  I realise there are some types of documents that
shouldn't be cached, but for a test system, a config file could
specify that 'http://www.some.host/auto/*' should not be cached.

I know the topic has been brought up on the list before, but is anyone
actually working on it?  Now that we have some clients that have been
modified for proxy support, perhaps the caching discussion could be
renewed?

rik.
--
Rik Harris - rik.harris@vifp.monash.edu.au              || Systems Programmer
+61 3 560-3265 (AH) +61 3 90-53227 (BH)                 || and Administrator
Fac. of Computing & Info.Tech., Monash Uni, Australia   || Vic. Institute of
http://www.vifp.monash.edu.au/people/rik.html           || Forensic Pathology



From altis@ibeam.jf.intel.com  Fri Feb 11 08:52:05 1994 --100
Message-Id: <m0pUmoN-00042lC@ibeam.intel.com>
Date: Fri, 11 Feb 1994 08:52:05 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: proxy server GET examples

Below are a number of requests showing what a WWW client sends to a proxy
gateway server, and what the server returns. There are examples for Gopher,
FTP, and HTTP. Notice that in all cases, the client actually speaks HTTP
with the proxy gateway server and uses the full URL in its GET method
request. The example proxy server is the cern_httpd 2.15 daemon. The client
is Lynx 2-2 with the modification that I used an Accept: */* rather than
the list of Accepts that Lynx normally sends. The client request is
separated from the server reply by a blank line. I've used "...data..." to
substitute for actual content returned by the proxy gateway server. I hope
this clarifies some of the confusion over what the client/server
conversation looks like.

ka

----A Gopher request for a gif file
GET gopher://host.domain/g9/test.gif HTTP/1.0
Accept: */*
User-Agent: Lynx/2.2  libwww/2.14
From: altis@ibeam.intel.com

HTTP/1.0 200 AA check bypassed (gatewaying)
MIME-Version: 1.0
Server: CERN/2.15
Content-Type: image/gif

..data...

----A Gopher request for an html file
GET gopher://host.domain/h0/test.html HTTP/1.0
Accept: */*
User-Agent: Lynx/2.2  libwww/2.14
From: altis@ibeam.intel.com

HTTP/1.0 200 AA check bypassed (gatewaying)
MIME-Version: 1.0
Server: CERN/2.15
Content-Type: text/html

..data...


----A Gopher request for a text file
GET gopher://host.domain/00/test.txt HTTP/1.0
Accept: */*
User-Agent: Lynx/2.2  libwww/2.14
From: altis@ibeam.intel.com

HTTP/1.0 200 AA check bypassed (gatewaying)
MIME-Version: 1.0
Server: CERN/2.15
Content-Type: text/plain

..data...

----An ftp request for a directory
GET ftp://host.domain/ HTTP/1.0
Accept: */*
User-Agent: Lynx/2.2  libwww/2.14
From: altis@ibeam.intel.com

HTTP/1.0 200 AA check bypassed (gatewaying)
MIME-Version: 1.0
Server: CERN/2.15
Content-Type: text/html

..data...

----An ftp request for a binary file
GET ftp://host.domain/test.au HTTP/1.0
Accept: */*
User-Agent: Lynx/2.2  libwww/2.14
From: altis@ibeam.intel.com

HTTP/1.0 200 AA check bypassed (gatewaying)
MIME-Version: 1.0
Server: CERN/2.15
Content-Type: audio/basic

..data...

----another ftp request for a binary file
GET ftp://host.domain/test.Z HTTP/1.0
Accept: */*
User-Agent: Lynx/2.2  libwww/2.14
From: altis@ibeam.intel.com

HTTP/1.0 200 AA check bypassed (gatewaying)
MIME-Version: 1.0
Server: CERN/2.15
Content-Type: application/x-compressed

..data...

----A proxied HTTP example
GET http://host.domain/test.au HTTP/1.0
Accept: */*
User-Agent: Lynx/2.2  libwww/2.14
From: altis@ibeam.intel.com

HTTP/1.0 200 OK
Date: Thursday, 10-Feb-94 22:32:29 GMT
Server: NCSA/1.1
MIME-version: 1.0
Content-type: audio/basic
Last-modified: Thursday, 10-Feb-94 22:22:49 GMT
Content-length: 27

..data...





From appel@cih.hcuge.ch  Tue Feb 15 10:56:45 1994 --100
Message-Id: <1603*/S=appel/OU=cih/O=hcuge/PRMD=switch/ADMD=arcom/C=ch/@MHS>
Date: Tue, 15 Feb 1994 10:56:45 --100
From: appel@cih.hcuge.ch (Ron D. Appel)
Subject: RE: proxy service


What is a proxy service?


-------------------------------------------------------------------------
| Ron D. Appel                             | Tel.:   (+41 22) 372 6264  |
| Hopital Cantonal Universitaire de Geneve | Fax.:   (+41 22) 372 6198  |
| Centre d'Informatique Hospitaliere       | e-mail: appel@cih.hcuge.ch |
| 24, rue Micheli-du-Crest                 |   (S=appel;OU=cih;O=hcuge; |
| CH-1211 Geneve 14                        |    P=switch;A=arcom;C=ch)  |
| Switzerland                              |                            |
|                                                                       |
|  Try the ExPASy Molecular Biology Server at http://expasy.hcuge.ch/   |
-------------------------------------------------------------------------



From luotonen@ptsun00.cern.ch  Tue Feb 15 11:04:21 1994 --100
Message-Id: <9402110937.AA17986@ptsun03.cern.ch>
Date: Tue, 15 Feb 1994 11:04:21 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: proxy gateway service announcement/testing


> [Caching on proxy]
> I know the topic has been brought up on the list before, but is anyone
> actually working on it?

Yes, we are.

--
Ari Luotonen		 |
World-Wide Web Project	 |
CERN			 | phone: +41 22 767 8583
CH - 1211 Geneve 23	 | email: luotonen@dxcern.cern.ch



From dsr@hplb.hpl.hp.com  Tue Feb 15 11:11:37 1994 --100
Message-Id: <9402111004.AA03928@manuel.hpl.hp.com>
Date: Tue, 15 Feb 1994 11:11:37 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Server control over history?

Neal Holtz writes:

> Is anyone else interested in having the server have some control over
> a clients history list?
> ... In these cases, you probably do not want to see these nodes put
> on the history list.

I have been worrying about this too. For instance if you follow a thread
from a main document, you will often be offered the chance to click an icon
to return to the main document. This is clearly an alternative to
backtracking through the history. It seems to me that clicking the icon
should be clean up the history so that it is directly equivalent to
backtracking to the main document. This would avoid the possibility of
confusion in the user's mind as to the effects of either course of action.

For this to work we need a hint to the browser to pop the history back to
the point just before the linked document *last* appeared. This could be
done via an attribute on the <A> element.

Another common case occurs with fill-out forms. Keeping a history of
the successive stages of filling out a form, and the updates to that
form by the server is of dubious value (it would only make sense in the
context of a multi-level UNDO mechanism).  So it makes sense to flag
intermediate steps as not to be placed on the history stack. This could
be done by an element in the document's head or an HTTP header (the two
are formally equivalent).

So how about:

        a) adding an attribute to the <A> element to rollback the
           history stack to the node before the *last* occurrence of
           the linked node, e.g.

           <A HREF="main_doc.html" ROLLBACK>Return to Home page</A>

        b) Defining a new HTTP header for history hints which servers
           can generate based on META elements in document heads, e.g.

           <META NAME="History" VALUE="discard">

For (b) clients would get the HTTP header

           History: discard

Other values such as "rollback" may be useful. This seems preferable
to adding yet another element to HTML+. Let me know what you think,
and I will ammend the HTML+ spec.

Dave Raggett



From decoux@moulon.inra.fr  Tue Feb 15 11:18:40 1994 --100
Message-Id: <9402111628.AA18116@moulon.moulon.inra.fr>
Date: Tue, 15 Feb 1994 11:18:40 --100
From: decoux@moulon.inra.fr (ts)
Subject: CERN Daemon 2.15beta and CGI script



 Cern Daemon 2.15beta support "nph-" CGI script ?

 I have not seen any reference to "nph" in  :

    http://info.cern.ch/hypertext/WWW/Daemon/User/CGIBinDoc.html

> Results From Scripts
> 
> Scripts return their results either outputting a document to their standard output, or by
> outputting the location of the result document (either a full URL or a local virtual
> path). 
> 
> Outputting a Document
> 
> Script result must begin with a Content-Type: line giving the document content
> type, followed by an empty line. The actual document follows the empty line.
> Example: 
> 
>         Content-Type: text/html
> 
>         <HEAD>
>         <TITLE>Script test>
>         </HEAD>
>         <BODY>
>         <H1>My First Virtual Document</H1>
>         ....
>         </BODY>
> 
> Giving Document Location
> 
> If the script wants to return an existing document (local or remote), it can give a 
> Location: header followed by an empty line: Example: 
> 
     .................................


Guy Decoux



From Jon.Tetzchner@nta.no  Tue Feb 15 11:24:13 1994 --100
Message-Id: <199402111634.AA03910@bang.nta.no>
Date: Tue, 15 Feb 1994 11:24:13 --100
From: Jon.Tetzchner@nta.no (Jon.Tetzchner@nta.no)
Subject: fm2html version 0.8.4 now available

Fm2html version 0.8.4 is now available. Get it from bang.nta.no. It is available
in the pub directory.

Below is some information about the filter and a list of changes from the
last version.

Enjoy!

Jon.

----------------------------
Jon Stephenson von Tetzchner	 
Norwegian Telecom Research  
      jons@hal.nta.no
----------------------------

-------------------------------------------------------------------------------


		Frame 2 Html filter
	====================================================

This filter has been made as part of my work at Norwegian Telecom Research.
All rights to the filter belong to Norwegian Telecom Research.
The filter can be modified as long as the top lines of each source file
indicating the original source of the filter is kept. We would also
appriciate being sent information about bugs, bug fixes, etc. 
The code can not be included in a commercial product.

The current version of the filter:

	o Handles frame files and books.
	o Is customable, through a tags file mapping frame tags to 
		logical tags used by the filter.
	o All frame X-refs become html links.
	o An index is automatically generated based on chapter
		headings in the frame documents.
	o The file structure of the frame document is kept in the
		html document. Single frame files become single
		html files. Frame books become multiple html files,
		one html file for each frame file. FrameMaker generated
		files are removed.
	o Graphics and maths are separated to files, which are then 
		translated to postscript and ultimately gif.
	o Tables are handled through the <pre> html tag.
	o Italics and bold parts of paragraphs are handled.

Problems and bugs:

	o Characters in frame not in html, including Greek character.
	o Documents that are divided into sections using the FrameMaker
		'frame'.
	o FrameMaker documents which do not use tags and operate on
		text directly.
	o Very large pictures are not handled well (bigger than an A4).
	o Only anchored frames are handled well. Other graphics may be lost.

This program was made for internal use only. I will try to make it portable,
but it is not unlikely that a few paths have to be changed.

Acknowledgement:

	I would like to thank Geir Ivarsoey of Norwegian Telecom Research,
	Duncan Fraser and Randy Roesler of MacDonald Detwiller, Daniel K.Schneider of
	TECFA (Educational Technologies and Learning), 
	Jan van der Steen of Computer Lab, Amsterdam,
	Stephen Martin of LSI Logic Corporation of Canada,  and 
	Rob Kooper and  Bob Jackson
	for their assistance in improving this program. 

Availability:

	ftp bang.nta.no:pub/fm2html.v.0.n.m.tar.Z.


Disclaimer:

	Use this program at your own risk.


----------------------------------------------------------------------------


Below is a list of environment variables that can be set:

	FRAME2HTMLPRINTFILE (e.g. /local/lib/mif2gif.printsettings)
	FRAME2HTMLTAGSFILE (e.g. /local/lib/mif2html.tags)
	DOCUMENTLANGUAGE (US OR NO, US is default. This results in "Innhold" being
			  used instead of "Table of Contents", etc.)

Useage: 
	fm2html frame-file [title] [author]
	Fm2html can take 3 parameters.
	The first specifies the name of the file or book to translate.
	The second optional parameter is the title of the file.
	The third optional parameter is the author of the file.

	mif2html can also be called instead of fm2html if you have
	transformed to mif already. mif2html takes the same parameters
	as fm2html (except the first that must be a mif file).

	fm2mif can be used to translate frame files to mif.
	It takes as its only parameter the name of the frame
	file.

Warning:
	The filter operates in the current version on
	your local directory. It uses a lot of memory and will
	very easily run out of space, unless you give it a lot
	of swap space.

Some comments:

	The tags files describe the connection between FrameMaker
	tags and the internal tags of the filter. These internal
	tags are mapped to HTML tags. When the filter program is 
	run, it checks whether there exists a file called <filename>.tags,
	where <filename> is the name of the file or book to 
	translate. If no such file is found, the filter checks whether
	the FRAME2HTMLTAGSFILE environment variable is set. If
	it is, the corresponding file is used. If the environment
	variable is not set, then the default file is used. The
	default file is $LIBDIR/mif2html.tags.

	The tags file uses TABs to separate internal tags and
	FrameMaker tags. To see a list of the internal tags,
	have a look at the tags file.

	You may have to change the line in the start of the perl
	scripts identifying the location of the perl interpreter.
	Do this in the Makefile. 

	You must edit the destination location for files 
	(LIBDIR and BINDIR) to a suitable
	directory. Then run 'make install'.
	
	You will probably have to edit the directories in ps2gif
	so that they show where the pnm and ppm filters lie. Other
	programs that have to be available on your machine include
	gs (GhostScript), FrameMaker (fmbatch), flex, yacc and cc.

	You can included links to other html documents in your FrameMaker
	document by using Markers of the type 'Hypertext'. The marker
	text is assumed to be an URL, e.g. "http://www.nta.no/".
	Any text following the marker (until the end of the line or until
	another marker or such in the text) becomes a link in your HTML
	file to the file given by the marker text. Tip: To avoid having too
	much text highlighted, have a marker (not Hypertext)
	denoting the end of the text to be highlighted. FrameMaker newlink,
	openlink and gotolink are now supported.
	
	Any paragraphs of the type 'TITLE' will become the title of
	the document and will be moved to the top of the document
	(or of the book document, if the file being translated
	is a book). 
	
	Have a look at the Demo.doc file (Demo.html) to see an example file.
	

Frequently asked questions:

	o Where can I find the pbmplus (pnm and ppm filters) package?

		See a list of locations given by archie in "WhereToFindPbmPlus".
	
	o Where can I find flex?

		See a list of locations given by archie in "WhereToFindFlex"

	o What is the fl (-lfl) library?

		It is the flex libary.

		

-----------------------------------------------------------------------------------

version 0.8.4

o Unlisted headings (UHEADING) added. Do not show in the table of contents.
o Table of contents cleaned up and changed in appearance (bold major headings
  and less space between lines).
o Table Headings and title are bold.
o Figure text is now bold.
o Fixed links to tables bug.
o Bold and italics are now handled in tables (works only with Mosaic 2.2 and newer).
o Figures and mathematic formulas are no longer totally removed from tables. Instead,
  a link with the tekst "(fig)" is generated. Clicking the link shows the missing
  graphic or math formula.
o FrameMaker hypertext handling extended.
o FrameMaker hypertext links (openlink and gotolink) are now handled (Stephen Martin).
o Removed any tags from <TITLE> field as WinMosaic displays them.
o Changed mif2gif so that the files are opened and closed in order instead
  of opening all at the same time. Saves space...
o Introduced use of TMPDIR variable instead of /tmp. /tmp is used when the
  TMPDIR variable is not set (Bob Jackson).
o Changed extractmifhead.pl and trans.l to follow FrameMaker 4 standard.
  (Bob Jackson).
o Modified yyerror and changed YYMAXDEPTH variable to allow the converter to
  work on SGI systems running IRIX-4.0.5 (Jan van der Steen).
o Fixed unfinished indenting bug.
o Norwegian Document Language now works.
o Fixed multiple .html`s in references between files.
o New tags, some old tags dropped.
o Updated and documented tags file.
o Changed insertcharacters.l so that special characters are handled correctly 
  when compiled with cc as well as gcc.
o Made links inside files skip filename, so that the file is not re-read.
  (Dropped because this still messes up back and forward in Mosaic).
o Made the use of the FRAME2HTMLPRINTFILE env variable optional
  (Duncan Fraser).
o Prepared special characters somewhat for the acceptance of the proposed HTML+
  character set (&alpha; etc.).

version 0.8.3 (Jan. 3. 1994)

o Fixed bug with links between files in a book (Daniel K. Schneider).
o Fixed continuing symbolfonts bug.
o Fixed symbolfonts and tags bug.
o Bugfixes. Hopefully the end of the 'Error in parsing. Got <Char whatever>.'

version 0.8.2  (Nov. 30. 1993)

o Fixed figure or table at end of file disappearance bug.
o Fixed footnote in table bug.
o Figures are now also links to themselves. This enables the reader
  to resize the figures if the viewer allows this (e.g. xv).
o Colour figures.
o .doc stripped (Duncan Fraser).
o Hypertext markers in Frame Documents are assumed to be HTML links.
  The following text becomes a link to the document given by the
  FrameMaker marker. Markers can be put in tables as well. However,
  figures can not be links... except to themselves... for now anyway.
o More bugfixes

version 0.8.1. (Nov. 24. 1993)

o Various bug fixes.
o More translation.
o Improved list handling (Duncan Fraser).
o Fixed Makefile bugs (missing addtextrect).
o Fixed bug with files ending in ".doc".


version 0.8  (Nov. 8. 1993)

o Book bug fixes.
o Translated to English
o Index generation improved.
o Bugfixes.
o Renamed files.
o Updated mif2html.tags file.
o Added consecutive numbered lists.













From (no email)  Tue Feb 15 11:29:42 1994 --100
Message-Id: <199402111903.AA07105@rock.west.ora.com>
Date: Tue, 15 Feb 1994 11:29:42 --100
From: (no email) ((no name))
Subject: 

Lou Montulli writes:
| >    I think you are thinkinf of the 'Referer:' field.  I send this when
| > Bill P.
| 
| One issue I'm not sure of is.  What should I send if the user
| types the URL in on the command line or with a "goto" command?
| Probably not sending the field would work, but maybe
| we should be exacting and send something like "User entered"

  How about being even more exacting and distinguish bewteen 'User',
'Startup', and 'Search'.  As a server admin, I might be interested to
know that a bunch of people like one of my pages so much that s/he
has configured it as their home page (thinking of the 'Internet Starting
Points' type services here).  'Search' may not be necessary because
it can be inferred from the '?' in the URL of the Referer.

Chris
-----------------------------------------------------------------------
Christopher McRae			            mcrae@ora.com
President, SIGWEB                                   415/242-9623
Project Manager
O'Reilly & Associates, Publishers                   510/540-6036



From connolly@hal.com  Tue Feb 15 12:12:03 1994 --100
Message-Id: <9402112238.AA02894@ulua.hal.com>
Date: Tue, 15 Feb 1994 12:12:03 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Identifying scripts by file extension?


In article <2jfcv4$c54@hal.com> nholtz@civeng.carleton.ca (Neal Holtz) writes:

   I think this was beat to death a month or two ago, but ...

   Currently, the only accepted way to designate a script gateway in a
   URL is to use some magic prefix such as "/cgi-bin/myscript/" to the
   the path.  Unfortunately, this co-exists very poorly with the rules
   for forming full URL's from relative ones (see below).  What is the
   general opinion on using the file extension to identify gateways or
   scripts?

..

   If the type information is encoded in the front portions of path
   names, and if the rules for generating full URL's from relative ones
   call for replacement of the rear portions of the path name, then it 
   is impossible to use a relative URL to link to a node of a different
   type (because the replacement rules wouldn't allow replacement of the
   type information).

Sorry... I haven't been around for a while... I find this interesting.
I barked a lot _long_ ago about the fact that the URL spec said both
	(1) a url is of the form scheme:string
where string is opaque, and
	(2) a url is of the form scheme://host/dir/dir/dir/file
		or just /dir/dir/dir/file
		or ../dir/file
		or ../dir/file#id
		or just #idd
		blah blah blah

The grammar in the URL spec is highly ambiguous. For example, how
does one parse the following?

	news:lkjlsdf#lksjdf@hal.com

The question arises: which part of a URL is opaque, and which parts
does the client get to peek into? If the clients are going to peek
into URL's (for example to resolve <A HREF="../foo/bar.html"> into a
global URL), then the servers can't use arbitrary strings as paths in
URL's. We've already seen news id's and WAIS doc-id's clash with
relative URL's.

I will again assert that what we should use the SGML parser to do
whatever parsing is going to be done on the client side, and make the
results opaque to the client, thereby allowing the server to use _any_
string it wants to encode info. We should also _allow_ a link to
contain content-type information. (How else do I link to a postscript
file on an ftp archive? By file extension? Come on!)

I'm pretty sure we can become HyTime compliant while were at it. Consider:

in stead of:
	See <A HREF="#z123">the para below</a> for more.
use:
	See <A linkend=z123>the para below</a> for more.

in stead of:
	See <A HREF="foo.html">the foo section</a> for more.
use:
	<httploc id="home" host="host.domain" path="/dir1/dir2/file">
	<relloc id=rel1 locsrc="home" path="foo.html">
	See <A linkend=rel1>the foo section</a> for more.

in stead of:
	See <A HREF="ftp://host/dir/file.tex">fred's thesis</a> for more.
use:
	<ftploc id=ftp1 host="info.cern.ch" dir="/host/dir" file="file.tex"
			content-type="text/x-latex">
	See <A linkend="http1">fred's thesis</a> for more.


The fact that relative HREFs are so widely used justifies support for
the feature. But I think you should be able to stick a tree of HTML
documents, gif files, postscript documents, etc. on an FTP server and
have it work just as well as putting them on an HTTP server. Also, you
should be able to copy those HTML documents to a local disk and use
them there.

The means we need an interoperable way to combine a relative link with
a global link to form a new global link. At first, it seems this
should be done on the server side so that the link strings can stay
opaque and the client can stay dumb.

But that doesn't work:
	* if you want to use FTP, or
	* if you want to be able to move the documents around 
		without changing them, or
	* if you want to serve the same files up via HTTP, gopher,
		 and FTP at the same time without filtering them.

Then perhaps we should just once and for all agree that a URL includes
a path that is a list of names, where the syntax of names is the
intersection of the POSIX portable filename syntax and the SGML token
syntax. (Yuk, but...) ULR's can alternatively contain a "selector
string" that is opaque and does not combine with relative locations.

So a client can reliably resolve:

	<ftploc id="ftp1" host="think.com" path="pub WAIS src readme.html">
	<relloc locsrc="ftp1" path=".. doc xyz.gif">

into:
	<ftploc id="ftp1" host="think.com" path="pub WAIS doc xyz.gif">

but it the following is an error:

	<waisloc id=wais1 host="think.com" doc-id="12l3kjl2k3jlk3jlj">
	<relloc locsrc="wais1" path=".. foo.html">

I suppose all this could be done with punctuation in stead of using
SGML syntax...

traditional style (servers _must_ be careful with syntax):

	scheme://host:9999/dir/dir/dir/file.ext#anchor
where scheme =~ /^[a-zA-Z][a-zA-Z0-9]*$/
	host =~ m-^[^:/]+$-
	dir,file =~ m-^[^/]+$-
	ext =~ m-^[^/\.#]+$-
	anchor =~ m-^[^/\.#]+$-

"opaque selector"style
	scheme://host:9999|selector
where scheme =~ /^[a-zA-Z][a-zA-Z0-9]*$/
	host =~ m-^[^:/]+$-
and selector is _completely_ opaque to the client. The context must
be able to allow a URL to be ANY string.

Once you look at how messy the punctuation strategies get in general,
SGML syntax is as good as anything. And since we're already
implementing an SGML parser, why implement a _separate_ URL parser? In
other contexts, we can use other syntaxes to represent URL's, e.g.:

	(httploc :host "www.hal.com" :path "/a/b/c.ext")
or
	Content-Type: message/external-body; access-type="http";
				site="www.hal.com"
				path="/a/b/c.ext"

But it's VERY important that we standardize on which parts of a URL
are opaque, and which are not. The current strategy is breaking down.

Dan

		



From montulli@stat1.cc.ukans.edu  Tue Feb 15 12:18:46 1994 --100
Message-Id: <9402112338.AA28524@stat1.cc.ukans.edu>
Date: Tue, 15 Feb 1994 12:18:46 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Changes to news:* handleing


Our local news guru (Bob Sloan) pointed out to me that the Web use
of "LIST" to retrieve all the articles on the system was rather ugly
and that what we should be using is "LIST NEWSGROUPS".  "LIST"
returns the name of each group plus the last article number and
the first article number and then a moderated code.  "LIST" looks like:

                                  NEWSGROUPS

     * alt.3d 5025 4760 y
     * alt.abortion.inequity 12827 12537 y
     * alt.abuse-recovery 2 3 m
     * alt.activism 60722 60194 y

Pretty darn ugly.  "LIST NEWSGROUPS" returns the name of each
newsgroup and a description.  With some slight code changes
we now get:

                                  NEWSGROUPS

   alt.3d 
          Three-dimensional imaging.
   alt.abortion.inequity
          Paternal obligations of failing to abort unwanted child.
   alt.abuse-recovery
          Companion to alt.sexual.abuse.recovery. (Moderated)
   alt.activism
          Activities for activists.


I have attached code diffs for both the Lynx version of
the WWW library changes and the XMosaic 2.2 version, since
there are differneces in the file.  I will have these changes
in Lynx version 2.2

Does anyone know of any server that doesn't support "LIST NEWSGROUPS"
I tried INN, ANU News and a CNEWS installation and they all worked.

:lou

------ Diffs to Lynx version of Libwww 2.1 HTNews.c -------------
602c602
<     (*targetClass.start_element)(target, HTML_DL , 0, 0);
---
>     (*targetClass.start_element)(target, HTML_MENU , 0, 0);
612c612
<     	    (*targetClass.start_element)(target, HTML_DT , 0, 0);
---
>     	    (*targetClass.start_element)(target, HTML_LI , 0, 0);
624,637c624,630
< 		int i=0;
< 
< 		/* find whitespace if it exits */
< 		for(; line[i] != '\0' && !WHITE(line[i]); i++)
< 		    ;  /* null body */
< 	
< 		if(line[i] != '\0') {
< 		    line[i] = '\0';
< 		    write_anchor(line, line);
<     	            (*targetClass.start_element)(target, HTML_DD , 0, 0);
< 		    PUTS(&line[i+1]); /* put description */
< 		} else {
< 		    write_anchor(line, line);
< 		}
---
> 		char group[LINE_LENGTH];
> 		int first, last;
> 		char postable;
> 		if (sscanf(line, "%s %d %d %c", group, &first, &last, &postable)==4)
> 		    write_anchor(line, group);
> 		else
> 		    PUTS(line);
642c635
<     (*targetClass.end_element)(target, HTML_DL);
---
>     (*targetClass.end_element)(target, HTML_MENU);
955c948
< 	    strcpy(command, "LIST NEWSGROUPS");
---
> 	    strcpy(command, "LIST ");

-------- Diffs to XMosaic 2.2 version of HTNews.c ---------

479c479
<     (*targetClass.start_element)(target, HTML_DL , 0, 0);
---
>     (*targetClass.start_element)(target, HTML_MENU , 0, 0);
489c489
<     	    (*targetClass.start_element)(target, HTML_DT , 0, 0);
---
>     	    (*targetClass.start_element)(target, HTML_LI , 0, 0);
501,514c501,507
< 		int i=0;
<                 /* find whitespace if it exits */
<                 for(; line[i] != '\0' && !WHITE(line[i]); i++)
<                    ;  /* null body */
<  
<                 if(line[i] != '\0') {
<                    line[i] = '\0';
<                    write_anchor(line, line);
<                    (*targetClass.start_element)(target, HTML_DD , 0, 0);
<                    PUTS(&line[i+1]); /* put description */
<                 } else {
<                    write_anchor(line, line);
< 		}
< 
---
> 		char group[LINE_LENGTH];
> 		int first, last;
> 		char postable;
> 		if (sscanf(line, "%s %d %d %c", group, &first, &last, &postable)==4)
> 		    write_anchor(line, group);
> 		else
> 		    PUTS(line);
519c512
<     (*targetClass.end_element)(target, HTML_DL);
---
>     (*targetClass.end_element)(target, HTML_MENU);
755c748
<         strcpy(command, "LIST NEWSGROUPS");
---
>         strcpy(command, "LIST ");
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From redback!jimmc@eskimo.com  Tue Feb 15 12:23:53 1994 --100
Message-Id: <9402121445.AA18231@redback.>
Date: Tue, 15 Feb 1994 12:23:53 --100
From: redback!jimmc@eskimo.com (Jim McBeath)
Subject: proxy server allows using std Mosaic with Term

The new proxy server announced by Kevin Altis makes it possible to run
off-the-shelf Mosaic 2.2 over a Term connection!  No more hacking Term
modifications int each new release of Mosaic.

After dialing into my network machine and starting up term on both ends,
here's the shell script I use to start up Mosaic on my home machine:

	#!/bin/sh -
	remotehost=www1.cern.ch:911
	lport=8080
	while [ $# -gt 0 ]; do
		case $1 in
		-r ) remotehost=$2; shift 2;;
		-l ) lport=$2; shift 2;;
		*) echo "Bad option"; exit 1;;
		esac
	done
	gateway=http://localhost:$lport/
	http_proxy=$gateway;	export http_proxy
	ftp_proxy=$gateway;	export ftp_proxy
	wais_proxy=$gateway;	export wais_proxy
	gopher_proxy=$gateway;	export gopher_proxy
	tredir $lport $remotehost
	exec Mosaic

This script sets up (using tredir) a local port (8080) which gets redirected
across the Term connection to the remote proxy server (www1.cern.ch:911).
Whenever Mosaic makes a request, it goes to tredir on port 8080, which
forwards it across the term line to term on the other end, which opens
a connection to www1.cern.ch:911 and forwards the request to it, which
forwards it to the real service.  Works great!  (Well, actually, I'm
getting a few core dumps in Mosaic 2.2, but I assume that's some other
problem, like the fact that I'm running Solaris 2.2).  The same approach
should work for any browser which supports proxying.  So now all the folks
out there with Unix machines and Term, but no Motif, can run Mosaic.

I also downloaded cern httpd 2.15 onto my network machine and ran it there,
so that I could use it as my server rather than www1.cern.ch, saving one
internet hop and reducing the load on cern.  Works fine also. (I did have to
relink it on my Sun4 to get the resolver linked in.)

Thanks to Ari and Kevin (and all the others for whom I don't have names)
for this latest improvement to the Web.

-Jim McBeath
jimmc@eskimo.com



From luotonen@ptsun00.cern.ch  Tue Feb 15 12:28:45 1994 --100
Message-Id: <9402140018.AA23524@ptsun03.cern.ch>
Date: Tue, 15 Feb 1994 12:28:45 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: CERN httpd 2.15beta released


CERN httpd 2.15beta is out:

	ftp://info.cern.ch/pub/www/src/cern_httpd_2.15beta.tar.Z

This package includes EVERYTHING, so don't ftp the libwww.  Also,
DO NOT use the libwww that comes with it for anything else, libwww 2.15
is NOT officially released.

Precompiled binaries exist for:

  Sun4: 	ftp://info.cern.ch/pub/www/bin/sun4/httpd_2.15beta.Z
  Solaris:	ftp://info.cern.ch/pub/www/bin/solaris/httpd_2.15beta.Z
  HP:		ftp://info.cern.ch/pub/www/bin/snake/httpd_2.15beta.Z
  NeXT:		ftp://info.cern.ch/pub/www/bin/next/httpd_2.15beta.Z
  DecStation:	ftp://info.cern.ch/pub/www/bin/decstation/httpd_2.15beta.Z
  OSF/1:	ftp://info.cern.ch/pub/www/bin/osf1/httpd_2.15beta.Z

For other platforms I'm happy to receive diffs.  This release is
mainly for CGI/1.0 and proxy gatewaying, but a lot of other new
features have been included (that's why I still call it beta).



                    CERN HTTPD 2.15beta RELEASE NOTES
 
   There is one single thing that needs to be done when changing over
   from httpd 2.14 to 2.15:
 
 
        Rename your old /htbin scripts to end in .pp suffix!
 
General Notes
 
     * Code tested under Purify -- all detected memory leaks and
       revealed bugs fixed.
     * Forking code enhanced -- no longer crashes when running
       standalone.
     * Documentation redesigned, but still under construction:

           http://info.cern.ch/hypertext/WWW/Daemon/User/Guide.html

     * Contains Solaris port!! (but not VMS :-( )
 
CGI/1.0, Common Gateway Interface
 
     * CGI/1.0 interface fully implemented
     * Old CERN httpd scripts will continue working if you rename them
       to end with .pp suffix. Links referencing these scrips do NOT
       need to be changed. (This feature does not add any overhead to
       CGI/1.0 script calls.)
     * New product cgiparse for CGI/1.0 scripts to parse QUERY_STRING
       env.var and to read CONTENT_LENGTH characters from stdin
     * htimage upgraded to CGI/1.0
     * The whole server-environment is propagated to CGI script, except
       for variables that are reserved for CGI/1.0.
     * Scripts are spawned by doing a fork() and exec() instead of
       system() -- more efficient and secure
 
Firewall Gateway Modifications
 
     * Access authorization works thru firewalls
     * So does POST, therefore forms also
     * -disable/-enable command line options and Disable/Enable
       configuration directives for dis/enabling HTTP methods. GET, HEAD
       and POST are enabled by default.
     * Fix: text/html and text/plain not passed multiply to servers when
       running as gateway
     * Fix: */*, image/* etc not expanded by the gateway
     * Fix: try local search ONLY when accessing local files
     * Known bug remaining: big binary files fail to transfer

Other New Features
 
     * When started standalone in non-verbose mode automatically
       disconnects from terminal session and goes background
     * User-supported directories enabling URLs starting with /~username
     * Redirection
     * Meta-information files to allow RFC-822-style headers to be
       appended to server response header section
     * New, common logfile format, localtime default, GMT as an option
     * Ability to suppress logging for certain hosts/domains according to
       given hostname template or IP number mask, like *.cern.ch or
       128.141.*.*
     * -setuid option to set server uid to authenticated uid (local)
     * Multilanguage support: same URL can be used to retrieve a document
       in different languages
     * AddLanguage, AddEncoding and AddType directives to configuration
       file (AddType replaces Suffix -- suffix still understood)
     * Better multiformat algorithm
     * HostName directive to config file for servers that want to give
       CGI/1.0 scripts a different hostname than the actual. Useful if
       machine has many aliases, or if httpd fails to get the full
       domainname.
     * Exec rule obsoliting HTBin directive -- now multiple script
       directories possible, with arbitrary mappings
     * Get-Mask, Post-Mask and Put-Mask for protection setup files.
       Get-Mask obsolites Mask-Group -- Mask-Group still understood
     * Groups All/Users and Anybody/Anyone/Anonymous automatically
       defined. All means anybody that has been authenticated, and
       Anybody is just anybody
     * Server:
     * Last-Modified:
     * Content-Length:
     * Content-Language:
     * Content-Encoding:
     * Scripts can output also Uri: and Expires: headers (this will
       eventually be made more general)
     * HEAD works, also with stupid scripts that also output the body
 
Enhancements, Fixes
 
     * The final explicit Map to filesystem in configuration file no
       longer required, because it was causing confusion
     * Assume Basic authentication scheme even if not explicitly
       mentioned in setup file
     * Get client DNS hostname, for the logfile among other things
     * Fail made the default when rules are translated to the end without
       coming accross with a Pass, Exec or Fail rule (this is to enhance
       security, it was too easy to forget the Fail * from the end of
       config file)
     * Made config (rule) file understand different ways of writing
       keywords, e.g.: UserDir, userdir, User-Dir, user_dir,
       UserDirectory and so on
     * The eight misplaced server-side access authorization files moved
       away from libwww
     * Fix: directory indexing works with a trailing slash
     * Fix: HTSimplify() called strcpy() with overlapping args
 
--
Ari Luotonen		 | httpd@info.cern.ch
World-Wide Web Project	 |
CERN			 | phone: +41 22 767 8583
CH - 1211 Geneve 23	 | email: luotonen@dxcern.cern.ch




From charlesa@cosmos.learned.co.uk  Tue Feb 15 12:33:40 1994 --100
Message-Id: <9402141043.AA22960@cosmos.learned.co.uk>
Date: Tue, 15 Feb 1994 12:33:40 --100
From: charlesa@cosmos.learned.co.uk (Charles Ashley)
Subject: Re: one author's perspective on HTML (a modest proposal)


Hear, here.

>I propose the command <NOVEL> that will tell the displayer to break
>paragraphs with indentations rather than blank lines.







From luotonen@ptsun00.cern.ch  Tue Feb 15 12:38:32 1994 --100
Message-Id: <9402141251.AA23679@ptsun03.cern.ch>
Date: Tue, 15 Feb 1994 12:38:32 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: CERN httpd 2.15beta released


----
Sorry if this comes many times, but it seems Anastasios' mailing
list software doesn't like me. :-(
----

CERN httpd 2.15beta is out:

	ftp://info.cern.ch/pub/www/src/cern_httpd_2.15beta.tar.Z

This package includes EVERYTHING, so don't ftp the libwww.  Also,
DO NOT use the libwww that comes with it for anything else, libwww 2.15
is NOT officially released.

Precompiled binaries exist for:

  Sun4: 	ftp://info.cern.ch/pub/www/bin/sun4/httpd_2.15beta.Z
  Solaris:	ftp://info.cern.ch/pub/www/bin/solaris/httpd_2.15beta.Z
  HP:		ftp://info.cern.ch/pub/www/bin/snake/httpd_2.15beta.Z
  NeXT:		ftp://info.cern.ch/pub/www/bin/next/httpd_2.15beta.Z
  DecStation:	ftp://info.cern.ch/pub/www/bin/decstation/httpd_2.15beta.Z
  OSF/1:	ftp://info.cern.ch/pub/www/bin/osf1/httpd_2.15beta.Z

For other platforms I'm happy to receive diffs.  This release is
mainly for CGI/1.0 and proxy gatewaying, but a lot of other new
features have been included (that's why I still call it beta).



                    CERN HTTPD 2.15beta RELEASE NOTES
 
   There is one single thing that needs to be done when changing over
   from httpd 2.14 to 2.15:
 
 
        Rename your old /htbin scripts to end in .pp suffix!
 
General Notes
 
     * Code tested under Purify -- all detected memory leaks and
       revealed bugs fixed.
     * Forking code enhanced -- no longer crashes when running
       standalone.
     * Documentation redesigned, but still under construction:

           http://info.cern.ch/hypertext/WWW/Daemon/User/Guide.html

     * Contains Solaris port!! (but not VMS :-( )
 
CGI/1.0, Common Gateway Interface
 
     * CGI/1.0 interface fully implemented
     * Old CERN httpd scripts will continue working if you rename them
       to end with .pp suffix. Links referencing these scrips do NOT
       need to be changed. (This feature does not add any overhead to
       CGI/1.0 script calls.)
     * New product cgiparse for CGI/1.0 scripts to parse QUERY_STRING
       env.var and to read CONTENT_LENGTH characters from stdin
     * htimage upgraded to CGI/1.0
     * The whole server-environment is propagated to CGI script, except
       for variables that are reserved for CGI/1.0.
     * Scripts are spawned by doing a fork() and exec() instead of
       system() -- more efficient and secure
 
Firewall Gateway Modifications
 
     * Access authorization works thru firewalls
     * So does POST, therefore forms also
     * -disable/-enable command line options and Disable/Enable
       configuration directives for dis/enabling HTTP methods. GET, HEAD
       and POST are enabled by default.
     * Fix: text/html and text/plain not passed multiply to servers when
       running as gateway
     * Fix: */*, image/* etc not expanded by the gateway
     * Fix: try local search ONLY when accessing local files
     * Known bug remaining: big binary files fail to transfer

Other New Features
 
     * When started standalone in non-verbose mode automatically
       disconnects from terminal session and goes background
     * User-supported directories enabling URLs starting with /~username
     * Redirection
     * Meta-information files to allow RFC-822-style headers to be
       appended to server response header section
     * New, common logfile format, localtime default, GMT as an option
     * Ability to suppress logging for certain hosts/domains according to
       given hostname template or IP number mask, like *.cern.ch or
       128.141.*.*
     * -setuid option to set server uid to authenticated uid (local)
     * Multilanguage support: same URL can be used to retrieve a document
       in different languages
     * AddLanguage, AddEncoding and AddType directives to configuration
       file (AddType replaces Suffix -- suffix still understood)
     * Better multiformat algorithm
     * HostName directive to config file for servers that want to give
       CGI/1.0 scripts a different hostname than the actual. Useful if
       machine has many aliases, or if httpd fails to get the full
       domainname.
     * Exec rule obsoliting HTBin directive -- now multiple script
       directories possible, with arbitrary mappings
     * Get-Mask, Post-Mask and Put-Mask for protection setup files.
       Get-Mask obsolites Mask-Group -- Mask-Group still understood
     * Groups All/Users and Anybody/Anyone/Anonymous automatically
       defined. All means anybody that has been authenticated, and
       Anybody is just anybody
     * Server:
     * Last-Modified:
     * Content-Length:
     * Content-Language:
     * Content-Encoding:
     * Scripts can output also Uri: and Expires: headers (this will
       eventually be made more general)
     * HEAD works, also with stupid scripts that also output the body
 
Enhancements, Fixes
 
     * The final explicit Map to filesystem in configuration file no
       longer required, because it was causing confusion
     * Assume Basic authentication scheme even if not explicitly
       mentioned in setup file
     * Get client DNS hostname, for the logfile among other things
     * Fail made the default when rules are translated to the end without
       coming accross with a Pass, Exec or Fail rule (this is to enhance
       security, it was too easy to forget the Fail * from the end of
       config file)
     * Made config (rule) file understand different ways of writing
       keywords, e.g.: UserDir, userdir, User-Dir, user_dir,
       UserDirectory and so on
     * The eight misplaced server-side access authorization files moved
       away from libwww
     * Fix: directory indexing works with a trailing slash
     * Fix: HTSimplify() called strcpy() with overlapping args
 
--
Ari Luotonen		 | httpd@info.cern.ch
World-Wide Web Project	 |
CERN			 | phone: +41 22 767 8583
CH - 1211 Geneve 23	 | email: luotonen@dxcern.cern.ch




From dalalk@vtaix.cc.vt.edu  Tue Feb 15 12:44:06 1994 --100
Message-Id: <199402141355.IAA23961@vtaix.cc.vt.edu>
Date: Tue, 15 Feb 1994 12:44:06 --100
From: dalalk@vtaix.cc.vt.edu (Kaushal R Dalal)
Subject: HTML+ and Mosaic

Hi,

I want to know if "HTML+" is supported by the latest version of 
NCSA Mosaic, Mosaic-2.2. 

Any info. on this would be highly appreciated.

Thanks,
Kaushal Dalal
Grad. Student,
Virginia Tech,
Blacksburg, VA 24060.



From neuss@igd.fhg.de  Tue Feb 15 11:56:18 1994 --100
Message-Id: <9402112038.AA02090@wildturkey.igd.fhg.de>
Date: Tue, 15 Feb 1994 11:56:18 --100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: Problem with "~user/file.html"

Dear fellow Webbers,

I'm currently setting up a a server for a larger network (over 700 accounts)
and I experienced some problems. Maybe you people have some comments
on my ideas. The problem we have mainly comes from having to provide access
to all kinds of documents via file access (this is what you get if you e.g.
do an "Open Local" in Xmosaic and then follow links that look like
  HREF="/usr/dir/somedir/file.html" or HREF="localfile.html"), but also
wanting to serve some documents or document trees to the world. 

It's a very nice feature that NCSAs httpd allows users to provide
access to local documents by putting them in a special folder in
their home directory.

Ok, let's say you want to do something like click *here* to get info
about the author of this document. The form "/~userid/me.html" only
works over the server, and the form "/whole/path/some/where/me.html"
only works with local access! What can I do??? 

I need to provide a solution that works both times - and I can not
tell the server to make the whole user directory tree accessible
(for obvious security reasons).

Yes, I could write an alias, but I cannot write 700+ aliases!
Am I overlooking something? IMHO, it would be a good solution if
browsers expanded the tilde operator to the users home dir whenever
they run in local mode. Is this behavior already standardized?

And another thing.. shouldn't "~userid/me.html" expand to the
home dir, too? Currently Mosaic see's that the path does not start 

with "/", and appends the current path to it.

Many thanks in advance for your help,
Chris

---
"I ride a tandem with the random.." 

Christian Neuss   # Fraunhofer Institute for Computer Graphics
Wilhelminenstr.7  #  64283 Darmstadt # Germany
e-mail: neuss@igd.fhg.de  finger: neuss@wildturkey.igd.fhg.de



From redback!jimmc@eskimo.com  Tue Feb 15 12:48:57 1994 --100
Message-Id: <9402141857.AA19702@redback.>
Date: Tue, 15 Feb 1994 12:48:57 --100
From: redback!jimmc@eskimo.com (Jim McBeath)
Subject: A new MIF to HTML converter, written in C

For those of you who have been frustrated by MIF converters written
in perl, lisp, or other advanced languages, here's one written entirely
in C.  OK, to be honest, there are also two 20 line sh scripts.

It comes with a preliminary manual which you can run through miftran
to make HTML pages.  This serves as a test to make sure miftran
compiled correctly on your system, as an example, and as the actual
documentation for miftran.

This is version 0.1, with all the caveats that a version number like
that implies.  You can pick it up (40K) from

    ftp://ftp.alumni.caltech.edu/pub/mcbeath/web/miftran/miftran.tgz

Below is the README file.

-Jim McBeath
jimmc@eskimo.com


    Miftran v0.1						14.Feb.94

    Miftran is a general purpose MIF (Framemaker's Maker Interchange
    Format) translation program.  It was designed primarily to translate
    to HTML, but is flexible enough that it could be used for other
    translations.

    The latest version of miftran is most likely to be found in
       ftp://ftp.alumni.caltech.edu/pub/mcbeath/web/miftran

    Miftran has the following features:

	- Written in C (plus a few very small sh scripts).

	- Externally configurable with a single configuration (RC) file,
	  which can be customized for each MIF file to be translated.
	  ALL output text is controlled by contents of RC file.

	- Based on paragraph and font tags.

	- Generates Table of Contents (based on chapters, etc.) and Index
	  (based on Index markers), controlled by configuration file.

	- Accepts a single MIF input file, but can produce multiple
	  output files (e.g. split by chapters, sections, etc.),
	  based on configuration file.

	- Converts cross-references to HTML links,
	  based on configuration file.

	- String substitution allows converting special characters.

	- For advanced users, the C source can be relatively easily
	  customized to recognize additional MIF constructs.

	- Freely redistributable with an MIT/X11 style copyright.
	  You can even put it into a commercial product and sell it
	  if you want, as long as you preserve and acknowlege our copyright.

    Miftran has the following limitations:

	- Only handles single MIF files.  If you have multiple MIF files,
	  you will have to process them one at a time.  You can probably
	  make this work by munging the html/Makefile and the sh scripts.

	- Does not handle anchored frames and inline images.

	- Does not handle conditional text.

	- No special handling of tables.

	- Does not handle paragraph and font formatting based on anything
	  other than tags.

    To compile and test miftran:
    1. Do a "make" in this directory.  This will build the miftran program.
    2. Cd to the html directory and do another "make".  This will create
       a set of HTML files, one per chapter, for the miftran.mif manual.
    3. In the html directory, do a "make diff" to compare the html/*.html
       files against the htmlref/*.html files.  They should be identical.
    4. Run your favorite HTML browser starting at TOC.html.

    This early version has been tried on a very limited sample of MIF files.

    Miftran has been compiled and run on the following machines:
	Sparc SunOS 4.1.3
	Sparc Solaris 2.2 with gcc

    If you use miftran, please send me email and let me know about it.
    If you are using a machine or OS that is not in the above list,
    please let me know, especially if there were any changes required
    to get it to work.

    Author: Jim McBeath
    jimmc@globes.com  jimmc@eskimo.com  jimmc@alumni.caltech.edu



From WIGGINS@msu.edu  Tue Feb 15 13:14:09 1994 --100
Message-Id: <9402150038.AA18156@dxmint.cern.ch>
Date: Tue, 15 Feb 1994 13:14:09 --100
From: WIGGINS@msu.edu (Rich Wiggins)
Subject: Mirror of CERN's list of sites?

Has anyone set up a mirror of the CERN list of sites?  I'm
especially curious about the geographical list, as CERN's
is arriving truncated after the UK right now.

/rich



From sanders@BSDI.COM  Tue Feb 15 18:03:46 1994 --100
Message-Id: <199402151659.KAA03437@austin.BSDI.COM>
Date: Tue, 15 Feb 1994 18:03:46 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Server control over history? 

Dave_Raggett writes:
> Neal Holtz writes:
> > Is anyone else interested in having the server have some control over
> > a clients history list?
> > ... In these cases, you probably do not want to see these nodes put
> > on the history list.
> 
> I have been worrying about this too. For instance if you follow a thread

How about just having the browser use <A HREF="..." REV="Precedes">...</A>
and if the previous link matches the HREF then go "back", if not, continue
adding things to the history list like normal.

I believe that having servers directly messing with client history lists
is a can of worms that we really don't want to open.  Whatever happens
on the client end should be implied by generic hypertext operations.

--sanders



From udi@ubique.co.il  Tue Feb 15 13:29:25 1994 --100
Message-Id: <199402150757.JAA00740@shaldag.ubique.co.il>
Date: Tue, 15 Feb 1994 13:29:25 --100
From: udi@ubique.co.il (Ehud Shapiro)
Subject: Which Database to use with HTML Forms/SQL Gateway?


Hi,
We are considering using a database to operate in conjuction with HTML
forms and an SQL gateway, and we are looking for advice on which
database to use.  Oracle and Sybase are our prime candidates. 

Any information on relevant experience will be appreciated.
Thanks,
-- Udi



From mcclanah@dlgeo.cr.usgs.gov  Tue Feb 15 12:58:24 1994 --100
Message-Id: <9402142038.AA04449@dlgeo.cr.usgs.gov>
Date: Tue, 15 Feb 1994 12:58:24 --100
From: mcclanah@dlgeo.cr.usgs.gov (mcclanah@dlgeo.cr.usgs.gov)
Subject: Conference

Seems to me I saw some announcement of a conference at NCSA in March - can't
seem to locate any reference to it anywhere and am in a BIG hurry - anyone know anything about this or am I just dreaming?

Thanks for any pointers,
Pat McClanahan		Internet:mcclanah@dlgeo.cr.usgs.gov
EROS Data Center 		 mcclanah@edcserver1.cr.usgs.gov
Sioux Falls, SD
605-361-4607



From drmacro@vnet.IBM.COM  Tue Feb 15 13:24:16 1994 --100
Message-Id: <9402150445.AA06793@dxmint.cern.ch>
Date: Tue, 15 Feb 1994 13:24:16 --100
From: drmacro@vnet.IBM.COM (W. Eliot Kimber)
Subject: <draft-ietf-iiir-html-01.txt, .ps> to be deleted.

Ref:  Your note of Mon, 14 Feb 1994 18:57:33 -0600 (attached)


At the risk of sidetracking what appears to be a productive dialog
with pedantry, I think it's important to clarify a few points that
I think are central to the issue of making HTML+ a real SGML
application.

| >> I think the HTML-Plus does a good job of getting a lot of interesting
| >> issues on the table, but it's approach of throwing all the stuff into
| >> one DTD, and making the DTD extensible (thereby forcing clients to
| >> know how to _parse_ SGML DTD's) is a little off track.
| >
| >Actually, once you state that HTML is an SGML format, then formally each
| >document can extend the DTD.
|
| Nope. I took great pains in the specification to prevent WWW clients
| from having to deal with anything but _instances_ of the DTD I wrote:
|
| <!--    Regarding clause 6.1, SGML Document:
|
|         [1] SGML document = SGML document entity,
|             (SGML subdocument entity |
|             SGML text entity | non-SGML data entity)*
|
|         The role of SGML document entity is filled by this DTD,
|         followed by the conventional HTML data stream.
| -->

I'm afraid that on this point there can be no compromise.  If a document
is an SGML document then it *must* start with a DOCTYPE declaration
and include the document element *in the same entity*.  The definition
of SGML document entity is quite clear on this.  In fact, it is impossible
to know whether or not a given stream of data is valid SGML *unless*
there is a doctype declaration (and an SGML declaration, which
may be implied by the processing system).

Therefore, if you have an SGML application it *must* parse DTDs.  There
are certain constraints you can apply, such as only recognizing certain
GIs (and issuing application error messages when other GIs are declared),
but you must be able to resolve entity references.  Anything less is
"SGML-like".

But this shouldn't be a problem because parsing DTDs is about the
only really easy part of SGML, and building an entity resolution table
is easy.  Having written crude parsers myself in Rexx and C, I have
a hard time buying the argument that such parsing is a burdensome
requirement to place on browsers.  Surely real-time formatting is a
much harder problem to solve.  Note that there is no requirement that
a conforming parser be a *validating* parser, nor that it support
the optional features of OMITTAG, SHORTTAG, and the like.

Note that given this requirement, you can cheat a little and refuse
to parse anything except entity declarations in DTD subsets.  In the
interest of compromise I, for one, would look the other way, for what
it's worth.  I don't think it's unreasonable for an application like
Mosaic, which is trying to stay as simple and easy to implement as
possible, to say "look, we just can't handle new element declarations
at the document level--the DTD's built in", as long as it does
require and recognize the doctype declaration.  This would make
a minimal HTML+ document something like:

<!DOCTYPE HTMLPlus PUBLIC "-//WWW//DTD HTMLPlus/version 0.0.0//EN">
<HTMLPlus>
  ...
</HTMLPlus>

Note that by using the public identifier for the DTD, you can define
the mapping of the public ID to the "public part of the DTD" to be
hard-coded and unchanging to the parsing algorithms built into the
browser rather than to a literal file containing element declarations
(this is analagous to applications using a "compiled" DTD).  I see
no difference between a DTD that is compiled into an application-specific
object and a DTD that is compiled into procedural code.

Certainly I would prefer to see the browsers be more general, but I
am willing to admit (if grudgingly) that practical considerations
may outweigh concerns for complete correctness and generality.


| >I have investigated HyTime compliance with Yuri Rubinsky and Elliot Kimber
| >(Dr Macro), and know how to add this in. At the moment though, most people
| >in the WWW community see little value in switching to a model which forces
| >you to declare hypertext links at the start of the document.
|
| There are ways to exploid HyTime without using <!ENTITYs for all
| links. More on that later too...

I'd like to know what these ways are.  The use of entities for hyperlinking
is really a base function of SGML that HyTime merely exploits.  In other
words, a reference to a separate document in an SGML context can only
(in the view of the pedant) be expressed as a data entity reference (by
the rule that if a mechanism exists in the standard, you must use that
mechanism).  This is a basic fundamental of SGML.  The one-level indirection
provided by entity-name/system-ID mapping is essential to any hope of
system and application independence for data, with the two-level indirection
of entity-name/public-ID/system-ID crucial for complete system and
application independence and wide interoperation.

You could, I suppose, use notation locations (notloc) in place of
entity references, but by my "in the standard" rule, the pedant in
me would have to object.  Thus you could cast a URL-based link
as something like:

<p>See
<notloc id=book-1-loc notation=URL>ULR//FTP::/a/b/c/</notloc>
<a target=book-1-loc>this book</a> for more

Where the Notloc element is an inclusion at the document element,
and thus valid anywhere and otherwise transparent to the main content
processing routine (not to mention that record ends caused purely
by the notloc are not treated as data).  Note that HyTime does let
you specify the constraint that forward references are not allowed,
removing the need to do lookahead to resolve references.  Or you
could instead make Notloc only valid within the link elements themselves,
ensuring that its in a predictable place (there's no real functional
difference between an attribute and a required subelement).

Once you can resolve an entity reference, resolving a reference to a
particular location within a referenced entity isn't that much more
difficult (you merely pass two values to your resolution function rather
than one).  If you can link to IDs within the document you're browsing,
you should be able to link to IDs within any referenced document
entity with equal ease.  Having done this, you've implemented as much
HyTime as you'll need for most online browsers.  An indirect HyTime
link to an ID in another document is nothing more than a two-element
address where one element is the entity name (resolved to a system location)
and the other element is a target ID.  Any system that does cross-book
linking to specific locations must already support some form of two-element
address, so doing it in a HyTime-compliant fashion must only involve
mapping the HyTime syntax onto the existing mechanism.  You may also need
to define constraints on the complexity of HyTime expressions supported,
which is reasonable and expected (which is one of the reasons there are
so many options in HyTime).

<aside>
Please note that my e-mail address has changed.  As of 2/14 I am
no longer an IBM employee.  I can be reached at kimber@passage.com
or drmacro@aol.com.
</aside>

--
<address id=drmacro HyTime=bibloc>
Eliot Kimber                      Internet: kimber@passage.com
Passage Systems                      Phone: 1-512-339-3618
9971 Quail Bldv, Suite 903     AltInternet: drmacro@aol.com
Austin, TX 78758
</address>



From luotonen@ptsun00.cern.ch  Tue Feb 15 15:12:08 1994 --100
Message-Id: <9402151137.AA24779@ptsun03.cern.ch>
Date: Tue, 15 Feb 1994 15:12:08 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: proxy server allows using std Mosaic with Term


> I also downloaded cern httpd 2.15 onto my network machine and ran it there,
> so that I could use it as my server rather than www1.cern.ch, saving one
> internet hop and reducing the load on cern.  Works fine also. (I did have to
> relink it on my Sun4 to get the resolver linked in.)

-lresolv is now default in precompiled binaries for Sun4.

And a correction to Kevin's message about cern_httpd 2.15: it does
not do caching yet, but 2.16 will (28.2.).


-- Cheers, Ari --




From altis@ibeam.jf.intel.com  Tue Feb 15 11:39:52 1994 --100
Message-Id: <m0pV3x4-00042SC@ibeam.intel.com>
Date: Tue, 15 Feb 1994 11:39:52 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Caching and metainformation

At  8:54 AM 2/11/94 +0000, Rik Harris wrote:
>Has this project involved any discussion wrt caching?  A proxy server
>would be the ideal place to cache documents that don't require
>authentication.  I realise there are some types of documents that
>shouldn't be cached, but for a test system, a config file could
>specify that 'http://www.some.host/auto/*' should not be cached.
>
>I know the topic has been brought up on the list before, but is anyone
>actually working on it?  Now that we have some clients that have been
>modified for proxy support, perhaps the caching discussion could be
>renewed?

Actually, the cern_httpd 2.15 supports caching in addition to proxying.
Unfourtunately, we don't have much client or server support right now for
all of the fields that you would want to check to do proper caching. Here
is an excerpt from an earlier message reply to me by TimBL.

At 12:20 PM 2/9/94 +0000, Tim Berners-Lee wrote:
>> Doing HEAD, expires, etc. is suddenly going to get important. Might put
>> some pressure on the URNs issue as well.
>
>Yes.  Also, the Public: is important.  We must get the default understanding
>completely clear.  At the moment in HTTP is seems as though Public: is
>just informational, as in fact if anyone really wants to test access then
>they can just try it.  With caching, the Public: allows the caching server
>to return it directly.  If we specify the current assumption that
>if nothing is specified then the document is public, This is
>
>NOT fail-safe.  Would it be better to make that assuption ONLY if no
>Authorization: header was sent?
>
>IE
>    If Public: present, it is definitive.
>    Else        if authorization was needed, then assume NO public access
>                else    if Allowed: is present, assume public access is same
>                        else assume public access is GET only.
>
>I'll put that in the spec -- if anyone has any troubles with this
>say now.

A document that required authentication should not be cached, since it
implies the caching server have the same authentication rules and
information as the server that the document came off of. Also, a caching
server is going to store its documents on a file system that is probably
accessible by people not authorized/authenticated to read the information.
Servers implementing something like ChargeTo: probably don't want their
information cached either :)

Object MetaInformation
In the cases of documents that are candidates to cache, we need to
explicitly state which HTTP metainformation "fields" a server needs to send
in order for the document to be cached correctly. Tim mentioned Public:,
there is also the need to send Version:, Date:, Expires:, and Last-Modified
which may imply different caching strategies depending on their values. You
can also ask how the HTTP server is going to fill in that metainformation.
Is it part of the <HEAD> of an HTML document? If so, what happens with
binary files where there is no <HEAD>? I have two or three documents that I
never want a client or proxy server to cache such as a stock price or
weather document, so those need an "Expires: Always" kind of
Metainformation. Most other objects that I serve can probably be cached
based on the modified date and time of the document in the file system or
modified field on information from a SQL database.

Given an URL, Clients and proxy servers need to be able to ask for the HEAD
or metainformation parts of an object. It might be beneficial for the
client and server writers to describe which fields they currently supply or
request so they can see and agree what to add.

ka





From omy@San-Jose.ate.slb.com  Tue Feb 15 18:48:10 1994 --100
Message-Id: <9402151735.AA10706@San-Jose.ate.slb.com>
Date: Tue, 15 Feb 1994 18:48:10 --100
From: omy@San-Jose.ate.slb.com (Omy Ronquillo)
Subject: Temporary file location.


	Hi!

	How can I change the /tmp or /var/tmp where Mosaic creates
	temporary files? Where it goes by default now is a small
	space so I'm running out of room when I retrieve files.

				Thanks.
				Omy



From altis@ibeam.jf.intel.com  Tue Feb 15 13:08:36 1994 --100
Message-Id: <m0pWCpu-00042YC@ibeam.intel.com>
Date: Tue, 15 Feb 1994 13:08:36 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: client local file system access issues

Getting all clients to do file://localhost
file://localhost/path
First of all, is this standardized or not? We have people all over the net,
using file:// instead of ftp:// for ftp accesses, which is a pain. Local
file access doesn't really seem to be standardized. Here are some examples:

Lynx and X Mosaic both do file://localhost/path
path is the full path from the root (/) of the local filesystem.

Cello does file://localhost/c:/path
c could be any drive letter.

Mac Mosaic does file:///drive name/path Spaces are okay at least in the
drive name.

Win Mosaic does file:///c|/path
c could be any drive letter.

I think that clients could probably treat the drive letter (DOS/WINDOWS) or
drive name (Mac), etc. as just the first path element, so that the path
wouldn't look any different than their Unix cousins:
file://localhost/c/path or file://localhost/drive name/path
The key is that all clients have to conform to using file://localhost/ to
mean part of the local file system.

---
Local aliases
Given that all clients will then reference local files in a standard way,
it becomes possible to imagine distributing a set of files across systems
and building hyperlinks to reference local files seamlessly with other Web
documents. An obvious reason to do this would be for inlined gif images,
help files, or large static files that you don't want clients pulling
across the net every time they access an HTML page (like Tim's Internet
Talk Radio interview). You could even use least common denominator file and
directory names (DOS style filename.ext) so that the same group of files
would work across Unix, Mac, and DOS/Windows. I can think of lots of static
pages on the net that I wouldn't mind keeping on my local filesystem
(especially my portable where I don't always have a net connection). Of
course, I would want to reference those local files and not have the URLs
break at a later date.

What's needed of course is the ability to do client side aliasing, just
like HTTP servers allow today. The client would support a setup such as:

Alias file://localhost/c/somedir/anotherdir file://localhost/i/anotherdir

All the URLs referencing those local documents would still work, even
though the actual file system location on the local machine has changed.

---
The last issue is when dealing with local files, there is no particular
reason that clients should bring a document from part of the local
filesystem to another part of the local filesystem before displaying the
file or handing it off to an external browser. So, the case of
file://localhost should be treated special be the client so that it just
goes and used the local file. You won't have to wait for a 14MB audio/video
file to get copied to a different directory before the external mpeg player
launches to play the file.

Comments?

ka





From connolly@hal.com  Tue Feb 15 13:19:15 1994 --100
Message-Id: <9402150057.AA04169@ulua.hal.com>
Date: Tue, 15 Feb 1994 13:19:15 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: <draft-ietf-iiir-html-01.txt, .ps> to be deleted. 

In message <9402141145.AA08042@manuel.hpl.hp.com>, Dave_Raggett writes:
>Dan, did you get my pointer to the current HTML+ DTD? 
>
>        ftp://15.254.100.100/pub/htmlplus.dtd.txt
>

I just now took a look at it. Very nice.

My major concern is that it implies a tremendous increase in the
complexity of the HTML parser. With my original HTML specification, an
HTML parser only parsed the instance part of the SGML document. With
this HTML+ specification, WWW clients will have to parse the prologue
as well. For example:

[[[[[[

<!DOCTYPE HTMLPLUS [

<!-- here's a blurb I hate to type all the time:
<!ENTITY sgml "SGML (Standard Generalized Markup Language)">

]>

<htmlplus><body>Parsing &sgml; in a general fashion is quite complicated!
</body></htmlplus>

]]]]]]

You have, however, simplified matters by not putting any parameter
entities in content models. This means that WWW clients won't have to
deal with individual documents introducing new element types "on the
fly."

But you've introduced OMITTAG, <!ENTITY> parsing, and lots of other
stuff. If we plan to include a full blown SGML parser in every WWW
client, why not use all the syntactic sugar like short references and
cool stuff like that while we're at it?

One of the things I released (or was just about to release when I
changed jobs...) was an SGML compliant HTML parser in a few hundred
lines of vanilla ANSI C.

>
>One issue in formalising HTML+ was in providing an adequate structure
>while dealing with legacy documents. As you can see in my current DTD,
>documents have a richer structure than with the old HTML DTD.
>

Yes... and it seems to me (at first glance... I'll have to look more
closely...) that we've lost the ability to translate HTML to Microsoft
Word or FrameMaker without any loss of information.

Let's get formal why don't we: I do not mean that we should be able to
take any RTF file and convert it to HTMLPLUS, or MIF for that matter.
But I think it's crucial that there exist invertible mappings

	h : HTML -> RTF
and
	g : HTML -> MIF
and
	h : HTML -> TeXinfo

so that I can take a given HTML document, convert it to RTF, and
convert it back and get exactly what I started with (the same ESIS,
that is... perhaps SGML comments and a few meaningless RE's would get
lost).

>For instance, document text is forced to appear within paragraph elements
>which act as containers. Documents are broken into divisions using the
>new DIVn elements which give substance to the notion that headers start
>sections which continue up to the next peer or larger header. 

If we're going to burden WWW clients with all this rich structure and
OMITTAG parsing, why don't we go with something like DocBook, which
has a proven ability to capture the structure of existing technical
documents, in stead of trying to roll our own.

>The ability to omit starting tags suggested a neat trick for handling
>existing HTML documents by defining DIVn and P as having omissable
>starting tags. Thus an <H1> tag can only occur as the first element
>of a DIV1 element, so browsers can infer missing DIVn start tags.

I'd like to see a more formal argument that this is a general
solution... Perhaps in the form of a short perl program that does the
inference.

>Similarly, missing <P> tags can be inferred when the browser sees
>something belonging to %text. This neatly deals with the common case
>where some authors think of <P> as a paragraph separator or something
>you should put at the end of each paragraph (this view is promulgated
>by Mosaic documentation). 

Is this form of inference consistent with the SGML standard? Or is
this a non-standard extension to support legacy HTML documents?

>My HTML+ browser works this way, using a top-down parser which permits
>most elements to have omissable start and end tags, using the context
>to identify missing tags. Each element is associated with a procedure.
>Its easy this way to recover the structure of badly authored documents
>e.g. with missing <DL> start tags.  BTW this browser will be demoed at
>the forthcoming WWW Conference in May.
>General purpose SGML parsers have difficulties with omitted start tags
>reflecting the outcome of a debate in the standards committee. Small
>print in the SGML standard limits the power of parsers to infer missing
>start tags. This restriction was added to simplify writing general parsers
>to handle DTDs in which the content model specifies exceptions.

If you're suggesting we use a parser that's "smarter" than standard
SGML parsers, I don't see the point. Either we buy into SGML, or we
make up something application-specific. And if we're going to make up
something application specific, we might as well scrap SGML syntax
all together and build something simple out of lex and yacc, or build
on TeXinfo.

>As a result the HTML+ DTD specifies the paragraph element as requring a
>start tag. The DTD can therefore be used with existing SGML authoring tools.
>HTML+ browsers are expected to exploit the DTD to infer missing tags, and
>hence deal with the wide variety of markup errors in existing documents.
>
>In future, we expect authors will use specialized wysiwyg editors for HTML+
>or automated document format conversion tools and hence produce documents
>which naturally conform to the DTD.
>

Ahh... now I am beginnig to understand the strategy, and I think I
like it: We begin anew with HTMLPLUS, defining a DTD that we expect to
be suitable to our needs. Then simply acknowledge that existing
documents contain a significant number of markup errors, and develop
heuristic techniques for inferring the ESIS from these "broken"
documents.

Hmmm... as long as there are no un-broken documents that would be
misinterpreted by these heuristics, I think it's a great idea. (Again,
though, I'd like to see a formal argument that this is the case.)

>> I think the HTML-Plus does a good job of getting a lot of interesting
>> issues on the table, but it's approach of throwing all the stuff into
>> one DTD, and making the DTD extensible (thereby forcing clients to
>> know how to _parse_ SGML DTD's) is a little off track.
>
>Actually, once you state that HTML is an SGML format, then formally each
>document can extend the DTD.

Nope. I took great pains in the specification to prevent WWW clients
from having to deal with anything but _instances_ of the DTD I wrote:

<!--    Regarding clause 6.1, SGML Document:

        [1] SGML document = SGML document entity,
            (SGML subdocument entity |
            SGML text entity | non-SGML data entity)*

        The role of SGML document entity is filled by this DTD,
        followed by the conventional HTML data stream.
-->

> HTML+ merely exploits this to show authors
>how to declare which extension they wish to use: forms, tables, figures etc.
>I owe a debt here to Lou Burnard and the TEI DTDs which showed me how and
>why to use this approach. It is also pivotal in addressing the problems
>in providing a wide enough range of semantic markup to cover all needs.
>In practice, this is a bottomless pit, and the best solution is for HTML+
>browsers to offer a small basis set of emphasis primitives and to allow
>authors to define their specific elements in terms of this basis set (see
>the RENDER element). At least one browser out there already supports this.

Another solution is to go with more of a MIME architecture, where HTML
is just one data format. TeXinfo is another handy one, and maybe
DocBook, etc. ... I'll have to explain my thoughts on this a little
more in another message.

>
>I have investigated HyTime compliance with Yuri Rubinsky and Elliot Kimber
>(Dr Macro), and know how to add this in. At the moment though, most people
>in the WWW community see little value in switching to a model which forces
>you to declare hypertext links at the start of the document.

There are ways to exploid HyTime without using <!ENTITYs for all
links. More on that later too...

> This no doubt
>will change if and when HyTime gets widely adopted. On the other hand, I
>feel it is essential for HTML+ to conform to SGML. Without this, publishers
>and businesses will tend to see WWW as a passing experiment that needs to
>be replaced by something on a more professional/commercial footing. This is
>why I am working so hard to extend HTML into something that meets publishers
>and users expectations for document delivery. NCSA have done their bit - now
>its my turn to roll up my sleeves and get down to serious programming :-)

There's a lot of good stuff in this latest DTD. I think we need a more
sophisticated, fault-tolerant linking element, and a few other things,
but you might be on the right track.

>
>> I've got a lot of catching up to do. I hope it's not too late to
>> keep folks from losing confidence in communicating with HTML.
>
>No problem! I am confident that html+ will go a long way to vitiating
>current objections and raising confidence in WWW as a model for the
>development of national information highways. I look forward to renewed
>vigour in the debate on where we should go next, and hope you can make it
>to the WWW Conference in Geneva.

I wish! Maybe...

Dan

p.s. I'd like to start some sort of html-successor-design discussion
form. Is comp.infosystems.www, comp.text.sgml, or www-talk a suitable
forum? Shall we create one?



From burchard@horizon.gw.umn.edu  Tue Feb 15 18:53:39 1994 --100
Message-Id: <9402151747.AA08949@horizon.gw.umn.edu>
Date: Tue, 15 Feb 1994 18:53:39 --100
From: burchard@horizon.gw.umn.edu (Paul Burchard)
Subject: Re: Server control over history?

Dave_Raggett <dsr@hplb.hpl.hp.com> writes:
> [Paul Burchard <burchard@geom.umn.edu> writes:]
> > Exactly---but this sort of automatic unlimited UNDO
> > capability is a wonderful thing for increasing interactivity.
> > The higher bandwidth of interaction you want to achieve,
> > the more useful it becomes.
> 

> But, how do you get the database at the other end to roll
> back in sync with the browser. At the very least the
> browser would have to send a message to the server to
> roll-back each step. This is well outside the current
> HTTP/fill-out forms mechanism. Worse, I can't see how
> its possible to roll-back the state in all cases. 


Fill-out forms, as they exist now, can hold arbitrary state  
information during a sequence of transactions (this will soon be  
completely straightforward and aboveboard using type="hidden"  
fields.)

In the W3Kit system used to build our interactive Web apps, the  
complete per-user application state is in fact encoded in the  
fill-out form, as an archive of persistent objects.  This allows  
rollback, even random access to history, since despite appearances  
the server remains fully stateless.

I realize that database front ends can't go quite as far, since the  
server has its own state independent of the state of ongoing  
transactions with each user.  But if users would frequently want to  
undo or edit transactions by backtracking, then the front end should  
probably use an explicit transaction commitment scheme anyway.  Under  
that assumption, random access could at least be provided within a  
sequence of uncommitted transactions.

Again, I'm not arguing that you should *always* be able to provide  
complete random access to the server, just that the hypertext format  
of the Web raises user expectations to a point where you ought to  
have some pretty good reasons for the limits you do place on random  
access.

--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------



From pflynn@curia.ucc.ie  Tue Feb 15 15:18:21 1994 --100
Message-Id: <9402151033.AA15861@curia.ucc.ie>
Date: Tue, 15 Feb 1994 15:18:21 --100
From: pflynn@curia.ucc.ie (Peter Flynn)
Subject: Re: <draft-ietf-iiir-html-01.txt, .ps> to be deleted.

> But this shouldn't be a problem because parsing DTDs is about the
> only really easy part of SGML, and building an entity resolution table
> is easy.  Having written crude parsers myself in Rexx and C, I have
> a hard time buying the argument that such parsing is a burdensome
> requirement to place on browsers.  Surely real-time formatting is a
> much harder problem to solve.  Note that there is no requirement that
> a conforming parser be a *validating* parser, nor that it support
> the optional features of OMITTAG, SHORTTAG, and the like.

It's the wall-clock time for the user that's important. OK, some of us
are lucky and work on fast machines: millions of potential users out
there are on dumb terminals on overloaded VAXen, old 386s, Goddess knows
what. 

Coming from the other end of the spectrum, real-time formatting is not
that hard either. After all, if you want robust fast formatting, you can
always lift the core code from TeX and use that, shorn of its batch
environment and syntactic dependencies; some systems already have.

> Note that given this requirement, you can cheat a little and refuse
> to parse anything except entity declarations in DTD subsets.  In the

I take back what I said about modifying the DTD not being permitted, I 
didn't understand fully what was being implied.

///Peter



From burchard@horizon.gw.umn.edu  Tue Feb 15 15:38:44 1994 --100
Message-Id: <9402151357.AA08820@horizon.gw.umn.edu>
Date: Tue, 15 Feb 1994 15:38:44 --100
From: burchard@horizon.gw.umn.edu (Paul Burchard)
Subject: Re: Caching and metainformation

altis@ibeam.jf.intel.com (Kevin Altis) writes:
> In the cases of documents that are candidates to cache, we
> need to explicitly state which HTTP metainformation
> "fields" a server needs to send in order for the document
> to be cached correctly.

Could you clarify how this would affect POST forms?  Are you  
suggesting that POST forms will have to include explicit  
metainformation to prevent themselves from being cached (which would  
seem to be undesirable in most cases)?

--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------



From burchard@horizon.gw.umn.edu  Tue Feb 15 15:33:22 1994 --100
Message-Id: <9402151355.AA08815@horizon.gw.umn.edu>
Date: Tue, 15 Feb 1994 15:33:22 --100
From: burchard@horizon.gw.umn.edu (Paul Burchard)
Subject: Re: Server control over history?

Dave_Raggett <dsr@hplb.hpl.hp.com> writes:
> Another common case occurs with fill-out forms. Keeping a
> history of the successive stages of filling out a form, and
> the updates to that form by the server is of dubious value

On the contrary---I would like to disagree here.

> (it would only make sense in the context of
> a multi-level UNDO mechanism).

Exactly---but this sort of automatic unlimited UNDO capability is a  
wonderful thing for increasing interactivity.  The higher bandwidth  
of interaction you want to achieve, the more useful it becomes.

At the Geometry Center, we have just released a suite of interactive  
graphical WWW applications based on fill-out forms, and have found  
the automatic UNDO capability to be *extremely* helpful.  Given the  
turnaround time of network transactions, correcting or fine-tuning  
one's input by taking advantage of the transparent UNDO feature can  
greatly boost the apparent interactivity of the application.  (Try it  
out for yourself at "http://www.geom.umn.edu/apps/gallery.html".)

I don't have anything against your proposal for "hints" that would  
allow history to be discarded.  I simply want to emphasize that the  
default behavior of browsers should always be to *keep* intermediate  
steps of the fill-out form.

Thanks for listening,
--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------



From letovsky-stan@CS.YALE.EDU  Tue Feb 15 12:53:38 1994 --100
Message-Id: <199402142004.AA07206@RA.DEPT.CS.YALE.EDU>
Date: Tue, 15 Feb 1994 12:53:38 --100
From: letovsky-stan@CS.YALE.EDU (Stan Letovsky)
Subject: HTML Extensions; www/SYBASE gateway

I am in the process of designing and implementing some extensions to
HTML / NCSA Mosaic Fill-Out forms capability, as part of a project to
develop a generic, end-user programmable Mosaic-based front-end for
SYBASE databases. For a description of either, see
	http://procrustes.biology.yale.edu/genera.html
Briefly the fill-out forms extensions include:
 o a widget for tabular arrays of input fields
 o a hash-table for associating arbitrary attributes with widgets
 o HTML directives for accessing and setting attributes
 o finer-grained client/server interaction than is currently supported by http, 
 to allow for interactive field-entry validation and such. 
I'd be curious to hear opinions or advice.

Cheers.

-Stan



From altis@ibeam.jf.intel.com  Tue Feb 15 19:26:18 1994 --100
Message-Id: <m0pWUPJ-000434C@ibeam.intel.com>
Date: Tue, 15 Feb 1994 19:26:18 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: Server control over history?

At 11:11 AM 2/15/94 +0000, Dave_Raggett wrote:
>Neal Holtz writes:
>
>> Is anyone else interested in having the server have some control over
>> a clients history list?
>> ... In these cases, you probably do not want to see these nodes put
>> on the history list.
>
>I have been worrying about this too. For instance if you follow a thread
>from a main document, you will often be offered the chance to click an icon
>to return to the main document. This is clearly an alternative to
>backtracking through the history. It seems to me that clicking the icon
>should be clean up the history so that it is directly equivalent to
>backtracking to the main document. This would avoid the possibility of
>confusion in the user's mind as to the effects of either course of action.
>
>For this to work we need a hint to the browser to pop the history back to
>the point just before the linked document *last* appeared. This could be
>done via an attribute on the <A> element.

In HyperCard (and some other systems), if you wanted to support the kind of
functionality you're talking about, then you used "push card" and "pop
card." Whenever you went to a "child" document, the "parent" would do:
push card
go card x

Then when the user clicked on the "go back" button, the script would do a
"pop card" clearing the history list and jumping back to the last pushed
card. This also allows you to lead the user down a sub-tree of several
links, then jump back to the lead off point.

So, I think what you're really talking about is stack based push/pop idea.

ka





From altis@ibeam.jf.intel.com  Tue Feb 15 19:31:54 1994 --100
Message-Id: <m0pWUPM-000436C@ibeam.intel.com>
Date: Tue, 15 Feb 1994 19:31:54 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: Identifying scripts by file extension?

At 12:12 PM 2/15/94 +0000, Daniel W. Connolly wrote:
>I'm pretty sure we can become HyTime compliant while were at it. Consider:
>
>in stead of:
>        See <A HREF="foo.html">the foo section</a> for more.
>use:
>        <httploc id="home" host="host.domain" path="/dir1/dir2/file">
>        <relloc id=rel1 locsrc="home" path="foo.html">
>        See <A linkend=rel1>the foo section</a> for more.
>
>in stead of:
>        See <A HREF="ftp://host/dir/file.tex">fred's thesis</a> for more.
>use:
>        <ftploc id=ftp1 host="info.cern.ch" dir="/host/dir" file="file.tex"
>                        content-type="text/x-latex">
>        See <A linkend="http1">fred's thesis</a> for more.

Regarding HyTime compliance: When I don't have to type that junk and when
my HTML(+) editor remembers all the syntaxes for me, then I'm all for
HypTime compliance. Until then, HyTime can stay outside in the vaporware
shed.

ka





From connolly@hal.com  Tue Feb 15 19:46:45 1994 --100
Message-Id: <9402151844.AA04406@ulua.hal.com>
Date: Tue, 15 Feb 1994 19:46:45 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Call For Discussion on HTML/HTML+ design

------- =_aaaaaaaaaa0
Content-Type: text/plain; charset="us-ascii"
Content-ID: <4404.761337859.1@ulua>

[I apologize for the wide distribution of this notice, but I need
to start getting this discussion archived, and www-talk has a
handy hypertext archive.]

The WWW project has recently experienced TREMENDOUS growth, and there
has been a lot of discussion lately about HTML standardization. The
current IETF draft specification of HTML has is expiring, and it's
time to replace it.

Attached are
(1) an mh-alias file that I'm using to define the forum.
(2) an edited collection of misc articles and relavent specs


------- =_aaaaaaaaaa0
Content-Type: text/plain; charset="us-ascii"
Content-ID: <4404.761337859.2@ulua>
Content-Description: html-design alias

# HTML design discussion forum...
html-design:	Dan Connolly -- HTML draft spec author <connolly@hal.com>,\
	Tim Berners-Lee -- WWW Project Lead <timbl@info.cern.ch>,\
	Dale Dougherty -- GNN Publisher <dale@ora.com>,\
	Dave_Raggett -- HTML+ draft spec author <dsr@hplb.hpl.hp.com>,\
	Dave Hollander -- SDL designer <dmh@hpfcma.fc.hp.com>,\
	Elliot Kimber -- HyTime advocate <kimber@passage.com>,\
	Peter Flynn -- HTML educator<pflynn@curia.ucc.ie>,\
	Rob McCool -- NCSA representative<robm@ncsa.uiuc.edu>,\
	Erik Naggum -- SGML advocate <enag@ifi.uio.no>,\
	WWW Talk List -- Archive Mechanism <www-talk@info.cern.ch>


------- =_aaaaaaaaaa0
Content-Type: text/x-html; charset="us-ascii"
Content-ID: <4404.761337859.3@ulua>
Content-Description: current notebook
Content-Transfer-Encoding: quoted-printable

<HTML>
<HEAD>
<TITLE>HTML Successor Design Notebook</TITLE>
</HEAD>
<BODY>

<ADDRESS>Daniel W. Connolly &lt;connolly@hal.com&gt; <P>
$Id: html_design.html,v 1.3 1994/02/15 18:35:08 connolly Exp connolly $</A=
DDRESS>

<H2>Background</H2>

<DL>
<DT>The World Wide Web Initiative:   The Project
<DD><A HREF=3D"http://info.cern.ch/hypertext/WWW/TheProject.html">http://i=
nfo.cern.ch/hypertext/WWW/TheProject.html</A>
<DT>People involved in the WorldWideWeb project
<DD><A HREF=3D"http://info.cern.ch/hypertext/WWW/People.html">http://info.=
cern.ch/hypertext/WWW/People.html</A>
</DL>

<H2>Current (Published Normative) Specs</H2>

<DL>
<DT>IETF draft spec of text/html content-type
<DD><A HREF=3D"ftp://ds.internic.net/internet-drafts/draft-ietf-iiir-html-=
01.txt">ftp://ds.internic.net/internet-drafts/draft-ietf-iiir-html-01.txt<=
/A>
<DT>Remote file 15.254.100.100/pub/htmlplus.dtd.txt
<DD><A HREF=3D"ftp://15.254.100.100/pub/htmlplus.dtd.txt">ftp://15.254.100=
.100/pub/htmlplus.dtd.txt</A>
</DL>

<H2>Current (Informative Changing) specs</H2>

<DL>
<DT>HyperText Mark-up Language
<DD><A HREF=3D"http://info.cern.ch/hypertext/WWW/MarkUp/MarkUp.html">http:=
//info.cern.ch/hypertext/WWW/MarkUp/MarkUp.html</A>
<DT>How to write HTML files
<DD><A HREF=3D"http://curia.ucc.ie/info/net/htmldoc.html">http://curia.ucc=
.ie/info/net/htmldoc.html</A>
</DL>


<H2>Meetings</H2>

<DL>
<DT>WWW-TEI Meeting 19-20th November 1993
<DD><A HREF=3D"http://itdsrv1.ul.ie/Research/WWW/cork_tei_www.html">http:/=
/itdsrv1.ul.ie/Research/WWW/cork_tei_www.html</A>
</DL>


<H2>Discussion</H2>

<DL>
<DT>WWW Talk hypertext archive
<DD><A HREF=3D"http://gummo.stanford.edu/html/hypermail/archives.html">htt=
p://gummo.stanford.edu/html/hypermail/archives.html</A>
<DT>comp.text.sgml index
<DD><A HREF=3D"wais://ifi.uio.no/comp.text.sgml">wais://ifi.uio.no/comp.te=
xt.sgml</A>
<DT>Remote file ds.internic.net/internet-drafts/draft-ietf-iiir-html-01.tx=
t
<DD><A HREF=3D"ftp://ds.internic.net/internet-drafts/draft-ietf-iiir-html-=
01.txt">ftp://ds.internic.net/internet-drafts/draft-ietf-iiir-html-01.txt<=
/A>
<DT> Re: WWW and Hytime (was Explicit Linking is Impossible)
<DD><A HREF=3D"wais://ifi.uio.no/comp.text.sgml/TEXT/11048/1=3Difi.uio.no%=
3A210;2=3D/local/ftp/lib/wais/comp.text.sgml;3=3D0%2011048%20/local/ftp/pu=
b/SGML/comp.text.sgml/by.msgid/19930511.074728.98%40almaden.ibm.com;4=3Dif=
i.uio.no%3A210;5=3D/local/ftp/lib/wais/comp.text.sgml;6=3D0%2011048%20/loc=
al/ftp/pub/SGML/comp.text.sgml/by.msgid/19930511.074728.98%40almaden.ibm.c=
om;7=3D%00;">wais://ifi.uio.no/comp.text.sgml/TEXT/11048/1=3Difi.uio.no%3A=
210;2=3D/local/ftp/lib/wais/comp.text.sgml;3=3D0%2011048%20/local/ftp/pub/=
SGML/comp.text.sgml/by.msgid/19930511.074728.98%40almaden.ibm.com;4=3Difi.=
uio.no%3A210;5=3D/local/ftp/lib/wais/comp.text.sgml;6=3D0%2011048%20/local=
/ftp/pub/SGML/comp.text.sgml/by.msgid/19930511.074728.98%40almaden.ibm.com=
;7=3D%00;</A>
<DT>WWW and Hytime %28was Explicit Linking is Impossible%29 (in comp.text.=
sgml)
<DD><A HREF=3D"wais://ifi.uio.no/comp.text.sgml?WWW+and+Hytime+%28was+Expl=
icit+Linking+is+Impossible%29">wais://ifi.uio.no/comp.text.sgml?WWW+and+Hy=
time+%28was+Explicit+Linking+is+Impossible%29</A>
<DT>WEK: HTML DTD and HyTime
<DD><A HREF=3D"wais://ifi.uio.no/comp.text.sgml/TEXT/12392/1=3Difi.uio.no%=
3A210;2=3D/local/ftp/lib/wais/comp.text.sgml;3=3D0%2012392%20/local/ftp/pu=
b/SGML/comp.text.sgml/by.msgid/19930524.152345.29%40almaden.ibm.com;4=3Dif=
i.uio.no%3A210;5=3D/local/ftp/lib/wais/comp.text.sgml;6=3D0%2012392%20/loc=
al/ftp/pub/SGML/comp.text.sgml/by.msgid/19930524.152345.29%40almaden.ibm.c=
om;7=3D%00;">wais://ifi.uio.no/comp.text.sgml/TEXT/12392/1=3Difi.uio.no%3A=
210;2=3D/local/ftp/lib/wais/comp.text.sgml;3=3D0%2012392%20/local/ftp/pub/=
SGML/comp.text.sgml/by.msgid/19930524.152345.29%40almaden.ibm.com;4=3Difi.=
uio.no%3A210;5=3D/local/ftp/lib/wais/comp.text.sgml;6=3D0%2012392%20/local=
/ftp/pub/SGML/comp.text.sgml/by.msgid/19930524.152345.29%40almaden.ibm.com=
;7=3D%00;</A>
<DT>Request for a valid HTML.DTD @ CERN
<DD><A HREF=3D"wais://ifi.uio.no/comp.text.sgml/TEXT/12992/1=3Difi.uio.no%=
3A210;2=3D/local/ftp/lib/wais/comp.text.sgml;3=3D0%2012992%20/local/ftp/pu=
b/SGML/comp.text.sgml/by.msgid/4E74%40cernvm.cern.ch;4=3Difi.uio.no%3A210;=
5=3D/local/ftp/lib/wais/comp.text.sgml;6=3D0%2012992%20/local/ftp/pub/SGML=
/comp.text.sgml/by.msgid/4E74%40cernvm.cern.ch;7=3D%00;">wais://ifi.uio.no=
/comp.text.sgml/TEXT/12992/1=3Difi.uio.no%3A210;2=3D/local/ftp/lib/wais/co=
mp.text.sgml;3=3D0%2012992%20/local/ftp/pub/SGML/comp.text.sgml/by.msgid/4=
E74%40cernvm.cern.ch;4=3Difi.uio.no%3A210;5=3D/local/ftp/lib/wais/comp.tex=
t.sgml;6=3D0%2012992%20/local/ftp/pub/SGML/comp.text.sgml/by.msgid/4E74%40=
cernvm.cern.ch;7=3D%00;</A>
</DL>

<H2>Related Specs</H2>

<DL>
<DT>internet-drafts/draft-levinson-sgml-00.txt
<DD><A HREF=3D"ftp://ds.internic.net/internet-drafts/draft-levinson-sgml-0=
0.txt">ftp://ds.internic.net/internet-drafts/draft-levinson-sgml-00.txt</A=
>
</DL>


<H2>Related Projects</H2>

<DL>
<DT>GNN Home Page
<DD><A HREF=3D"http://nearnet.gnn.com/gnn/GNNhome.html">http://nearnet.gnn=
.com/gnn/GNNhome.html</A>
</DL>

<H2>Notes on Tools</H2>

This document was composed by:
<OL>
<LI> Reading news with GNUS on lemacs at an X terminal, saving articles in=
 mh-mail folders
<LI> Browsing the comp.text.sgml archive with the emacs interface
to WAIS over a dial-up connection.
<LI> Retracing my steps with Mosiac and creating a hot-list.
<LI> Mailing the hotlist to myself to get it into HTML.
<LI> Editing the resulting hotlist with html-mode in lemacs.
<LI> Validating the result with sgmls and a local copy of the HTML DTD.
<LI> Archiving the result locally with RCS
<LI> Using mhn to build a MIME message out of the results.
</OL>

<P>
It's too bad that a lot of the audit trail gets lost when building
these things. For example, I've coded links to some USENET news
articles as WAIS URL's. Even though I know the message-id of the news
article and some of my readers may have copies of those articles
locally, I cannot express the link to them in HTML.

 <P>It's also too bad that the hotlist feature of Mosiac doesn't use HTML
in the first place.


 <P>These are things I hope we fix soon.

 <P>I tried to use www -listrefs to generate a plaintext version, but
I couldn't get it to work like I wanted... I think I gotta get tkWWW
installed around here!

</BODY>
</HTML>

------- =_aaaaaaaaaa0--



From letovsky-stan@CS.YALE.EDU  Tue Feb 15 16:05:40 1994 --100
Message-Id: <199402151423.AA28819@RA.DEPT.CS.YALE.EDU>
Date: Tue, 15 Feb 1994 16:05:40 --100
From: letovsky-stan@CS.YALE.EDU (Stan Letovsky)
Subject: Default URLs

Connolly's recent discussion of URL syntax (See 

	Subject: Re: Identifying scripts by file extension?
	From:    "Daniel W. Connolly" <connolly@hal.com>
	Date:    Tue, 15 Feb 94 12:04:04 )

brings to mind a related problem I encountered recently, in using the
PATH_INFO hack to try to communicate extra information -- which
database to access -- to a general purpose DB accessing script, which
returns dynamically generated HTML docs full of links to DB-objects.
It would be convenient for the default URL on these links to be a
reference to the same DB -- so convenient, in fact, that it would
result in a 20% reduction in the size of some documents. Mosaic 2.1
uses the current URL as the default, but does not pass on the current
PATH_INFO, which means the URL must include a pathname. I think this a
bug, personally. 

-Stan



From dsr@hplb.hpl.hp.com  Tue Feb 15 19:51:28 1994 --100
Message-Id: <9402151841.AA13772@manuel.hpl.hp.com>
Date: Tue, 15 Feb 1994 19:51:28 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Server control over history?

> In HyperCard (and some other systems), if you wanted to support the kind of
> functionality you're talking about, then you used "push card" and "pop
> card." Whenever you went to a "child" document, the "parent" would do:
>    push card
>    go card x

> Then when the user clicked on the "go back" button, the script would do a
> "pop card" clearing the history list and jumping back to the last pushed
> card. This also allows you to lead the user down a sub-tree of several
> links, then jump back to the lead off point.

> So, I think what you're really talking about is stack based push/pop idea.

Can you explain the idea in more detail? Here is my current understanding of it:

Right now each time you follow a link the current document is pushed. The "Back"
button pops the document of the top of the stack, while a "Home" link on a document
behaves like any other link and stupidly pushes the current document, when what we
want it to do is to pop the stack right back to just before the home document.

I guess we could add actions to:

    a) push a marker to the current history stack position
    b) pop a marker and clear the history stack accordingly

These could be implemented using a couple of new attributes for links.
Another idea is to encourage people to use <LINK> rather than <A> for such
"Go Home" links. This means that such links couldn't be scrolled off the window.
For this to be attractive, we also need to allow images for links, perhaps by
adding a SRC attribute to LINK. What do people think of this?

Dave



From enag@ifi.uio.no  Tue Feb 15 15:27:43 1994 --100
Message-Id: <19940215134758.3289.gyda.ifi.uio.no@ifi.uio.no>
Date: Tue, 15 Feb 1994 15:27:43 --100
From: enag@ifi.uio.no (Erik Naggum)
Subject: Re: <draft-ietf-iiir-html-01.txt, .ps> to be deleted.

[Eliot Kimber] (1994-02-14 23:02:36 -0500)

|   I'm afraid that on this point there can be no compromise.  If a
|   document is an SGML document then it *must* start with a DOCTYPE
|   declaration and include the document element *in the same entity*.  The
|   definition of SGML document entity is quite clear on this.  In fact, it
|   is impossible to know whether or not a given stream of data is valid
|   SGML *unless* there is a doctype declaration (and an SGML declaration,
|   which may be implied by the processing system).

I beg to differ.

This is a complicated issue, and I'm at work now, so I can't elaborate
until sometime tonight (or this afternoon, EST), but the reason it has
become complicated is that there has been a general failure to understand
the distinction between what an SGML parser will see, and what was really
there.  Charles Goldfarb and I quickly came to the conclusion that the
record boundary characters were figments of the SGML entity manager's
interface, and we have worked hard to specify a mechanism that allows
"storage objects" (a generalization of "file") to identify their record
boundary convention (a generalization of "line terminator") such that the
entity manager could do the right thing with them.  We also took this
argument further, realizing that the "entity" is not a file, or a string of
characters "out there".  It's a string of characters as seen by the parser.
We allowed substrings of storage objects, concatentation of (substrings of)
storage objects, and the reason we have "storage object" instead of "file"
is the realization that the user needs the ability to identify the "storage
manager" that can take a "storage object specification" and convert it to a
string of characters.  We provide two default storage managers: "file", and
"literal".  A user can thereby provide his own storage manager to read text
from an in-memory buffer, from a network resource, from a database, from
the execution of a program, etc.

Conceptually, there is no limit to the number of transformations that could
be applied to the storage objects before they were presented to the parser
as the string of characters of an entity.

As an extension of this idea was the realization that people work with one
particular document type much more than they work with others.  It would be
a waste to parse the same DTD thousands of times a day, and we got the idea
that a pre-parsed DTD could be stored in some way transparent to the user,
which would be used by the SGML parser.  This folds itself neatly into the
idea of a resumable parser that stores enough state information that it can
resume from any point in the parsing process.  Right after the DTD parsing
is just one example.  The idea was that a parser client could parse up to a
certain point, keep a "bookmark", and resume parsing from there if the text
following this point changed.  Well, apply this to a DTD, and the whole
instance could change.

I believe I have outlined a standards-conforming process that can be used
to support the initial view that HTML+ need not include a DTD in every file
(a reference would suffice), and need not parse the DTD itself (a pre-
parsed version will do).  Since the HTML+ application is restrictive, the
number of document type declarations that will conform to the application
is small, and can, for all practical uses, be limited to one, which is the
one that all HTML+ processor implement.

However, this is not really such a big deal.  I have argued that validating
a DTD is a different task than using it, and some parsers implement this
distinction.  Validating is _hard_.  Parsing it to use it is relatively
easy, and takes almost the same amount of resources required to read and
process a binary format resulting from the pre-parsed DTD.  (Barring tons
of comments in the DTD, or lots of small files with DTD fragments.)

If we also assume that HTML+ document authors validate their documents
before they ship them (a not unfriendly requirement when you consider the
alternative), parsing relative to a DTD is a relatively simple process.  If
done with something other than an SGML parser, however, it can be
expensive, hard to get right, and terribly complicated.  Therefore, an SGML
parser should be used for this purpose.  Whatever it is that actually does
the job will be an "SGML parser", although probably not a _conforming_ SGML
parser.  Using a publicly available tool that can communicate with its
client in the way I have outlined should offer some significant advantages.
I also believe using POEM would solve many problems in retrieving files
over the network, and would thus simplify the entire parsing process.

Well, duty calls.  I will have to continue later.

Best regards,
</Erik>
--
Erik Naggum <erik@naggum.no> <SGML@ifi.uio.no>  |  Memento, terrigena.
ISO 8879 SGML, ISO 10744 HyTime, ISO 10646 UCS  |  Memento, vita brevis.



From connolly@hal.com  Tue Feb 15 19:58:11 1994 --100
Message-Id: <9402151853.AA04421@ulua.hal.com>
Date: Tue, 15 Feb 1994 19:58:11 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Identifying scripts by file extension? 

In message <m0pWUPM-000436C@ibeam.intel.com>, Kevin Altis writes:
>
>Regarding HyTime compliance: When I don't have to type that junk and when
>my HTML(+) editor remembers all the syntaxes for me, then I'm all for
>HypTime compliance. Until then, HyTime can stay outside in the vaporware
>shed.
>

Thank you for your enlightening and persuasive editorial comments on
the future of building high-integrity Web documents. ;-)

Get a clue. Think ahead a little. Information is too valuable to trust
to hacked systems. We need to get this right. We don't have to build a
whole HyTime system to write HyTime compliant documents. Hacked up
tools are fine, as long as the data is compliant with a well-defined
formalism.  If we develop HyTime compliant docs now, and full-blown
HyTime systems come along, we will have all our valuable information
ready to use.

Dan



From pflynn@curia.ucc.ie  Tue Feb 15 15:06:37 1994 --100
Message-Id: <9402151025.AA15748@curia.ucc.ie>
Date: Tue, 15 Feb 1994 15:06:37 --100
From: pflynn@curia.ucc.ie (Peter Flynn)
Subject: Re: <draft-ietf-iiir-html-01.txt, .ps> to be deleted.

> My major concern is that it implies a tremendous increase in the
> complexity of the HTML parser. With my original HTML specification, an
> HTML parser only parsed the instance part of the SGML document. With
> this HTML+ specification, WWW clients will have to parse the prologue
> as well. 

If the prolog is valid SGML, this should not be a problem to code (just
borrow it) but what kind of slow-down will this mean for users? My guess
is that it won't be used heavily by most authors until SGML itself
becomes more widespread and they start using compliant editors.

Q. By "parse" do we mean "check for conformance and reject if in error"
as well as "extract anything which needs acting upon". The whole area
of what a browser should do when it encounters invalid HTML+ needs some
more thought.

Rob: at the Nov meeting, both Lou M and Chris W were of the opinion that
whatever the formalities of SGML, browsers ought just to make the best
effort they can and not reject a document (ever?). Bill P felt they
should be more critical. At the other extreme, browsers _could_ actually
refuse to display an invalid instance, and mail the author (or webmaster)
with a parser log (probably not a useful thing to do, though). Somewhere
between these lies a workable solution: what's the Mosaic team's take?

> You have, however, simplified matters by not putting any parameter
> entities in content models. This means that WWW clients won't have to
> deal with individual documents introducing new element types "on the
> fly."

This is what the RENDER tag is for, surely?

> But you've introduced OMITTAG, <!ENTITY> parsing, and lots of other
> stuff. If we plan to include a full blown SGML parser in every WWW
> client, why not use all the syntactic sugar like short references and
> cool stuff like that while we're at it?

Why?

> Let's get formal why don't we: I do not mean that we should be able to
> take any RTF file and convert it to HTMLPLUS, or MIF for that matter.
> But I think it's crucial that there exist invertible mappings
> 
> 	h : HTML -> RTF
> and
> 	g : HTML -> MIF
> and
> 	h : HTML -> TeXinfo
> 
> so that I can take a given HTML document, convert it to RTF, and
> convert it back and get exactly what I started with (the same ESIS,
> that is... perhaps SGML comments and a few meaningless RE's would get
> lost).
> 
> >For instance, document text is forced to appear within paragraph elements
> >which act as containers. Documents are broken into divisions using the
> >new DIVn elements which give substance to the notion that headers start
> >sections which continue up to the next peer or larger header. 

It's worth pointing out what several users have already said to me when
I suggested making <P> a container (let me quote from one of them):

>>> By the way, you get weird results when you try an SGML-based
>>> filter (e.g., Omnimark) on an HTML file in which P is empty.
>>> There's all that uncontained PCDATA sloshing around.

Omnimark is arguably the robustest *-->SGML-->* converter around. If
we want a completely (well, 99%) invertible conversion (which I think
we do), then we should look to sticking with concepts that are known
to work.

> If we're going to burden WWW clients with all this rich structure and
> OMITTAG parsing, why don't we go with something like DocBook, which
> has a proven ability to capture the structure of existing technical
> documents, in stead of trying to roll our own.

Because a large amount of what people may want to put up on web servers
is not necessarily technical documentation. This is why we went to the
trouble of bringing together the TEI and some browser writers. Using
DocBook would be _way_ too limiting.

> >Similarly, missing <P> tags can be inferred when the browser sees
> >something belonging to %text. This neatly deals with the common case
> >where some authors think of <P> as a paragraph separator or something
> >you should put at the end of each paragraph (this view is promulgated
> >by Mosaic documentation). 

I think this is a dangerous path to tread. Yes, what you suggest could be 
done, but I think it's going too far down the path of trying to let
plaintext documents masquerade as HTML.

> Is this form of inference consistent with the SGML standard? Or is
> this a non-standard extension to support legacy HTML documents?

Goldfarb discusses a similar concept in The Book. I haven't got it here
but I can look it up. It involves redefining record-end and record-start
so that they can act as GIs, I think.

> >My HTML+ browser works this way, using a top-down parser which permits
> >most elements to have omissable start and end tags, using the context
> >to identify missing tags. Each element is associated with a procedure.
> >Its easy this way to recover the structure of badly authored documents
> >e.g. with missing <DL> start tags.  

It also makes it much easier for the user to foul things up if they don't
appreciate what they are doing. It's "bad" enough for non-SGML people to
cope with remembering to insert tags to do things, but if they have to
remember what the effect is of _not_ inserting tags, just when we were
getting to the stage of getting them used to SGML....well...

> >In future, we expect authors will use specialized wysiwyg editors for HTML+
> >or automated document format conversion tools and hence produce documents
> >which naturally conform to the DTD.

Don't even have to be WYSIWYG...but they will need to be conformant.

> Hmmm... as long as there are no un-broken documents that would be
> misinterpreted by these heuristics, I think it's a great idea. (Again,
> though, I'd like to see a formal argument that this is the case.)

Which is why we need to formalise the frozen HTML right now, including
making <p> a container, so that we have (a) a benchmark for existing
browsers; (b) a standard for authors to use that works properly and (c)
something we can base future HTML+ engines on for the bits where they have 
to deal with legacy docs.

> >Actually, once you state that HTML is an SGML format, then formally each
> >document can extend the DTD.

I don't think that's meant to be the case. You can allow the inclusion of 
entity definitions but I don't think you can let people rewrite the DTD 
on the fly.

> > HTML+ merely exploits this to show authors
> >how to declare which extension they wish to use: forms, tables, figures etc.
> >I owe a debt here to Lou Burnard and the TEI DTDs which showed me how and
> >why to use this approach. 

The TEI DTD is somewhat bigger and more complex than HTML+. If we were to
go for something else, this would be the one, not DocBook.

> >I have investigated HyTime compliance with Yuri Rubinsky and Elliot Kimber
> >(Dr Macro), and know how to add this in. At the moment though, most people
> >in the WWW community see little value in switching to a model which forces
> >you to declare hypertext links at the start of the document.

Huh. Authors of large documents see it.

> >will change if and when HyTime gets widely adopted. On the other hand, I
> >feel it is essential for HTML+ to conform to SGML. Without this, publishers
> >and businesses will tend to see WWW as a passing experiment that needs to
> >be replaced by something on a more professional/commercial footing. This is
> >why I am working so hard to extend HTML into something that meets publishers
> >and users expectations for document delivery. NCSA have done their bit - now
> >its my turn to roll up my sleeves and get down to serious programming :-)
> 
> There's a lot of good stuff in this latest DTD. I think we need a more
> sophisticated, fault-tolerant linking element, and a few other things,
> but you might be on the right track.

I think Dave is. I'm a tad worried about the emphasis on publishing: still
way too many publishers think of SGML as a wordprocessor, and spend $000s
on in-house DTDs which implement dozens of attributes for each tag, giving
font info and hard-coded positional information. I would be much happier
seeing HTML+ start to educate them.

I'm going to inject a plea here for one small tweak to HTML+ which I
mentioned in November. A NUM attribute for <P>, <LI>, <DT> and <DD> for
recording (not displaying or predicating) the original numbering sequence 
in cases where the HTML has been generated by a converter. This would 
make finding the original a whole lot easier.

> p.s. I'd like to start some sort of html-successor-design discussion
> form. Is comp.infosystems.www, comp.text.sgml, or www-talk a suitable
> forum? Shall we create one?

I think it should carry the string `html' or `htmlplus' rather than `www'.
comp.text.sgml.html might do.

///Peter



From letovsky-stan@CS.YALE.EDU  Tue Feb 15 16:00:10 1994 --100
Message-Id: <199402151409.AA28474@RA.DEPT.CS.YALE.EDU>
Date: Tue, 15 Feb 1994 16:00:10 --100
From: letovsky-stan@CS.YALE.EDU (Stan Letovsky)
Subject: Re: Server control over history? 

From: Dave_Raggett <dsr@hplb.hpl.hp.com>
To: Multiple recipients of list <www-talk@www0.cern.ch>
Subject: Re: Server control over history?

Neal Holtz writes:

> Is anyone else interested in having the server have some control over
> a clients history list?
> ... In these cases, you probably do not want to see these nodes put
> on the history list.

Dave Raggett replied:
I have been worrying about this too. ...
Another common case occurs with fill-out forms. Keeping a history of
the successive stages of filling out a form, and the updates to that
form by the server is of dubious value (it would only make sense in the
context of a multi-level UNDO mechanism).  So it makes sense to flag
intermediate steps as not to be placed on the history stack. This could
be done by an element in the document's head or an HTTP header (the two
are formally equivalent).

So how about:

        a) adding an attribute to the <A> element to rollback the
           history stack to the node before the *last* occurrence of
           the linked node, e.g.

           <A HREF="main_doc.html" ROLLBACK>Return to Home page</A>

        b) Defining a new HTTP header for history hints which servers
           can generate based on META elements in document heads, e.g.

           <META NAME="History" VALUE="discard">

For (b) clients would get the HTTP header

           History: discard

Other values such as "rollback" may be useful. This seems preferable
to adding yet another element to HTML+. Let me know what you think,
and I will ammend the HTML+ spec.

Dave Raggett

----- End Included Message -----

>> Letovsky replies:

In http://procrustes.biology.yale.edu/genera.html#Extensions
(Subsection on Immediate Commands) I propose another approach to this
issue, based on techniques common in database interface tools.
Specifically, I propose:

	o a hash table allowing arbitrary attribute+value pairs to
	be associated with widgets (and documents).
	o html extensions allowing those attributes to be accessed and set.
	These include a header directive analogous -- I think -- to Dave's proposed
           <META NAME="History" VALUE="discard">
	which I called 
		<NOPUSH>
	the idea being that inhibits pushing a new document onto the stack,
	which is the standard client action.

	The GET and PUT directives proposed there constitute an approach to
	implementing a means of communicating fill-out form deltas in a
	non-pushing context. I urge people thinking about this issue to look
	beyond simply setting the visible values of the fields, and to consider
	the need to manipulate invisible associated values. These may be previous
	values of the fields, needed in order to create, e.g. SQL statements along
	the lines of
		update table set field = newvalue where field = oldvalue
	as well as for undo change commands; also to store surrogate keys (ID#s)
	which you may not want visible, but which the form needs to know about
	in order to send the server a message that allows SQL to be generated that
	refers to those keys.

Cheers. 

-Stan




From connolly@hal.com  Tue Feb 15 20:09:20 1994 --100
Message-Id: <9402151906.AA04444@ulua.hal.com>
Date: Tue, 15 Feb 1994 20:09:20 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Server control over history? 

In message <9402151841.AA13772@manuel.hpl.hp.com>, Dave_Raggett writes:
>I guess we could add actions to:
>
>    a) push a marker to the current history stack position
>    b) pop a marker and clear the history stack accordingly
>
>These could be implemented using a couple of new attributes for links.
>Another idea is to encourage people to use <LINK> rather than <A> for such
>"Go Home" links. This means that such links couldn't be scrolled off the window.
>For this to be attractive, we also need to allow images for links, perhaps by
>adding a SRC attribute to LINK. What do people think of this?

I think a whole lot of folks have already thought a whole lot about
problems like this, and they thought this stuff was important enough
to standardize on.

They call their work HyTime.

We should take a look at it and see if we can't borrow from what they
did.

For example, it should be trivial to derive this LINK element from the
hytime ilink architectural form. See

Message-ID: <19930511.074728.98@almaden.ibm.com>
Date: 11 May 1993 09:36:53 -0400 (19930511.133653)
Newsgroups: comp.text.sgml,alt.hypertext
Subject: Re: WWW and Hytime (was Explicit Linking is Impossible)

at:

wais://ifi.uio.no/comp.text.sgml/TEXT/11048/1=ifi.uio.no%3A210;2=/local/ftp/lib/wais/comp.text.sgml;3=0%2011048%20/local/ftp/pub/SGML/comp.text.sgml/by.msgid/19930511.074728.98%40almaden.ibm.com;4=ifi.uio.no%3A210;5=/local/ftp/lib/wais/comp.text.sgml;6=0%2011048%20/local/ftp/pub/SGML/comp.text.sgml/by.msgid/19930511.074728.98%40almaden.ibm.com;7=%00;

for one way of doing it.

Dan



From atotic@ncsa.uiuc.edu  Tue Feb 15 16:55:36 1994 --100
Message-Id: <9402151551.AA01157@void.ncsa.uiuc.edu>
Date: Tue, 15 Feb 1994 16:55:36 --100
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: client local file system access issues

> Mac Mosaic does file:///drive name/path Spaces are okay at least in the
> drive name.

MacMosaic actually accepts:

file://atotic1.ncsa.uiuc.edu/AlexWork/Test files/main.html
file://localhost/AlexWork/Test files/main.html
file:///AlexWork/Test files/main.html

as valid paths for the local file. There are certain problems with using / in
your names, but other than that, I think its local file handling is
completer.

> file or handing it off to an external browser. So, the case of
> file://localhost should be treated special be the client so that it just
> goes and used the local file. You won't have to wait for a 14MB audio/video
> file to get copied to a different directory before the external mpeg player
> launches to play the file.

Yeah, we just have not gotten around to doing it.

Aleks




From roeber@vscrna.cern.ch  Tue Feb 15 17:18:37 1994 --100
Message-Id: <9402151615.AA14103@dxmint.cern.ch>
Date: Tue, 15 Feb 1994 17:18:37 --100
From: roeber@vscrna.cern.ch (Frederick G.M. Roeber)
Subject: Re: Server control over history?

>From: burchard@horizon.gw.umn.edu (Paul Burchard )
>Message-ID:<5BC8@cernvm.cern.ch>

>At the Geometry Center, we have just released a suite of interactive
>graphical WWW applications based on fill-out forms, and have found
>the automatic UNDO capability to be *extremely* helpful.

It is also extremely useful in the on-line sokoban game.
--
<a href="http://info.cern.ch/roeber/fgmr.html">Frederick.</a>



From MMASRUR@smail.srl.ford.com  Tue Feb 15 20:29:06 1994 --100
Message-Id: <9402151926.AA11497@dxmint.cern.ch>
Date: Tue, 15 Feb 1994 20:29:06 --100
From: MMASRUR@smail.srl.ford.com (M. Abul Masrur)
Subject: Message sent from mmasrur.



Thanks to all of you who responded to my message yesterday on how
to acquire the mosaic software and use it for stand alone PC's.

After getting the files from the proper ftp site, I tried to run it and I
found the followings:  whereas it worked ok with a particular machine
(a stand alone personal computer) which has the PCTCP Version 2.11,
--  it did not work in my own machine which has the PCTCP Version
2.2 (a newer version of PCTCP).  I got the error message which reads
like this :
**************************************************
      PCTCPAPI088: Incompatible PCTCPAPI.DLL & WNET386.DLL
      versions detected.

      PCTCPAPI.DLL version 2.1 
      WNET386.DLL version 2.2 patch level 2

      Your application would be adversely affected.  DLL
      initialization failed.
**************************************************

I am not sure if the mosaic.exe is capable of running with only older
versions of PCTCP.  I would appreciate your advice.

Thank you for your help.

Regards.
ABUL

ABUL MASRUR
Sci. Rsch. Labs, Ford Motor Co.
E-Mail: MMASRUR@SMAIL.SRL.FORD.COM

****  This message was sent to the address(es) listed below.  ****
ncsa.uiuc.edu	mosaic-x
info.cern.ch	www-talk




From dsr@hplb.hpl.hp.com  Tue Feb 15 17:24:29 1994 --100
Message-Id: <9402151611.AA13602@manuel.hpl.hp.com>
Date: Tue, 15 Feb 1994 17:24:29 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Server control over history?

>> Another common case occurs with fill-out forms. Keeping a
>> history of the successive stages of filling out a form, and
>> the updates to that form by the server is of dubious value
>> (it would only make sense in the context of
>> a multi-level UNDO mechanism).

> Exactly---but this sort of automatic unlimited UNDO capability is a  
> wonderful thing for increasing interactivity.  The higher bandwidth  
> of interaction you want to achieve, the more useful it becomes.

But, how do you get the database at the other end to roll back
in sync with the browser. At the very least the browser would have
to send a message to the server to roll-back each step. This is well
outside the current HTTP/fill-out forms mechanism. Worse, I can't
see how its possible to roll-back the state in all cases.

Dave



From altis@ibeam.jf.intel.com  Tue Feb 15 21:11:01 1994 --100
Message-Id: <m0pWW2T-000437C@ibeam.intel.com>
Date: Tue, 15 Feb 1994 21:11:01 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: SGML parsing, moving between formats

At  1:19 PM 2/15/94 +0000, Daniel W. Connolly wrote:
>One of the things I released (or was just about to release when I
>changed jobs...) was an SGML compliant HTML parser in a few hundred
>lines of vanilla ANSI C.

Great! But did this code go into a black hole with the job change or is it
available somewhere, need some testing, or what? :)

>>One issue in formalising HTML+ was in providing an adequate structure
>>while dealing with legacy documents. As you can see in my current DTD,
>>documents have a richer structure than with the old HTML DTD.
>>
>
>Yes... and it seems to me (at first glance... I'll have to look more
>closely...) that we've lost the ability to translate HTML to Microsoft
>Word or FrameMaker without any loss of information.
>
>Let's get formal why don't we: I do not mean that we should be able to
>take any RTF file and convert it to HTMLPLUS, or MIF for that matter.
>But I think it's crucial that there exist invertible mappings
>
>        h : HTML -> RTF
>and
>        g : HTML -> MIF
>and
>        h : HTML -> TeXinfo
>
>so that I can take a given HTML document, convert it to RTF, and
>convert it back and get exactly what I started with (the same ESIS,
>that is... perhaps SGML comments and a few meaningless RE's would get
>lost).

Folks, this is extremely important. It may be a b*tch to move documents
from RTF, MIF, etc. to HTML right now, but it should be a breeze to go the
other direction, at least. If we make it difficult or impossible to go from
HTML to RTF, MIF, etc., then we've blown it. If we make it possible to go
from HTML -> RTF -> HTML without loss of information, formatting
information, etc. then we win!

Brief reality check sermon follows:
Keep in mind that the world will neither go whole hog into SGML (HTML), nor
continue to deal execlusively with application specific document formats
such as Microsoft Word or Quark Xpress. We're going to have to deal with
both camps. There isn't a single big company (or small company) or
organization that isn't sick and tired (mad as hell to be precise) of NOT
being able to move formatted information between multiple applications
without loss. Frankly, I'm surprised that the blood of vendors hasn't been
spilt at trade shows (it almost was at Seybold last October). However, it
isn't our job to come up with the ULTIMATE SGML DTD holy grail document
format, etc., though what Dave is doing is extremely useful for us as a
measuring stick and focal point. It IS our job to make it easy for users to
publish information on the Web, HyperText or otherwise. Concentrate on
making authoring/publishing easy and keep in mind that document formats
have to rendered at some point. If you make rendering too hard, the only
people that will be able to spend the bucks to write clients capable of
doing the rendering will be big companies like HP, Intel, and Microsoft.
Plus, capabilities between clients will vary so much, that the presentation
independence will be completely lost, because half of what client A can
render, can't be rendered by client B in any meaningful way. If you make it
too difficult to author/publish hypertext, then most of the information on
the Web will just end up being documents to be displayed by applications
external to the WWW client browser and the browser ends up being an
elaborate Gopher substitute.
Sermon off

ka





From altis@ibeam.jf.intel.com  Tue Feb 15 21:16:44 1994 --100
Message-Id: <m0pWW2W-000439C@ibeam.intel.com>
Date: Tue, 15 Feb 1994 21:16:44 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: Caching and metainformation

At  3:38 PM 2/15/94 +0000, Paul Burchard wrote:
>altis@ibeam.jf.intel.com (Kevin Altis) writes:
>> In the cases of documents that are candidates to cache, we
>> need to explicitly state which HTTP metainformation
>> "fields" a server needs to send in order for the document
>> to be cached correctly.
>
>Could you clarify how this would affect POST forms?  Are you
>suggesting that POST forms will have to include explicit
>metainformation to prevent themselves from being cached (which would
>seem to be undesirable in most cases)?

A Caching/Proxy server might cache a FORM coming from a server, but would
never cache the FORM being POSTed to that server. Perhaps, caching FORMs
from a server is always a bad idea, in which case, a tag in the HEAD area
needs to identify that a FORM shouldn't be cached, say Expires: always.

ka





From altis@ibeam.jf.intel.com  Tue Feb 15 21:21:43 1994 --100
Message-Id: <m0pWW2Y-00043AC@ibeam.intel.com>
Date: Tue, 15 Feb 1994 21:21:43 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: Problem with "~user/file.html"

At 11:56 AM 2/15/94 +0000, neuss@igd.fhg.de wrote:
>Ok, let's say you want to do something like click *here* to get info
>about the author of this document. The form "/~userid/me.html" only
>works over the server, and the form "/whole/path/some/where/me.html"
>only works with local access! What can I do???

Two things to be aware of. One, I added a users directory under htdocs for
our server, then added:
Alias /~/ /users/
to my server setup (ncsa_httpd). So, a http server relative reference of
~username or /users/username is equivelant. What you put in the
htdocs/users is your business, it could alias to the public_html for each
username or it could point only to those users that want to have their
directories available or it could point to the users home directory. BTW,
creating 700 aliases is a piece of cake if you just write a script to go
through your /etc/passwd file.

In your Unix file system (not the httpd server root), if you had a /users
directory, so that at the shell prompt, each user could do a "cd
/users/username" to easy users home directory, then a local reference
should work the same as a server reference for user directories.

Anybody else doing something like this?

ka





From connolly@hal.com  Tue Feb 15 21:27:38 1994 --100
Message-Id: <9402152009.AA04644@ulua.hal.com>
Date: Tue, 15 Feb 1994 21:27:38 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: HTML DTD: past, present, future [was: HTML DTD and HyTime]

------- =_aaaaaaaaaa0
Content-Type: text/plain; charset="us-ascii"
Content-ID: <4639.761342909.2@ulua>
Content-Description: plain text version

                                Toward a Formalism for Communication On the Web
                                          Daniel W. Connolly <connolly@hal.com>
                                                                               
         $Id: html_essay.html,v 1.2 1994/02/15 20:07:12 connolly Exp connolly $
                                                                               
Status

   I had hoped to polish this more before publishing it, but I can't seem to
   get caught up... there's so much new stuff all the time!
   
                SOME BACKGROUND ON SGML FOR THE WORLD-WIDE WEB
                                       
   In late 1992 and early 1993, I did quite a bit of work on the HTML DTD while
   I was working at Convex in the online documentation group.
   
   When I began, there was the LineMode browser and the NeXT implementation,
   and a few nodes in The Web describing HTML with some oblique references to
   SGML. I was not intimately familiar with SGML, but I was quite familiar with
   the problems of document interchange, and I was eager to apply some of my
   formal systems background to the problem.
   
On Formally Unconvertable Document Formats

   My experience with document interchange led me to classify document formats
   using the essential distinction that some are "programmable" and some are
   not. Most widely used source forms are programmable: TeX, troff, postscript,
   and the like. On the other hand, there are several "static" formats: plain
   text, Microsoft RTF, FrameMaker MIF, GNU's TeXinfo,
   
   The reason that this distinction is essential with respect to document
   interchange is that extracting information from documents in "programmable"
   document formats is equivalent to the halting problem. That is, it is
   arbitrarily difficult and cannot be automated in a general fashion.
   
   For example, I conjecture that it is impossible to write a program that will
   extract the third word from a TeX document. It would be an easy task for 80%
   of the TeX documents out there -- just skip over some formatting stuff and
   grab the third bunch of characters surrounded by whitespace. But that
   "formatting stuff" might be a program that generates 100 words from the
   hypenation dictionary. So the simple lexical scan of the TeX source would
   find a word that is not third word of the document when printed.
   
   This may seem like an obscure and unimportant problem, but I assure you that
   the problem of converting TeX tables to FrameMaker MIF is just as
   unsolvable.
   
   So while "programmable" document formats have the advantage that features
   can be added on a per-document basis, they suffer the disadvantage that
   these features cannot be recovered by the machine and translated in an
   automated fashion.
   
Document Formats as Communications Media

   If we look at document formats in light of the conventional
   sender/message/medium/receiver communications model, we see that document
   formats capture the message at various levels of "concreteness".
   
   The message begins as a collection of concepts and ideas in the mind of the
   sender. In order to communicate, the sender and receiver must share some
   language. That is, they must both understand some common set of symbols and
   the way those symbols combine to represent ideas. The senders job is to
   express the message in terms of the common symbols and express them on the
   medium -- that is "render" or "present" them. The the medium stimulates the
   receiver to reconstruct the symbols in his/her brain -- that is, the
   receiver "interprets" or "recognizes" the symbols from the medium. Those
   symbols interact with other symbols in the receiver's brain, and the
   receiver "gets the message."
   
   The communications medium is often a layered combination of more and less
   concrete media. For example, folks first render their ideas in the symbology
   of the English language, and then render those symbols as sequences of
   spoken phonemes or written characters. Those written characters are in turn
   combinations of lines, curves, strokes, and points. The receiving folks then
   assemble the strokes into characters, the characters into words, the words
   into phrases, sentences, thoughts, ideas, and so on.
   
   The most common and ubiquitous document format, plain ASCII text, captures
   or digitizes messages at the level of written characters. PostScript
   captures the characters as lines, curves, and paths. The GIF format captures
   a document as an array of pixels. GIF is in many ways infinitely more
   expressive than plain text, which is limited to arrangements of the 96 ASCII
   characters.
   
   The RTF, TeX, nroff, etc. document formats provide very sophisticated
   automated techniques for authors of documents to express their ideas. It
   seems strange at first to see that plain text is still so widely used. It
   would seem that PostScript is the ultimate document format, in that its
   expressive capabilities include essentially anything that the human eye is
   capable of perceiving, and yet it is device-independent.
   
   And yet if we take a look at the task of interpreting data back into the
   ideas that they represent, we find that plain text is much to be preferred,
   since reading plain text is so much easier to automate than reading GIF
   files (optical character recognition) or postscript documents (halting
   problem). In the end, while the source to a various TeX or troff documents
   may correspond closely to the structure of the ideas of the author, and
   while PostScript allows the author very precise control and tremenous
   expressive capability, all these documents ultimately capture an image of a
   document for presentation to the human eye. They don't capture the original
   information as symbols that can be processed by machine.
   
   To put it another way, rendering ideas in PostScript is not going to help
   solve the problem of information overload -- it will only compound the
   situation.
   
   As a real world example, suppose you had a 5000 page document in PostScript,
   and you wanted to find a particular piece of information inside it. The
   author may have organized the document very well, but you'd have to print it
   to use those clues. If the characters aren't kerned much, you might be able
   to use grep or sick a WAIS indexing engine on it. Then, once you've found
   what looks like postscript code for some relavent information, you'd pray
   that the document adheres to the Adobe Document Structuring conventions so
   that you could pick out the page containing the information you need and
   view that page.
   
   If that's too perverse, look at the problem of navigating a large collection
   of technical papers coded in TeX. Many of the authors use LaTeX, and you may
   be able to convince the indexing engine to filter out common LaTeX
   formatting idioms -- or better yet, weight headings, abstracts, etc. more
   heavily than other sections based on the formatting idioms. While there are
   heuristic solutions to this problem that will work in the typical 80%/20%
   fashion, the general solution is once again equivalent to the halting
   problem; for example, individual documents might have bits of TeX
   programming that change the significance of words in a way that the indexing
   engine won't be able to understand.
   
SGML as a Layered Communications Medium

   So where does SGML fit into the sender/message/medium/receiver game?
   
   I'll use PostScript as a basis of comparison. The PostScript model consists
   of a fairly powerful and general purpose two dimensional imaging model, that
   is, a set of primitive symbols for specifying sets of points in two
   dimensions using handy computational techniques, and a general purpose
   programming model for building complex symbols out of those primitives. That
   model is applied extensively to the problem of typography, and there is a an
   architecture (that is, a set of well known symbols derived from the
   primitives) for using and building fonts.
   
   So to communicate message consisting of symbols from human communications in
   PostScript, one may choose from a well known set of typefaces, or create a
   new typeface using the well known font architecture, or free-hand draw some
   characters using postscript primitives, or draw lines, boxes, circles and
   such using postscript primitives, or scribble on a piece of paper, scan it,
   and convert the bits to use the postscript image operator. The space of
   symbols is nearly limitless, as long as those symbols can be expressed
   ultimately as pixels on a page.
   
   The distinctive feature of PostScript (an advantage at times, and a
   disadvantage at others) is that whether you print it and deliver the paper
   or you deliver the PostScript and the receiver prints it out, the result is
   the same bunch of images.
   
   The SGML model, on the other hand, specifies no general purpose programming
   model where complex symbols can be defined in terms of primitive symbols.
   The meaning of a symbol is either found in the SGML standard itself, or in
   some PUBLIC document (which may or may not be machine readable), or in some
   SYSTEM specific manner, or defined by an SGML application. The only real
   primitives are the character and the "non-SGML data entity".
   
   The model perscribes that a document consist of a declaration, a prologue,
   and an instance. The declaration is expressed in ASCII and specifies the
   character sets and syntactic symbols used by the prologue and instance. The
   prologue is expressed in a standard language using the syntactic symbols
   from the delcaration, and specifies a set of entities and a grammar of
   element types available to the instance.
   
   The instance is a sequence of elements, character data, and entities
   constrained by the grammar set forth in the prologue, and the SGML standard
   does not specify any semantics or meaning for the instance.
   
   So to communicate using SGML, the sender first chooses a character set and
   certain processing quatities and capacities. For example "I'm writing in
   ASCII, and I'll never use an element name more than 40 characters long" is
   some information that can be expressed in the SGML declaration. [The
   standard allows the SGML declaration to be implicitly agreed upon by sender
   and receiver, and this is generally the case].
   
   The tricky part is the prologue, where the sender gives a grammar that
   constrains the structure of the document. Along with the information
   actually expressed in SGML in the prologue, there is usually some amound of
   application defined semantics attached to the element types. For example,
   the prologue may express in SGML that an H2 element must occur within the
   content of an H1 element. But the convention that text in an H1 is usually
   displayed larger and considered more important is application defined.
   
   Once the prologue is determined (this usually involves considerable
   discussion between a collection of authors and consumers in some domain --
   in the end, there may be some "parameter entities" in the prologue which
   allow some variation on a per-document basis), the sender is constrained to
   a rigorous structure for the organization of the symbols and character data
   of the document. On the other hand, s/he has an automated technique for
   verifying that s/he has not viloated the structure, and hence there is some
   confidence that the document can be consumed and processed by machine.
   
                  THE HTML DTD: CONFORMING, THOUGH EXPEDIENT
                                       
Design Constraints of the HTML DTD

   Tim's original conception of HTML is that it should be about as expressive
   as RTF. In contrast to traditional SGML applications where documents might
   be batch processed and complex structure is the norm, HTML documents are
   intended to be processed interactively. And the widespread success of
   WYSIWYG word processors based on fairly flat paragraph structure was proof
   that something like RTF was suitable for a fairly wide variety of tasks.
   
   As I learned a little about SGML, it was clear that the WWW browser
   implementation of HTML sorely lacked anything resembling an SGML entity
   manager. And there were some syntactic inconsitencies with the SGML
   standard. And it didn't use the ID/IDREF feature where it should have...
   
   Then, as I began to comprehend SGML with all its warts, (who's idea was it
   to attach the significance of a newline character to the phase of the moon
   anyway?) I was less gung-ho about declaring all the HTML out there to be
   blasphemy to the One True SGML Way.
   
   Thus I chose for my battle to find some formal relationship between the SGML
   standard and the  HTML that was "out there." The quest was:
   
  FIND SOME DTD SUCH THAT THE VAST MAJORITY OF HTML DOCUMENTS ARE INSTANCES OF
  THAT DTD, CONVERSELY, SUCH THAT ALL ITS INSTANCES MAKE SENSE TO THE EXISTING
  WWW CLIENTS.
  
   I struggled mightily with such issues as:
   
      Should we be sticking <! DOCTYPE HTML SYSTEM> in .html files? What if
      somebody puts an entity declaration in there? (And does that mean that
      WWW clients have to be able to parse SGML prologues in general?
      
      What's the syntax of an attribute value? If we allow SHORTTAG YES, does
      that mean we have to parse <em/this/ style of markup too?
      
      Can we put some short reference maps in the DTD that will cause real SGML
      parsers and current WWW browsers to do the same thing w.r.t newlines?
      (i.e. can we make all that phase-of-the-moon processing with newlines a
      moot issue)
      
      What about marked sections? Short reference maps?
      
      What character set should we be using? How do I express ISO-Latin-1 in
      the SGML declaration? How should authors express the 'How do you put
      quotes in an attribute value literal?
      
      How can I deal with the current paragraph element idioms without using
      minimization?
      
      Can I stick base64 encoded stuff in a CDATA element? Do I have to watch
      out for How do we combine SGML and multimedia data in the same data
      stream?
      
   I found solutions to some problems, and punted on others. I probably should
   have put more comments in the DTD regarding the compromises. But I wanted to
   keep the DTD stripped down to the normative information and keep the
   informative information in other documents.
   
   I did, by the way, draft a series of 4 or 5 documents demonstrating various
   structural and syntactic features of SGML -- a sort of validation suite. I'm
   not sure where it went.
   
   I'd like to respond to Elliot Kimber's critique of the HTML DTD that I
   posted.
   

>At the bottom of this posting is a slightly modified copy of the
>HTML DTD that conforms to the HyTime standard.  I have not modified
>the elements or content models in any way.  I have not added any
>new elements.  I have only added to the attribute lists of a few
>elements.
>
>The biggest change I made was to the way URL addresses are handled.
>In order to use HyTime (as opposed to application-specific)
>methods for doing addressing, I had to change the URL address
>from a direct reference into an entity reference where the
>entity's system identifier is its URL address.

   I suggested this long ago, but Tim shot the idea down. As I recall, he said
   that all that extra markup was a waste. On the one hand, I agree with him --
   the purpose of a language is to be able to express common idioms succinctly,
   and SGML/HyTime are poor in that respect. On the other hand, once you've
   chosen SGML, you might as do as the Romans do.
   

>  This makes
>the link elements conform to the architectural forms and puts
>in enough indirection to allow other addressing methods to
>be used to locate the objects without having to modify the
>links, only the entity declarations.

   Why is it easier to modify entity declarations than links? Six of one,
   half-dozen of the other if you ask me.
   

>  I use SUBDOC entities
>for refering to other complete documents, although I'm not
>sure this the best thing, but there's no other construct in
>SGML that works as well.  Note that nowwhere in 8879 does it
>define what must happen as the result of a SUBDOC reference,
>except that a new parsing context is established.  The actual
>result of a SUBDOC reference is a matter of style and presumably
>in a WWW context it would result in the retrieval of the document
>and its presentation in a seperate window.  The key is that
>the subdoc reference establishes a specific relationship between
>the source of the link and the target, namely one document
>refering to another.  The target document could also be defined
>as a data entity with whatever notation is appropriate (possibly
>even SGML if it's another SGML document).  This may be the better
>approach, I don't know.

   I don't expect that the data entity/subdocument entity distinction matters
   one hill of beans to contemporary WWW clients. I'm interested to know if it
   means anything to HyTime engines.
   

>If I were re-designing the HTML, I would add direct support
>for HyTime location ladders using at a minimum the nameloc,
>notloc, and dataloc addressing elements.  However, if these
>elements are needed for interchange they could be generated
>from the information contained in WWW documents using the
>DTD below, so it's not critical.
>

   Could you expand on that? If we'll be "generating" compliant SGML for
   interchange, we might as well use TeXinfo or something practical like that
   for application-specific purposes.
   

>This is just one attempt at applying HyTime to the HTML.
>I'm sure there are other equally-valid (or more valid)
>ways it could be done.  Given the current functionality
>of the WWW, I'm sure there are ways to express that functionality
>using HyTime constructs.  HyTime constructs may also suggest
>useful ways to extend the WWW functionality, who knows.

   I finally got to actually read the HyTime standard the other day, and  the
   clink and noteloc forms looked most useful. I'm also interested in
   expressing some of the "relative link" idioms used in HTML. (e.g how would
   we express HREF="../foo/bar.html#zabc" using HyTime? The object of the game
   is to do it in such a way that the markup can be copied verbatim from one
   system to another (say unix to VMS) and have the right meaning)
   

><!ENTITY % URL "CDATA"
>        -- The term URL means a CDATA attribute
>           whose value is a Universal Resource Locator,
>           as defined in ftp://info.cern.ch/pub/www/doc/url3.txt
>        -->
><!--=====================================================================
>    WEK:  I have defined URL addresses as a notation so that they can
>          be then used in a notloc element.
>    =====================================================================-->
><!NOTATION url PUBLIC "-//WWW//NOTATION URL/Universal Resource Locator
>                             /'ftp: info.cern.ch/pub/www/doc/url3.txt'
>                             //EN"
>>

   Cool good idea.
   

>
><!ENTITY % linkattributes
>        "NAME NMTOKEN #IMPLIED
>        HREF ENTITY #IMPLIED
>
> --=== WEK =======================================================
>
>      HREF is now an entity attribute rather than containing a
>      URL address directly.  To create a link using a URL address,
>      declare a SUBDOC or data entity and make the system
>      identifier the URL address of the object:
>
>      <!ENTITY  mydoc SYSTEM "URL address of document " SUBDOC >
>
>      This indirection gives to things:
>
>      1. A way to protect links in the source from changes in the
>         location of a document since the physical address is only
>         specified once.

   Ah... now I get it... in case you have lots of links to mydoc or parts of
   mydoc, you only have one place that defines where mydoc is. Nifty.
   

>
>      2. An opportunity to use other addressing methods, including
>         possibly replacing the URL with an ISO formal public
>         identifier.
>    =================================================================-->
>
>        TYPE NAME #IMPLIED -- type of relashionship to referent data:
>                                PARENT CHILD, SIBLING, NEXT, TOP,
>                                 DEFINITION, UPDATE, ORIGINAL etc. --
>        URN CDATA #IMPLIED -- universal resource number. unique doc id --
>        TITLE CDATA #IMPLIED -- advisory only --
>        METHODS NAMES #IMPLIED -- supported methods of the object:
>                                        TEXTSEARCH, GET, HEAD, ... --
>        -- WEK: --
>        LINKENDS  NAMES #IMPLIED
>          -- Linkends takes one or more NAME= values for local links--
>        HyNames  CDATA #FIXED 'TYPE ANCHROLE URN DOCORSUB'
>        ">

   I thought the ANCHROLEs of a clink were defined by HyTime to be REFsomething
   and REFSUB. Or are those just defaults? Also... does the HyNames think work
   locally like this? What a HACK!
   

>
><!--=== WEK ==========================
>
>    The HyNames= attribute maps the local attribute names to their
>    cooresponding HyTime forms.
>
>    The Methods= attribute is bit of a puzzle since it is really
>    a part of the hyperlink presentation/processing style, not
>    a property of the anchors, but there's nothing wrong with
>    having application-specific stuff in your HyTime application.

   The Methods= attribute has been striken :-(. It was motivated by the
   observation that textsearch interactions in WWW go like this:
   
      Doc A says "click here[23] to see the index"
      
      user clicks
      
      client fetches link 23, "http://host/index"
      
      displays "cover page" document
      
      user enters FIND abc
      
      client fetches "http://host/index?abc"
      
      search results are displayed
      
   Wheras in gopher, you get to save a step if you like:
   
      Doc A says "click here[23] to search the index"
      
      user clicks
      
      client displayes "enter search words here: " dialog
      
      user enters FIND abc
      
      client fetches "http://host/index?abc"
      
      search results are displayed
      
   So to specify the latter, you would create a link with Methods=textsearch.
   

>    I added LinkEnds= so that the various linking elements will
>    completely conform to the clink and ilink forms.  The presence
>    of the LinkEnds= attribute does not imply required support
>    for this type of linking, but it does make HTML more consistent
>    with other DTDs that do use the LinkEnds= attribute form.
>
>    Note that 10744 shows the attribute name for the ILINK form
>    to be 'linkend', not 'linkends'.  I consider this to be a
>    typo, as there's no logical reason to disallow multiple anchors
>    from a clink and lack of it puts an undue requirement of
>    specifying otherwise unneeded nameloc elements.  In any case,
>    an application can transform linkends= to linkend= plus a
>    nameloc, so it doesn't matter in practice.

   Are there any HyTime implementations out there? Do they use 'linkend' or
   'linkends'? It's hard to beleive that HyTime became a standard without a
   proof-of-concept implementation.
   

>
><!ELEMENT P     - O EMPTY -- separates paragraphs -->
><!--=== WEK ==========================================================
>
>    Design note:  This seems like a clumsy way to structure information.
>                  One would expect paragraphs to be containing.
>
>    ==================================================================-->

   Yeah, well, try implementing end tag inference in <1000 or so lines of code.
   Maybe we'll get it right next time...
   

><!ELEMENT DL    - -  (DT | DD | P | %hypertext;)*>
><!--    Content should match ((DT,(%hypertext;)+)+,(DD,(%hypertext;)+))
>        But mixed content is messy.
>  -->
><!--=== WEK ============================================================
>
>    Design note:  This content should be:
>
>    <!ELEMENT DL  - - (DT+, DD)+ >
>    <!ELEMENT (DT | DD) - O (%hypertext;)* >
>
>    There's no reason for DT and DD to be empty.  Perhaps there was
>    some confusion about the problems with mixed content?  There are
>    none here.
>
>    These comments apply to the other list elements as well.
>
>    ====================================================================-->

   The problem is that DL, DT, DD, UL, OL, and LI were marked up in extant HTML
   documents as if minimization were supported. But I didn't want to introduce
   minimization into the implementation, so I made the DT, DD, and LI elements
   empty.
   
   It's possible I'm confused about mixed content, but the way I understand it,
   you don't want to use mixed content except in repeatable or groups because
   authors will stick whitespace in where it is meant to be ignored but it
   won't be.
   

>
><!-- Character entities omitted.  These should be separate from
>     the main DTD so specific applications can define their values.
>     ISO entity sets could be used for this.
>  -->

   Another point I should have explained in the DTD: the WWW application
   specifies that HTML uses the Latin-1 character set, and that the Ouml entity
   represents exactly that character from the Latin-1 character and not some
   system specific thingy. Translation to system character sets is done outside
   of the SGML parser.

------- =_aaaaaaaaaa0
Content-Type: text/html; charset="us-ascii"
Content-ID: <4639.761342909.3@ulua>
Content-Description: html version
Content-Transfer-Encoding: quoted-printable

<!-- $Id: html_essay.html,v 1.2 1994/02/15 20:07:12 connolly Exp connolly =
$ -->
<html>
<head>
<title>Toward a Formalism for Communication On the Web</title>
</head>
<body>

<ADDRESS>Daniel W. Connolly &lt;connolly@hal.com&gt; <P>
$Id: html_essay.html,v 1.2 1994/02/15 20:07:12 connolly Exp connolly $
</ADDRESS>

<H2>Status</H2>

 <P>I had hoped to polish this more before publishing it, but I can't seem
to get caught up... there's so much new stuff all the time!

<H1>Some Background on SGML for the World-Wide Web
</H1>

 <p>In late 1992 and early 1993, I did quite a bit of work on the HTML DTD
while I was working at Convex in the online documentation group.

 <p>When I began, there was the LineMode browser and the NeXT
implementation, and a few nodes in The Web describing HTML with some
oblique references to SGML. I was not intimately familiar with SGML, but
I was quite familiar with the problems of document interchange, and I
was eager to apply some of my formal systems background to the problem.

<H2>On Formally Unconvertable Document Formats
</H2>

 <P>My experience with document interchange led me to classify document
formats using the essential distinction that some are "programmable" and
some are not. Most widely used source forms are programmable: TeX,
troff, postscript, and the like. On the other hand, there are several "sta=
tic"
formats: plain text, Microsoft RTF, FrameMaker MIF, GNU's TeXinfo,

 <P>The reason that this distinction is essential with respect to document
interchange is that extracting information from documents in
"programmable" document formats is equivalent to the halting problem.
That is, it is arbitrarily difficult and cannot be automated in a
general fashion.

 <P>For example, I conjecture that it is impossible to write a program tha=
t
will extract the third word from a TeX document. It would be an easy
task for 80% of the TeX documents out there -- just skip over some
formatting stuff and grab the third bunch of characters surrounded by
whitespace. But that "formatting stuff" might be a program that
generates 100 words from the hypenation dictionary. So the simple
lexical scan of the TeX source would find a word that is <em>not</em> thir=
d
word of the document when printed.

 <P>This may seem like an obscure and unimportant problem, but I assure yo=
u
that the problem of converting TeX tables to FrameMaker MIF is just as
unsolvable.

 <P>So while "programmable" document formats have the advantage that
features can be added on a per-document basis, they suffer the
disadvantage that these features cannot be recovered by the machine and
translated in an automated fashion.


<H2>Document Formats as Communications Media
</H2>

 <P>If we look at document formats in light of the conventional
sender/message/medium/receiver communications model, we see that
document formats capture the message at various levels of
"concreteness".

 <P>The message begins as a collection of concepts and ideas in the mind o=
f
the sender. In order to communicate, the sender and receiver must share
some language. That is, they must both understand some common set of
symbols and the way those symbols combine to represent ideas. The
senders job is to express the message in terms of the common symbols and
express them on the medium -- that is "render" or "present" them. The
the medium stimulates the receiver to reconstruct the symbols in his/her
brain -- that is, the receiver "interprets" or "recognizes" the symbols
from the medium. Those symbols interact with other symbols in the
receiver's brain, and the receiver "gets the message."

 <P>The communications medium is often a layered combination of more and
less concrete media. For example, folks first render their ideas in the
symbology of the English language, and then render those symbols as
sequences of spoken phonemes or written characters. Those written
characters are in turn combinations of lines, curves, strokes, and
points. The receiving folks then assemble the strokes into characters,
the characters into words, the words into phrases, sentences, thoughts,
ideas, and so on.

 <P>The most common and ubiquitous document format, plain ASCII text,
captures or digitizes messages at the level of written characters.
PostScript captures the characters as lines, curves, and paths. The GIF
format captures a document as an array of pixels. GIF is in many ways
infinitely more expressive than plain text, which is limited to
arrangements of the 96 ASCII characters.

 <P>The RTF, TeX, nroff, etc. document formats provide very sophisticated
automated techniques for authors of documents to express their ideas. It
seems strange at first to see that plain text is still so widely used.
It would seem that PostScript is the ultimate document format, in that
its expressive capabilities include essentially anything that the human
eye is capable of perceiving, and yet it is device-independent.

 <P>And yet if we take a look at the task of interpreting data back into
the ideas that they represent, we find that plain text is much to be
preferred, since reading plain text is so much easier to automate than
reading GIF files (optical character recognition) or postscript
documents (halting problem). In the end, while the source to a various
TeX or troff documents may correspond closely to the structure of the
ideas of the author, and while PostScript allows the author very precise
control and tremenous expressive capability, all these documents
ultimately capture an image of a document for presentation to the human
eye. They don't capture the original information as symbols that can be
processed by machine.

 <P>To put it another way, rendering ideas in PostScript is not going to
help solve the problem of information overload -- it will only compound
the situation.

 <P>As a real world example, suppose you had a 5000 page document in
PostScript, and you wanted to find a particular piece of information
inside it. The author may have organized the document very well, but
you'd have to print it to use those clues. If the characters aren't
kerned much, you might be able to use grep or sick a WAIS indexing
engine on it. Then, once you've found what looks like postscript code
for some relavent information, you'd pray that the document adheres to
the Adobe Document Structuring conventions so that you could pick out
the page containing the information you need and view that page.

 <P>If that's too perverse, look at the problem of navigating a large
collection of technical papers coded in TeX. Many of the authors use
LaTeX, and you may be able to convince the indexing engine to filter
out common LaTeX formatting idioms -- or better yet, weight headings,
abstracts, etc. more heavily than other sections based on the
formatting idioms. While there are heuristic solutions to this problem
that will work in the typical 80%/20% fashion, the general solution is
once again equivalent to the halting problem; for example, individual
documents might have bits of TeX programming that change the
significance of words in a way that the indexing engine won't be able
to understand.


<H2>SGML as a Layered Communications Medium
</H2>

 <P>So where does SGML fit into the sender/message/medium/receiver game?

 <P>I'll use PostScript as a basis of comparison. The PostScript model
consists of a fairly powerful and general purpose two dimensional
imaging model, that is, a set of primitive symbols for specifying sets
of points in two dimensions using handy computational techniques, and a
general purpose programming model for building complex symbols out of
those primitives. That model is applied extensively to the problem of
typography, and there is a an architecture (that is, a set of well known
symbols derived from the primitives) for using and building fonts.

 <P>So to communicate message consisting of symbols from human
communications in PostScript, one may choose from a well known set of
typefaces, or create a new typeface using the well known font
architecture, or free-hand draw some characters using postscript
primitives, or draw lines, boxes, circles and such using postscript
primitives, or scribble on a piece of paper, scan it, and convert the
bits to use the postscript image operator. The space of symbols is
nearly limitless, as long as those symbols can be expressed ultimately
as pixels on a page.

 <P>The distinctive feature of PostScript (an advantage at times, and a
disadvantage at others) is that whether you print it and deliver the
paper or you deliver the PostScript and the receiver prints it out, the
result is the same bunch of images.

 <P>The SGML model, on the other hand, specifies no general purpose
programming model where complex symbols can be defined in terms of
primitive symbols. The meaning of a symbol is either found in the SGML
standard itself, or in some PUBLIC document (which may or may not be
machine readable), or in some SYSTEM specific manner, or defined by an
SGML application. The only real primitives are the character and the
"non-SGML data entity".

 <P>The model perscribes that a document consist of a declaration, a
prologue, and an instance. The declaration is expressed in ASCII and
specifies the character sets and syntactic symbols used by the prologue
and instance. The prologue is expressed in a standard language using the
syntactic symbols from the delcaration, and specifies a set of entities
and a grammar of element types available to the instance.

 <P>The instance is a sequence of elements, character data, and entities
constrained by the grammar set forth in the prologue, and the SGML
standard does not specify any semantics or meaning for the instance.

 <P>So to communicate using SGML, the sender first chooses a character set
and certain processing quatities and capacities. For example "I'm
writing in ASCII, and I'll never use an element name more than 40
characters long" is some information that can be expressed in the SGML
declaration. [The standard allows the SGML declaration to be implicitly
agreed upon by sender and receiver, and this is generally the case].

 <P>The tricky part is the prologue, where the sender gives a grammar that
constrains the structure of the document. Along with the information
actually expressed in SGML in the prologue, there is usually some amound
of application defined semantics attached to the element types. For
example, the prologue may express in SGML that an H2 element must occur
within the content of an H1 element. But the convention that text in an
H1 is usually displayed larger and considered more important is
application defined.

 <P>Once the prologue is determined (this usually involves considerable
discussion between a collection of authors and consumers in some
domain -- in the end, there may be some "parameter entities" in the
prologue which allow some variation on a per-document basis), the sender
is constrained to a rigorous structure for the organization of the
symbols and character data of the document. On the other hand, s/he has
an automated technique for verifying that s/he has not viloated the
structure, and hence there is some confidence that the document can be
consumed and processed by machine.


<H1>The HTML DTD: Conforming, though Expedient
</H1>

<H2>Design Constraints of the HTML DTD
</H2>

 <P>Tim's original conception of HTML is that it should be about as
expressive as RTF. In contrast to traditional SGML applications where
documents might be batch processed and complex structure is the norm,
HTML documents are intended to be processed interactively. And the
widespread success of WYSIWYG word processors based on fairly flat
paragraph structure was proof that something like RTF was suitable for a
fairly wide variety of tasks.

 <P>As I learned a little about SGML, it was clear that the WWW browser
implementation of HTML sorely lacked anything resembling an SGML entity
manager. And there were some syntactic inconsitencies with the SGML
standard. And it didn't use the ID/IDREF feature where it should have...

 <P>Then, as I began to comprehend SGML with all its warts, (who's idea wa=
s
it to attach the significance of a newline character to the phase of the
moon anyway?) I was less gung-ho about declaring all the HTML out there
to be blasphemy to the One True SGML Way.

 <P>Thus I chose for my battle to find some formal relationship between th=
e
SGML standard and the  HTML that was "out there." The quest was:

<H3>Find some DTD such that the vast majority of HTML documents are
instances of that DTD, conversely, such that all its instances make
sense to the existing WWW clients.
</H3>

 <P>I struggled mightily with such issues as:

<UL>
<LI>Should we be sticking &lt;! DOCTYPE HTML SYSTEM> in .html files? What
if somebody puts an entity declaration in there? (And does that mean
that WWW clients have to be able to parse SGML prologues in general?

<LI>What's the syntax of an attribute value? If we allow SHORTTAG YES,
does that mean we have to parse <CODE>&lt;em/this/</CODE> style of
markup too?

<LI>Can we put some short reference maps in the DTD that will cause real
SGML parsers and current WWW browsers to do the same thing w.r.t
newlines? (i.e. can we make all that phase-of-the-moon processing with
newlines a moot issue)

<LI>What about marked sections? Short reference maps?

<LI>What character set should we be using? How do I express ISO-Latin-1
in the SGML declaration? How should authors express the '<' character?
How should this be expressed in the DTD?

<LI>How do you put quotes in an attribute value literal?

<LI>How can I deal with the current paragraph element idioms without
using minimization?

<LI>Can I stick base64 encoded stuff in a CDATA element? Do I have to
watch out for <'s and such?

<LI>How do we combine SGML and multimedia data in the same data stream?

</UL>


 <P>I found solutions to some problems, and punted on others. I probably
should have put more comments in the DTD regarding the compromises. But
I wanted to keep the DTD stripped down to the normative information and
keep the informative information in other documents.

 <P>I did, by the way, draft a series of 4 or 5 documents demonstrating
various structural and syntactic features of SGML -- a sort of
validation suite. I'm not sure where it went.

 <P>I'd like to respond to Elliot Kimber's critique of the HTML DTD that I
posted.

<pre>
>At the bottom of this posting is a slightly modified copy of the
>HTML DTD that conforms to the HyTime standard.  I have not modified
>the elements or content models in any way.  I have not added any
>new elements.  I have only added to the attribute lists of a few
>elements.
>
>The biggest change I made was to the way URL addresses are handled.
>In order to use HyTime (as opposed to application-specific)
>methods for doing addressing, I had to change the URL address
>from a direct reference into an entity reference where the
>entity's system identifier is its URL address.
</pre>

 <P>I suggested this long ago, but Tim shot the idea down. As I recall, he
said that all that extra markup was a waste. On the one hand, I agree
with him -- the purpose of a language is to be able to express common
idioms succinctly, and SGML/HyTime are poor in that respect. On the
other hand, once you've chosen SGML, you might as do as the Romans do.

<pre>
>  This makes
>the link elements conform to the architectural forms and puts
>in enough indirection to allow other addressing methods to
>be used to locate the objects without having to modify the
>links, only the entity declarations.
</pre>

 <P>Why is it easier to modify entity declarations than links? Six of one,
half-dozen of the other if you ask me.

<pre>
>  I use SUBDOC entities
>for refering to other complete documents, although I'm not
>sure this the best thing, but there's no other construct in
>SGML that works as well.  Note that nowwhere in 8879 does it
>define what must happen as the result of a SUBDOC reference,
>except that a new parsing context is established.  The actual
>result of a SUBDOC reference is a matter of style and presumably
>in a WWW context it would result in the retrieval of the document
>and its presentation in a seperate window.  The key is that
>the subdoc reference establishes a specific relationship between
>the source of the link and the target, namely one document
>refering to another.  The target document could also be defined
>as a data entity with whatever notation is appropriate (possibly
>even SGML if it's another SGML document).  This may be the better
>approach, I don't know.
</pre>

 <P>I don't expect that the data entity/subdocument entity distinction
matters one hill of beans to contemporary WWW clients. I'm interested to
know if it means anything to HyTime engines.

<pre>
>If I were re-designing the HTML, I would add direct support
>for HyTime location ladders using at a minimum the nameloc,
>notloc, and dataloc addressing elements.  However, if these
>elements are needed for interchange they could be generated
>from the information contained in WWW documents using the
>DTD below, so it's not critical.
>
</pre>

 <P>Could you expand on that? If we'll be "generating" compliant SGML for
interchange, we might as well use TeXinfo or something practical like
that for application-specific purposes.

<pre>
>This is just one attempt at applying HyTime to the HTML.
>I'm sure there are other equally-valid (or more valid)
>ways it could be done.  Given the current functionality
>of the WWW, I'm sure there are ways to express that functionality
>using HyTime constructs.  HyTime constructs may also suggest
>useful ways to extend the WWW functionality, who knows.
</pre>

 <P>I finally got to actually read the HyTime standard the other day, and =

the clink and noteloc forms looked most useful. I'm also interested in
expressing some of the "relative link" idioms used in HTML.
(e.g how would we express HREF=3D"../foo/bar.html#zabc" using HyTime? The
object of the game is to do it in such a way that the markup can be
copied verbatim from one system to another (say unix to VMS) and have
the right meaning)

<pre>
>&lt;!ENTITY % URL "CDATA"
>        -- The term URL means a CDATA attribute
>           whose value is a Universal Resource Locator,
>           as defined in ftp://info.cern.ch/pub/www/doc/url3.txt
>        -->
>&lt;!--=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>    WEK:  I have defined URL addresses as a notation so that they can
>          be then used in a notloc element.
>    =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D-->
>&lt;!NOTATION url PUBLIC "-//WWW//NOTATION URL/Universal Resource Locator
>                             /'ftp: info.cern.ch/pub/www/doc/url3.txt'
>                             //EN"
>>
</pre>

 <P>Cool good idea.

<pre>
>
>&lt;!ENTITY % linkattributes
>        "NAME NMTOKEN #IMPLIED
>        HREF ENTITY #IMPLIED
>
> --=3D=3D=3D WEK =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>
>      HREF is now an entity attribute rather than containing a
>      URL address directly.  To create a link using a URL address,
>      declare a SUBDOC or data entity and make the system
>      identifier the URL address of the object:
>
>      &lt;!ENTITY  mydoc SYSTEM "URL address of document " SUBDOC >
>
>      This indirection gives to things:
>
>      1. A way to protect links in the source from changes in the
>         location of a document since the physical address is only
>         specified once.
</pre>

 <P>Ah... now I get it... in case you have lots of links to mydoc or parts
of mydoc, you only have one place that defines where mydoc is. Nifty.

<pre>
>
>      2. An opportunity to use other addressing methods, including
>         possibly replacing the URL with an ISO formal public
>         identifier.
>    =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D-->
>
>        TYPE NAME #IMPLIED -- type of relashionship to referent data:
>                                PARENT CHILD, SIBLING, NEXT, TOP,
>                                 DEFINITION, UPDATE, ORIGINAL etc. --
>        URN CDATA #IMPLIED -- universal resource number. unique doc id --
>        TITLE CDATA #IMPLIED -- advisory only --
>        METHODS NAMES #IMPLIED -- supported methods of the object:
>                                        TEXTSEARCH, GET, HEAD, ... --
>        -- WEK: --
>        LINKENDS  NAMES #IMPLIED
>          -- Linkends takes one or more NAME=3D values for local links--
>        HyNames  CDATA #FIXED 'TYPE ANCHROLE URN DOCORSUB'
>        ">
</pre>

 <P>I thought the ANCHROLEs of a clink were defined by HyTime to be
REFsomething and REFSUB. Or are those just defaults? Also... does the
HyNames think work locally like this? What a HACK!

<pre>
>
>&lt;!--=3D=3D=3D WEK =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D
>
>    The HyNames=3D attribute maps the local attribute names to their
>    cooresponding HyTime forms.
>
>    The Methods=3D attribute is bit of a puzzle since it is really
>    a part of the hyperlink presentation/processing style, not
>    a property of the anchors, but there's nothing wrong with
>    having application-specific stuff in your HyTime application.
</pre>

The Methods=3D attribute has been striken :-(. It was motivated by the
observation that textsearch interactions in WWW go like this:

<OL>
<LI>Doc A says "click here[23] to see the index"
<LI>user clicks
<LI>client fetches link 23, "http://host/index"
<LI>displays "cover page" document
<LI>user enters FIND abc
<LI>client fetches "http://host/index?abc"
<LI>search results are displayed
</OL>

Wheras in gopher, you get to save a step if you like:

<OL>
<LI>Doc A says "click here[23] to search the index"
<LI>user clicks
<LI>client displayes "enter search words here: " dialog
<LI>user enters FIND abc
<LI>client fetches "http://host/index?abc"
<LI>search results are displayed
</OL>

So to specify the latter, you would create a link with Methods=3Dtextsearc=
h.

<pre>
>    I added LinkEnds=3D so that the various linking elements will
>    completely conform to the clink and ilink forms.  The presence
>    of the LinkEnds=3D attribute does not imply required support
>    for this type of linking, but it does make HTML more consistent
>    with other DTDs that do use the LinkEnds=3D attribute form.
>
>    Note that 10744 shows the attribute name for the ILINK form
>    to be 'linkend', not 'linkends'.  I consider this to be a
>    typo, as there's no logical reason to disallow multiple anchors
>    from a clink and lack of it puts an undue requirement of
>    specifying otherwise unneeded nameloc elements.  In any case,
>    an application can transform linkends=3D to linkend=3D plus a
>    nameloc, so it doesn't matter in practice.
</pre>

Are there <EM>any</EM> HyTime implementations out there? Do they use
'linkend' or 'linkends'? It's hard to beleive that HyTime became a
standard without a proof-of-concept implementation.

<pre>
>
>&lt;!ELEMENT P     - O EMPTY -- separates paragraphs -->
>&lt;!--=3D=3D=3D WEK =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>
>    Design note:  This seems like a clumsy way to structure information.
>                  One would expect paragraphs to be containing.
>
>    =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D-->
</pre>

Yeah, well, try implementing end tag inference in &lt;1000 or so lines of =
code.
Maybe we'll get it right next time...

<pre>
>&lt;!ELEMENT DL    - -  (DT | DD | P | %hypertext;)*>
>&lt;!--    Content should match ((DT,(%hypertext;)+)+,(DD,(%hypertext;)+)=
)
>        But mixed content is messy.
>  -->
>&lt;!--=3D=3D=3D WEK =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>
>    Design note:  This content should be:
>
>    &lt;!ELEMENT DL  - - (DT+, DD)+ >
>    &lt;!ELEMENT (DT | DD) - O (%hypertext;)* >
>
>    There's no reason for DT and DD to be empty.  Perhaps there was
>    some confusion about the problems with mixed content?  There are
>    none here.
>
>    These comments apply to the other list elements as well.
>
>    =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D-->
</pre>

The problem is that DL, DT, DD, UL, OL, and LI were marked up in extant
HTML documents as if minimization were supported. But I didn't want to
introduce minimization into the implementation, so I made the DT, DD,
and LI elements empty.
<p>

It's possible I'm confused about mixed content, but the way I understand
it, you don't want to use mixed content except in repeatable or groups
because authors will stick whitespace in where it is meant to be ignored
but it won't be.

<pre>
>
>&lt;!-- Character entities omitted.  These should be separate from
>     the main DTD so specific applications can define their values.
>     ISO entity sets could be used for this.
>  --&gt;
</pre>

Another point I should have explained in the DTD: the WWW application
specifies that HTML uses the Latin-1 character set, and that the Ouml
entity represents exactly that character from the Latin-1 character and
not some system specific thingy. Translation to system character sets is
done <em>outside</em> of the SGML parser.

</body>
</html>

------- =_aaaaaaaaaa0--



From connolly@hal.com  Tue Feb 15 21:39:02 1994 --100
Message-Id: <9402152035.AA04673@ulua.hal.com>
Date: Tue, 15 Feb 1994 21:39:02 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Problem with "~user/file.html" 

In message <m0pWW2Y-00043AC@ibeam.intel.com>, Kevin Altis writes:
>At 11:56 AM 2/15/94 +0000, neuss@igd.fhg.de wrote:
>>Ok, let's say you want to do something like click *here* to get info
>>about the author of this document.

I'd suggest the following:

	... click <A HREF="finger://my.domain/myloginname">here</a>...

I'm not sure about the URL syntax for a finger query... you should check
the spec or the code or something.

But the finger application is already quite widely deployed. Let's not
get into the mindset that the HTTP server should solve all the world's
problems.

Hmm.... how does one specify that the data coming back from the finger
request is to be interpreted as HTML? With the DTD I wrote a long time
ago, it would be:

	... click <A HREF="finger://my.domain/myloginname"
			CONTENT-TYPE="text/html">here</a>...

But nobody else seemed to think that content-type was a good thing to
put in a link. I sure think it's useful.

Dan



From neuss@igd.fhg.de  Tue Feb 15 22:12:46 1994 --100
Message-Id: <9402152111.AA04347@wildturkey.igd.fhg.de>
Date: Tue, 15 Feb 1994 22:12:46 --100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: Re: Problem with "~user/file.html" 

Hello the Web,

> But the finger application is already quite widely deployed. Let's not
> get into the mindset that the HTTP server should solve all the world's
> problems.
> 


Yuh.. decidedly not. I merely wondered whether UNIX browser should support
the ~operator whenever they're in local mode.

Thanx for the help,
Chris
---
"I ride a tandem with the random.." 

Christian Neuss   # Fraunhofer Institute for Computer Graphics
Wilhelminenstr.7  #  64283 Darmstadt # Germany
e-mail: neuss@igd.fhg.de  finger: neuss@wildturkey.igd.fhg.de



From dmh@hpfcma.fc.hp.com  Tue Feb 15 22:41:05 1994 --100
Message-Id: <9402152136.AA28566@hpfcma.fc.hp.com>
Date: Tue, 15 Feb 1994 22:41:05 --100
From: dmh@hpfcma.fc.hp.com (Dave Hollander)
Subject: Re: <draft-ietf-iiir-html-01.txt, .ps> to be deleted. 


> = Peter Flynn
> > > = Dave Raggett

> Q. By "parse" do we mean "check for conformance and reject if in error"
> as well as "extract anything which needs acting upon". The whole area
> of what a browser should do when it encounters invalid HTML+ needs some
> more thought.
> 
One opinion: browsers should be as robust as possible; however, to
help people make the document truly portable, there should also be
a validating parser tool available, perhaps built into the servers.

..

> Because a large amount of what people may want to put up on web servers
> is not necessarily technical documentation. This is why we went to the
> trouble of bringing together the TEI and some browser writers. Using
> DocBook would be _way_ too limiting.
> 
Indeed. For something like the web, I can not think of any semantic domain
for a tag set, except display semantics, that is not limiting. For example,
I am looking into commercial use of the internet and this would be all 
sorts of non-hierarchy stuff. Any D1...Dn based structure will be in the
way (but, allowing for tag abuse, ways to do it can be found).

Likewise, I am concerned about the expressiveness of HTML+. There are 
many document types today that can not be reproduced faithfully using 
this specification; interactive catalogs, technical magazines are two
examples that will not be able to be done without significant compromise
to their design specifications.  

Many of those who will be funding the growth of the Internet (and other 
networks) will be looking to reproduce their information in the visual
form they are used to. IF we wish this to be a tool used (and thereby
moved forward by) these types of applications then we need to consider
their needs.

..

> > >In future, we expect authors will use specialized wysiwyg editors for 
> > >HTML+ or automated document format conversion tools and hence produce 
> > >documents which naturally conform to the DTD.
> 
> Don't even have to be WYSIWYG...but they will need to be conformant.
> 
This is a fundamental issue. I would propose that you can not reach 
agreement as to what a good DTD is with out agreeing to this aspect of
it usage. So, what will it be?  
  ____ Full support for hand tagging
  ____ Limited support for hand tagging, limited support for specialized editor
  ____ specialized editor or tool

I assume that we should support hand tagging as much as possible up to the 
point where "real" SGML tools can support, but not beyond! However, there
may be a way to get the tools community to provide specialized tools
customized for HTML+ at little or no cost. This is such an attractive idea
that we should consider it.


Regards,

Dave Hollander



From masinter@parc.xerox.com  Tue Feb 15 23:20:14 1994 --100
Message-Id: <94Feb15.141634pst.2732@golden.parc.xerox.com>
Date: Tue, 15 Feb 1994 23:20:14 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: ... standard flame about `... click <a ...>here</a>...'

I know the discussion was about a totally different topic, but I
thought I'd repeat a standard flame:
`click *here*' is bad style:

   a) not everybody clicks.
   b) `here' isn't a good thing to link to.

Instead of 
	click *here* to see today's weather
try
	you can see *today's weather*





From montulli@stat1.cc.ukans.edu  Tue Feb 15 23:57:17 1994 --100
Message-Id: <9402152254.AA56088@stat1.cc.ukans.edu>
Date: Tue, 15 Feb 1994 23:57:17 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: proxy server allows using std Mosaic with Term

> a connection to www1.cern.ch:911 and forwards the request to it, which
> forwards it to the real service.  Works great!  (Well, actually, I'm
> getting a few core dumps in Mosaic 2.2, but I assume that's some other
> problem, like the fact that I'm running Solaris 2.2).  The same approach
> should work for any browser which supports proxying.  So now all the folks
> out there with Unix machines and Term, but no Motif, can run Mosaic.
> 
Actually the core dumps are my fault :(  I sent out diffs with a
serious bug. :( :( :(
on line 175 of libwww2/HTAccess.c change:
        /* proxy servers have precedence over gateway servers */
        if (proxy) {
                char * gatewayed;
                StrAllocCopy(gatewayed,proxy);
                StrAllocCat(gatewayed,addr);

to:
        /* proxy servers have precedence over gateway servers */
        if (proxy) {
                char * gatewayed=0;
                StrAllocCopy(gatewayed,proxy);
                StrAllocCat(gatewayed,addr);

Sorry about the bug but I figured xmosaic needed at least one so
that we would have something to complain about. :)

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From dmh@hpfcma.fc.hp.com  Wed Feb 16 00:33:58 1994 --100
Message-Id: <9402152329.AA01341@hpfcma.fc.hp.com>
Date: Wed, 16 Feb 1994 00:33:58 --100
From: dmh@hpfcma.fc.hp.com (Dave Hollander)
Subject: Re: <draft-ietf-iiir-html-01.txt, .ps> to be deleted. 


My last posting, and Dan and O'Reilly's answers,  got me to thinking 
about the success factors of the Web today and in the future. I think 
the factors are going to change, and quickly. I assume that HTML+ 
should address the success factors of the future. 

Success factors up until the near future includes "Tim's original 
conception of HTML is that it should be about as expressive as RTF",
the robustness of the HTML language and browsers, browser ease of use 
(everyone I have shown it to [NCSA Mosaic] understands how to use 
immediately!), easy to establish servers and the bridging to
ftp/gopher/wais.

In the future, I expect several other factors to become important. These
are:
    1) the willingness of large information providers to subsidize the
		interconnections 
    2) the affordable creation and maintenance of webs
    3) continual browser and server improvements and platform support

In essence, I do believe that commercial web applications are necessary
and that improvements to the interchange language, APIs, etc should
work to support commercial needs. Finally, do this while keeping the 
technology accessible as it is today.

How do I see this impacting the design of HTML+?

   * expressive - every company has an "corporate identity", every publisher
	has their "look". We need style sheets if we keep the language
	semantic (and I hope we do). We could also use an style element
	like the Table of Semantic Styles in SDL.

   * programmable and portable - document publishers have to know what is
	widely supported and what is not. (the same phenomena as PC software)

   * linkage - lots of consideration needs to go into links. The maintenance
	of links is a real problem in my community (online UNIX technical 
	document publishers). At least one level of abstraction should be
	required and specified.

Please let me know if I am off the mark as these thoughts are the basis 
for making judgments about HTML+.

Regards,
Dave Hollander



From mcharity@hq.lcs.mit.edu  Wed Feb 16 01:20:35 1994 --100
Message-Id: <9402160017.AA09178@hq.lcs.mit.edu>
Date: Wed, 16 Feb 1994 01:20:35 --100
From: mcharity@hq.lcs.mit.edu (Mitchell N Charity)
Subject: Composable CGI scripts?

I wonder if one might "compose" CGI scripts.
That is, one CGI script calling another.

One might then use one CGI script to do logging, another to do
authentication, and then the final one to do a search (for instance).
Different subtrees could have their own logging/authentication/etc
properties.  Use kerberos here, but IP address there.  Offer automatic
decompression on these files, but not on those.

Basically, the server implementation is getting spread over URL space.
Server simplicity, modularity, and service flexibility bought with
system load (multiple script processes).

Just a thought,
Mitchell



From stumpf@informatik.tu-muenchen.de  Wed Feb 16 02:11:05 1994 --100
Message-Id: <94Feb16.020715mesz.311358@hprbg5.informatik.tu-muenchen.de>
Date: Wed, 16 Feb 1994 02:11:05 --100
From: stumpf@informatik.tu-muenchen.de (Markus Stumpf)
Subject: Proxy Servers

Hello folx,

looks like lots of people are hacking on proxy clients/servers these days.
Could we please agree on some standards ... on the server and on the client
side?

The proxy environment variables cause the URLs to be sent with
the protocol, the others without (I never checked what the real
difference is in the use of WWW_xxx_gateway and the proxy one,
and I never understood why the protocol info was omitted around
libwww-2.09).

And while we're on it I'd really like to have some mechanisms to only
use a gateway at all, if the clients cannot connect directly. (IMHO
it doesn't make too much sense to connect to servers on the same
subnet/domain, that are e.g. on the same side of a firewall through
a gatway server.)

But before we have 20 environment variables to control the client
and 2 different URLs that are sent out on gateway requests, could we
please agree on some "standards" ?

	\Maex
-- 
______________________________________________________________________________
 Markus Stumpf                        Markus.Stumpf@Informatik.TU-Muenchen.DE 
                                http://www.informatik.tu-muenchen.de/~stumpf/



From mcarthur@fit.qut.edu.au  Wed Feb 16 02:55:36 1994 --100
Message-Id: <199402160153.LAA03746@fitmail.fit.qut.edu.au>
Date: Wed, 16 Feb 1994 02:55:36 --100
From: mcarthur@fit.qut.edu.au (Mr Robert McArthur)
Subject: Defaults - client side, server side or what?

Hello everyone.

With the proliferation of form-based documents I see a need for a system
whereby people can setup defaults for documents that they visit.  For
instance, an archie frontend document provides you with the choice of which
archie server around the world to use.  Someone accessing the frontend may
use Europe, another maybe Australia depending upon where they are located
and other things.  It would be useful to have defaults for a particular
person for a document.  So, whenever I use the frontend it will default to
Australia, but still give me the option of course for somewhere else.
Like wise in other forms and, I think, in many other documents.

It does not seem like a server side feature because it would mean the server
keeping track of individuals preferences.  It seems slightly more of a client
feature where you can set defaults any time you get to a document and these
are stored in your local config file.  Perhaps the document author can
provide details on what choices the reader has for default status, and what
values can be set as defaults.

Hows all that sound...
Robert




From altis@ibeam.jf.intel.com  Wed Feb 16 03:12:37 1994 --100
Message-Id: <m0pWbh5-000434C@ibeam.intel.com>
Date: Wed, 16 Feb 1994 03:12:37 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: Proxy Servers

At  2:11 AM 2/16/94 +0000, Markus Stumpf wrote:
>looks like lots of people are hacking on proxy clients/servers these days.
>Could we please agree on some standards ... on the server and on the client
>side?

Well, that's what Lou, Ari, and I are doing, setting the application level
proxy standard for the Web. We've defined how a client can speak HTTP to a
proxy server in order to interact (GET, POST, PUT...) with the Web without
losing any functionality on the client side. This is necessary for clients
behind firewalls, but is also useful when you want the proxy server to act
as a caching server for a site, minimizing Internet traffic to and from
that site. The client always speaks HTTP to the proxy server and results
are always returned via HTTP (actually in the same connection, usual
stuff). The proxy server in turn speaks HTTP, FTP, Gopher, WAIS, whatever,
in order to retrieve the actual data, but always returns the data as an
HTTP MIME message, doing MIME typing on the fly; The GET examples I posted
a few days ago were intended to show the client/server conversation.

>The proxy environment variables cause the URLs to be sent with
>the protocol, the others without (I never checked what the real
>difference is in the use of WWW_xxx_gateway and the proxy one,
>and I never understood why the protocol info was omitted around
>libwww-2.09).

Only four environment variables: http_proxy, gopher_proxy, ftp_proxy,
wais_proxy. All expect to be set to a full URL, for example in bourne
shell:
ftp_proxy=http://host.domain:911/
export ftp_proxy

Actually, you can proxy news as well via the cern_httpd, but that's not
such a great idea. The environment variable names are different than the
old WWW_protocol_GATEWAY environment variables so that they don't get
confused with the old mechanism and allow sites using the older mechanism
to migrate smoothly to the new standard method. I think people mainly used
the old method as a WAIS gateway with the oddball double URLs.

The big difference between the old WWW_protocol_GATEWAY proxy and the new
standard, is that in the new method the client always sends a full URL (a
real URL, like what the user would see in the client, not a double URL) to
the proxy and the client and server only talk HTTP between themselves. This
was shown by the GET examples I posted. Since the proxy server, gets a full
URL from the client, the same proxy server can proxy requests for all
destination protocols (http://, ftp://, gopher://, wais://). Also, the
proxy simply sends along all of the metainformation fields, Accepts, etc.
from the client when the URL is for an HTTP server (http://). This way, as
the HTTP protocol support expands in our clients and servers to include
more metainformation and so on, your site proxy server doesn't have to be
upgraded. The proxy server is just that, a proxy between clients and
servers on the Internet.

>And while we're on it I'd really like to have some mechanisms to only
>use a gateway at all, if the clients cannot connect directly. (IMHO
>it doesn't make too much sense to connect to servers on the same
>subnet/domain, that are e.g. on the same side of a firewall through
>a gatway server.)

Each client application will have to decide when to proxy and when not to.
A few messages went across this list about standardizing how clients should
make that decision, but we need more discussion. For now, all clients use
all or nothing proxying on a protocol by protocol basis.

>But before we have 20 environment variables to control the client
>and 2 different URLs that are sent out on gateway requests, could we
>please agree on some "standards" ?

I need to write up all the HTML to document this and get it posted on the
CERN server, etc. However, that doesn't prevent people from running clients
via the cern_httpd as a proxy, testing, and feeding results back to myself,
and this list. Proxy support is fairly easy for clients; I think the record
to add support right now is five minutes, but it didn't take much longer
for the NCSA folks to add proxy support given Lou's diffs. Server writers
that want to add proxy support should contact me if they don't understand
any of the workings described so far.

ka





From burchard@geom.umn.edu  Wed Feb 16 03:36:26 1994 --100
Message-Id: <9402160233.AA17609@mobius.geom.umn.edu>
Date: Wed, 16 Feb 1994 03:36:26 --100
From: burchard@geom.umn.edu (burchard@geom.umn.edu)
Subject: Re: Defaults - client side, server side or what?

> With the proliferation of form-based documents I see a
> need for a system whereby people can setup defaults for
> documents that they visit.

No need to invent an idiosyncratic feature for this---just save a  
copy of the form and edit the values to taste.

The only problem with this right now is that some browsers don't seem  
to be very smart in the way they save HTML into local files.   
Specifically, they don't resolve relative links, even though they  
know perfectly well what the full URLs are, and even though the  
document is useless without such resolution.  Right now you have to  
edit the URLs in the saved file yourself.

--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------



From pflynn@curia.ucc.ie  Wed Feb 16 11:01:38 1994 --100
Message-Id: <9402160934.AA08795@curia.ucc.ie>
Date: Wed, 16 Feb 1994 11:01:38 --100
From: pflynn@curia.ucc.ie (Peter Flynn)
Subject: Re: <draft-ietf-iiir-html-01.txt, .ps> to be deleted.

> One opinion: browsers should be as robust as possible; however, to
> help people make the document truly portable, there should also be
> a validating parser tool available, perhaps built into the servers.

Particularly if it mailed the document owner (<link href> value) each
time the document was accessed, to say it was invalid :-)  Actually,
doing that kind of parsing server-side is a nice idea.

> Likewise, I am concerned about the expressiveness of HTML+. There are 
> many document types today that can not be reproduced faithfully using 
> this specification; interactive catalogs, technical magazines are two
> examples that will not be able to be done without significant compromise
> to their design specifications.  

I don't think we can realistically expect to be all things to all people.
At some stage we have to bring the axe down and say "this is what HTML++@
will do, and this is what it will not do". So long as we document clearly
what users cannot expect to do, we will have a stable platform for other
and future developments.

> Many of those who will be funding the growth of the Internet (and other 
> networks) will be looking to reproduce their information in the visual
> form they are used to. IF we wish this to be a tool used (and thereby
> moved forward by) these types of applications then we need to consider
> their needs.

Consider, perhaps, but they are going to have to learn the facts of life.
Concentration on visual-only documents will kill the whole concept stone
dead. I know they're funders and we may have to kiss the right ass from
time to time, but I don't see why we should compromise the structural
validity of what we are doing just to please some semiliterates in govt
or biz. They want a pictorial network, let them go help Ted Turner.

[wysiwyg editors]
> This is a fundamental issue. I would propose that you cannot reach 
> agreement as to what a good DTD is without agreeing to this aspect of
> it usage. So, what will it be?  
>  ___ Full support for hand tagging
>  ___ Limited support for hand tagging, limited support for specialized editor
>  ___ specialized editor or tool

I'm not clear what the issue of WYSIWYG editors has to do with a "good" DTD.
You can use any SGML editor you wish with any DTD, as far as I know.

> I assume that we should support hand tagging as much as possible up to the 
> point where "real" SGML tools can support, but not beyond! However, there
> may be a way to get the tools community to provide specialized tools
> customized for HTML+ at little or no cost. This is such an attractive idea
> that we should consider it.

SGML usage is growing very fast, and most people expect it to grow much faster.
I would suspect that a dedicated HTML-only tool would have a very short
lifespan, when people will discover they want to use other DTDs for other
(un-web-related) tasks elsewhere in their work: so they'll want a general-
purpose editor instead.

Having said that, yes, there will be an interim need for simple HTML+
editors which guarantee to produce valid HTML+.

///Peter



From timbl@ptpc00.cern.ch  Wed Feb 16 12:13:27 1994 --100
Message-Id: <9402161111.AA03325@ptpc00.cern.ch>
Date: Wed, 16 Feb 1994 12:13:27 --100
From: timbl@ptpc00.cern.ch (Tim Berners-Lee)
Subject: Re: Problem with "~user/file.html" 


Dan suggests,

> 	... click <A HREF="finger://my.domain/myloginname">here</a>...
> 

> I'm not sure about the URL syntax for a finger query... you should check
> the spec or the code or something.

this is almost a FAQ.  The gopher URL can be used to make a finger query.
gopher://my.domain:79/0myloginname
where the 0 indicates plain text coming back, and 79 is the finger port  
number.

> But the finger application is already quite widely deployed. Let's not
> get into the mindset that the HTTP server should solve all the world's
> problems.

Sure,  if what you want is plain text about a user, finger is fine.
(We'd hate to solve all the worlds problems wouldn't we ;-))

> Hmm.... how does one specify that the data coming back from the finger
> request is to be interpreted as HTML?

Oh dear.  You certainly can't do that.  The finger protocol is
that you send a user name and you get back plain text. If you
return anything else, you are breaking finger.  You can't do
that to existing protocols.  You will have complaints from
finger client users.

Of course, you could define an enhanced  "htfinger" protocol, which had a
header on the response to say what the data type was coming back,
or a "gofinger" protocol which had a letter prepended to the
username to say what data type one should expect...  :-)))

> With the DTD I wrote a long time
> ago, it would be:
> 

> 	... click <A HREF="finger://my.domain/myloginname"
> 			CONTENT-TYPE="text/html">here</a>...
> 

> But nobody else seemed to think that content-type was a good thing to
> put in a link. I sure think it's useful.

That's what gopher0 does. It doesn't work when you get the same thing
available in many formats, or when the format changes.  But it does mean
that you can put up an appropriate icon on eth link automatically. 


> Dan

Tim



From rodw@cbl.leeds.ac.uk  Wed Feb 16 13:31:45 1994 --100
Message-Id: <3622.9402161226@cblelcc.cbl.leeds.ac.uk>
Date: Wed, 16 Feb 1994 13:31:45 --100
From: rodw@cbl.leeds.ac.uk (rodw@cbl.leeds.ac.uk)
Subject: Re: Server control over history?

Paul Burchard <burchard@edu.umn.gw.horizon> writes:
>
>Fill-out forms, as they exist now, can hold arbitrary state  
>information during a sequence of transactions (this will soon be  
>completely straightforward and aboveboard using type="hidden"  
>fields.)
>
>In the W3Kit system used to build our interactive Web apps, the  
>complete per-user application state is in fact encoded in the  
>fill-out form, as an archive of persistent objects.  This allows  
>rollback, even random access to history, since despite appearances  
>the server remains fully stateless.

A result of this approach is that all the forms with instantiated
state have the same url.  Therefore, to go to the same state again
you either have to save a local copy of the form or follow a set of links to 
reach the desired state.  Saving the page's url on your hotlist won't get
you back to the same place, same goes for mailing the url to someone -- they
will not be able to get to the same place.  I think these failing underline
that a url should contain enough state to enable a script to regenerate the 
html page.

Rod
----
Roderick Williams          R.J.Williams@cbl.leeds.ac.uk



From timbl@ptpc00.cern.ch  Wed Feb 16 11:37:28 1994 --100
Message-Id: <9402161035.AA03315@ptpc00.cern.ch>
Date: Wed, 16 Feb 1994 11:37:28 --100
From: timbl@ptpc00.cern.ch (Tim Berners-Lee)
Subject: Re: Proxy Servers


> Date: Wed, 16 Feb 1994 03:11:37 --100
> From: altis@ibeam.jf.intel.com (Kevin Altis)

> At  2:11 AM 2/16/94 +0000, Markus Stumpf wrote:
> >looks like lots of people are hacking on proxy clients/servers these days.
> >Could we please agree on some standards ... on the server and on the  
client
> >side?

I am sorry that I have been late in replying to this stuff to
clear up some misunderstandings.  The fact is that the "proxy",
or gateway operation, has been in the WWW code for a very long time.
It was commented out when the client developed a rule file
for one release, because I stupidly imagined that the rule file
would do the job when it won't for a HTTP-HTTP gateway.
But it was put back in again, and then removed recently when
the code was reorganised, and I forgot to put it back in.
In fact, to get wais: urls into more common usage, the
code had the address of the cern gateway for wais built in as
a default at one stage.

> Well, that's what Lou, Ari, and I are doing, setting the application level
> proxy standard for the Web. We've defined how a client can speak HTTP to a
> proxy server in order to interact (GET, POST, PUT...) with the Web without
> losing any functionality on the client side. This is necessary for clients
> behind firewalls, but is also useful when you want the proxy server to act
> as a caching server for a site, minimizing Internet traffic to and from
> that site. The client always speaks HTTP to the proxy server and results
> are always returned via HTTP (actually in the same connection, usual
> stuff). The proxy server in turn speaks HTTP, FTP, Gopher, WAIS, whatever,
> in order to retrieve the actual data, but always returns the data as an
> HTTP MIME message, doing MIME typing on the fly; The GET examples I posted
> a few days ago were intended to show the client/server conversation.
> 

> >The proxy environment variables cause the URLs to be sent with
> >the protocol, the others without (I never checked what the real
> >difference is in the use of WWW_xxx_gateway and the proxy one,
> >and I never understood why the protocol info was omitted around
> >libwww-2.09).

There is no difference.
Sorry for the code drop-out, it will go back in.  (We didn't have Ari at
the time!)

> Only four environment variables: http_proxy, gopher_proxy, ftp_proxy,
> wais_proxy. All expect to be set to a full URL, for example in bourne
> shell:
> ftp_proxy=http://host.domain:911/
> export ftp_proxy
> 

> Actually, you can proxy news as well via the cern_httpd, but that's not
> such a great idea. The environment variable names are different than the
> old WWW_protocol_GATEWAY environment variables so that they don't get
> confused with the old mechanism and allow sites using the older mechanism
> to migrate smoothly to the new standard method. 


Aaagh!  The old and new methods are protocol-wise the same.  I like
the name "proxy" but I don't think that changing the environment variables
helps.  The effect is exactly the same.

> I think people mainly used
> the old method as a WAIS gateway with the oddball double URLs.

What are oddball double URLs?  There were two methods of using the
wais gateway. You could eiethr use it in gateway mode, by setting

	WWW_wais_GATEWAY=http://info.cern.ch:8001/
	exprt WWW_wais_GATEWAY

in which case the client sent the wais URL unmodified

	(connect to info.cern.ch on 8001)
	GET wais:/wais.dom.ain/database?query

OR, someone refering to a wais database who didn't want to rely on users
having that gatway set up would put a pointer to a mapping onto
http space http://info.cern.ch:8001/wais.dom.ain/database?query
In this case the server would get

	GET /wais.dom.ain/database?query

but being a smart server it would handle that too.  So you could have your  
cake or eat it.  In fact becaus a lot of things made explict reference
to http-mapped gateways, there were a lot of mapped URLs about,
and that will continue to be the case for anything which doesn't have
its own access protocol (like hytelnet) and when there is a wais gateway
running very efficiently on the same machine as a given wais database, in
which case to force access to go through that gateway saves everyone time.

> The big difference between the old WWW_protocol_GATEWAY proxy and the new
> standard, is that in the new method the client always sends a full URL (a
> real URL, like what the user would see in the client, not a double URL)

There is no such difference.  If by "double URL' you mean the mapping of
wais space onto http space, then that was just a convenient extra.
Check out the HTTP spec,

"Unless the server is being used as a gateway, a partial URL shall be given  
with the assuptions of the protocol (HTTP:) and server (the server) being  
obvious." (from
<http://infoc.ern.ch//hypertext/WWW/Protocols/HTTP/Request.html>)

> Since the proxy server, gets a full
> URL from the client, the same proxy server can proxy requests for all
> destination protocols (http://, ftp://, gopher://, wais://).

In fact the info.cern.ch:80 server was working in this mode when it
was running on a NeXT.  When we moved to solaris   :-(  the wais code broke,
and we ain't fixed it yet, which is why info.cern.ch:8001 is down.

Two reasons that all this has been less evident and hasn't worked.
One is that the code was taken out of the libwww. The other
is that the wais gateway not there for all to use since Christmas.

> Also, the
> proxy simply sends along all of the metainformation fields, Accepts, etc.
> from the client when the URL is for an HTTP server (http://). This way, as
> the HTTP protocol support expands in our clients and servers to include
> more metainformation and so on, your site proxy server doesn't have to be
> upgraded. The proxy server is just that, a proxy between clients and
> servers on the Internet.

This is something which the old server didn't do, but Ari's new release will.

> >And while we're on it I'd really like to have some mechanisms to only
> >use a gateway at all, if the clients cannot connect directly. (IMHO
> >it doesn't make too much sense to connect to servers on the same
> >subnet/domain, that are e.g. on the same side of a firewall through
> >a gatway server.)

I agree.

> Each client application will have to decide when to proxy and when not to.
> A few messages went across this list about standardizing how clients should
> make that decision, but we need more discussion. For now, all clients use
> all or nothing proxying on a protocol by protocol basis.

I agree.

> >But before we have 20 environment variables to control the client
> >and 2 different URLs that are sent out on gateway requests, could we
> >please agree on some "standards" ?

Yes.  I think that a simple and common case will be that
anything within a certain single domain will be local access.
Generally the firewall or the weak link is at a domain boundary,
to all intents and purposes. One possibility is to force ALL traffic
outide a domain to use a server, which would need two env variables

	WWW_FIREWALL_GATEWAY	http://gateway.acme.com/
	WWW_FIREWALL_DOMAIN	acme.com

of couse a good default would be to guess that the domain was the 

domain of the gateway server, which would just mean one env variable.
Another would be to do it separately by URL scheme.

	WWW_http_GATEWAY	http://gateway.acme.com/
	WWW_http_DIRECT_DOMAIN	acme.com

Any thoughts on this?  Kev like to propose something and the code
as a function of any other comments?  I agree we want to keep it
simple.

Tim Berners-Lee




From dsr@hplb.hpl.hp.com  Wed Feb 16 14:44:37 1994 --100
Message-Id: <9402161337.AA17424@manuel.hpl.hp.com>
Date: Wed, 16 Feb 1994 14:44:37 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Server control over history? 

Dan,

I couldn't retrieve the references you gave for managing history
using HyTime. In reading through the HyTime architectural forms
and background material, I didn't see anything directly relevant
to issues such as what can and can't be scrolled away.

My suggestion for extending the LINK attribute is a very natural
extension to HTML and builds on earlier precedents. We have
recently developed a flexible scheme for expressing hypertext
paths, where paths are themselves HTML documents. The ability
to specify images for such navigation buttons is already popular
in the WWW community, and my suggestion extends this to include
a document toolbar, using the LINK element.

HyTime's location ladders and event mapping etc. are largely
orthogonal to these ideas. Indeed, it is already clear how to
add the HyTime clink architectural form into HTML, as an alternative
to the current HREF and SRC attributes. Perhaps you could make
a more concrete proposal for how this would effect issues such
as management of history and visibility of navigation buttons.

Dave



From dsr@hplb.hpl.hp.com  Wed Feb 16 15:03:23 1994 --100
Message-Id: <9402161357.AA17436@manuel.hpl.hp.com>
Date: Wed, 16 Feb 1994 15:03:23 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: SGML parsing, moving between formats

At  1:19 PM 2/15/94 +0000, Daniel W. Connolly wrote:

> Yes... and it seems to me (at first glance... I'll have to look more
> closely...) that we've lost the ability to translate HTML to Microsoft
> Word or FrameMaker without any loss of information.
>
> Let's get formal why don't we: I do not mean that we should be able to
> take any RTF file and convert it to HTMLPLUS, or MIF for that matter.
> But I think it's crucial that there exist invertible mappings
>
>        h : HTML -> RTF
> and
>        g : HTML -> MIF
> and
>        h : HTML -> TeXinfo
>
> so that I can take a given HTML document, convert it to RTF, and
> convert it back and get exactly what I started with (the same ESIS,
> that is... perhaps SGML comments and a few meaningless RE's would get
> lost).

I don't understand why Dan thinks that mapping from HTML+ to the other
formats will be impossible. It seems straightforward enough to me.
The main decision is what style to assume for each logical element.

Reversibility *is* a big problem with HTML as current filters from
FrameMaker or LaTeX to HTML tend to translate tables and math into
inline images. This problem goes away when we switch to HTML+ as
you can then translate reversibly into HTML+ tables and math. Thats
why adoption of these features is so important. I will be demoing them
at the forthcoming WWW Conference with a new X11 HTML+ browser.

You would still lose any style settings in the process, but even this
can be dealt with by associating the HTML+ document with as style sheet
as suggested by O'Reilly & Associates.

Dave Raggett



From swb1@cornell.edu  Wed Feb 16 15:42:09 1994 --100
Message-Id: <199402161438.JAA05295@postoffice3.mail.cornell.edu>
Date: Wed, 16 Feb 1994 15:42:09 --100
From: swb1@cornell.edu (Scott W Brim)
Subject: Re: Problem with "~user/file.html"

  >> Hmm.... how does one specify that the data coming back from the finger
  >> request is to be interpreted as HTML?

By having a script do the finger for you?





From dsr@hplb.hpl.hp.com  Wed Feb 16 16:11:30 1994 --100
Message-Id: <9402161505.AA17851@manuel.hpl.hp.com>
Date: Wed, 16 Feb 1994 16:11:30 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: <draft-ietf-iiir-html-01.txt, .ps> to be deleted. 

Dave,

> In the future, I expect several other factors to become important. These
> are:
>     1) the willingness of large information providers to subsidize the
>                 interconnections 
>     2) the affordable creation and maintenance of webs
>     3) continual browser and server improvements and platform support

I think this is a good list, but you may be forgetting the opportunities
for recompensing information providers. Right now, there is nothing to stop
providers from offering a subscription-only service. A related approach is
to encrypt documents and sell people decryption keys for use with suitable
browsers. There are many other possibilities such as pay as you go, which
will depend on third party billing mechanisms. It would be great to involve
the telcos and credit card companies in helping to set this up.

The affordability of creating webs is in little doubt and requires nothing
more that what is available with current billboard systems. Tools for
supporting maintenance of Web servers and caches are rapidly evolving. We
still need to make progress on URNs though.

I hope that my HTML+ browser will greatly stimulate browser developments
this year, and expect the trend to continue. As the number of browsers takes
off dramatically, it becomes a realistic proposition for commercial developpers
to provide professional tools. Some degree of continuity in standards is going
to be essential to allow commercial developpers a chance though.

> How do I see this impacting the design of HTML+?

>   * expressive - every company has an "corporate identity", every publisher
>        has their "look". We need style sheets if we keep the language
>        semantic (and I hope we do). We could also use an style element
>        like the Table of Semantic Styles in SDL.

You can see this now, in the way people are using graphics to embellish their
documents.  I am currently proposing an extension that would allow authors to
customise a document toolbar in this way for this very reason. I believe that
styles shouldn't be part of the document directly though. Instead it seems
preferable to link the HTML document to stylesheets and to allow the opportunity
for servers to send stylesheets appropriate to each class of browser. This
makes for a much cleaner design, considering the very wide range of browser
platforms.

>   * programmable and portable - document publishers have to know what is
>        widely supported and what is not. (the same phenomena as PC software)

HTML is already very good at this - browsers are available for a staggering
number of platforms. HTML+ will soon follow suit. HTML+ will be more portable
than HTML as you won't need to translate tables and math into inline images etc.

>   * linkage - lots of consideration needs to go into links. The maintenance
>        of links is a real problem in my community (online UNIX technical 
>        document publishers). At least one level of abstraction should be
>        required and specified.

By using URNs we can decouple this from the document format itself. I have
recently posted a note on how this can be achieved using a DNS like
distributed name lookup scheme for lifetime identifiers and a mechanism for
selecting between variants. This could be available by the end of this year.

Regards,

Dave Raggett,

-----------------------------------------------------------------------------
Hewlett Packard Laboratories,           +44 272 228046
Bristol, England                        dsr@hplb.hpl.hp.com



From dkulp@gdb.org  Wed Feb 16 17:45:30 1994 --100
Message-Id: <9402161642.AA09097@dev.gdb.org>
Date: Wed, 16 Feb 1994 17:45:30 --100
From: dkulp@gdb.org (David Kulp)
Subject: HTTP Server Load

Webmasters:

We're about to bring on-line a WWW server which we expect to
be heavily utilized.  The production plans include buying a new machine,
but we're not sure about the system load of a high use server.

Currently I'm running a development server (NCSA's httpd) standalone on a 
Sparc 10 Model 30 with no problems, but usage is, of course, quite low.
The server is used to perform some heavy-duty database (Sybase) querying.
I suspect that performance problems will be due to the query processes
and not the httpd server itself; nevertheless, server performance is
a concern.

If anyone has any performance stats or anecdotal evidence for memory
requirements, CPU usage, or response time vs connections on Sun machines
I'd be very interested.  Direct e-mail replies to dkulp@gdb.org.

Thanks very much,
-David Kulp.

Genome DataBase
Johns Hopkins Univ



From wade@cs.utk.edu  Wed Feb 16 17:56:57 1994 --100
Message-Id: <9402161651.AA19639@honk.cs.utk.edu>
Date: Wed, 16 Feb 1994 17:56:57 --100
From: wade@cs.utk.edu (Reed Wade)
Subject: Re: client local file system access issues 



Is the 'file' URL documented anywhere?



>Getting all clients to do file://localhost

I was noticing that XMosaic did this. My understanding of the 'file'
protocol specifier was:
  1. try to get it via ftp
  2. if that fails and no host name was specified, try to treat it
     as a local file

I don't know if maybe 2. comes before 1. This would be pretty important 
in the following case-

  file://localhost/tuna

Suppose there is a /tuna file on my machine AND a file called
tuna in the root directory of the anonymous ftp server on this
same machine?


If I were writing a WWW client (which, in fact, I am) I'd be inclined
to say that if a host is specified (even localhost) then 'file'
_really_ means 'ftp'; otherwise it means 'file'.

As an aside, the URL spec <file://info.cern.ch/pub/www/doc/url-spec.txt>
makes no mention of the 'file' protocol type.




>file://localhost/path
>Local
>file access doesn't really seem to be standardized. Here are some examples:
>
>Lynx and X Mosaic both do file://localhost/path
>path is the full path from the root (/) of the local filesystem.

>Cello does file://localhost/c:/path
>c could be any drive letter.
>
>Mac Mosaic does file:///drive name/path Spaces are okay at least in the
>drive name.
>
>Win Mosaic does file:///c|/path
>c could be any drive letter.
>
>
>I think that clients could probably treat the drive letter (DOS/WINDOWS) or
>drive name (Mac), etc. as just the first path element, so that the path
>wouldn't look any different than their Unix cousins:
>file://localhost/c/path or file://localhost/drive name/path
>The key is that all clients have to conform to using file://localhost/ to
>mean part of the local file system.


What's so great about /'s?

I don't know too much about DOS and Mac's but isn't it possible
to construct a single absolute (path/string) that points to a file
including drive? If that's true, what's the need to litter it with /'s?
I'm nearly certain that's the case w/DOS.

This causes a problem if you still want to use partial URLs.

I think there's a lot of confusion regarding the semantics of the /
in urls (especially for nonUnix-style systems). I know I'm confused.


>---
>Local aliases
[ stuff deleted ]
>(especially my portable where I don't always have a net connection). Of
>course, I would want to reference those local files and not have the URLs
>break at a later date.
>
>What's needed of course is the ability to do client side aliasing, just
>like HTTP servers allow today. The client would support a setup such as:
>
>Alias file://localhost/c/somedir/anotherdir file://localhost/i/anotherdir
>
>All the URLs referencing those local documents would still work, even
>though the actual file system location on the local machine has changed.

Don't partial URLs fix this problem already? Or at least part of it.


>---
>The last issue is when dealing with local files, there is no particular
>reason that clients should bring a document from part of the local
>filesystem to another part of the local filesystem before displaying the
>file or handing it off to an external browser. So, the case of
>file://localhost should be treated special be the client so that it just
>goes and used the local file. You won't have to wait for a 14MB audio/video
>file to get copied to a different directory before the external mpeg player
>launches to play the file.
>
>Comments?

This is strictly a client design decision. If you're looking at a local
file then, naturally, you shouldn't copy it around. If 'localhost' has
magical meaning--the thought of which makes my skin crawl, then that's
another matter.


>
>ka
>
>


-reed

--
wade@cs.utk.edu -- http://netlib2.cs.utk.edu/people/ReedWade.html



From connolly@hal.com  Wed Feb 16 18:06:06 1994 --100
Message-Id: <9402161703.AA05676@ulua.hal.com>
Date: Wed, 16 Feb 1994 18:06:06 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: client local file system access issues 

In message <9402161651.AA19639@honk.cs.utk.edu>, Reed Wade writes:
>
>
>Is the 'file' URL documented anywhere?
>
>

Yeah, it's in the URL spec. But it's somewhat muddled.

Here's what I suggest we standardize on:

1. ftp://host/dir/dir/dir/file
	and we define the semantics in terms of the FTP RFC
	and/or the MIME ftp access-type.

2. local-file:/dir/dir/dir/file
	and
   local-file://host/dir/dir/dir/file
	again, see the MIME RFC for interpretation

And we support, for backwards compatibility

3. file://host/dir/file.ext
	which means "try local-file, then ftp or rcp or whatever
	other hacks are locally available"

Dan




From paulp@is.internic.net  Wed Feb 16 18:26:27 1994 --100
Message-Id: <Pine.3.89.9402160927.B3380-0100000@is.internic.net>
Date: Wed, 16 Feb 1994 18:26:27 --100
From: paulp@is.internic.net (Paul Phillips)
Subject: Re: Problem with "~user/file.html" 

Or, execute a script that runs the finger, replaces the newlines with <br>, 
and return the output in HTML.  Not only does this look nicer, but you 
can put anchors and images in your .plan and have them come up.  :-)

|-------------------------------------------------------------------------|
| Paul Phillips                  | EMAIL: paulp@is.internic.net           |
| InterNIC Information Services  |   WWW: http://www.internic.net/~paulp/ |
| Reference Desk Staff           | PHONE: 619-455-4626 FAX: 619-455-4640  |
|-------------------------------------------------------------------------|


On Wed, 16 Feb 1994, Tim Berners-Lee wrote:

> > Hmm.... how does one specify that the data coming back from the finger
> > request is to be interpreted as HTML?
> 
> Oh dear.  You certainly can't do that.  The finger protocol is
> that you send a user name and you get back plain text. If you
> return anything else, you are breaking finger.  You can't do
> that to existing protocols.  You will have complaints from
> finger client users.
> 
> Of course, you could define an enhanced  "htfinger" protocol, which had a
> header on the response to say what the data type was coming back,
> or a "gofinger" protocol which had a letter prepended to the
> username to say what data type one should expect...  :-)))



From connolly@hal.com  Wed Feb 16 18:41:24 1994 --100
Message-Id: <9402161738.AA05710@ulua.hal.com>
Date: Wed, 16 Feb 1994 18:41:24 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Problem with "~user/file.html" 

In message <9402161111.AA03325@ptpc00.cern.ch>, Tim Berners-Lee writes:
>
>Oh dear.  You certainly can't do that.  The finger protocol is
>that you send a user name and you get back plain text. If you
>return anything else, you are breaking finger.  You can't do
>that to existing protocols.  You will have complaints from
>finger client users.
>
>Of course, you could define an enhanced  "htfinger" protocol, which had a
>header on the response to say what the data type was coming back,
>or a "gofinger" protocol which had a letter prepended to the
>username to say what data type one should expect...  :-)))

I disagree. RFC822 says that the body of an internet message is plain
text in the US-ASCII charset. That doesn't stop us from sticking stuff
in internet messages that satisfies the letter of RFC822 and applying
a different interpretation to it -- MIME.

Suppose we leave the normative part of the finger protocol alone,
but we extend our usage of it so that we interpret what comes back
as a text/plain body part, unless by some means (e.g. an HTML link)
we can determine that it should be some other text/* content type.

Note that what comes back still satisfies the constraints of the
finger protocol -- we just interpret it a little differently.
Normal finger clients see some SGML <tags> that they didn't bargain
for, but I don't see much harm in that.

Now folks can't stick image/gif data in their .plan file, but they
can put pointers to them.

Dan




From connolly@hal.com  Wed Feb 16 18:46:50 1994 --100
Message-Id: <9402161741.AA05724@ulua.hal.com>
Date: Wed, 16 Feb 1994 18:46:50 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Problem with "~user/file.html" 

In message <Pine.3.89.9402160927.B3380-0100000@is.internic.net>, Paul Phillips 
writes:
>Or, execute a script that runs the finger, replaces the newlines with <br>, 
>and return the output in HTML.  Not only does this look nicer, but you 
>can put anchors and images in your .plan and have them come up.  :-)

But it requires an HTTP server somewhere between the client and
the finger server. In sites where finger is already deployed, why
do this? It's a pain especially w.r.t. security, firewalls and such.

Dan



From wade@cs.utk.edu  Wed Feb 16 19:00:52 1994 --100
Message-Id: <9402161757.AA19894@honk.cs.utk.edu>
Date: Wed, 16 Feb 1994 19:00:52 --100
From: wade@cs.utk.edu (Reed Wade)
Subject: Re: Problem with "~user/file.html" 


Daniel W. Connolly writes:
>Suppose we leave the normative part of the finger protocol alone,
>but we extend our usage of it so that we interpret what comes back
>as a text/plain body part, unless by some means (e.g. an HTML link)
>we can determine that it should be some other text/* content type.
>
>Note that what comes back still satisfies the constraints of the
>finger protocol -- we just interpret it a little differently.
>Normal finger clients see some SGML <tags> that they didn't bargain
>for, but I don't see much harm in that.
>
>Dan


One solution would be to treat it simply as preformatted html.
That way imbedded tags would be easily dealt with if they occur.

<pre>
  ( result of finger lookup )
</pre>


-reed

--
wade@cs.utk.edu -- http://netlib2.cs.utk.edu/people/ReedWade.html



From paulp@is.internic.net  Wed Feb 16 19:09:12 1994 --100
Message-Id: <Pine.3.89.9402161059.B8029-0100000@is.internic.net>
Date: Wed, 16 Feb 1994 19:09:12 --100
From: paulp@is.internic.net (Paul Phillips)
Subject: Re: Problem with "~user/file.html" 

It's definitely not strictly necessary, and it's inappropriate at some 
sites, but that doesn't mean it's not useful.  I do not like the look of 
plaintext when using web browsers, so I take pains to convert it to HTML 
where possible.  In my particular case, it's possible.  YMMV.

|-------------------------------------------------------------------------|
| Paul Phillips                  | EMAIL: paulp@is.internic.net           |
| InterNIC Information Services  |   WWW: http://www.internic.net/~paulp/ |
| Reference Desk Staff           | PHONE: 619-455-4626 FAX: 619-455-4640  |
|-------------------------------------------------------------------------|


On Wed, 16 Feb 1994, Daniel W. Connolly wrote:

> But it requires an HTTP server somewhere between the client and
> the finger server. In sites where finger is already deployed, why
> do this? It's a pain especially w.r.t. security, firewalls and such.
> 
> Dan
> 



From junga@informatik.tu-muenchen.de  Wed Feb 16 18:15:40 1994 --100
Message-Id: <1994Feb16.171149.15183@Informatik.TU-Muenchen.DE>
Date: Wed, 16 Feb 1994 18:15:40 --100
From: junga@informatik.tu-muenchen.de (Achim Jung)
Subject: Collection of Perl-Scripts for modifiing URLs and HTML-documents


Hi!

Why invenmting the wheel twice? I`m searching for some useful Perl 
scripts (scripts in other languages as well) which are used in a 
WWW-server. Has anyone collected an archive of some?
(If not, I will create it.)

What I`m especially searching for is a program/script which fully
expands relative URLs in a html-document.

Regards, Achim
-------------------------------------------------------------------
Achim Jung        IRC: Flops        junga@informatik.tu-muenchen.de
WWW: http://www.informatik.tu-muenchen.de/personen/junga/junga.html




From montulli@stat1.cc.ukans.edu  Wed Feb 16 20:26:15 1994 --100
Message-Id: <9402161923.AA67580@stat1.cc.ukans.edu>
Date: Wed, 16 Feb 1994 20:26:15 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Proxy Servers

> > Date: Wed, 16 Feb 1994 03:11:37 --100
> > From: altis@ibeam.jf.intel.com (Kevin Altis)
> 
> > At  2:11 AM 2/16/94 +0000, Markus Stumpf wrote:
> > >looks like lots of people are hacking on proxy clients/servers these days.
> > >Could we please agree on some standards ... on the server and on the  
> client
> > >side?
> 
> I am sorry that I have been late in replying to this stuff to
> clear up some misunderstandings.  The fact is that the "proxy",
> or gateway operation, has been in the WWW code for a very long time.
> It was commented out when the client developed a rule file
> for one release, because I stupidly imagined that the rule file
> would do the job when it won't for a HTTP-HTTP gateway.
> But it was put back in again, and then removed recently when
> the code was reorganised, and I forgot to put it back in.
> In fact, to get wais: urls into more common usage, the
> code had the address of the cern gateway for wais built in as
> a default at one stage.
> 
Tim I have to disagree.  The code that was in the WWWlibrary 
DID NOT send the entire URL unmodified, it sent only the
host and path part and it has worked that way for as long
as I can remember.  The code to do that _IS_ in version 2.14
of the library and it works with existing wais gateway
servers (like the one at NCSA).  The reason we picked a
different environment variable was so that we didn't break
whatever other people might have been doing with
gateways using the old form. (with just host and path)

The use of a new environment variable is justified.

:lou



From wade@cs.utk.edu  Wed Feb 16 20:33:39 1994 --100
Message-Id: <9402161930.AA20076@honk.cs.utk.edu>
Date: Wed, 16 Feb 1994 20:33:39 --100
From: wade@cs.utk.edu (Reed Wade)
Subject: Re: Problem with "~user/file.html" 


I wrote:
>One solution would be to treat it simply as preformatted html.
>That way imbedded tags would be easily dealt with if they occur.
>
><pre>
>  ( result of finger lookup )
></pre>
>
>
>-reed


Actually, that's a pretty stupid idea. 
It would break in too many odd ways. 

Forget I mentioned it. 

How did this thread get started? What's wrong with pointing at
random different http servers for finger info (like a lot of people
do now)? Maybe whois++ will fix this.


-reed

--
wade@cs.utk.edu -- http://netlib2.cs.utk.edu/people/ReedWade.html



From dmh@hpfcma.fc.hp.com  Wed Feb 16 21:33:42 1994 --100
Message-Id: <9402162028.AA11899@hpfcma.fc.hp.com>
Date: Wed, 16 Feb 1994 21:33:42 --100
From: dmh@hpfcma.fc.hp.com (Dave Hollander)
Subject: Re: <draft-ietf-iiir-html-01.txt, .ps> to be deleted. 

> > = Dave Hollander
> = Dave Raggett

> You can see this now, in the way people are using graphics to embellish their
> documents.  I am currently proposing an extension that would allow authors to
> customise a document toolbar in this way for this very reason. I believe that
> styles shouldn't be part of the document directly though. Instead it seems
> preferable to link the HTML document to stylesheets and to allow the 
> opportunity for servers to send stylesheets appropriate to each class of 
> browser. This makes for a much cleaner design, considering the very wide 
> range of browser platforms.
> 

I am not sure that separate style sheets vs. embedded style information
necessarily make for a cleaner design. The real design elegance is in
how you express the style both in terms of axis of control and syntax.

Either way, obviously from my mail, I think that some more control over
style is important to the future of the web.


_________________

> >   * programmable and portable - document publishers have to know what is
> >        widely supported and what is not. (the same phenomena as PC software
)
> 
> HTML is already very good at this - browsers are available for a staggering
> number of platforms. HTML+ will soon follow suit. HTML+ will be more portable
> than HTML as you won't need to translate tables and math into inline images 
> etc.

In this case, the issue is not platforms (thanks to the efforts to date)
but validation. How can a publisher verify their information can reach
a wide audience without testing on each browser?

Not really...It is hard to know what html constructs are legal vs supported
on the particular browser you are using. Your "lint" concept for checking
a document would help a lot.


Regards,
Dave Hollander


___________________________________________________________________
Dave Hollander 				Hewlett-Packard
Document Interchange Specialist 	3404 East Harmony Road, MS. 1U11
Learning Products Services 		Fort Collins, Colorado  80525
303-229-3192 				dmh@ce.hp.com
___________________________________________________________________



From dmh@hpfcma.fc.hp.com  Wed Feb 16 21:41:01 1994 --100
Message-Id: <9402162036.AA12214@hpfcma.fc.hp.com>
Date: Wed, 16 Feb 1994 21:41:01 --100
From: dmh@hpfcma.fc.hp.com (Dave Hollander)
Subject: Re: <draft-ietf-iiir-html-01.txt, .ps> to be deleted. 


> > Dave H
>   Dave R 

> > One opinion: browsers should be as robust as possible; however, to
> > help people make the document truly portable, there should also be
> > a validating parser tool available, perhaps built into the servers.
>
> I have long argued for an equivalent of lint and beatify for HTML.
> IMHO browsers should indicate if a document doesn't conform to the DTD,
> perhaps by a simple indicator light, along with an ability to get
> a report on the errors detected. I don't want the overhead of the
> server running this validation tool each time I retrieve a document
> though. This should be an offline job for the WebMaster of that server.

AGREE.

__________________________________________________________________________
> > Dave H
>   peter 

> > Likewise, I am concerned about the expressiveness of HTML+. There are 
> > many document types today that can not be reproduced faithfully using 
> > this specification; interactive catalogs, technical magazines are two
> > examples that will not be able to be done without significant compromise
> > to their design specifications.  
> 
> I don't think we can realistically expect to be all things to all people.
> At some stage we have to bring the axe down and say "this is what HTML++@
> will do, and this is what it will not do". So long as we document clearly
> what users cannot expect to do, we will have a stable platform for other
> and future developments.

Yes, any system will impose limitations. In the past and in talking
to people about the web, I have found that one common issue is 
expressiveness. In pure semantic languages this issue is in the
size and range of the tag set. In SDL, it is the factors controlled
by the table of semantic styles (TOSS).

If the goal of HTML+ is to provide a more standard, computable, rational, 
stable basis for what is done today, fine--the future will take care
of itself. On the other hand, if we are to look to the future, then we
must consider the limitations in light of the anticipated needs of the
future.


> > Many of those who will be funding the growth of the Internet (and other 
> > networks) will be looking to reproduce their information in the visual
> > form they are used to. IF we wish this to be a tool used (and thereby
> > moved forward by) these types of applications then we need to consider
> > their needs.
> 
> Consider, perhaps, but they are going to have to learn the facts of life.
> Concentration on visual-only documents will kill the whole concept stone
> dead. I know they're funders and we may have to kiss the right ass from
> time to time, but I don't see why we should compromise the structural
> validity of what we are doing just to please some semiliterates in govt
> or biz. They want a pictorial network, let them go help Ted Turner.

True enough, most of "those" people need to wake up to the true value
of their materials. I DO NOT ADVOCATE PS and or ACROBAT or anything like
that! However, we should also take care to not impose limitations that 
are not of value to the implementation of a semantic/structure based
environment and tools. 

I would hope a way could be found to maintain the "structural validity"
and provide control over style. We are trying to do both right now with 
the COSE/Common Desktop Environment Help viewer.

_____________________
> 
> [wysiwyg editors]
> > This is a fundamental issue. I would propose that you cannot reach 
> > agreement as to what a good DTD is without agreeing to this aspect of
> > it usage. So, what will it be?  
> >  ___ Full support for hand tagging
> >  ___ Limited support for hand tagging, limited support for specialized 
> >      editor
> >  ___ specialized editor or tool
> 
> I'm not clear what the issue of WYSIWYG editors has to do with a "good" DTD.
> You can use any SGML editor you wish with any DTD, as far as I know.
> 
You can use any editor with any DTD assuming the DTD does not use 
features (Link, Concur, short reference maps etc) that are not supported
by the editor. In adopting an old HP dtd to ArborText, Interleaf, and 
FrameBuilder I found that the design of the DTD had a large impact on 
the usability of the combined tool.  

Two examples, our old DTD used every minimization trick in the book 
(and a few that were not) resulting in literally dozens of 
valid elements in most contexts. GUI menu systems are not good at 
controlling that many choices and our authors became confused.

Another example is the choice of tag names. The original dtd had the shortest
tag names possible to minimize keyboarding. When put in GUI editors (that
do not map from GI to menu item) the odd order and cryptic names were a 
problem.


Regards,
Dave H



From montulli@stat1.cc.ukans.edu  Wed Feb 16 22:29:44 1994 --100
Message-Id: <9402162127.AA60786@stat1.cc.ukans.edu>
Date: Wed, 16 Feb 1994 22:29:44 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Idea for new form input type

I have an suggestion for a new form input type that I would
like to input on.

In order to support the posting of large files using form based
machanisms I suggest that a new input type "include-file" (or
something similar) be added.  This new type would be a file
that exists on the local machine whose data would be posted
to the forms server during form submission.  This method
would require the file to be posted using a MIME multipart
message.

The reasons this will be useful are as follows:
* large text files can be sent without having to cut and paste
  them into a textarea window as they are now.
* no memory limits on the size of the file.  (currently all input
  data is held in memory)
* arbitrary binary files can be sent 

The particular reason I would like to see this type is that I would
like to design a forms based database control system to simplify
management of HTTP servers.  Users could use forms to upload their
HTML or binary files up onto the server thereby eliminating the
need for them to either run their own server or know the commands
of the remote server's operating system.  Additional features would
allow editing and deletion of files and other database management
functions.

This should be an easy thing to add to existing browsers since
all the GUI browsers have file selection dialog boxs already in
use.  The difficult part will be revamping the form post method
to use mime multipart messaging.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From jonl@hal.com  Wed Feb 16 22:45:07 1994 --100
Message-Id: <9402162142.AA24386@gorn.hal.com>
Date: Wed, 16 Feb 1994 22:45:07 --100
From: jonl@hal.com (frederick smythe, esquire)
Subject: Possibly useful httpd extension (based on httpd 1.1)


I was setting up two http servers, one internal and one external,
and decided that the way I wanted to do the external one was to
have one nfs-mounted partition which our firewall machine mounted
from inside to contain data which users wanted to make available
to people outside.   Since I don't have users in /etc/passwd, I
couldn't use the ~user method of things, and I didn't want to have
to keep adding symlinks, or Alias directives everytime a new user
wanted put something up, so I ended up add a HomeOverride variable
in srm.conf, vaguely similar to UserDir.

If HomeOverride is defined, it is used as the directory to expand
the the ~ in ~user to.  Ie, specifying 

HomeOverride  /www-external/users

makes requests for ~user to become /www-external/users/user.

I then set UserDir to DISABLED since i had no need for the 
public_html subdir method.

If people think this is more generally useful, I'd be happy to
send Rob my patches to stock 1.1.

-- 
<a href="http://www.echo.com/~falcon/sg.html">jon r. luini</a>



From Jonathan_Roberts@transarc.com  Wed Feb 16 23:31:50 1994 --100
Message-Id: <EhMdkxk00k00JEvY5W@transarc.com>
Date: Wed, 16 Feb 1994 23:31:50 --100
From: Jonathan_Roberts@transarc.com (Jonathan_Roberts@transarc.com)
Subject: Corporate Use of Mosaic

My company is actively seeking a tool to use as a hypertext interface
for browsing our documents.  On the side I have converted a small
number of our SGML docs (from Publisher) to HTML as a "proof of
technology".  This conversion is now automatic and I think Transarc
would benefit greatly from this time savings.

Transarc, however, currently favors a product called OpenView (I
think).  This support stems from the fact that OpenView is supported
by a corporation with a professional reputation to uphold.  They fear
being left in the lurch if NCSA should someday stop supporting Mosaic.
My opinion is that we would get a fix from the Mosaic community more
quickly than we would from the OpenView folks, and if we cannot get a
fix from outside we at least have the chance to fix it ourselves.


Specifically, I need answers to the following questions:


    - Is there a royalty fee for redistributing Mosaic when it is
      packaged with lots of our own stuff (e.g. docs and programs
      on a CDROM)?

    - What sort of support can be expected?

    - Are there other corporations which depend on Mosaic for their
      hypertext needs?  If so, what are their experiences?


I know, the source is available and we could fix any defect we found.
I agree, but this is seen as a bad solution by the PTB.

My take is "Hundreds of thousands of people use it daily and it has
become a fairly robust application with powerful features.  It's free.
It will further empower our customers in a way unavailable with
commercial solutions.  Let's use it."

Jonathan



From connolly@hal.com  Thu Feb 17 00:58:58 1994 --100
Message-Id: <9402162355.AA06475@ulua.hal.com>
Date: Thu, 17 Feb 1994 00:58:58 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: A thought on implementation...


It occurs to me that it is unjustifiably difficult to do things to
HTML documents like:

	* list all the URL's in a node
	* list all the H1,H2,H3s in a node
	* find the title of a node

correctly because of bleed between the regular, context free, and
context sensitive idioms of SGML. For example:

	<XMP>this: <A HREF="abc"> looks like a link, but it's
	not because <XMP> is an RCDATA element, and STAGO is not
	recognized in RCDATA</XMP>

	<!-- this: <A HREF="abc"> looks like a link too! -->

	And this: a < b > c has no markup at all, even though it
	uses the "magic" < and > chars.

	<A HREF='<A HREF="wierd, but possible">'>I bet this would
	break most contemporary implementations!</a>

Suppose we decide to standardize on two things:

	(1) a DTD in the strictest sense of SGML compliance
	(or better yet, a set of architectural forms...)
	that defines HTML in a somewhat abstract sense in terms
	of elements, and character data (and entities?)

	(2) a context-free interchange language which is a subset
	of the SGML syntax.

You could use the DTD if you have real SGML tools and you want to
use minimization, comments, and < chars as data.

But for interchange within the WWW application, we'd agree that, for
example, the < character is _always_ markup, and we'd use &#60; for
the data character '<'.

Here are (at least some of) the rules we'd adopt over and above SGML:

* No <!-- --> comments
* No <![foo] .. ]]> marked sections
* Always use numeric character references for '<', '>', and '&'
	(no harm in using &lt;, &gt;, &amp; forms, I suppose)
* Use numeric character references for ", \n, \t inside attribute
value literals
* Always quote attribute value literals with double-quotes, not singe
quotes.
* Don't split attribute values across lines (Hmmm...)

Then the "search for HREF's" problem could be coded in ~20 lines of perl:

	while(<>){	# read a line
		while(/<[^>]*$/){	# line looks like ...<TAG
					# with no >... read another line
			$_ .= <>;
		}
		while(s/^<(\w+)([^>])*)>/){ # find a start tag
			local($gi) = $1;
			local($attrs) = $2;
			$gi =~ tr/a-z/A-Z/; # convert to upper-case
			if($gi eq 'A'){
				# for each attr...
				while($attrs =~ s/^(\w+)\s*=\s*"([^"])"\s*//){
					local($name, $val) = ($1, $2);
					print "HREF: val\n" if $name eq 'HREF';
				}
			}
		}
	}

I can almost guarantee that those 20 lines of perl are already in use
as a heuristic solution to that problem now (I looked at the elisp
code for emacs w3 client, and beleive me: it's all over the place).

It's clear to me that folks are going to write HTML parsers based on
intuition and experience with context-free languages. Learning exactly
how SGML parsing works is sufficiently difficult that folks won't do it.

In stead of declaring that perl code to be busted, why don't we agree
the SGML folks didn't know much about autonoma theory and tighten up
the definition of HTML a little?

Dan





From terry@ora.com  Thu Feb 17 01:43:38 1994 --100
Message-Id: <199402170040.AA13406@rock.west.ora.com>
Date: Thu, 17 Feb 1994 01:43:38 --100
From: terry@ora.com (Terry Allen)
Subject: Re Dan on implementation

Some friendly (truly) comments:

| It occurs to me that it is unjustifiably difficult to do things to
| HTML documents like:
| 	* list all the URL's in a node
| 	* list all the H1,H2,H3s in a node
| 	* find the title of a node

I have no trouble doing this.  Unless you mean by node, "more than
one HTML instance."  As the sequel shows, you mean "without using
SGML-aware tools."

| correctly because of bleed between the regular, context free, and
| context sensitive idioms of SGML. For example:
| 
| 	<XMP>this: <A HREF="abc"> looks like a link, but it's
| 	not because <XMP> is an RCDATA element, and STAGO is not
| 	recognized in RCDATA</XMP>

Off the point, I'll bet Mosaic sees it as a link.  But presumably
the author put this in an XMP because it's part of the example, not
a real link.  The doc should make clear that you can't put a 
link in an XMP.  Of course the </a> tag, which you've omitted
to show, *is* detected.

| 	<!-- this: <A HREF="abc"> looks like a link too! -->

How so?  It's in a comment, and so will be ignored by a parser.

| 	And this: a < b > c has no markup at all, even though it
| 	uses the "magic" < and > chars.

But not in the magic combinations <[A-Za-z] etc.

| 	<A HREF='<A HREF="wierd, but possible">'>I bet this would
| 	break most contemporary implementations!</a>

But it's valid SGML according to my local version of the HTML DTD
and sgmls.

| Suppose we decide to standardize on two things:
| 	(1) a DTD in the strictest sense of SGML compliance
| 	(or better yet, a set of architectural forms...)
| 	that defines HTML in a somewhat abstract sense in terms
| 	of elements, and character data (and entities?)

That is, a proper DTD that parses.

| 	(2) a context-free interchange language which is a subset
| 	of the SGML syntax.

Your argument so far does not indicate a need for this.  You have
simply remarked that some conventions of SGML are not what you'd
like them to be.  They're not what I'd like them to be, either,
but SGML is where we're at so far as document markup, today.

| You could use the DTD if you have real SGML tools and you want to
| use minimization, comments, and < chars as data.
| 
| But for interchange within the WWW application, we'd agree that, for
| example, the < character is _always_ markup, and we'd use &#60; for
| the data character '<'.

Dan, HTML is defined as an SGML DTD.  If that's to continue to be so,
you can't apply these restrictions---unless you want to write a
crippled SGML parser that complains about free-floating < > etc.  

Furthermore, there is no reason at all to
use &#60; for &lt;, and it is a weakness of the present DTD that 
it doesn't use the standard ISO pub and num entity sets.  Read
the newbies on comp.infosystems.www; ask Peter Flynn, who 
nobly spends a lot of time answering them.  

| Here are (at least some of) the rules we'd adopt over and above SGML:
| 
| * No <!-- --> comments

Over my dead body.  This is SGML.  Run it through a parser and you'll
never have trouble with comments.  Lots of people want them, and
it's a problem now that Mosaic, incorrectly, renders tagged text within 
comments.

| * No <![foo] .. ]]> marked sections

I don't care about this, but someone else may.  Why forbid them?

| * Always use numeric character references for '<', '>', and '&'
| 	(no harm in using &lt;, &gt;, &amp; forms, I suppose)

So let's do it right and use the &lt; forms.

| * Use numeric character references for ", \n, \t inside attribute
| value literals
| * Always quote attribute value literals with double-quotes, not singe
| quotes.
| * Don't split attribute values across lines (Hmmm...)
| 
| Then the "search for HREF's" problem could be coded in ~20 lines of perl:

[deleted]
| 
| I can almost guarantee that those 20 lines of perl are already in use
| as a heuristic solution to that problem now (I looked at the elisp
| code for emacs w3 client, and beleive me: it's all over the place).
| 
| It's clear to me that folks are going to write HTML parsers based on
| intuition and experience with context-free languages. Learning exactly
| how SGML parsing works is sufficiently difficult that folks won't do it.

Too bad for them.  They aren't following the spec, then.  Please don't
tell us we can't follow the spec.  I fully understand that the Webbers
who first decided to define HTML as SGML bit off far more than they
knew about, but for those of us who want to get our documents online,
the HTML DTD is where the rubber hits the road.  Browser writers
have to learn to live with SGML, warts and all.

| In stead of declaring that perl code to be busted, why don't we agree
| the SGML folks didn't know much about autonoma theory and tighten up
| the definition of HTML a little?

You mean define a new "Dan's SGML."  I don't think this is a reasonable
solution.  But, Dan, you have the energy to write TGML (my pet name for
a hypothetical successor to SGML) that would do these things right, and 
some others, too.  When someone gets around to writing it, whether it
is part of a standards process or no, TGML will replace SGML in short 
order, especially if it has a readable manual and a free parser.

BTW, would explain "autonoma theory" and how it relates
to such mundane things as the syntax for comments?  


-- 
Terry Allen  (terry@ora.com)
Editor, Digital Media Group
O'Reilly & Associates, Inc.
Sebastopol, Calif., 95472



From jmyers@eecs.nwu.edu  Thu Feb 17 01:52:22 1994 --100
Message-Id: <9402170049.AA09940@marigold.eecs.nwu.edu>
Date: Thu, 17 Feb 1994 01:52:22 --100
From: jmyers@eecs.nwu.edu (Jennifer Myers)
Subject: man page for X Mosaic available

In concert with the installation of X Mosaic on the machines in my
department, there was a desire for a locally-resident man page.

I took the text of the User's manual maintained by Alan Braverman 
at NCSA, and converted it to standard troff man page form, as well
as make a few minor changes to upgrade it to 2.2.

The man page is 35K and prints on 12 pages of troff-formatted text.

NCSA has done a wonderful job putting together the User's manual.  It
is nice, though, to have the information available locally, and in
standard man page format.

I've placed a copy of the man page here:

	http://www.eecs.nwu.edu:8001/jmyers/Mosaic.1

If you do not maintain a man directory, the procedure for viewing this
file is as follows:

Once you have downloaded the file to your local directory, either
view it from the shell by typing:

	nroff -man Mosaic.1 | page

or, print it on a PostScript printer with a command such as the
following:

	psroff -man Mosaic.1 | lpr

-Jennifer




From burchard@geom.umn.edu  Thu Feb 17 02:18:22 1994 --100
Message-Id: <9402170115.AA19032@mobius.geom.umn.edu>
Date: Thu, 17 Feb 1994 02:18:22 --100
From: burchard@geom.umn.edu (burchard@geom.umn.edu)
Subject: Re: Idea for new form input type

> In order to support the posting of large files using form
> based machanisms I suggest that a new input type
> "include-file" (or something similar) be added.  This
> new type would be a file that exists on the local machine
> whose data would be posted to the forms server during form
> submission.  This method would require the file to be
> posted using a MIME multipart message.

This would be very useful....but it is unclear to me how the file  
name information is supposed to be maintained and governed.

Does the user have to choose a file name every time the form is  
submitted?  If so, it would be far less useful.  For example, it  
would not solve the problem of entering large data sets into  
Cyberview, our form-based 3D viewer, where the same data set is  
typically used repeatedly.

On the other hand, if the file name info is maintained as part of the  
form and transferred back and forth, where does that info fit into  
the name/value scheme?

Is this an input-only mechanism?  Why go half-way?  Interactive data  
processing programs on the Web will want to be able to output large  
amounts of out-of-line data as well.

--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------



From connolly@hal.com  Thu Feb 17 02:50:55 1994 --100
Message-Id: <9402170147.AA06698@ulua.hal.com>
Date: Thu, 17 Feb 1994 02:50:55 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Re Dan on implementation 

In message <199402170040.AA13406@rock.west.ora.com>, Terry Allen writes:
>
>BTW, would explain "autonoma theory" and how it relates
>to such mundane things as the syntax for comments?  
>

So... you're one of these folks that drive the SGML bandwagon without
any background in formal systems, huh? ;-)

Ok... I'll try to summarize a 3 semester hour computer science class
in a few paragraphs:

About 50 years ago, some linguists figured out that there are a
handful of useful classes of languages:

	"Regular" languages: those that can be recognized with
	a finite state machine (hence the term "regular expression
	which is so pervasive about the unix priesthood)

	e.g. "any string consisting of all A's"

	The classical example of a language which is _not_ regular
	is "any string with ()'s balanced" -- it takes an arbitrary
	amount of state (i.e not finite) to remember how many ('s
	are still pending.

	"Context Free" languages: those that can be recognized
	with a pushdown autonomon (ala yacc)

	Examples of context free languages:
	"any string with ()'s balanced"
	"any string of the form xyz.zyx, i.e. fred.dref wilma.amliw"
	C (mostly -- "undeclared identifiers" and such are context-sens.)
	lisp s-expressions

	Examples of languages that are _not_ context free:
	"any string of the form xyz.xyz, i.e. fred.fred, wilma.wilma"

	"Context Sensitive" languages: nobody cares about these. They
	just lump them in with "General" languages.

	"General" aka "Turing Decidable" languages: languages
	that can be recognized by a turing machine (i.e. C-program).

	"Turing Undecidable" languages: languages that a computer
	can't recognize.

	Examples: "a C program that has no infinite loops"

If we keep HTML down to a context-free language composed of regular tokens,
then folks can write little 20-line ditties in perl, elisp, lex, yacc,
etc. and get real work done.

If we require real-time processing of all legal SGML documents,
we buy nothing in terms of functionality, and we render almost
all current implementations broken.


>| 	<XMP>this: <A HREF="abc"> looks like a link, but it's
>| 	not because <XMP> is an RCDATA element, and STAGO is not
>| 	recognized in RCDATA</XMP>
>
>Off the point, I'll bet Mosaic sees it as a link.

Exactly. So let me ask you: do you use mosaic anyway? I thought so.
Do you really think it's worth-while to implement a full SGML parser
in Mosaic just so you can use raw < in stead of &lt; ?

>| 	<!-- this: <A HREF="abc"> looks like a link too! -->
>
>How so?  It's in a comment, and so will be ignored by a parser.

Yes, by an SMGL compliant parser, but not by any parser built
out of standard parsing tools like regular expressions, lex, and yacc.
(well, actually, you could do it with lex, but it's a pain...)

>| 	And this: a < b > c has no markup at all, even though it
>| 	uses the "magic" < and > chars.
>
>But not in the magic combinations <[A-Za-z] etc.

Right. The famous "delimiter in context". Contrast this with the
vast majority of "context free" languages in use.

>Your argument so far does not indicate a need for this.  You have
>simply remarked that some conventions of SGML are not what you'd
>like them to be.  They're not what I'd like them to be, either,
>but SGML is where we're at so far as document markup, today.

My argument is based on the fact that HTML documents must be
parsed _interactively_, i.e. at reasonably high speed, i.e. in
real time. The run-time cost of full SGML parsing far outweighs
the benefits.

>| You could use the DTD if you have real SGML tools and you want to
>| use minimization, comments, and < chars as data.
>| 
>| But for interchange within the WWW application, we'd agree that, for
>| example, the < character is _always_ markup, and we'd use &#60; for
>| the data character '<'.
>
>Dan, HTML is defined as an SGML DTD.  If that's to continue to be so,
>you can't apply these restrictions---unless you want to write a
>crippled SGML parser that complains about free-floating < > etc.  

You say "crippled", I say "expedient". Remember: the documents are
still conforming. It's just the WWW client parser that's non-standard.

>Furthermore, there is no reason at all to
>use &#60; for &lt;, and it is a weakness of the present DTD that 
>it doesn't use the standard ISO pub and num entity sets.

No, it's not. The ISO pub and num entity sets are designed for
situations where the <, #, etc. characters represent system-specific
data entities, e.g. characters from a special "math" font.

The HTML DTD uses ISO-latin1 as its character set, and '<' is a
perfectly normal ISO latin 1 character -- number 60 in the collating
sequence. A parser is required to treat the strings "&#60;" and "<"
identically, unless "<" is followed by a letter...). It was a mistake
on my part early on to confuse &lt; with &#60;.

But as it turns out, the ISO pub and num entity names make handy
mnemonics for ISO characters. So we use them. But according to
the DTD, &lt; is defined to mean exactly the string "<" -- not
"whatever your system would like to use for a less-than sign"
as in the ISO pub entity set.

>
>| Here are (at least some of) the rules we'd adopt over and above SGML:
>| 
>| * No <!-- --> comments
>
>Over my dead body.  This is SGML.  Run it through a parser and you'll
>never have trouble with comments.

Hey, buddy, YOU run it through a parser :-). Seriously: if you want
to put these idioms in your source, then have your server remove them
before sending them to other WWW clients. Or batch-convert them. But
DON'T require every WWW client to parse it.

>  Lots of people want them, and
>it's a problem now that Mosaic, incorrectly, renders tagged text within 
>comments.


Ok... comments are useful. How about this: Comments must be of the form:

<!-- comment -->

and not

lksjdflkj <!-- comment in middle of line -->

nor

<!-- comment
split accross lines -->

>
>| * No <![foo] .. ]]> marked sections
>
>I don't care about this, but someone else may.  Why forbid them?

Because of the processing overhead! They're context-sensitive at
least, if not turing-decidable. This means adding thousands of lines
of code. And maintaining it...

>| It's clear to me that folks are going to write HTML parsers based on
>| intuition and experience with context-free languages. Learning exactly
>| how SGML parsing works is sufficiently difficult that folks won't do it.
>
>Too bad for them.  They aren't following the spec, then.  Please don't
>tell us we can't follow the spec.  I fully understand that the Webbers
>who first decided to define HTML as SGML bit off far more than they
>knew about, but for those of us who want to get our documents online,
>the HTML DTD is where the rubber hits the road.  Browser writers
>have to learn to live with SGML, warts and all.

The one thing I've learned about the internet is that the party who
writes and distributes code to implement his spec is the guy who
sets the standard. I bet I can get client developers to agree on
my idea sooner than you can get them to adopt SGML.

We'd accomplish these objectives:

	(1) These restricted HTML documents are still compliant.
	They still work with SGML tools.
	(1) We could teach folks what HTML looks like a whole lot easier.
	(2) We could write HTML processing software easier
	(3) We would increase confidence among authors that their
	documents will be rendered (and searched, indexed, outlined,
	and other wise processed) accurately.

I bet most SGML-producing tools follow these restrictions anyway. It's
just a question of getting the SGML-producing people to do it.

>
>| In stead of declaring that perl code to be busted, why don't we agree
>| the SGML folks didn't know much about autonoma theory and tighten up
>| the definition of HTML a little?
>
>You mean define a new "Dan's SGML."

Look: I tried _real_hard_ to define HTML in terms of SGML and get the
community to back me up. It didn't work. So I'm willing to shed some
of SGML's obscure features in favor of reliability.

>  I don't think this is a reasonable
>solution.  But, Dan, you have the energy to write TGML (my pet name for
>a hypothetical successor to SGML) that would do these things right, and 
>some others, too.  When someone gets around to writing it, whether it
>is part of a standards process or no, TGML will replace SGML in short 
>order, especially if it has a readable manual and a free parser.

Ah... so I guess we agree on that part! Keep in mind that these TGML
documents are still SGML documents in every sense of the word. It is
only the TGML parser which is not an SGML parser.

So if you have some SGML documents which are not TGML documents, just
run them through sgmls (in batch) and I'll provide a 20-line perl script
to output a "super-normalized" version acceptable to a TGML parser.

Dan




From Paul.Wain@brunel.ac.uk  Thu Feb 17 10:16:18 1994 --100
Message-Id: <21558.9402170911@thor.brunel.ac.uk>
Date: Thu, 17 Feb 1994 10:16:18 --100
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Re: Re Dan on implementation

Hi,

erm correct me if Im missing something here but...

[On marked sections in HTML]
@ >| * No <![foo] .. ]]> marked sections
@ >
@ >I don't care about this, but someone else may.  Why forbid them?
@ 
@ Because of the processing overhead! They're context-sensitive at
@ least, if not turing-decidable. This means adding thousands of lines
@ of code. And maintaining it...

Then...

[On HTML/SGML to TGML]
@ So if you have some SGML documents which are not TGML documents, just
@ run them through sgmls (in batch) and I'll provide a 20-line perl script
@ to output a "super-normalized" version acceptable to a TGML parser.

Am I missing something here :) (erm dont take that too seriously btw,
its been a long hour ploughing through the mailbox so I need some light
relief).

Seriously though, can someone explain to me why say YACC couldnt be used
to parse the <!-- --> comments, even if they are split over multiple
lines. I can see it would be hard, but it initially ``looks'' possible.

Paul

.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From koblas@netcom.com  Thu Feb 17 10:37:37 1994 --100
Message-Id: <199402170935.BAA19565@mail.netcom.com>
Date: Thu, 17 Feb 1994 10:37:37 --100
From: koblas@netcom.com (David Koblas)
Subject: Has somebody written 'htcp'?


With the mirroring that some sites do, along with the occasional need to
just copy over files from a remote site via a URL specification.  I was
wondering if anyone had gotten around to writting a copy program that
would do the work for you.

Ideally, it would have a '-r' option that would parse HTML documents
and look for children...

--koblas@netcom.com



From tkevans@barkeep.es.dupont.com  Thu Feb 17 11:23:40 1994 --100
Message-Id: <9402171018.AA23339@barkeep>
Date: Thu, 17 Feb 1994 11:23:40 --100
From: tkevans@barkeep.es.dupont.com (Tim Evans)
Subject: URL Syntax for Passthrough ftp?

Is there a syntax for passthrough ftp?  That is, in our firewalled
network, we can use ftp via passthrough on the firewall machine 
by specifying a port number and username of the syntax 
"anonymous@remote.ftp.site"  Sequence is something like this:

ftp -n firewall:portnumber

ftp-prompt-fro-firewall>  user anonymous@remote.ftp.site

The outgoing connection is then set up and functions like a normal
ftp session.  Can such a connection be set up using an URL, and
what it its syntax?

Thanks

-- 
Tim Evans                     |    E.I. du Pont de Nemours & Co.
tkevans@eplrx7.es.dupont.com  |    Experimental Station
(302) 695-9353/8638 (FAX)     |    P.O. Box 80357
EVANSTK AT A1 AT ESVAX        |    Wilmington, Delaware 19880-0357



From Jean-Christophe.Touvet@inria.fr  Thu Feb 17 11:51:45 1994 --100
Message-Id: <199402171048.AA14044@perignon.inria.fr>
Date: Thu, 17 Feb 1994 11:51:45 --100
From: Jean-Christophe.Touvet@inria.fr (JC Touvet)
Subject: "finger" and URN -> URL


 Hello happy Webbers,

 you certainly wonder what could be in my mind the link between yesterday's
"finger" and URNs discussions.

 Dan suggested:
 
>       ... click <A HREF="finger://my.domain/myloginname">here</a>...
> 
 
> I'm not sure about the URL syntax for a finger query... you should check
> the spec or the code or something.

 and Dave wrote:

> By using URNs we can decouple this from the document format itself. I have
> recently posted a note on how this can be achieved using a DNS like
> distributed name lookup scheme for lifetime identifiers and a mechanism for
> selecting between variants. This could be available by the end of this year.

 Now, let's imagine we have a very strong directory service that:

	- provides white or yellow pages informations
	- has powerful searching capabilities
	- can easily store Web URLs
	- is the perfect response to a "distributed name lookup scheme"

 And, on the other hand, a protocol that:

	- allows you to access this directory service
	- but is not restricted to it (finger, whois+*... compatibility)
	- uses User Friendly Naming
	- has a centroid approach of servers networking
	- is so simple that a nice client module could be easily integrated in
	the common WWW library

 maybe we'd have a solution to both problems.

 But those things exist yet, namely X.500 and SOLO:

	- X.500 is THE directory service, but it's HUGE to implement, even as
	a client.

	- SOLO has only one command: SOLO <ufn>?attributes. Its results are
	easily parsable, and fit perfectly in an hypertext browser: if the
	name is not found, we get suggestions, and we can create hypertext
	links with that.

 On server side, you can implement SOLO just a la finger, but you can
also build an X.500 access server with it. Thus we have done a WWW/SOLO
gateway that provides X.500 and SOLO servers access from a WWW client. And, of
course, we have put URL attributes in our X.500 directory.

 But more efficient is is what we're doing now: add to libwww a module (smaller
than gopher one) that:

 - given a "solo://server/" URL
		example "solo://mitsou.inria.fr/"

	returns you a FORM to query the server

 - given a "solo://server/ufn?attributes" URL
 		example "solo://mitsou.inria.fr/huitema?*"

	sends the query to the server, and parses results:
	 - if suggestions returned -> hypertext links to other queries
	 - if name found -> HTML result
	 - if URL attributes found in entry -> hypertext links
	 - and if a "SmallPhoto" URL found -> include it in result
	 - could be extended (suggestions ?), but we want to keep it SIMPLE

 - given a "x500:ufn?attributes" URL
		example "x500:kostner,nexor,england?phone,address,email,url"

	does the same, but using an X.500 access server defined in your
	environment (which must be a SOLO server, of course ;-). So, you can
	use the nearest entry point in X.500 world.

 - given a "urn:name" URL
		example "urn:CN=testdoc,O=inria,C=fr"

	returns the latest version of a document using the same X.500 access
	server

 - given a "urn:name#version" URL
		example "urn:CN=testdoc,O=inria,C=fr#1.0.1"

	returns the named version of the document


 We are submitting a paper on this topic for the WWW-94 conference, so if
you're interested in it, you'll find further explanations on CERN's server.

 If you want more informations about SOLO, have a look at the draft RFC:
	draft-huitema-solo-00.txt


 Cheers,

    -JCT-

<A HREF="solo://perignon.inria.fr:2222/touvet?*">	JC Touvet	 </A>
<A HREF="x500:pap,inria,france?*">			Paul-Andre Pays	 </A>
<A HREF="urn:CN=Solo/WWW%20paper,O=inria,C=fr">		The paper (soon) </A>



From Jean-Christophe.Touvet@inria.fr  Thu Feb 17 12:14:05 1994 --100
Message-Id: <199402171110.AA15202@perignon.inria.fr>
Date: Thu, 17 Feb 1994 12:14:05 --100
From: Jean-Christophe.Touvet@inria.fr (JC Touvet)
Subject: Please, NO !!!!  8-o


 Please, don't send HTTP requests to our SOLO server !!!!!! You should use our
gateway, not the SOLO server itself.

    -JCT-



From Paul.Wain@brunel.ac.uk  Thu Feb 17 12:45:28 1994 --100
Message-Id: <22942.9402171140@thor.brunel.ac.uk>
Date: Thu, 17 Feb 1994 12:45:28 --100
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Indented <MENU>s

Whilst automatically generating some indented menu's (upto 4 deep) I
created the following code:

GB
<MENU>
Brunel University
<MENU>
Counsellors
<MENU>
<BR>
<A HREF="http://molnir.brunel.ac.uk:4321/...>Dr S Smith</A>
</MENU>
<BR>
Economics
<MENU>
<BR>
<A HREF="http://molnir.brunel.ac.uk:4321/...>Dr J Smith</A>
</MENU>
</MENU>
</MENU>

Now when viewing this under Lynx I get what I think is the correct
response:

==============================LYNX DISPLAY=======================
   GB
   Brunel University
        Counsellors
 
                 [1]Dr S Smith
	
	Economics

		 [2]Dr J Smith
==============================LYNX DISPLAY=======================

*BUT* when I view the same HTML with Mosaic I get:

==============================MOSAIC DISPLAY=======================
   GB 

      Brunel University 
          Counsellors 

             Dr S Smith 

          Economics 

             Dr J Smith
==============================MOSAIC DISPLAY=======================

As you can see Mosaic has added in an extra blank line. Am I right in
assuming that this is infact a bug in the mosaic rendering an that the
LYNX output is correct.

If not, how would I go about doing it?

.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From marca@eit.COM  Thu Feb 17 13:11:31 1994 --100
Message-Id: <199402171208.MAA13991@threejane>
Date: Thu, 17 Feb 1994 13:11:31 --100
From: marca@eit.COM (Marc Andreessen)
Subject: Indented <MENU>s

Paul "S." Wain writes:
> Whilst automatically generating some indented menu's (upto 4 deep) I
> created the following code:
> 
> GB
> <MENU>
> Brunel University
> <MENU>
> Counsellors
> <MENU>
> <BR>
> <A HREF="http://molnir.brunel.ac.uk:4321/...>Dr S Smith</A>
> </MENU>
> <BR>
> Economics
> <MENU>
> <BR>
> <A HREF="http://molnir.brunel.ac.uk:4321/...>Dr J Smith</A>
> </MENU>
> </MENU>
> </MENU>
> 
> Now when viewing this under Lynx I get what I think is the correct
> response:
> 
> ==============================LYNX DISPLAY=======================
>    GB
>    Brunel University
>         Counsellors
>  
>                  [1]Dr S Smith
> 	
> 	Economics
> 
> 		 [2]Dr J Smith
> ==============================LYNX DISPLAY=======================
> 
> *BUT* when I view the same HTML with Mosaic I get:
> 
> ==============================MOSAIC DISPLAY=======================
>    GB 
> 
>       Brunel University 
>           Counsellors 
> 
>              Dr S Smith 
> 
>           Economics 
> 
>              Dr J Smith
> ==============================MOSAIC DISPLAY=======================
> 
> As you can see Mosaic has added in an extra blank line. Am I right in
> assuming that this is infact a bug in the mosaic rendering an that the
> LYNX output is correct.
> 
> If not, how would I go about doing it?

It's not a bug -- it's a rendering choice made by the browser.  As we
kneel before the altar of the "ha ha, you can't control what your
documents look like in HTML" philosophy, you, dear sinner (nay,
blasphemer), can but take solace in the fact that Mosaic is putting a
blank line at the start of all toplevel lists, including yours,
entirely on purpose.  (Why?  Primarily because we thought that looked
the best for most documents on the Web at the time, as I remember.)

What can you do about it?  Probably nothing.  Isn't that cheery news?
I think so.  In fact, it has been a constant source of delight for me
over the past year to get to continually tell hordes (literally) of
people who want to -- strap yourselves in, here it comes -- control
what their documents look like in ways that would be trivial in TeX,
Microsoft Word, and every other common text processing environment:
"Sorry, you're screwed."

Ah well, live and learn.  Or not.

Cheers,
Marc

--
Marc Andreessen
Enterprise Integration Technologies
Palo Alto, California
marca@eit.com



From stefank@math.chalmers.se  Thu Feb 17 14:03:43 1994 --100
Message-Id: <199402171300.OAA04790@knaster.math.chalmers.se>
Date: Thu, 17 Feb 1994 14:03:43 --100
From: stefank@math.chalmers.se (Stefan Karlsson)
Subject: Re: Please, NO !!!!  8-o

> 
> 
>  Please, don't send HTTP requests to our SOLO server !!!!!! You should use our
> gateway, not the SOLO server itself.
> 
>     -JCT-
> 

What's the URL to the gateway, I don't think you mentioned that?

  --
  Stefan Karlsson
						|
| Matematiska institutionen	|  Hogenskildsgatan 19	|
| CTH/GU			|  416 57 G|teborg	|
| 412 96 G|teborg		| 			|
| 031-772 35 77			|  031-84 21 99		|
|				| 			|



From Jean-Christophe.Touvet@inria.fr  Thu Feb 17 14:58:08 1994 --100
Message-Id: <199402171354.AA22467@perignon.inria.fr>
Date: Thu, 17 Feb 1994 14:58:08 --100
From: Jean-Christophe.Touvet@inria.fr (JC Touvet)
Subject: Re: Please, NO !!!! 8-o 


 Hello,

> What's the URL to the gateway, I don't think you mentioned that?

 I didn't give the address of our gateway because we don't want to have
numbers of simultaneous connections "just to try it" when people in US side
will wake up ! Our machine wouldn't support it, since it's not its main
purpose. Let's wait a bit, we'll have a dedicated server soon...

 People REALLY interested in our proposition will find pointers on WWW-94
conference server, where is our paper abstract. I think it will reduce sensibly
connections rate ;-)

 Cheers,

    -JCT-



From dolesa@smtp-gw.spawar.navy.mil  Thu Feb 17 15:35:10 1994 --100
Message-Id: <9401177615.AA761506295@smtp-gw.spawar.navy.mil>
Date: Thu, 17 Feb 1994 15:35:10 --100
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: Re: Indented <MENU>s


My compliments!  That has got to be the best, most honest reply I've ever
seen in my entire lifetime!  The POWER!!!! muhahahhahahaha.

        Andre'
-----

It's not a bug -- it's a rendering choice made by the browser.  As we
kneel before the altar of the "ha ha, you can't control what your
documents look like in HTML" philosophy, you, dear sinner (nay,
blasphemer), can but take solace in the fact that Mosaic is putting a
blank line at the start of all toplevel lists, including yours,
entirely on purpose.  (Why?  Primarily because we thought that looked
the best for most documents on the Web at the time, as I remember.)

What can you do about it?  Probably nothing.  Isn't that cheery news?
I think so.  In fact, it has been a constant source of delight for me
over the past year to get to continually tell hordes (literally) of
people who want to -- strap yourselves in, here it comes -- control
what their documents look like in ways that would be trivial in TeX,
Microsoft Word, and every other common text processing environment:
"Sorry, you're screwed."

Ah well, live and learn.  Or not.

Cheers,
Marc

--
Marc Andreessen
Enterprise Integration Technologies
Palo Alto, California
marca@eit.com




From terry@ora.com  Thu Feb 17 16:18:50 1994 --100
Message-Id: <199402171514.AA24107@rock.west.ora.com>
Date: Thu, 17 Feb 1994 16:18:50 --100
From: terry@ora.com (Terry Allen)
Subject: HTML/EML implementation

(Context:  Dan Connolly proposed some simplifications to the use
of SGML on the Web, which I opposed.)

| So... you're one of these folks that drive the SGML bandwagon without
| any background in formal systems, huh? ;-)

Chuckle.  Just trying to use available tools *for document processing*
to get my docs online.  Show me a better bandwagon.

| If we require real-time processing of all legal SGML documents,
| we buy nothing in terms of functionality, and we render almost
| all current implementations broken.

You don't have to parse any SGML except docs conforming to the 
HTML DTD and its SGML declaration (in which you can turn off
several features, though not the ones that concern you).

| Yes, by an SMGL compliant parser, but not by any parser built
| out of standard parsing tools like regular expressions, lex, and yacc.
| (well, actually, you could do it with lex, but it's a pain...)

| You say "crippled", I say "expedient". Remember: the documents are
| still conforming. It's just the WWW client parser that's non-standard.

What it comes down to is that you want to make your job easier
by eliminating some of the basic functionality of SGML.  There
is no way of doing this in the HTML DTD.  You are advocating  
another ML, call it EML for "expedient."

But HTML-conformant docs won't necessarily parse through your client,
e.g., if they have comments in them.  (What will you do about those?
render them as normal text?)  My problem is that 
all the other tools I have will tell me that such docs are valid.
To make this work, I'd have to take my SGML docs and preprocess
them, then check to see that they're valid according to your EML.

| The one thing I've learned about the internet is that the party who
| writes and distributes code to implement his spec is the guy who
| sets the standard. I bet I can get client developers to agree on
| my idea sooner than you can get them to adopt SGML.

I know of two browsers in development that use SGML, and existing
SGML browsers (freestanding, not net-ready) are not much slower
than (cited for example only) Mosaic at rendering docs.

| We'd accomplish these objectives:
| 	(1) These restricted HTML documents are still compliant.
| 	They still work with SGML tools.

But the converse is not true:  SGML tools will pass as valid
docs that aren't valid EML (or maybe just aren't properly 
formatted for EML).  See above.

| 	(1) We could teach folks what HTML looks like a whole lot easier.
| 	(2) We could write HTML processing software easier

Unworthy arguments.  We could do this even better by staying in
ASCII.

| 	(3) We would increase confidence among authors that their
| 	documents will be rendered (and searched, indexed, outlined,
| 	and other wise processed) accurately.

Oh come on.  Build the software right, don't say that you'll make
mistakes if it has to be complicated.

[[this part totally parenthetical on both sides!
| Ah... so I guess we agree on that part! Keep in mind that these TGML
| documents are still SGML documents in every sense of the word. It is
| only the TGML parser which is not an SGML parser.

No, you're proposing EML.  The mythical TGML will have inline 
comments and entities for special characters, and all sorts of 
things that you would object to as slowing up processing,  but 
which document managers want to make *their* lives easier.  
It will have better attribute typing, and will eliminate the 
record end problem in mixed content,  etc.  It will be SGML
done over again right, rather than SGML stripped down.
]]

To sum up:  The EML you're proposing would make browser 
construction easier at the expense of document management in SGML.
It can't be specified in SGML because it cuts out basic features
of SGML, and SGML tools won't necessarily work right with documents 
intended for EML browsers.


-- 
Terry Allen  (terry@ora.com)
Editor, Digital Media Group
O'Reilly & Associates, Inc.
Sebastopol, Calif., 95472



From erik@naggum.no  Thu Feb 17 16:30:44 1994 --100
Message-Id: <19940217.0690.erik@naggum.no>
Date: Thu, 17 Feb 1994 16:30:44 --100
From: erik@naggum.no (Erik Naggum)
Subject: Re: A thought on implementation...

(I'm still busy, so this is just a quick reply to Dan Connolly's message of
yesterday.)

Briefly put: The problems that people seem to have with SGML appears to
come from the idea that SGML is so much like text that it can be treated as
text _without_ markup.  This is not true.

That one should be able to process an SGML file without doing any SGML
parsing is not a particularly good idea, nor is it desirable.  Rather than
roll your own primitive and dysfunctional SGML processors, why not use an
actual SGML parser?  Some of them are heavy-weights, and some insist on
being run as separate processes, but the project I'm working on has shown
that one can build a small and conforming SGML parser that is also fast.
Commercial implementations are usually built to be separate programs that
run script languages to do conversion to some other language for use by
other applications.  This has made it very hard for people to use "native
SGML" in their applications, for things to which that SGML would otherwise
be eminently suited.  I find HTML to be a very interesting example of such
an application, but I am concerned with the amount of heuristics involved
in the "pseudo-parsing", and particularly concerned that more of the same
heuristics-based approach is suggested for HTML+.

The idea that comments should be disallowed has one major disadvantage: If
I can't use comments or marked sections in my HTML+ files, it means I must
have two copies, one published, the other not.  Such comments and marked
sections are necessary if I want to keep important information about the
document available over its lifetime.  Since we are not talking about
snapshort documents and ephemeral information _all_ the time, lack of this
feature may be an untenable situation in practice.  Marked sections are
sometimes the only means to "comment out" a larger block of text that
already has comments in it, or hyphens that could terminate the comments
prematurely.  One of the beauties of HTML today is precisely that one can
reference the source documents without any of the stupid pre-processing and
multiple sources that is required in other hypertext systems, which remain
closed systems because of these limitations.

A restriction on the legality of SGML constructs also means that we can no
longer use ordinary SGML tools to test for conformance to the HTML+ DTD and
document conventions, but will have to build new tools to validate already
valid SGML documents.  I maintain that this is a very bad idea.  The cost
of the alternative is small in comparison.

Please note that my main mail address is <erik@naggum.no>.

Best regards,
</Erik>
--
Erik Naggum <erik@naggum.no> <SGML@ifi.uio.no>  |  Memento, terrigena.
ISO 8879 SGML, ISO 10744 HyTime, ISO 10646 UCS  |  Memento, vita brevis.



From luotonen@ptsun00.cern.ch  Thu Feb 17 16:46:05 1994 --100
Message-Id: <9402171542.AA25872@ptsun03.cern.ch>
Date: Thu, 17 Feb 1994 16:46:05 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: cern_httpd 2.15beta on HP


There was a bug in HP version of cern_httpd 2.15beta which caused
zombie children to gather around and have their suspicious little
rituals.

There is now a new binary for HP in

	ftp://info.cern.ch/pub/www/bin/hp/httpd_2.15beta.Z

Many thanks to Erling Andersen for his bug report.

By the way, there are now binaries available also for NeXT-Intel in
../next-386/...  and Linux in .../linux/...

Thanks to Mr Kwun for the Linux binary, and Rainer Klute for his
patch.


-- Cheers, Ari --




From speyer@mcc.com  Thu Feb 17 17:06:24 1994 --100
Message-Id: <9402171542.AA07286@faith.mcc.com>
Date: Thu, 17 Feb 1994 17:06:24 --100
From: speyer@mcc.com (Bruce Speyer)
Subject: Re: HTML/EML implementation

We too found raw HTML to be difficult to maintain.  Since we aren't (yet)
a SGML shop we are temporarily using the C preprocessor (cpp) since it
was handy and a "no-brainer" to use.  So our preprocessed sources include
C style comments and macros that expand with argument substitution.
That is how we impose a consistent style to our documents at this time.
-spy (http://galaxy.einet.net/galaxy.html)




From burchard@geom.umn.edu  Thu Feb 17 17:15:50 1994 --100
Message-Id: <9402171604.AA20038@mobius.geom.umn.edu>
Date: Thu, 17 Feb 1994 17:15:50 --100
From: burchard@geom.umn.edu (burchard@geom.umn.edu)
Subject: Yikes!  What happened to "image" INPUTs?

I just took a look at the draft HTML+ DTD  
(ftp://15.254.100.100/pub/htmlplus.dtd.txt) and noticed that "image"  
inputs were not mentioned in the description of fill-out forms.   
Alarmed, I looked back at the forms documentation at NCSA and found  
that information about "image" inputs was beginning to disappear at  
key points.

Although the draft HTML+ DTD appears to suggest
    <INPUT TYPE="submit" SRC="image.gif">
as an eventual replacement for
    <INPUT TYPE="image" SRC="image.gif">
what can I do in the meantime?

Scrapping "image" inputs represents a backwards-compatibility  
nightmare for anyone doing graphical fill-out forms, because there  
will be an interim period in which there is *no* reliable way to  
support graphical input in fill-out forms.  Right now, "image" inputs  
are the only way.

Thanks for your consideration of this matter.

--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------




From connolly@hal.com  Thu Feb 17 17:22:50 1994 --100
Message-Id: <9402171605.AA08714@ulua.hal.com>
Date: Thu, 17 Feb 1994 17:22:50 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: HTML/EML implementation 

In message <199402171514.AA24107@rock.west.ora.com>, Terry Allen writes:
>(Context:  Dan Connolly proposed some simplifications to the use
>of SGML on the Web, which I opposed.)
>
>What it comes down to is that you want to make your job easier
>by eliminating some of the basic functionality of SGML.  There
>is no way of doing this in the HTML DTD.  You are advocating  
>another ML, call it EML for "expedient."
>
>But HTML-conformant docs won't necessarily parse through your client,
>e.g., if they have comments in them.  (What will you do about those?
>render them as normal text?)  My problem is that 
>all the other tools I have will tell me that such docs are valid.
>To make this work, I'd have to take my SGML docs and preprocess
>them, then check to see that they're valid according to your EML.
>

This business of author-validation is a compelling counter-argument
that I only fully considered after I had posted my idea.

I certainly detect a sincere commitment to SGML on the part of
information providers. Now... we just need freely distributable,
_correct_, efficient, reusable implementations in C, perl, and
elisp... That's what constitues a net.standard, I'd say. Hmmm...  I
guess we'd better decide on a DTD (or set of architectural forms...)
pretty soon!

Dan






From wei@xcf.Berkeley.EDU  Thu Feb 17 17:29:47 1994 --100
Message-Id: <9402171625.AA15193@xcf.Berkeley.EDU>
Date: Thu, 17 Feb 1994 17:29:47 --100
From: wei@xcf.Berkeley.EDU (Pei Y. Wei)
Subject: Re: A thought on implementation...

Erik Naggum wrote:
> Rather than
> roll your own primitive and dysfunctional SGML processors, why not use an
> actual SGML parser?  Some of them are heavy-weights, and some insist on
> being run as separate processes, but the project I'm working on has shown
> that one can build a small and conforming SGML parser that is also fast.

Interesting. Does this small+conforming+fast SGML parser exist, and is
it going to be freely available on the net?

BTW, If anyone has succeeded in, or would like to try, making sgmls into 
a linkable library (as opposed to being run restarting the process), 
please drop me a line. I tried it once but it wasn't fun.


-Pei



From erik@naggum.no  Thu Feb 17 17:36:40 1994 --100
Message-Id: <19940217.0698.erik@naggum.no>
Date: Thu, 17 Feb 1994 17:36:40 --100
From: erik@naggum.no (Erik Naggum)
Subject: Re: A thought on implementation...

Pei,

The parser and its entity manager are in alpha testing right now.  We
expect it to be out in beta test "relatively soon", such as two to three
weeks from now.

However, ARC SGML, on which SGMLS was built, was intended to be used as a
linkable library.  SGMLS unfortunately, took a rather different approach,
and retrofitting the fixes that SGMLS made, back into the ARC SGML parser
has proved to be a most painful task that I soon dropped.  Part of the
reason for that is that the source code was gratuitously reformatted,
making use of existing tools (diff, patch, etc) nigh impossible.

Best regards,
</Erik>
--
Erik Naggum <erik@naggum.no> <SGML@ifi.uio.no>  |  Memento, terrigena.
ISO 8879 SGML, ISO 10744 HyTime, ISO 10646 UCS  |  Memento, vita brevis.



From sanders@BSDI.COM  Thu Feb 17 17:50:21 1994 --100
Message-Id: <199402171646.KAA02381@austin.BSDI.COM>
Date: Thu, 17 Feb 1994 17:50:21 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Indented <MENU>s 

> What can you do about it?  Probably nothing.  Isn't that cheery news?
> I think so.  In fact, it has been a constant source of delight for me
> over the past year to get to continually tell hordes (literally) of
> people who want to -- strap yourselves in, here it comes -- control
> what their documents look like in ways that would be trivial in TeX,
> Microsoft Word, and every other common text processing environment:
> "Sorry, you're screwed."

Then why don't you just implement one of the many style
sheet proposals that are on the table.  This would pretty much
solve the problem if done correctly.

--sanders



From dave@tis.com  Thu Feb 17 18:03:10 1994 --100
Message-Id: <9402171658.AA10771@tis.com>
Date: Thu, 17 Feb 1994 18:03:10 --100
From: dave@tis.com (David I. Dalva)
Subject: Re: proxy gateway service announcement/testing

Hi, I'm running httpd_2.15beta on a firewall, and am trying to take advantage
of the proxy option.  It's listening on port 911.

I've set up a shell script as follows:

	#!/bin/sh
	http_proxy=http://relay.tis.com:911/
	wais_proxy=http://relay.tis.com:911/
	gopher_proxy=http://relay.tis.com:911/

	export http_proxy wais_proxy gopher_proxy

	exec /usr/local/X11R5/bin/Mosaic-hp700

I see that another httpd is forked on the firewall when I run this shell
script, but the client eventually times out.  The log file only shows the
access attempt, that is, no errors are reported.

This functionality is very important for us, and several of our clients also
need this service.

Can you help?

Dave Dalva <dave@tis.com>		Trusted Information Systems, Inc.
+1.301.854-6889				Glenwood, MD  21738
+1.301.854-5363 FAX



From terry@ora.com  Thu Feb 17 18:13:01 1994 --100
Message-Id: <199402171709.AA26265@rock.west.ora.com>
Date: Thu, 17 Feb 1994 18:13:01 --100
From: terry@ora.com (Terry Allen)
Subject: Re implementation

| This business of author-validation is a compelling counter-argument
| that I only fully considered after I had posted my idea.
| 
| I certainly detect a sincere commitment to SGML on the part of
| information providers. Now... we just need freely distributable,
| _correct_, efficient, reusable implementations in C, perl, and
| elisp... That's what constitues a net.standard, I'd say. Hmmm...  I
| guess we'd better decide on a DTD (or set of architectural forms...)
| pretty soon!

Handsomely said, Dan.  Thanks.  Yes, we need a DTD and a style
sheet mechanism, too.

Regards,

-- 
Terry Allen  (terry@ora.com)
Editor, Digital Media Group
O'Reilly & Associates, Inc.
Sebastopol, Calif., 95472



From john@math.nwu.edu  Thu Feb 17 19:04:37 1994 --100
Message-Id: <9402171758.AA27100@hopf.math.nwu.edu>
Date: Thu, 17 Feb 1994 19:04:37 --100
From: john@math.nwu.edu (John Franks)
Subject: Re: A thought on implementation...

According to Erik Naggum:
> 
> A restriction on the legality of SGML constructs also means that we can no
> longer use ordinary SGML tools to test for conformance to the HTML+ DTD and
> document conventions, but will have to build new tools to validate already
> valid SGML documents.  I maintain that this is a very bad idea.  The cost
> of the alternative is small in comparison.
> 

One fact of life which this discussion (except for Dan Connolly) seems
to be conveniently ignoring is that creating standards does not compel
browser writers to comply with them.  It is fine to say, "Well yes,
from the point of view of formal languages SGML was badly designed and
that makes correct parsing complicated, but it is our standard we have
to stick to it.  Just do it right!"  But I think that Dan's point was
that browser writers haven't stuck to it and I am not sure just why we
can expect them to in the future.  Writing a browser is very
difficult.  It is not clear just how much of HTML+ will actually be
implemented in any form in popular browsers.

For many information providers today, what Mosaic and lynx will process
is much more important than what sgmls will validate.  It is fine to
say that validation is very important to you and you aren't willing
to compromise in order to make the browser writer's life easier and 
browser performance/reliability better.  But unless you plan on writing
your own browser and distributing it to your target audience, you 
may want to try to *persuade* browser writers that there is something
in it for them if they put in all this extra work.


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu




From montulli@stat1.cc.ukans.edu  Thu Feb 17 19:14:17 1994 --100
Message-Id: <9402171809.AA34779@stat1.cc.ukans.edu>
Date: Thu, 17 Feb 1994 19:14:17 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Release of Lynx version 2.2

Lynx Ver. 2.2 is now available for anonymous ftp from
 FTP2.cc.ukans.edu    as   /pub/lynx/lynx2-2.tar.Z
                     and   /pub/lynx/lynx2-2.zip

( ftp://ftp2.cc.ukans.edu/pub/lynx/lynx2-2.tar.Z )
( ftp://ftp2.cc.ukans.edu/pub/lynx/lynx2-2.zip )

Lynx is a distributed hypertext browser with full World Wide Web
capibilities.  For an explanation of features and a demo, 
telnet to "www.cc.ukans.edu" and login as "www".

This release of Lynx has been compiled by me on the following platforms:

 o  IBM (AIX 3.2)
 o  DEC Ultrix
 o  DEC Alpha OSF/1
 o  Sun 4
 o  NeXT (Mine is an older version of NeXTStep, but it should work 
          with newer ones too.)
 o  OpenVMS for Alpha AXP (Multinet)

This release is rumored to compile on the following platforms:
 o  HP-UX (snake)
 o  Solaris 2
 o  SVR4
 o  VMS (UCX)
 o  LINUX
 o  SGI 
 o  SUN 3
 o  AIX 3.1
 o  NeXTStep 3.x
 o  SCO
 o  BSDI
 o  Apollo

Binaries for the following platforms are available:

 o  IBM (AIX 3.2, will work with 3.1 as well)
 o  Ultrix
 o  Alpha OSF/1
 o  Sun 4
 o  OpenVMS for Alpha AXP (Multinet)
 
A listserv list exists for the distribution of
Lynx related information, updates, and development discussion.
  o  Lynx-Dev@ukanaix.cc.ukans.edu
 
Send a subscribe request to listserv@ukanaix.cc.ukans.edu to
be added to the list.  All new releases will be anounced on this
list.  Please do not send subscribe requests to the the Lynx-Dev
list directly.

    The following new features have been added/changed:

Please see http://www.cc.ukans.edu/about_lynx/lynx2-2.html
for more details on new features.

* Interruptable I/0 now completely works.
  Just hit a 'z' anytime during a transfer to abort.  
  If there is a partial file to show, it will be shown.  
* bold and emphasis now use curses underlining to represent
  ephasized text.  The use of _underline_ chars before and after
  the text is now removed.
* added new more informative messages for form links and transfers.
* added 'd' for download current link.  Can be used to force a download
  of any file.
* Command keys are now completely configureable through the
  lynx.cfg file. (thanks go to David Trueman)
* 'r' removed as a comment key.  Use 'c' to comment or reconfigure 
  your keys to add 'r' back in.  (I would like to use 'r' in the 
  future to mean remove current bookmark link)
* Added support for HTTP redirection.  (I haven't tested it
  heavily because I don't know of many servers using
  redirection, so let me know if something breaks)
* Added Referer: header to HTTP request to specify the
  URI of the document from which the URI in the request
  was obtained.  This allows a server to generate lists of back-links 
  to documents, for interest, logging, etc. It also allows bad links 
  to be traced for maintenance.
* fixed -dump and -source options so that they work for
  binary files.
* Added NNTP posting capibilities.  Currently uses external inews
  program which will be included in the utils directory.
  Would someone like to patch in some freeware inews code?
* Added configurable download menu so that binary files may
  be downloaded using any protocol.  Download menu is displayed
  after selecting a non-displayable file.
* added configurable character sets.  Can be set in lynx.cfg,
  by the user in the options screen, or in the users .lynxrc file
  current sets are: ISO Latin I, DEC Multinational, IBM PC Character set, 
  & 7 bit approximations.  New sets can be added to src/LYCharSets.c
* slight change in forms user interface.  For the better I hope you will
  agree.  Text input fields are now active as soon as the cursor pointer
  is over them.  Therefore you may begin typing into the text field
  as soon as you come to it.  Tab, and up and down arrow keys will
  move off of the text input field.  Return will also move to the
  next link.  The only problem with this is as follows.  
  If you are in the habit of using the number keys (keypad)
  the or h,j,k,l VI keys, as soon as you move over the text link
  you will begin seeing numbers or hjk or l show up in the text field.
  In other words, your movement commands are now broken :(  You must
  use true arrow keys, return or the tab key to move off of the text field.
  I've tried this out and it seems to work alright.  I don't think
  that users unfamiliar with this will have too hard a time figureing
  it out.  Once you see numbers or letter appearing it becomes fairly
  obvious whats going on.  Lynx puts a message at the bottom of the
  screen saying "use tab or arrow keys to move off of link."
* added mods by David Trueman to implement -restrictions commandline
  option.  -restrictions allows a list of services to be disabled
  selectively. -restrictions takes the form of
  "lynx -restrictions=[default],[all],[inside_telnet],[outside_telnet],[shell],\
	[editor],[bookmark],[option_save],[print],[file_url],[download],[exec]"

   all             restricts all options.
   default         same as commandline option -anonymous.  Disables
	           default services for anonymous users.  Currently set to,
	           all restricted except for: inside_telnet, outside_telnet, and
	           goto.  Defaults settable within userdefs.h
   inside_telnet   disallow telnets for people coming from inside your
		   domain.
   outside_telnet  disallow telnets for people coming from outside your
		   domain.
   shell	   disallow shell escapes
   editor	   disallow editing
   bookmark	   disallow changing the location of the bookmark file.
   options_save    disallow saving options in .lynxrc
   print           disallow most print options
   goto            disable the 'g' (goto) command.
   file_url        disallow using G)oto to go to file: URL's
   download        disallow saving binary files to disk in the download menu.
   exec            disable execution scripts
   exec_frozen     disallow the user from changing the execution link
		   setting in the O)ptions menu.
* Added command line option -show_curser.  If enabled the curser
  will not be hidden in the right hand corner but will instead 
  be positioned at the start of the currently selected link.
  show curser is the default for systems without FANCY_CURSES
  capibilities, and the default configuration can be changed in
  userdefs.h
* fixed bug in password handling which cuased it to be displayed
  accidentally :(  (Danny Mayer)
* Added exec links.  4 types currently defined: (* compiled out by default! *)
    files ending in:  .csh, .ksh, .sh, (for UNIX)  and .com (for VMS);
  Use exec link controls to turn on and off exec links.
* Gopher menues are now within <pre> so that spaces are not
  collapsed in menu entries.
* Ability to FTP to VMS systems. (Foteos Macrides)
* Lynx FTP now supports PASV code. (fixes to CERN (Dave Raggert?) PASV code
  from John Ellson)  It is not enabled by default.  Line 43 in
  WWW/Library/Implementation/HTFTP must be commented out for
  PASV code to be used.  This was done because not all FTP sites
  can support PASV FTP.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From stumpf@informatik.tu-muenchen.de  Thu Feb 17 19:45:41 1994 --100
Message-Id: <1994Feb17.182421.6427@Informatik.TU-Muenchen.DE>
Date: Thu, 17 Feb 1994 19:45:41 --100
From: stumpf@informatik.tu-muenchen.de (Markus Stumpf)
Subject: Re: Proxy Servers


In article <9402161923.AA67580@stat1.cc.ukans.edu>, montulli@stat1.cc.ukans.edu (Lou Montulli) writes:
|> Tim I have to disagree.  The code that was in the WWWlibrary 
|> DID NOT send the entire URL unmodified, it sent only the
|> host and path part and it has worked that way for as long
|> as I can remember.

I can remember that the libs used with Mosaic 1.2 and the early 2.0alphas
sent the full URL. I know, 'cause I had to modify the gopher and http
gateways I'd written, after that versions :)

Another proposal :)

|> From: altis@ibeam.jf.intel.com (Kevin Altis)
|> Date: Fri, 11 Feb 94 15:09:43 GMT
|>
|> of the HTTP message is the same. For gopher and ftp, the proxy gateway
|> server will return the data encapsulated as a MIME content type to the
|> client like a normal HTTP message. HTTP MIME content types are returned for
|> all URL requests, regardless of the protocol type of the URL. FTP
|> directories, Gopher directories, etc. are returned as text/html.

I have always wondered, why e.g. gopher gateways should have to
convert the data they gate.
With the new gateway code I hoped we might have an *easy* way to gate
things, simply manage a binary 1:1 connection between client and destination.
But as I understand it this is not the case?
But why should a gateway encode the answer from a gopher server e.g.
wenn a client like Mosaic could do it himself and in a much neater
way with (internal-) icons and so on.

This is what I think we should have:  ;)

WWW_xxx_GATEWAY   send full URL, expect answer in  text/html

WWW_xxx_PROXY     send full URL, expect answer as raw data as from
                  destination server

	\Maex
-- 
______________________________________________________________________________
 Markus Stumpf                        Markus.Stumpf@Informatik.TU-Muenchen.DE 
                                http://www.informatik.tu-muenchen.de/~stumpf/



From Paul.Everitt@ncts.navy.mil  Thu Feb 17 19:53:16 1994 --100
Message-Id: <9402171835.AC27459@pandora.ncts.navy.mil>
Date: Thu, 17 Feb 1994 19:53:16 --100
From: Paul.Everitt@ncts.navy.mil (Paul Everitt)
Subject: Re: HTTP Server Load

To add to the thought process on this -- we are moving our 
httpd from a Sun ELC running Solaris 2.3 (don't laugh!) to 
an MP machine, either a SS10 or 1000.

I was told that apps don't necessarily have to be ported 
and recompiled to take advantage of MP stuff, since they
network and OS libraries that call take up ~70% of the 
processing time.  Therefore, that 70% would be dynamically
linked, and would inherit MP abilities.

Any thoughts on this?  We are also considering moving WAIS,
INN, etc. to this machine.  Are there any MP types out there
that would care to comment?

Ciao.
--Paul

>Webmasters:
>
>We're about to bring on-line a WWW server which we expect to
>be heavily utilized.  The production plans include buying a new machine,
>but we're not sure about the system load of a high use server.
>
>Currently I'm running a development server (NCSA's httpd) standalone on a 
>Sparc 10 Model 30 with no problems, but usage is, of course, quite low.
>The server is used to perform some heavy-duty database (Sybase) querying.
>I suspect that performance problems will be due to the query processes
>and not the httpd server itself; nevertheless, server performance is
>a concern.
>
>If anyone has any performance stats or anecdotal evidence for memory
>requirements, CPU usage, or response time vs connections on Sun machines
>I'd be very interested.  Direct e-mail replies to dkulp@gdb.org.
>
>Thanks very much,
>-David Kulp.
>
>Genome DataBase
>Johns Hopkins Univ
>




From dsr@hplb.hpl.hp.com  Thu Feb 17 20:03:12 1994 --100
Message-Id: <9402171845.AA21194@manuel.hpl.hp.com>
Date: Thu, 17 Feb 1994 20:03:12 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: A thought on implementation...

> Writing a browser is very difficult.

Too right it is, although parsing with error correction
is comparatively simple part of the job!

>  It is not clear just how much of HTML+ will actually be
> implemented in any form in popular browsers.

A full version except for math will be made freely available
for X windows in June following the WWW Conference where I
will be using it to illustrate the HTML+ spec.

Dave Raggett



From sanders@BSDI.COM  Thu Feb 17 21:08:55 1994 --100
Message-Id: <199402172004.OAA03676@austin.BSDI.COM>
Date: Thu, 17 Feb 1994 21:08:55 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Indented <MENU>s 

[on stylesheets]
> Any pointers to the page where they are defined?
Ok, a couple of people asked so I dug it up.

    http://www.vuw.ac.nz/non-local/gnat/ora-stylesheet-proposal.txt

This was the one discussed at WWWWW in Mass.

--sanders



From montulli@stat1.cc.ukans.edu  Thu Feb 17 21:26:25 1994 --100
Message-Id: <9402172022.AA47049@stat1.cc.ukans.edu>
Date: Thu, 17 Feb 1994 21:26:25 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Proxy Servers

> 
> 
> In article <9402161923.AA67580@stat1.cc.ukans.edu>, montulli@stat1.cc.ukans.edu (Lou Montulli) writes:
> |> Tim I have to disagree.  The code that was in the WWWlibrary 
> |> DID NOT send the entire URL unmodified, it sent only the
> |> host and path part and it has worked that way for as long
> |> as I can remember.
> 
> I can remember that the libs used with Mosaic 1.2 and the early 2.0alphas
> sent the full URL. I know, 'cause I had to modify the gopher and http
> gateways I'd written, after that versions :)

Whatever the case may be, WWW_protocol_GATEWAY has acted in its
current manner for a very long period of time.  I think that
it is unwise to change its action suddenly.  A new environment
variable is still justified.


> 
> Another proposal :)
> 
> |> From: altis@ibeam.jf.intel.com (Kevin Altis)
> |> Date: Fri, 11 Feb 94 15:09:43 GMT
> |>
> |> of the HTTP message is the same. For gopher and ftp, the proxy gateway
> |> server will return the data encapsulated as a MIME content type to the
> |> client like a normal HTTP message. HTTP MIME content types are returned for
> |> all URL requests, regardless of the protocol type of the URL. FTP
> |> directories, Gopher directories, etc. are returned as text/html.
> 
> I have always wondered, why e.g. gopher gateways should have to
> convert the data they gate.
> With the new gateway code I hoped we might have an *easy* way to gate
> things, simply manage a binary 1:1 connection between client and destination.
> But as I understand it this is not the case?
> But why should a gateway encode the answer from a gopher server e.g.
> wenn a client like Mosaic could do it himself and in a much neater
> way with (internal-) icons and so on.
> 
That could be possible (although I'm not sure it's wise) by
specifying a mime type for the gopher directory syntax and
passing that mime type and the gopher syntax to the client.

We can do just as good a job by specifying a common set of
icons with unique names (similar to the internal images
mosaic supports now) and using those names in gopher and
ftp directory listings.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From wei@xcf.Berkeley.EDU  Thu Feb 17 21:49:16 1994 --100
Message-Id: <9402172046.AA17850@xcf.Berkeley.EDU>
Date: Thu, 17 Feb 1994 21:49:16 --100
From: wei@xcf.Berkeley.EDU (Pei Y. Wei)
Subject: Re: A thought on implementation...

John Franks wrote:
> It is fine to say, "Well yes,
> from the point of view of formal languages SGML was badly designed and
> that makes correct parsing complicated, but it is our standard we have
> to stick to it.  Just do it right!"  But I think that Dan's point was
> that browser writers haven't stuck to it and I am not sure just why we
> can expect them to in the future. 

At one point, ViolaWWW used a real SGML parser (sgmls) entirely 
(without CERN's parser) for its parsing of HTML (using Dan's DTD) and 
also HMML (something I had cooked up; this was pre HTML+ DTD days). 
Trouble was, it seemed few people were validating their HTML documents
with a DTD. Consequently, viola had a hard time with lots of the HTML
documents on the web...

So, in order to be more roubust, that SGML parsing process was put on ice, 
and ViolaWWW is now back to using CERN's parser again. Thou, I ended up
making a bunch of modifications to the CERN parser s.t.it would know things
like what tag elements are valid within what tag elements. This is so that
the parser can generate implied close tags.

Awaiting for a DTD to solidify, *and people to actually validate with it*,
and an efficient SGML parser implementation...

John Franks wrote:
> Writing a browser is very difficult.  
> It is not clear just how much of HTML+ will actually be
> implemented in any form in popular browsers.
> But unless you plan on writing your own browser and distributing it 
> to your target audience, you may want to try to *persuade* browser 
> writers that there is something in it for them if they put in all this
> extra work.

ViolaWWW supports lots of HTML+ now, including tables, but no math. 

For the long run, if we can agree on some sort of architectural form,
and have a nice SGML parser to attach to the browser, then we're closer
to multiple DTD capable browsers.


-Pei



From marc@library.ucsf.edu  Thu Feb 17 22:33:11 1994 --100
Message-Id: <199402172127.AA25256@library.ucsf.edu >
Date: Thu, 17 Feb 1994 22:33:11 --100
From: marc@library.ucsf.edu (Marc Salomon)
Subject: Re: A thought on implementation...

Pei Y. Wei wrote:

|For the long run, if we can agree on some sort of architectural form,
|and have a nice SGML parser to attach to the browser, then we're closer
|to multiple DTD capable browsers.

I was thinking that instead of building a full SGML parser onto each browser
that the browser could open a connection to a TCP/IP-based SGML parser
(forgiving a la mosaic)  server that would take as input a document instance
and return an ESIS that the browser could use for rendering.

-marc
marc@ckm.ucsf.edu



From mkcho@nic.nm.kr  Fri Feb 18 03:33:45 1994 --100
Message-Id: <199402180234.LAA27320@nic.nm.kr>
Date: Fri, 18 Feb 1994 03:33:45 --100
From: mkcho@nic.nm.kr (Minkyung Cho)
Subject: searching in the WWW


hi,

I have questions about searching in the WWW.
I would like to know many WWW servers provide searching in the WWW space.
in most case, they seem to maintain their own index and make the index
referenced in the search request.

but probably fish search announced a few months ago does not work like that.
so I am wondering if the two mixed search is.
is there any server whose search works like above?

I want to get a reply!

- minkyoung -
Korea Networked Information Center



From neuss@igd.fhg.de  Fri Feb 18 11:28:56 1994 --100
Message-Id: <9402181026.AA04855@wildturkey.igd.fhg.de>
Date: Fri, 18 Feb 1994 11:28:56 --100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: Re: A thought on implementation...

Pei Y. Wei,

> At one point, ViolaWWW used a real SGML parser (sgmls) entirely 

> (without CERN's parser) for its parsing of HTML (using Dan's DTD) and 

> also HMML (something I had cooked up; this was pre HTML+ DTD days). 

> Trouble was, it seemed few people were validating their HTML documents
> with a DTD. Consequently, viola had a hard time with lots of the HTML
> documents on the web...

Sad 'nuff. We'll all have a hell of a time when people start using
WYSYWIG editors (which are probably SGML based, and will _have_ to
be more strict about syntax). IMHO, SGML frontend based systems
are the way to go for the simple HTML editors we'll see in the
near future.

Chris
---
"I ride a tandem with the random.." 

Christian Neuss   # Fraunhofer Institute for Computer Graphics
Wilhelminenstr.7  #  64283 Darmstadt # Germany
e-mail: neuss@igd.fhg.de  finger: neuss@wildturkey.igd.fhg.de



From relihanl@ul.ie  Fri Feb 18 12:36:41 1994 --100
Message-Id: <Pine.3.05.9402181151.B2376-b100000@itdsrv1.ul.ie>
Date: Fri, 18 Feb 1994 12:36:41 --100
From: relihanl@ul.ie (Liam Relihan)
Subject: Re: Idea for new form input type

> Lou Montulli writes:
> 
 > I have an suggestion for a new form input type that I would
 > like to input on.
 > 
 > In order to support the posting of large files using form based
 > machanisms I suggest that a new input type "include-file" (or
 > something similar) be added.  This new type would be a file
 > that exists on the local machine whose data would be posted
 > to the forms server during form submission.  This method
 > would require the file to be posted using a MIME multipart
 > message.
 > 
 > The reasons this will be useful are as follows:
 > * large text files can be sent without having to cut and paste
 >   them into a textarea window as they are now.
 > * no memory limits on the size of the file.  (currently all input
 >   data is held in memory)
 > * arbitrary binary files can be sent 

It could be dodgy if a form had some kind of a "hidden" field pointing to a
default, eg. something like /etc/password (or something else sensitive). 
Imagine, that in the middle of a massive form, a little file specification
field existed specifying a file as a default. 

Would default filenames be allowed ?

Maybe I'm being a little paranoid.

I agree, however, that it is a good idea.


Liam
--
 Liam Relihan,                 |   |\       Voice: +353-61-333644 ext.5015
 CSIS, Schumann Building,   -  |   |_/  -               Fax:+353-61-330876
 University Of Limerick,       |__ | \              E-mail: relihanl@ul.ie
 Ireland.                     http://itdsrv1.ul.ie/PERSONNEL/lrelihan.html




From waterbug@epims1.gsfc.nasa.gov  Fri Feb 18 11:37:47 1994 --100
Message-Id: <9402181027.AA04721@epims1>
Date: Fri, 18 Feb 1994 11:37:47 --100
From: waterbug@epims1.gsfc.nasa.gov (Steve Waterbury)
Subject: Re: Idea for new form input type



Lou Montulli writes:

> I have an suggestion for a new form input type that I would
> like to input on.
> 
> In order to support the posting of large files using form based
> machanisms I suggest that a new input type "include-file" (or
> something similar) be added.  This new type would be a file
> that exists on the local machine whose data would be posted
> to the forms server during form submission.  This method
> would require the file to be posted using a MIME multipart
> message.
> 
> The reasons this will be useful are as follows:
> * large text files can be sent without having to cut and paste
>   them into a textarea window as they are now.
> * no memory limits on the size of the file.  (currently all input
>   data is held in memory)
> * arbitrary binary files can be sent 

Enthusiastically seconded!!  This capability would be extremely 
useful for some forms applications I would like to implement.  

- Steve Waterbury

=====================================================================
Stephen C. Waterbury		Phone:	301-286-7557     FAX:	-1695
EEE Parts Information Management System (EPIMS)
Code 310.A			email:	waterbug@epims1.gsfc.nasa.gov
NASA/GSFC			"Sometimes you're the windshield;
Greenbelt, MD 20771			sometimes you're the bug."
=====================================================================



From dsr@hplb.hpl.hp.com  Fri Feb 18 13:11:14 1994 --100
Message-Id: <9402181204.AA23346@manuel.hpl.hp.com>
Date: Fri, 18 Feb 1994 13:11:14 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Idea for new form input type

Lou,

> In order to support the posting of large files using form based
> machanisms I suggest that a new input type "include-file" (or
> something similar) be added.  This new type would be a file
> that exists on the local machine whose data would be posted
> to the forms server during form submission.  This method
> would require the file to be posted using a MIME multipart
> message.

How do you imagine this appearing to users? Do they see an input field
for typing (pasting) a file name into? Or is it a button which pops up
the file dialog, then when closed changes its name to that of the
selected file?

> The reasons this will be useful are as follows:
> * large text files can be sent without having to cut and paste
>   them into a textarea window as they are now.
> * no memory limits on the size of the file.  (currently all input
>   data is held in memory)
> * arbitrary binary files can be sent 

Seems good to me, I guess smart browser could also support drag 'n drop.
Tim Berners-Lee suggested a while back now, that you should be able to
paste (or drag 'n drop) arbitary data into a TEXTAREA field with the
browser being responsible for managing the encapsulation when sending
it to the server, and also how to present the pasted data to the user.
The browser could default to showing the file name and size if it doesn't
have any other way of displaying the file/object in the widget.

One way of combining the two ideas is for the TEXTAREA widget to show
a file menu in addition to the vertical and horizontal scrollbars.
Surely this more general approach would be better than a simple file
name widget?

Dave



From rcp@austin.slcs.slb.com  Fri Feb 25 10:19:46 1994 --100
Message-Id: <9402181521.AB21912@genoa>
Date: Fri, 25 Feb 1994 10:19:46 --100
From: rcp@austin.slcs.slb.com (Robert C. Pettengill)
Subject: Re: Idea for new form input type

Dave,
>Lou,
>
>> In order to support the posting of large files using form based
>> machanisms I suggest that a new input type "include-file" (or
..
>> The reasons this will be useful are as follows:
>> * large text files can be sent without having to cut and paste
>>   them into a textarea window as they are now.
>> * no memory limits on the size of the file.  (currently all input
>>   data is held in memory)
>> * arbitrary binary files can be sent
>
>Seems good to me, I guess smart browser could also support drag 'n drop.
>Tim Berners-Lee suggested a while back now, that you should be able to
>paste (or drag 'n drop) arbitary data into a TEXTAREA field with the
>browser being responsible for managing the encapsulation when sending
>it to the server, and also how to present the pasted data to the user.
>The browser could default to showing the file name and size if it doesn't
>have any other way of displaying the file/object in the widget.
>
>One way of combining the two ideas is for the TEXTAREA widget to show
>a file menu in addition to the vertical and horizontal scrollbars.
>Surely this more general approach would be better than a simple file
>name widget?
>
>Dave

This approach would not address the problem of non text files very well.
Should a CGI script have to be prepared to handle a MS Word document as
well as ASCII text from a text area field?

Should the form allow the specification of a desired/required MIME content
type for the file to be named?

Should there be a mechanism for specifying the MIME content type of the
file to be sent?

;rob

--
Robert C. Pettengill
Schlumberger Lab for Computer Science, P.O. Box 200015, Austin, Texas 78720
rcp@austin.slcs.slb.com Internet, +1 512-331-3728 Voice, +1 512-331-3760 Fax





From bert@let.rug.nl  Fri Feb 25 10:25:56 1994 --100
Message-Id: <9402181656.AA10833@freya.let.rug.nl>
Date: Fri, 25 Feb 1994 10:25:56 --100
From: bert@let.rug.nl (Bert Bos)
Subject: Proposal for standard icons/symbols

Some time ago there was discussion on this list about defining a set
of standard icons for things like Gopher types, "home" buttons, etc.
The discussion didn't reach a conclusion. Below is a proposal.
Reactions please!

The text and two sets of example icons are also available at:

gopher://gopher.let.rug.nl/00/ftp/pub/Bert/WWWicn.940218
gopher://gopher.let.rug.nl/99/ftp/pub/Bert/ISO-bitmaps.shar.gz
gopher://gopher.let.rug.nl/99/ftp/pub/Bert/Hughes-icons.shar.gz

and a similar FTP address.


<!--======================================================================

Status:
     Proposal

File:
     WWWicn

Version:
     0.0

Formal Public Identifier:
     Most recent version:
     -//WWW//ENTITIES WWW standard icons//EN

     This version:
     -//WWW//ENTITIES WWW standard icons:0.0//EN

Description:
     A set of entities for common icons

History:
     17 Feb 1994 - version 0.0

Rationale:
     An HTML document often contains inlined graphic material,
     sometimes as decoration, sometimes to convey information that
     would be less clear in words. Some of these graphical objects
     function much like mathematical or other symbols; they have
     become part of the World-Wide Web language. E.g., in Gopher
     menus the type of a menu entry is often indicated with such a
     symbol.

     Traditionally, such symbols have been added to the text by means
     of in-lined images. But there has been a demand for
     standardization of a small set of symbols which every HTML
     browser would be able to recognize and display in some way. It
     has been suggested that a naming scheme could be invented whereby
     reserved names in <IMAGE> tags would flag to the browser that a
     special action was required. Another suggested approach is to use
     the (as yet undefined) URN mechanism.

     The first solution - reserved names in the attributes of <IMAGE>
     tags - has the drawback that it uses a name-space that is not
     (and should not be) under the control of HTML. It also requires
     browsers that normally ignore in-line images to analyze the tags
     for special names.

     The URN method has the drawback that it specifies a single,
     concrete image, instead of the more abstract symbol, that can
     vary according to context (bold, large, 3D, in color, etc.)

     The solution in this document uses features of SGML (of which
     HTML is a specialization) to abstract away from specific images.
     Every symbol is represented by an (external) entity, in much the
     same way as other non-ASCII symbols are specified. The HTML+ DTD
     would contain something like:

     ...
     <!ENTITY % ISOlat1 PUBLIC "ISO 8879-1986//ENTITIES Added Latin 1//EN">
     %ISOlat1;
     <!ENTITY % WWWicn PUBLIC "-//WWW//ENTITIES WWW standard icons//EN">
     %WWWicn;
     <!ENTITY % ISOdia PUBLIC "ISO 8879-1986//ENTITIES Diacritical Marks//EN">
     %ISOdia;
     ...

Notes:
     This document defines a number of entities for common icons, such
     as "folder", "document", "home" and "sound".  Some icons are also
     defined in ISO/IEC CD 11581 "icon symbols and functions". Such
     icons also have a recommended shape ("ISO SHAPE"), the others
     have only suggested shapes ("SHAPE").

     A typical HTML application such as a World Wide Web browser will
     have its own way of rendering the icons. It may use a collection
     of bitmaps in any of the well-known graphics formats, it may have
     a special symbol font, or it may try to aproximate the shape with
     ASCII characters.

     The shapes leave a lot of room for variations. Applications may
     use color, shadows, 3D drawings and perhaps even photographs or
     holograms to adapt the basic shapes to their own style. The size
     of the icons is likewise left unspecified, but it is assumed that
     icons are displayed in running text, so they should normally be
     no larger than 2 to 4 times the size of a character.

     The entities have names of at most 32 characters, which is the
     declared NAMELEN of the HTML+ DTD. This precludes using of the
     names from ISO-????, since they tend to be longer. The longer
     names are shown as a comment where appropriate.

     A NOTATION "WWWicn" is declared, but the semantics are
     deliberately left undefined. An application that processes HTML
     must have its own way of resolving the entity.

References:
     1) HTML+ DTD, draft of 24th January 1994

     2) ISO/IEC CD 11581 "Information technology - user-system
     interfaces - icon symbols and functions" (30 Nov 1993)

=======================================================================-->


<!-- Object icons ========================================================

     The icons are not really divided into groups, but indications
     such as object icons, navigation icons, etc. make it easier to
     document them. The object icons in this section are normally used
     to represent objects, not actions. In practice the distinction
     may be difficult to make, however.
-->

<!ENTITY folder			SDATA "folder">

<!--
     Represents a folder or directory or anything that holds other
     documents. If an action is associated with it (a hyper-jump or
     "open" action), it should gives access to the (table of) contents.

     ISO SHAPE: a horizontal rectangle with a small tab sticking out of the
     top, near the left side. Reminiscent of folders as used in the office.

     ISO NAME: folder
-->

<!ENTITY filing.cabinet		SDATA "filing.cabinet">

<!--
     ...

     ISO NAME: filing cabinet
-->

<!ENTITY fixed.disk		SDATA "fixed.disk">

<!--
     ...

     ISO NAME: fixed storage device
-->

<!ENTITY disk.drive		SDATA "disk.drive">

<!--
     ...

     ISO NAME: drive for removable disk
-->

<!ENTITY document		SDATA "document">

<!--
     Represents a general document the type of which is unimportant.
     When the type is unknown rather than unimportant, it is better to
     use the "unknown.document" entity.

     ISO SHAPE: a vertical rectangle with a "dog's ear" in the upper
     right corner.

     ISO NAME: document
-->

<!ENTITY unknown.document	SDATA "unknown.document">

<!--
     A document of unknown or unrecognized type. If there is an action
     associated with this document, it is often an "unsafe" action, in
     the sense that the contents may or may not be processable by the
     application.

     SHAPE: as "document" above, but with a question mark inside.
-->

<!ENTITY text.document		SDATA "text.document">

<!--
     A document containing (mostly) text, such as text/plain,
     text/html, text/enriched, etc.

     SHAPE: as "document" above, but with "greeked" text inside
     (usually just a few horizontal lines).
-->

<!ENTITY binary.document	SDATA "binary.document">

<!--
     ...
-->

<!ENTITY binhex.document	SDATA "binhex.document">

<!--
     ...
-->

<!ENTITY ftp			SDATA "ftp">

<!--
     ...
-->

<!ENTITY archive		SDATA "archive">

<!--
     ...
-->

<!ENTITY telnet			SDATA "telnet">

<!--
     ...
-->

<!ENTITY form			SDATA "form">

<!--
     ...
-->

<!ENTITY audio			SDATA "audio">

<!--
     An audio object. "Opening" it should play the sound on the
     system's speaker(s) or other audio output equipment.

     ISO SHAPE: a stylized loud speaker pointing to the right. (My
     suggestion: optionally accompanied by a few semicircular "sound
     waves".)

     ISO NAME: audio device
-->

<!ENTITY image			SDATA "image">

<!--

     A photograph, drawing or graphic of any kind. The associated
     object might be a file in any still image format, whether in
     raster or vector format. "Opening" it should result in the image
     being displayed.

     SHAPE: a "landscape", e.g., with two shallow mountains and a sun
     in the upper left corner.
-->

<!ENTITY map			SDATA "map">

<!--

     A map, either geographical or a schematic map showing relations
     between objects or actions. An interactive map ("hypermap") is
     definitily a map, but a non-interactive map might also be
     classified as an image. If the purpose of the map to explain or
     to instructm then it is probably a map; if it is just an
     illustration for other information it is probably an image.

     SHAPE: ...
-->

<!ENTITY film			SDATA "film">

<!--
     A film or animation, such as an MPEG movie. It may have a
     soundtrack.

     SHAPE: a film projector, pointing right: a rectangle with a large
     and a small circle on top and a smaller rectangle representing
     the lens.

     or: (TBD)

     SHAPE: two blank frames from a film reel, with rows of holes
     along the left and right side.
-->

<!ENTITY mail			SDATA "mail">

<!--
     ...

     ISO NAME: mail
-->


<!-- Navigation icons ====================================================

     Icons that represent documents not by their contents, but by
     their relation to the document in which they occur. Relations can
     be hierarchical: a parent document with children; or linear: a
     successor and a predecessor; or semantical: a table of contents,
     an index, a glossary, etc.

-->

<!ENTITY parent			SDATA "parent">

<!--
     Represents the parent of the current document. There should also
     be a way to go from the parent back to this child in one step.

     SHAPE: a triangle pointing upwards inside a square.
-->

<!ENTITY next			SDATA "next">

<!--
     The next document in a sequence of documents, such as in an HTML
     PATH.

     SHAPE: a triangle pointing to the right inside a square.
-->

<!ENTITY previous		SDATA "previous">

<!--
     The predecessor of the current document in a sequence of
     documents.

     SHAPE: a triangle pointing to the left inside a square.
-->

<!ENTITY home			SDATA "home">

<!--
     ...
-->

<!ENTITY toc			SDATA "toc">

<!--
     ...
-->

<!ENTITY glossary		SDATA "glossary">

<!--
     ...
-->

<!ENTITY index			SDATA "index">

<!--
     ...
-->

<!ENTITY summary		SDATA "summary">

<!--
     ...
-->


<!-- Miscellaneous =======================================================

-->


-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|



From burchard@geom.umn.edu  Fri Feb 25 10:30:32 1994 --100
Message-Id: <9402181710.AA21071@mobius.geom.umn.edu>
Date: Fri, 25 Feb 1994 10:30:32 --100
From: burchard@geom.umn.edu (burchard@geom.umn.edu)
Subject: Announcing W3Kit: a toolkit for interactive Web application development

-- The Geometry Center announces W3Kit version 1.0 --

W3Kit is a system for building interactive graphical
applications for the World Wide Web.

Any graphical Web browser supporting Mosaic-compatible
Fill-Out Forms can be used as the front end of a W3Kit
application.  The back end takes the form of a
CGI-compliant server script, and W3Kit is principally
an object-oriented class library for easing the
development of sophisticated server scripts.

W3Kit supports 2D and 3D graphics.  It also provides the
application developer with a familiar programming
model involving an event loop and the usual cast of
interface widgets. 


W3Kit has been tested under SGI Irix and NeXTSTEP.  It is
intended to be compatible with other UNIX/X11 setups
supporting the Display PostScript extensions to X. 

W3Kit also requires GCC and the Geomview/X11 source
distribution (available through the references below).

The kit, along with preliminary documentation, can be found at:
 <A HREF="http://www.geom.umn.edu/docs/W3Kit/W3Kit.html">W3Kit</A>
The Geometry Center's recently announced Interactive Gallery
contains several examples of what can be done with W3Kit:
 <A HREF="http://www.geom.umn.edu/apps/gallery.html">W3Kit Demos</A>

--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------



From ajs@merck.com  Fri Feb 25 10:38:48 1994 --100
Message-Id: <9402181758.AA12773@dxmint.cern.ch>
Date: Fri, 25 Feb 1994 10:38:48 --100
From: ajs@merck.com (ajs@merck.com)
Subject: Versatile proxy code

I've had success with using the new proxy
code in lynx and mosaic, but I cannot figure
out how to make only *certain* connections
use the proxy. For example given a URL inside
our domain, the connection should be direct.
For example:

	http://www.merck.com/home.html

should make a direct connection, whereas,

	http://www.research.att.com/

should use the proxy. Any ideas?



From montulli@stat1.cc.ukans.edu  Fri Feb 25 10:43:18 1994 --100
Message-Id: <9402181824.AA79438@stat1.cc.ukans.edu>
Date: Fri, 25 Feb 1994 10:43:18 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Idea for new form input type

> 
> Lou,
> 
> > In order to support the posting of large files using form based
> > machanisms I suggest that a new input type "include-file" (or
> > something similar) be added.  This new type would be a file
> > that exists on the local machine whose data would be posted
> > to the forms server during form submission.  This method
> > would require the file to be posted using a MIME multipart
> > message.
> 
> How do you imagine this appearing to users? Do they see an input field
> for typing (pasting) a file name into? Or is it a button which pops up
> the file dialog, then when closed changes its name to that of the
> selected file?

That would of course be at the discretion of the browser writer, but
I was visualizing it as a button with a changing label.

> 
> > The reasons this will be useful are as follows:
> > * large text files can be sent without having to cut and paste
> >   them into a textarea window as they are now.
> > * no memory limits on the size of the file.  (currently all input
> >   data is held in memory)
> > * arbitrary binary files can be sent 
> 
> Seems good to me, I guess smart browser could also support drag 'n drop.
> Tim Berners-Lee suggested a while back now, that you should be able to
> paste (or drag 'n drop) arbitary data into a TEXTAREA field with the
> browser being responsible for managing the encapsulation when sending
> it to the server, and also how to present the pasted data to the user.
> The browser could default to showing the file name and size if it doesn't
> have any other way of displaying the file/object in the widget.
> 
> One way of combining the two ideas is for the TEXTAREA widget to show
> a file menu in addition to the vertical and horizontal scrollbars.
> Surely this more general approach would be better than a simple file
> name widget?
> 
Are you really comfortable with dragging an executable or image file
into a TEXTAREA window.  The conception of dropping binary data into
a text window seems very odd.  Not to mention the fact that the
implementation would be very difficult.  What do you do if someone
has typed in data and then drops in a binary file?  Also, all current
implementations of TEXTAREA would have to be completely rewritten.
(you can't simply use a standard text widget anymore)

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *             UNIX! Cool! I know that!  Jurassic Park - The Movie        *
  **************************************************************************



From dsr@hplb.hpl.hp.com  Fri Feb 25 10:46:57 1994 --100
Message-Id: <9402181841.AA23780@manuel.hpl.hp.com>
Date: Fri, 25 Feb 1994 10:46:57 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Idea for new form input type

Robert C. Pettengill writes in response to Dave Raggett

>> One way of combining the two ideas is for the TEXTAREA widget to show
>> a file menu in addition to the vertical and horizontal scrollbars.
>> Surely this more general approach would be better than a simple file
>> name widget?

> This approach would not address the problem of non text files very well.
> Should a CGI script have to be prepared to handle a MS Word document as
> well as ASCII text from a text area field?

As you go on to suggest the content type of the TEXTAREA field would be
encoded using MIME. This also allows the same field to include multiple
objects of varying types. The CGI script would be able to break apart
and process the objects accordingly.

This entails switching to a MIME encapsulation for form contents as
suggested by Lou.

Lou comments:

> Are you really comfortable with dragging an executable or image file
> into a TEXTAREA window.  The conception of dropping binary data into
> a text window seems very odd.  Not to mention the fact that the
> implementation would be very difficult.  What do you do if someone
> has typed in data and then drops in a binary file?  Also, all current
> implementations of TEXTAREA would have to be completely rewritten.
> (you can't simply use a standard text widget anymore)

The idea here is to generalise the window to handle arbitrary objects
and to exploit MIME for encapsulation. Yes the implementation would be
harder but much of the work is in switching to a MIME encapsulation of
form contents which is needed anyway for the simpler file name field.
Object level embedding is an issue we need to think about and my idea
is a step in this direction.

Dave



From marca@eit.COM  Fri Feb 25 10:51:10 1994 --100
Message-Id: <199402181955.TAA03012@threejane>
Date: Fri, 25 Feb 1994 10:51:10 --100
From: marca@eit.COM (Marc Andreessen)
Subject: Re: Indented <MENU>s 

Tony Sanders writes:
> > What can you do about it?  Probably nothing.  Isn't that cheery news?
> > I think so.  In fact, it has been a constant source of delight for me
> > over the past year to get to continually tell hordes (literally) of
> > people who want to -- strap yourselves in, here it comes -- control
> > what their documents look like in ways that would be trivial in TeX,
> > Microsoft Word, and every other common text processing environment:
> > "Sorry, you're screwed."
> 
> Then why don't you just implement one of the many style sheet
> proposals that are on the table.  This would pretty much solve the
> problem if done correctly.

So then I get to tell people, "Well, you get to learn *this* language
to write your document, and then you get to learn *that* language for
actually making your document look like you want it to."  Oh, they'll
love that.

Marc's viewpoint: style sheets are an artificial construct inflicted
on us because of the whole non-presentation philosophy we've been
using (more or less, but enough to keep this particular set of
problems alive), which I argue is wholly inappropriate for document
delivery front-ends and is crippling our system.  Why not strip out a
level of complexity (and user headeaches) by having our front ends
simply handle a layout format suitable from the ground up for
front-end display of documents?  Publishers can still use SGML out the
wazoo on the back end if they want; if they don't and all they want is
documents that look the way they want them to look, we don't inflict
it on them.

(Actually, "why don't I just implement..." is a moot point right now,
as I'm not in the client business anymore.  But anyway.)

Cheers,
Marc



From FisherM@is3.indy.tce.com  Fri Feb 25 10:55:07 1994 --100
Message-Id: <2D651E43@MSMAIL.INDY.TCE.COM>
Date: Fri, 25 Feb 1994 10:55:07 --100
From: FisherM@is3.indy.tce.com (Fisher Mark)
Subject: Style Sheets (was: RE: Indented <MENU>s)


Style sheets (as I understand the concept :)) are really needed.  Too much 
discussion in this list has centered around how to get typographical effects 
from a content (not format) oriented language, HTML.  Attaching style sheets 
to documents would allow authors to get their message across in the format 
they intended, while the idea of multiple levels of rendering would let both 
Mosaic/Cello/Viola users and Lynx/linemode users view a document without 
being confused by typographical errors that are now generated in an effort 
to get the typography as correct as the content.  As an old-time UNIX 
nroff/troff user, one of things I enjoyed about those languages was that 
format was separated (at least somewhat) from content by macros -- you 
wanted 5x8 manuals instead of 8.5x11, you changed the macros.  Let's not 
lose the distinction between form and content, but simultaneously let's try 
to find a way to help authors (me included!) get the format they want (at 
least in most cases :().
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From connolly@hal.com  Fri Feb 25 10:59:17 1994 --100
Message-Id: <9402190010.AA10063@ulua.hal.com>
Date: Fri, 25 Feb 1994 10:59:17 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Comments on MIME/SGML

------- =_aaaaaaaaaa0
Content-Type: text/plain; charset="us-ascii"
Content-ID: <10024.761615492.2@ulua>
Content-Description: MIME/SGML as text

                                                          Comments on MIME/SGML
               PROPOSED CHANGES FOR MIME REPRESENTATION OF SGML
                                       
                                          Daniel W. Connolly <connolly@hal.com>
                                                                               
SGML Entites in MIME

   I believe that the goals of Mr. Levinson's MIME Content-types for SGML
   Documents[1] are essential to the success of the intenet as an Integrated
   Open Hypermedia (c.f. HyTime[2]) system.
   
   However, after a careful reading of the SGML standard[3] (specifically
   section 6: Entity Structure), I believe that SGML/MIME[4] fails to specify
   the most important machine representation of an SGML document; that is, the
   SGML document entity (production 2, section 6.2).
   
   In conventional practice, an SGML document "doc" of type "T" is represented
   as a file doc.sgml, which looks like:
   

<!DOCTYPE T SYSTEM "t.dtd" [
<!ENTITY fig1 SYSTEM "foo.ps" postscript>
]>
<T>blah blah blah <figure graphic=fig1> blah blah blah</T>

   along with an SGML declaration in the file T.decl. Technically speaking, the
   SGML document entity is the concatenation of T.decl and doc.sgml (with
   perhaps some system-specific newline->RS/RE conversions). But according to
   the standard it's OK to interchange SGML documents with an implied SGML
   delcaration, and in practice, the SGML declaration is often compiled into
   the processing software.
   
   So for all intents and purposes, the file doc.html is an SGML document
   entity. And it seems critical that the file doc.html should correspond to
   the body of some MIME body part.
   
   The draft[5] misuses the term "DTD", aka "Document Type Definition" (defn
   4.104). An SGML document indeed has three parts: the SGML declaration, the
   prologue, and the instance. And the distinction between the term prologue
   and the term DTD is not trivial.
   
   First, to be pedantic, a DTD is not generally representable in SGML syntax.
   The concept of the DTD includes not only the SGML-representable formal part,
   but also the associated application conventions which cannot be represented
   in SGML. Also -- a document may have more than one DTD in its prologue.
   
   Second, to be practical, the conventional machine representation of a DTD is
   just an SGML text entity in the file t.dtd which looks like:
   

<!NOTATION postscript PUBLIC "-/Adobe/Postscript">
<!ELEMENT T - - (#PCDATA) >
..

   Note that it does not correspond exactly to the prologue of the document, in
   that it does not contain the <!DOCTYPE [ ... ]> markup.
   
   For these reasons, I suggest the following modifications to the proposed
   MIME representation of SGML documents:
   
   1. We make the following correspondence between the terms of the SGML
   standard and the MIME RFC:
   
      SGML notation => MIME content-type
      
       SGML SYSTEM identifier => MIME Content-ID
      
       SGML data entity (= notation, data)
      
       => MIME body part (=content type, body)
      
       SGML text entity (= sequence of characters)
      
       => body of MIME text/sgml body part (= seq of chars)
      
       SGML document => a MIME multipart/SGML body part
      
   2. We use text/sgml in stead of application/sgml for the SGML "files", since
   they are in general readable on teletype devices.e
   
   3. We change the parameters of the multipart/SGML content type from
   

               sgml-part       := "intance" / "declaration"
                                / "dtd" / "fosi" / extension-token

   to
   

               sgml-part       := "document" / "declaration"
                                / "dtd" / "fosi" / extension-token

   where "document" is required, declaration is optional, and dtd is acutally
   redundant (since it's in the document entity) but useful, since a MIME UA
   might want to know what kind of document it is without parsing the document.
   
References

        Network Working Group, Internet Draft: MIME/SGML,
      <draft-levinson-sgml-01.txt>, E. Levinson, Accurate Information Systems,
      Inc., January 17, 1993
      
        ISO 8879:1986, Information Processing: Text and Office Systems:
      Standard Generalized Markup Language (SGML)
      
      ISO/IEC 10744 Information technology -- Hypermedia/Time-based Structuring
      Language (HyTime)
      
Editorial Note

   I used HTML because it works to a certain extent, not because I think it's
   exactly how I think internet IOH should work. My comments on HTML are still
   under development. See my notebook on the design on an HTML successor[6].
   
Production Note

   This document is brought to you by the following tools:
   
       Lucid emacs
      
       html-mode by Marc Andressen
      
       SGMLs
      
       NCSA Mosaic
      
   

------- =_aaaaaaaaaa0
Content-Type: multipart/x-sgml; boundary="----- =_aaaaaaaaaa1";
	document="10024.761615492.5@ulua";
	dtd="10024.761615492.6@ulua";
	declaration="10024.761615492.7@ulua"
Content-ID: <10024.761615492.3@ulua>

------- =_aaaaaaaaaa1
Content-Type: text/x-html; charset="us-ascii"
Content-ID: <10024.761615492.4@ulua>
Content-Description: comments on MIME/SGML as html
Content-Transfer-Encoding: quoted-printable

<HEAD>
<TITLE>Comments on MIME/SGML</TITLE>
</HEAD>
<BODY>
<H1>Proposed Changes for MIME representation of SGML
</H1>

<ADDRESS>Daniel W. Connolly &lt;connolly@hal.com&gt;</ADDRESS>

<H2>SGML Entites in MIME</H2>

I believe that the goals of Mr. Levinson's <CITE><A HREF=3D"#r1">MIME
Content-types for SGML Documents</A></CITE> are essential to the
success of the intenet as an Integrated Open Hypermedia (c.f. <A
HREF=3D"#HyTime">HyTime</A>) system. <P>

However, after a careful reading of <A HREF=3D"#SGML">the SGML
standard</A> (specifically section 6: Entity Structure), I believe
that <A HREF=3D"#r1">SGML/MIME</A> fails to specify the most
important machine representation of an SGML document; that is, the
SGML document entity (production 2, section 6.2). <P>

In conventional practice, an SGML document "doc" of type "T" is
represented as a file doc.sgml, which looks like:

<PRE>
&lt;!DOCTYPE T SYSTEM "t.dtd" [
&lt;!ENTITY fig1 SYSTEM "foo.ps" postscript&gt;
]&gt;
&lt;T&gt;blah blah blah &lt;figure graphic=3Dfig1&gt; blah blah blah&lt;/T=
&gt;
</PRE>

along with an SGML declaration in the file T.decl. Technically
speaking, the SGML document entity is the concatenation of T.decl and
doc.sgml (with perhaps some system-specific newline-&gt;RS/RE
conversions). But according to the standard it's OK to interchange
SGML documents with an implied SGML delcaration, and in practice, the
SGML declaration is often compiled into the processing software. <P>

So for all intents and purposes, the file doc.html is an SGML document
entity. And it seems critical that the file doc.html should correspond
to the body of some MIME body part. <P>

<A HREF=3D"#r1">The draft</A> misuses the term "DTD", aka "Document Type
Definition" (defn 4.104). An SGML document indeed has three parts: the
SGML declaration, the <EM>prologue</EM>, and the instance. And the
distinction between the term prologue and the term DTD is not trivial. <P>

First, to be pedantic, a DTD is <EM>not</EM> generally representable
in SGML syntax. The concept of the DTD includes not only the
SGML-representable formal part, but also the associated application
conventions which cannot be represented in SGML. Also -- a document
may have more than one DTD in its prologue. <P>

Second, to be practical, the conventional machine representation of a
DTD is just an SGML text entity in the file t.dtd which looks like: <P>

<PRE>
&lt;!NOTATION postscript PUBLIC "-/Adobe/Postscript"&gt;
&lt;!ELEMENT T - - (#PCDATA) &gt;
..
</PRE>


Note that it does <EM>not</EM> correspond exactly to the prologue of
the document, in that it does not contain the <CODE>&lt;!DOCTYPE [ ...
]&gt;</CODE> markup. <P>

For these reasons, I suggest the following modifications to the
proposed MIME representation of SGML documents: <P>

1. We make the following correspondence between the terms of the SGML
standard and the MIME RFC:

<UL>
<LI>SGML notation =3D> MIME content-type
<LI> SGML SYSTEM identifier =3D> MIME Content-ID
<LI> SGML data entity (=3D notation, data)
		 =3D> MIME body part (=3Dcontent type, body)
<LI> SGML text entity (=3D sequence of characters)
		 =3D> body of MIME text/sgml body part (=3D seq of chars)
<LI> SGML document =3D> a MIME multipart/SGML body part
</UL>

2. We use text/sgml in stead of application/sgml for the SGML "files",
since they are in general readable on teletype devices.e <P>

3. We change the parameters of the multipart/SGML content type from

<PRE>
               sgml-part       :=3D "intance" / "declaration"
                                / "dtd" / "fosi" / extension-token

</PRE>

to

<PRE>
               sgml-part       :=3D "document" / "declaration"
                                / "dtd" / "fosi" / extension-token

</PRE>

where "document" is required, declaration is optional, and dtd is
acutally redundant (since it's in the document entity) but useful,
since a MIME UA might want to know what kind of document it is without
parsing the document.

<H2>References</H2>

<OL>
<LI> =

<A NAME=3D"r1">
Network Working Group,
Internet Draft: MIME/SGML,
&lt;draft-levinson-sgml-01.txt&gt;,
E. Levinson,
Accurate Information Systems, Inc.,
January 17, 1993
</A>

<LI> =

<A NAME=3D"SGML">ISO 8879:1986, Information Processing: Text and Office Sy=
stems:
Standard Generalized Markup Language (SGML)
</A>

<LI><A NAME=3D"HyTime">ISO/IEC 10744 Information technology -- Hypermedia/=
Time-based
Structuring Language (HyTime)
</A>
</OL>

<H2>Editorial Note</H2>

I used HTML because it works to a certain extent, not because I think
it's exactly how I think internet IOH should work. My comments on HTML
are still under development. See <A
HREF=3D"http://www-external.hal.com/~connolly/html-design.html">my
notebook on the design on an HTML successor</A>.

<H2>Production Note</H2>

This document is brought to you by the following tools:

<UL>
<LI> Lucid emacs
<LI> html-mode by Marc Andressen
<LI> SGMLs
<LI> NCSA Mosaic
</UL>

</BODY>

------- =_aaaaaaaaaa1
Content-Type: text/x-sgml; charset="us-ascii"
Content-ID: <10024.761615492.5@ulua>
Content-Description: SGML document wrapper

<!DOCTYPE HTML SYSTEM "10024.761615492.6@ulua"
	-- PUBLIC "-//IETF/DRAFT/ietf-iiir-html-01" @@ -- [
<!-- $Id$ -->

<!ENTITY web-node SYSTEM "10024.761615492.4@ulua">
]>
<HTML>
&web-node;
</HTML>

------- =_aaaaaaaaaa1
Content-Type: text/x-sgml; charset="us-ascii"
Content-ID: <10024.761615492.6@ulua>
Content-Description: HTML dtd
Content-Transfer-Encoding: quoted-printable

<!-- Jul 1 93 -->
<!--    Regarding clause 6.1, SGML Document:

        [1] SGML document =3D SGML document entity,
            (SGML subdocument entity |
            SGML text entity | non-SGML data entity)*

        The role of SGML document entity is filled by this DTD,
        followed by the conventional HTML data stream.
-->

<!-- DTD definitions -->

<!ENTITY % heading "H1|H2|H3|H4|H5|H6" >
<!ENTITY % list " UL | OL | DIR | MENU ">
<!ENTITY % literal " XMP | LISTING ">

<!ENTITY % headelement
         " TITLE | NEXTID |ISINDEX" >

<!ENTITY % bodyelement
         "P | HR | %heading |
         %list | DL | ADDRESS | PRE | BLOCKQUOTE
        | %literal">

<!ENTITY % oldstyle "%headelement | %bodyelement | #PCDATA">

<!ENTITY % URL "CDATA"
        -- The term URL means a CDATA attribute
           whose value is a Uniform Resource Locator,
           as defined. (A URN may also be usable here when defined.)
        -->

<!ENTITY % linkattributes
        "NAME NMTOKEN #IMPLIED
        HREF %URL;  #IMPLIED
        REL CDATA #IMPLIED -- forward relationship type --
        REV CDATA #IMPLIED -- reversed relationship type
                              to referent data:

                                PARENT CHILD, SIBLING, NEXT, TOP,
                                DEFINITION, UPDATE, ORIGINAL etc. --

        URN CDATA #IMPLIED -- universal resource number --

        TITLE CDATA #IMPLIED -- advisory only --

        METHODS NAMES #IMPLIED -- supported public methods of the object:
                                        TEXTSEARCH, GET, HEAD, ... --

        ">


<!-- Document Element -->

<!ELEMENT HTML O O  (( HEAD | BODY | %oldstyle )*, PLAINTEXT?)>

<!ELEMENT HEAD - -  ( TITLE?  & ISINDEX?  & NEXTID?  & LINK*
                              & BASE?)>

<!ELEMENT TITLE - -  RCDATA
          -- The TITLE element is not considered part of the flow of text.
             It should be displayed, for example as the page header or
             window title.
          -->

<!ELEMENT ISINDEX - O EMPTY
          -- WWW clients should offer the option to perform a search on
             documents containing ISINDEX.
          -->

<!ELEMENT NEXTID - O EMPTY>
<!ATTLIST NEXTID N NAME #REQUIRED
          -- The number should be a name suitable for use
             for the ID of a new element. When used, the value
             has its numeric part incremented. EG Z67 becomes Z68
          -->
<!ELEMENT LINK - O EMPTY>
<!ATTLIST LINK
        %linkattributes>
        =

<!ELEMENT BASE - O EMPTY    -- Reference context for URLS -->
<!ATTLIST BASE

        HREF %URL; #IMPLIED

        >
<!ENTITY % inline "EM | TT | STRONG | B | I | U |
                        CODE | SAMP | KBD | KEY | VAR | DFN | CITE "
        >

<!ELEMENT (%inline;) - - (#PCDATA)>

<!ENTITY % text "#PCDATA | IMG | %inline;">

<!ENTITY % htext "A | %text"    -- Plus links, no structure -->

<!ENTITY % stext                -- as htext but also nested structure --
                        "P | HR | %list | DL | ADDRESS
                        | PRE | BLOCKQUOTE
                        | %literal | %htext">


<!ELEMENT BODY - -  (%bodyelement|%htext;)*>


<!ELEMENT A     - -  (%text)>
<!ATTLIST A
        %linkattributes;
        >

<!ELEMENT IMG    - O EMPTY --  Embedded image -->
<!ATTLIST IMG
        SRC %URL;  #IMPLIED     -- URL of document to embed --
        >


<!ELEMENT P     - O EMPTY -- separates paragraphs -->
<!ELEMENT HR    - O EMPTY -- horizontal rule -->

<!ELEMENT ( %heading )  - -  (%htext;)+>

<!ELEMENT DL    - -  (DT | DD | %stext;)*>
<!--    Content should match ((DT,(%htext;)+)+,(DD,(%stext;)+))
        But mixed content is messy.  -Dan Connolly
  -->

<!ELEMENT DT    - O EMPTY>
<!ELEMENT DD    - O EMPTY>

<!ELEMENT (UL|OL) - -  (%htext;|LI|P)+>
<!ELEMENT (DIR|MENU) - -  (%htext;|LI)+>
<!--    Content should match ((LI,(%htext;)+)+)
        But mixed content is messy.
  -->
<!ATTLIST (%list)
        COMPACT NAME #IMPLIED -- COMPACT, etc.--
        >

<!ELEMENT LI    - O EMPTY>

<!ELEMENT BLOCKQUOTE - - (%htext;|P)+
        -- for quoting some other source -->

<!ELEMENT ADDRESS - - (%htext;|P)+>

<!ELEMENT PRE - - (#PCDATA|%inline|A|P)+>
<!ATTLIST PRE
        WIDTH NUMBER #implied
        >

<!-- Mnemonic character entities. -->
<!ENTITY AElig "&#198;"  -- capital AE diphthong (ligature) -->
<!ENTITY Aacute "&#193;" -- capital A, acute accent -->
<!ENTITY Acirc "&#194;"  -- capital A, circumflex accent -->
<!ENTITY Agrave "&#192;" -- capital A, grave accent -->
<!ENTITY Aring "&#197;"  -- capital A, ring -->
<!ENTITY Atilde "&#195;" -- capital A, tilde -->
<!ENTITY Auml "&#196;"   -- capital A, dieresis or umlaut mark -->
<!ENTITY Ccedil "&#199;" -- capital C, cedilla -->
<!ENTITY ETH "&#208;"    -- capital Eth, Icelandic -->
<!ENTITY Eacute "&#201;" -- capital E, acute accent -->
<!ENTITY Ecirc "&#202;"  -- capital E, circumflex accent -->
<!ENTITY Egrave "&#200;" -- capital E, grave accent -->
<!ENTITY Euml "&#203;"   -- capital E, dieresis or umlaut mark -->
<!ENTITY Iacute "&#205;" -- capital I, acute accent -->
<!ENTITY Icirc "&#206;"  -- capital I, circumflex accent -->
<!ENTITY Igrave "&#204;" -- capital I, grave accent -->
<!ENTITY Iuml "&#207;"   -- capital I, dieresis or umlaut mark -->
<!ENTITY Ntilde "&#209;" -- capital N, tilde -->
<!ENTITY Oacute "&#211;" -- capital O, acute accent -->
<!ENTITY Ocirc "&#212;"  -- capital O, circumflex accent -->
<!ENTITY Ograve "&#210;" -- capital O, grave accent -->
<!ENTITY Oslash "&#216;" -- capital O, slash -->
<!ENTITY Otilde "&#213;" -- capital O, tilde -->
<!ENTITY Ouml "&#214;"   -- capital O, dieresis or umlaut mark -->
<!ENTITY THORN "&#222;"  -- capital THORN, Icelandic -->
<!ENTITY Uacute "&#218;" -- capital U, acute accent -->
<!ENTITY Ucirc "&#219;"  -- capital U, circumflex accent -->
<!ENTITY Ugrave "&#217;" -- capital U, grave accent -->
<!ENTITY Uuml "&#220;"   -- capital U, dieresis or umlaut mark -->
<!ENTITY Yacute "&#221;" -- capital Y, acute accent -->
<!ENTITY aacute "&#225;" -- small a, acute accent -->
<!ENTITY acirc "&#226;"  -- small a, circumflex accent -->
<!ENTITY aelig "&#230;"  -- small ae diphthong (ligature) -->
<!ENTITY agrave "&#224;" -- small a, grave accent -->
<!ENTITY amp "&#38;"     -- ampersand -->
<!ENTITY aring "&#229;"  -- small a, ring -->
<!ENTITY atilde "&#227;" -- small a, tilde -->
<!ENTITY auml "&#228;"   -- small a, dieresis or umlaut mark -->
<!ENTITY ccedil "&#231;" -- small c, cedilla -->
<!ENTITY eacute "&#233;" -- small e, acute accent -->
<!ENTITY ecirc "&#234;"  -- small e, circumflex accent -->
<!ENTITY egrave "&#232;" -- small e, grave accent -->
<!ENTITY eth "&#240;"    -- small eth, Icelandic -->
<!ENTITY euml "&#235;"   -- small e, dieresis or umlaut mark -->
<!ENTITY gt "&#62;"      -- greater than -->
<!ENTITY iacute "&#237;" -- small i, acute accent -->
<!ENTITY icirc "&#238;"  -- small i, circumflex accent -->
<!ENTITY igrave "&#236;" -- small i, grave accent -->
<!ENTITY iuml "&#239;"   -- small i, dieresis or umlaut mark -->
<!ENTITY lt "&#60;"      -- less than -->
<!ENTITY nbsp "&#32;"    --  should be NON_BREAKING space -->
<!ENTITY ntilde "&#241;" -- small n, tilde -->
<!ENTITY oacute "&#243;" -- small o, acute accent -->
<!ENTITY ocirc "&#244;"  -- small o, circumflex accent -->
<!ENTITY ograve "&#242;" -- small o, grave accent -->
<!ENTITY oslash "&#248;" -- small o, slash -->
<!ENTITY otilde "&#245;" -- small o, tilde -->
<!ENTITY ouml "&#246;"   -- small o, dieresis or umlaut mark -->
<!ENTITY szlig "&#223;"  -- small sharp s, German (sz ligature) -->
<!ENTITY thorn "&#254;"  -- small thorn, Icelandic -->
<!ENTITY uacute "&#250;" -- small u, acute accent -->
<!ENTITY ucirc "&#251;"  -- small u, circumflex accent -->
<!ENTITY ugrave "&#249;" -- small u, grave accent -->
<!ENTITY uuml "&#252;"   -- small u, dieresis or umlaut mark -->
<!ENTITY yacute "&#253;" -- small y, acute accent -->
<!ENTITY yuml "&#255;"   -- small y, dieresis or umlaut mark -->

<!-- deprecated elements -->

<!ELEMENT (%literal) - -  CDATA>

<!ELEMENT PLAINTEXT - O EMPTY>

<!-- Local Variables: -->
<!-- mode: sgml -->
<!-- compile-command: "sgmls -s -p " -->
<!-- end: -->

------- =_aaaaaaaaaa1
Content-Type: text/x-sgml; charset="us-ascii"
Content-ID: <10024.761615492.7@ulua>
Content-Description: HTML SGML declaration

<!SGML  "ISO 8879:1986"
--
        Document Type Definition for the HyperText Markup Language
        as used by the World Wide Web application (HTML DTD).

        NOTE: This is a definition of HTML with respect to
        SGML, and assumes an understanding of SGML terms.

        If you find bugs in this DTD or find it does not compile
        under some circumstances please mail www-bug@info.cern.ch
--

CHARSET
         BASESET  "ISO 646:1983//CHARSET
                   International Reference Version (IRV)//ESC 2/5 4/0"
         DESCSET  0   9   UNUSED
                  9   2   9
                  11  2   UNUSED
                  13  1   13
                  14  18  UNUSED
                  32  95  32
                  127 1   UNUSED
     BASESET   "ISO Registration Number 100//CHARSET
                ECMA-94 Right Part of Latin Alphabet Nr. 1//ESC 2/13 4/1"
     DESCSET   128 32 UNUSED
               160 95 32
               255  1 UNUSED


CAPACITY        SGMLREF
                TOTALCAP        150000
                GRPCAP          150000

SCOPE    DOCUMENT
SYNTAX
         SHUNCHAR CONTROLS 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
                           19 20 21 22 23 24 25 26 27 28 29 30 31 127 255
         BASESET  "ISO 646:1983//CHARSET
                   International Reference Version (IRV)//ESC 2/5 4/0"
         DESCSET  0 128 0
         FUNCTION RE          13
                  RS          10
                  SPACE       32
                  TAB SEPCHAR  9
         NAMING   LCNMSTRT ""
                  UCNMSTRT ""
                  LCNMCHAR ".-"
                  UCNMCHAR ".-"
                  NAMECASE GENERAL YES
                           ENTITY  NO
         DELIM    GENERAL  SGMLREF
                  SHORTREF SGMLREF
         NAMES    SGMLREF
         QUANTITY SGMLREF
                  NAMELEN  34
                  TAGLVL   100
                  LITLEN   1024
                  GRPGTCNT 150
                  GRPCNT   64

FEATURES
  MINIMIZE
    DATATAG  NO
    OMITTAG  NO
    RANK     NO
    SHORTTAG NO
  LINK
    SIMPLE   NO
    IMPLICIT NO
    EXPLICIT NO
  OTHER
    CONCUR   NO
    SUBDOC   NO
    FORMAL   YES
  APPINFO    NONE
>


------- =_aaaaaaaaaa1--

------- =_aaaaaaaaaa0--



From dmh@hpfcma.fc.hp.com  Fri Feb 25 11:03:32 1994 --100
Message-Id: <9402190101.AA17555@hpfcma.fc.hp.com>
Date: Fri, 25 Feb 1994 11:03:32 --100
From: dmh@hpfcma.fc.hp.com (Dave Hollander)
Subject: Virtual Pages


The attached posting reminded me of another document design and maintenance
issue. The current model for creating and accessing HTML documents
forces one to build rather small documents that are heavly linked.
This can be difficult to maintain particularlly in a production
environment because the unit of display (html files) have no relationship
to the unit of submission/processing/maintenance.

The approach used in the SDL is to have a document be a collection
of virtual pages where a virtual page is much like today's HTML document.
However, the virtual page collection can be managed as a single entity,
and sytles sheets or other doc set information only needs to be 
instanced once.

In addition, it provides a comfortable place to place an index, table
of contents, glossary, etc.

What do you think? Would this help HTML+?

Regards,

Dave Hollander
_____________________________________________________-

Xref: fc.hp.com comp.infosystems.www:1097
Path: fc.hp.com!col.hp.com!sdd.hp.com!elroy.jpl.nasa.gov!usc!math.ohio-state.edu!magnus.acs.ohio-state.edu!usenet.ins.cwru.edu!po.CWRU.Edu!ccy
From: ccy@po.CWRU.Edu (Cheung C. Yue)
Newsgroups: comp.infosystems.www
Subject: Forced page break in HTML
Date: 17 Feb 1994 04:14:41 GMT
Organization: Case Western Reserve University, Cleveland, OH (USA)
Lines: 13
Message-ID: <2juqvh$iuv@usenet.INS.CWRU.Edu>
Reply-To: ccy@po.CWRU.Edu (Cheung C. Yue)
NNTP-Posting-Host: slc10.ins.cwru.edu


Just curious if there is a code for a forced page break in a HTML
document.  I think that way one can feel more comfortable about
putting links to locations within the same file rather than to another
file.  I kind of hate to write multiple small html files just so I
can keep the appearance of the main document neat (sort of like Nova
Links).  I suppose I might have to accept just the code for a line, 
although that line will probably not show up on a text-terminal client
such as Lynx.  Any other suggestions?

C Cho Yue
ccy@po.cwru.edu
- -- 

------- End of Forwarded Message




From masinter@parc.xerox.com  Fri Feb 25 11:07:44 1994 --100
Message-Id: <94Feb16.174146pst.2735@golden.parc.xerox.com>
Date: Fri, 25 Feb 1994 11:07:44 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Proxy Servers

> Yes.  I think that a simple and common case will be that
> anything within a certain single domain will be local access.
> Generally the firewall or the weak link is at a domain boundary,
> to all intents and purposes. One possibility is to force ALL traffic
> outide a domain to use a server, which would need two env variables

> 	WWW_FIREWALL_GATEWAY	http://gateway.acme.com/
> 	WWW_FIREWALL_DOMAIN	acme.com

> of couse a good default would be to guess that the domain was the 
> domain of the gateway server, which would just mean one env variable.
> Another would be to do it separately by URL scheme.

> 	WWW_http_GATEWAY	http://gateway.acme.com/
> 	WWW_http_DIRECT_DOMAIN	acme.com

> Any thoughts on this?  Kev like to propose something and the code
> as a function of any other comments?  I agree we want to keep it
> simple.

> Tim Berners-Lee

Well, I'm sympathetic to keeping it simple, but I think we want some
finer grain control with more information -- even a table of addresses
and protocols  might be necessary. How about a configuration file
instead?




From masinter@parc.xerox.com  Fri Feb 25 11:11:33 1994 --100
Message-Id: <94Feb18.145019pst.2795@golden.parc.xerox.com>
Date: Fri, 25 Feb 1994 11:11:33 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Re Dan on implementation 

FWIW (For What It's Worth), I'm with Dan Connolly: let HTML be a small
context-free subset of SGML. Simpler, almost all the expressive power,
easier to write compliant browsers, and a little harder to write
validators.

(Forgive me if I'm behind, our mail is backlogged a couple of days.)



From decoux@moulon.inra.fr  Fri Feb 25 11:16:13 1994 --100
Message-Id: <9402191527.AA23345@moulon.moulon.inra.fr>
Date: Fri, 25 Feb 1994 11:16:13 --100
From: decoux@moulon.inra.fr (ts)
Subject: CGI Script and PATH_INFO


 Apparently, CERN Daemon, NCSA Daemon and Plexus don't give the same
"PATH_INFO" when there is a "+" in the pathname.

 Who are right ?

 Example : with the simple script "a"

#!/bin/sh
echo Content-type: text/plain
echo
echo $PATH_INFO
echo

 -------------------- NCSA  -----------------------------

moulon% telnet moulon 9000
Trying 192.93.96.1 ...
Connected to moulon.
Escape character is '^]'.
GET /cgi-bin/a/essai[+%3B+]?a HTTP/1.0
Accept: text/html
 
HTTP/1.0 200 OK
Date: Saturday, 19-Feb-94 15:12:21 GMT
Server: NCSA/1.1
MIME-version: 1.0
Content-type: text/plain
 
/essai[ ; ]

Connection closed by foreign host.


 ------------------- CERN  ------------------------------ 

moulon% telnet moulon 80
Trying 192.93.96.1 ...
Connected to moulon.
Escape character is '^]'.
GET /cgi-bin/a/essai[+%3B+]?a HTTP/1.0
Accept: text/html
 
HTTP/1.0 200 Document follows
MIME-Version: 1.0
Server: CERN/2.15
Content-Type: text/plain
 
/essai[+;+]
 
Connection closed by foreign host.

 ------------------- Plexus  ------------------------------ 

moulon% telnet moulon 8001
Trying 192.93.96.1 ...
Connected to moulon.
Escape character is '^]'.
GET /cgi-bin/a/essai[+%3B+]?a HTTP/1.0
Accept: text/html
 
HTTP/1.0 200 Document follows
Date: Saturday, 19-Feb-94 15:12:34 GMT
Server: plexus/3.0j
MIME-version: 1.0
Content-type: text/plain
 
/essai[+;+]
 
Connection closed by foreign host.
moulon% 
 -------------------------------------------------------------------


Guy Decoux



From letovsky-stan@CS.YALE.EDU  Fri Feb 25 11:20:23 1994 --100
Message-Id: <199402191617.AA07355@RA.DEPT.CS.YALE.EDU>
Date: Fri, 25 Feb 1994 11:20:23 --100
From: letovsky-stan@CS.YALE.EDU (Stan Letovsky)
Subject: Mosaic Mods

[Note: this letter is part of an ongoing effort to
extend Mosaic in directions of interest to database
applications. I am copying to www-talk in case anyone
has suggestions. This issue involved how to implement
an interactive subquery (query-within-query) capability
within Mosaic.]

Kei: after the array input widget, I think the next priority
should be the minimal modifications necessary to
support a reasonable subquery capability. The scenario
I am thinking of is this, with (*#)'s indicating a need
for extension:

	o Mosaic displays entity query-form document Q.

	o User invokes subbing on a particular entity-valued
	field F, perhaps by middle-mouse buttoning it(*1). This results
	in a "sub-on-F" URL being sent to the server.

	o The server responds with the query-form for the
	F entity -- call it R.

	o User fills in R, submits. Submit-URL is
	sent to server, which responds with subquery results
	list L, each item of which has a checkbox and a link
	to the corresponding object. User selects some/all items.
	Stack is now Q R L.

	o Submit on L results in pop(L)(*2), pop(R)(*2), and assignment
	of a hidden value to F(*3) listing the selected items,
	expressed as a SQL expression. Stack is now Q.

	o Submit on Q results in the hidden SQL expression
	being included in the URL. (*4)

	o Server constructs SQL query including subquery.
	Executes query, returns results as list of links.

To do this we need 

(*1) input-widget mouse-events, specified as
<INPUT ... SELECT2=action-spec ...>
Select1, Select2, .. being different selection actions --
1-3 would be mouse buttons in a 3-button world.
Action spec is, in general, an URL or a script invocation in a form
scripting language. We can get by with just an URL for this
application.

(*2) FORM submit must be able to cause immediate actions
on the form, with or without server involvement, either
by generalizing the ACTION=URL to ACTION=action-spec
and adding a script language that can express POP and
value storage, or by going through the server and
adding these as directives in HTML. Since at this point
we want to minimize our requirements for a form scripting
language (not because this is not desirable but because
we don't want to implement it) let's take the latter approach.
The form is submitted in the normal manner, but the document
that comes back contains some new HTML commands:
	<NOPUSH>   #inhibit pushing a new document onto stack
	<POP>	     #pop current document - Mosaic "Back"
	<POP>
and
(*3)
	<PUT OBJECT="F-name" ATTRIBUTE="SQL" VALUE="SQL-expr">
			#store SQL-expr on F's SQL attribute.
This last presumes the internal hash-table for
widget attributes discussed in 
http://cgsc.biology.yale.edu/genera.html. 

(*4) The search string encoded by a FORM submit must be extended
to include widget-attributes stored in the hash table. For
now we can assume all attributes get included in the 
search string, to avoid the need for a message-spec language.
We must, however, devise an extension to the search string
=/& packaging that allows attribute (and input array item)
values to be described.

Cheers. -Stan



From joe@astro.as.utexas.edu  Fri Feb 25 11:24:10 1994 --100
Message-Id: <9402191755.AA07206@astro.as.utexas.edu>
Date: Fri, 25 Feb 1994 11:24:10 --100
From: joe@astro.as.utexas.edu (Joe Wang)
Subject: Hard copy generation of book from HTML


Is something that can take a set of HTML files and generate a "book"
from them?  Something like html2latex that follows links.





From hgs@research.att.com  Fri Feb 25 11:28:11 1994 --100
Message-Id: <9402192256.AA13481@dxmint.cern.ch>
Date: Fri, 25 Feb 1994 11:28:11 --100
From: hgs@research.att.com (Henning G. Schulzrinne)
Subject: Tools for BibTeX use with WWW

A set of tools for creating a CGI-accessible BibTeX database is
now available on ftp://gaia.cs.umass.edu/pub/hgschulz/windex-1.1.tar.Z.
(It was mentioned by Craig Steinberger recently.) It contains a
BibTeX style file and translator that converts BibTeX output into
HTML as well as an indexing program that creates a wordlist-based index
of a set of BibTeX records in one or more files. The functionality
is demonstrated on http://www.research.att.com/.
 
Henning Schulzrinne
hgs@research.att.com



From lenst@lysator.liu.se  Fri Feb 25 11:32:43 1994 --100
Message-Id: <199402200023.BAA05251@lysita>
Date: Fri, 25 Feb 1994 11:32:43 --100
From: lenst@lysator.liu.se (lenst@lysator.liu.se)
Subject: Re: Re Dan on implementation 


In message <9402170147.AA06698@ulua.hal.com>, "Daniel W. Connolly"
<connolly@hal.com> uses C as an example of a context free language and
then goes on to write:

>If we keep HTML down to a context-free language composed of regular
>tokens, then folks can write little 20-line ditties in perl, elisp,
>lex, yacc, etc. and get real work done.

Can you write a little 20-line perl program that lists the variables
of a C program?

>If we require real-time processing of all legal SGML documents,
>we buy nothing in terms of functionality, and we render almost
>all current implementations broken.

I don't think it has been suggested that browsers need to be able to 
process *all* legal SGML documents.  It is after all a specific DTD
and a specific SGML declaration.

>>| 	<!-- this: <A HREF="abc"> looks like a link too! -->
>>
>>How so?  It's in a comment, and so will be ignored by a parser.
>
>Yes, by an SMGL compliant parser, but not by any parser built
>out of standard parsing tools like regular expressions, lex, and yacc.
>(well, actually, you could do it with lex, but it's a pain...)

Recognising a comment can be done with regular expressions.  If you
have trouble making lex and yacc handle this, I don't think it is
because the limitations of lex and yacc.

>>| 	And this: a < b > c has no markup at all, even though it
>>| 	uses the "magic" < and > chars.
>>
>>But not in the magic combinations <[A-Za-z] etc.
>
>Right. The famous "delimiter in context". Contrast this with the
>vast majority of "context free" languages in use.

I will compare this with C. In C "/" is a token used for the division
operator and "*" is a token used for the multiplication operator, but
when "/" is followed by "*" it is a comment start.  This is consistent
with a "context free" language as is recognising a "<" as a start tag
opener when it is followed by a letter.


>You say "crippled", I say "expedient". Remember: the documents are
>still conforming. It's just the WWW client parser that's non-standard.

It is harder to make SGML tools produce correct HTML if HTML has a lot
of arbitrary restrictions.

--
Lennart Staflin  <lenst@lysator.liu.se>



From robm@ncsa.uiuc.edu  Fri Feb 25 11:37:08 1994 --100
Message-Id: <9402200204.AA04247@void.ncsa.uiuc.edu>
Date: Fri, 25 Feb 1994 11:37:08 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: complaint about CGI



One of the most popular (in fact, the only one I've gotten) complaints about
CGI is the fact that the non-nph script's output is stripped of everything
except Location: and Content-type:. Since stuff like content-encoding wasn't
really an issue at the time, it was okay that the server stripped them out.
In fact, the spec. states that only content-type and location are valid
output headers.

The question is, what do we do about it? Many people want me to ``fix''
httpd so that it sends any unknown headers back to the client. This isn't so
bad, and would be backward compatible, but I don't want people to depend on
it without formally changing the spec (John, Ari, what did you guys do?).

Ultimately, though, I'd like to discourage use of parsed scripts in favor of
nph scripts since they're cleaner (server doesn't touch their output). Also,
the script that goes to nph only loses two things: ease of output because it
has to check REQUEST_METHOD etc., and Location: to a file. I don't really
view these as a big loss.

So, the question is, do we change the spec., or do we educate people and
give them library functions to output the header?

--Rob




From luotonen@ptsun00.cern.ch  Fri Feb 25 11:41:01 1994 --100
Message-Id: <9402201912.AA27911@ptsun03.cern.ch>
Date: Fri, 25 Feb 1994 11:41:01 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: complaint about CGI


> The question is, what do we do about it? Many people want me to ``fix''
> httpd so that it sends any unknown headers back to the client. This isn't so
> bad, and would be backward compatible, but I don't want people to depend on
> it without formally changing the spec

I'm all for it.


> (John, Ari, what did you guys do?).

Originally I sent back all the headers, but took that off since the
spec marked this as illegal.


BTW, Rob: Content-Encoding should be Content-Transfer-Encoding :-(

-- Cheers, Ari --




From robm@ncsa.uiuc.edu  Fri Feb 25 11:49:15 1994 --100
Message-Id: <9402202309.AA10155@void.ncsa.uiuc.edu>
Date: Fri, 25 Feb 1994 11:49:15 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: complaint about CGI

/*
 * Re: complaint about CGI  by John Franks
 *    written on Feb 20,  3:07pm.
 *
 * According to Ari Luotonen:
 * > > The question is, what do we do about it? Many people want me to ``fix''
 * > > httpd so that it sends any unknown headers back to the client. This 
 * > > isn't so bad, and would be backward compatible, but I don't 
 * > > want people to depend on it without formally changing the spec
 * > 
 * > I'm all for it.
 * 
 * I am happy with either way as long as we have a clear cut spec that is
 * well publicized. It is not enough for us to agree among ourselves or
 * agree on www-talk. If we agree to change, someone (presumably Rob) has
 * to document the changes.  I think it is unacceptable if people have to
 * look at the source code of a server to understand the spec.

I agree here. One of the things which is a source of endless frustration for
me is the fact that three quarters of the HTTP spec is TBS and things seem
to change without much notice (Location->Uri).

 * > > (John, Ari, what did you guys do?).
 * > 
 * > Originally I sent back all the headers, but took that off since the
 * > spec marked this as illegal.
 * 
 * I followed the spec and take only Content-type from the script.

Okay, then the question is, do we change the spec, or encourage nph- as the
viable alternative? If we change the spec, should we make it CGI/1.1 so that
the script knows that the server will accept its headers?

 * > BTW, Rob: Content-Encoding should be Content-Transfer-Encoding :-(
 * 
 * I agree with this!  But it isn't only Rob's problem. I originally used
 * Content-Transfer-Encoding until I found that Mosaic won't accept that,
 * only Content-encoding.  I am also still campaigning for browsers which
 * handle decoding to announce that fact in an Accept-Encoding (or
 * whatever the correct name is) header.  Gif and au files may be pretty
 * well compressed already but for Postscript files, for example,
 * compression is a *big* win.  Postscript is likely to be a standard way
 * to distribute math journal articles and it would be very nice to
 * compress on the fly if the browser can handle it (or decompress if it
 * can't).
 */

Thus said the HTTP spec:

> Content-Transfer-Encoding

> As in MIME. Some profiling and/or extension may be necessary, TBS.
> (Compression is not treated as transfer encoding by MIME). 

said the HTTP spec.

As I recall, the compression encoding we implemented in Mosaic and httpd was
based on a proposal bouncing back and forth between Marc, Tony, and I. I
don't know how much basis in MIME it had (if MIME doesn't use
Content-transfer-encoding for compression, then what does it use?)

--Rob



From robm@ncsa.uiuc.edu  Fri Feb 25 11:53:41 1994 --100
Message-Id: <9402210011.AA10489@void.ncsa.uiuc.edu>
Date: Fri, 25 Feb 1994 11:53:41 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: using PGP/PEM? using Kerberos?


I'm interested in knowing if any of you out there are using the PEM/PGP
access authentication in Mosaic 2.2/NCSA httpd 1.1. I think I'm going to
change the protocol for a couple of reasons, and would like to develop a
group of people to try the changes out on. Also, there are some problems in
the client side of PEM, and the server side of PGP that need fixing. Nothing
security related, just core dumps and unrecognized spaces in entity names.

Also, I'm looking for experienced Kerberos site administrators who are
willing to try out a Kerberized version of Mosaic/httpd. No, they're not
done yet, but they will be relatively soon. As with the PGP/PEM stuff, this
should be considered experimental.

--Rob



From felicia@sprecher.cs.uwm.edu  Fri Feb 25 11:57:58 1994 --100
Message-Id: <199402210653.AAA09011@sprecher.cs.uwm.edu>
Date: Fri, 25 Feb 1994 11:57:58 --100
From: felicia@sprecher.cs.uwm.edu (Chu-Peng Cheong)
Subject: Editor info wanted.


  Did someone mention there is a emac editor for HTML?  How may I get that?

  And, does it allow me to program how I want to interface with the user?

  I am currently looking for a way to allow user to enter new documents
  in a simple and standard format.  The user group is targeted to be
  physicians MD.  Therefore, the input screen is expect to be very form
  driven and simple.

  Is there any other Editor/Form HTML editor that I should be looking into
  before spinning off to develop one?

  Does every browser expect to write its own Editor?  Who has a nice Editor
  interface that can be used as reference?

  Sorry for the thousand questions, I am just learning W3, HTML and all the
  goodies while trying to port a system to it.

  Thank you in advance.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2626 N. Frederick Ave.			felicia@sprecher.cs.uwm.edu
Milwaukee, WI 53211 			felicia@csd4.csd.uwm.edu

    "It does not matter if you win or lose, until you lose." -  Angie Papadakis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From timbl@ptpc00.cern.ch  Fri Feb 25 12:02:13 1994 --100
Message-Id: <9402211509.AA01936@ptpc00.cern.ch>
Date: Fri, 25 Feb 1994 12:02:13 --100
From: timbl@ptpc00.cern.ch (Tim Berners-Lee)
Subject: Re: Comments on MIME/SGML

Dan,

>    1. We make the following correspondence between the terms of the SGML
>    standard and the MIME RFC:
    

..
>        SGML SYSTEM identifier => MIME Content-ID

	I proposed in an earlier note that the SGML SYSTEM identifier
	should be a URI and that a special form of URI (cid:) should
	be used to specify a content identifier within a MIME
	multipart message.
	

	SGM SYSTEM identifier  => URI
	
	
	This allows SGML to reference anything in the universal
	syntax, inclusing now URLs and later URNs, and
	other parts of a MIME object using the cid: form.
	I think this is important, as external references
	are important too, and one will need to mix them.
	
	Tim BL
	



From ZAPANTIS@uvphys.phys.UVic.CA  Fri Feb 25 12:06:07 1994 --100
Message-Id: <940221130845.206001d7@uvphys.phys.UVic.CA>
Date: Fri, 25 Feb 1994 12:06:07 --100
From: ZAPANTIS@uvphys.phys.UVic.CA (Nik Zapantis, UVic Physics, Victoria BC)
Subject: Rules file for CERN server question

From:	UVPHYS::ZAPANTIS     "Nik Zapantis, UVic Physics, Victoria BC (604)721-7729" 21-FEB-1994 13:07:41.32
To:	SMTP%"www-talk@info,cern.ch"
CC:	ZAPANTIS
Subj:	Rules file for CERN server question

I am using CERN's WWW server on a VMS 5.5-2 system with MULTINET 3.2C.
I am trying to serve documents located in different directories (and disks)
but I have not been able to figure out the exact syntax for my rules file.
Or, maybe, what I am trying to do cannot be done.
Here is the rules file I am using. I have no problem http serving docs under
the /www/* directory tree. Anything outside this tree, is not getting served.

# httpd.conf
# Rules file for my WWW server
pass	/		file:/diska/www/welcome.html
pass	/*		file:/diska/www/*
map     /ftp/*		file:/diska/www/ftp/*   
map	/star/*		file:/diskb/star/*
map	/opal/*		file:/diskc/opal/*
pass	file:/diska/www/ftp/*	<-- this works ok
pass	file:/diskb/star/*	<-- does not work
pass	file:/diskc/opal/*	<-- does not work
fail	*
htbin	/diska/www/htbin


I have also tried passing the files without mapping, but it did not work either.

Any help appreciated.

thanks,
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
| Nik Zapantis                  | zapantis@uvphys     (BITNET)  |
| Dept. of Physics & Astronomy  | 45393::zapantis(HEPnet/SPAN)  |
| University of Victoria        | zapantis@uvphys.phys.UVic.CA  |
| Victoria, BC                  | Phone: (604)721-7729          |
| V8W 3P6                       | FAX:   (604)721-7715          |
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++




From sg04%kesser@gte.com  Fri Feb 25 12:10:09 1994 --100
Message-Id: <9402212150.AA08047@kesser.cisl214>
Date: Fri, 25 Feb 1994 12:10:09 --100
From: sg04%kesser@gte.com (sg04%kesser@gte.com)
Subject: Re: Idea for new form input type

The form construct has been somewhat successful in obtaining queries
for small items of information from an INTERACTIVE user.

We are now beginning to talk about queries that involve:

1. Large image files
2. Large text files
3. queries for infomation from local databases.

There needs to be mechanism, let's call it FETCH, which can
do automated retrieval from the users machine to snarf in
things that the user would not have the patience to enter
in interactively. (but he would like to act as gatemaster on
this info going out).

For example, when a 1040 arrives, one would like to be able to
have automated incluedes for charitable expenses, cap-gains 
history for the current FY.

That is, it should be less painful to include such large tabular
databases, created by external programs, into a FORMS like mechanism.

Think of this as the dual to the external viewer mechanism.

The way I would see this is that the FORM could include various
FETCH queries. There would be something simliar to a mailcap file
that would specify how to these external data queries are to
be filled in.

I have done a lot of brainstorming on these lines in my Mosaic
Accessories document: ftp:/ftp.gte.com/pub/circus/accessories.html

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Yechezkal-Shimon Gutfreund		 	   sgutfreund@gte.com [MIME]
GTE Laboratories, Waltham MA     ftp://ftp.gte.com/pub/circus/home/home.html
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=



From alb@cupido.inesc.pt  Fri Feb 25 12:14:12 1994 --100
Message-Id: <9402221058.AA16117@cupido.inesc.pt>
Date: Fri, 25 Feb 1994 12:14:12 --100
From: alb@cupido.inesc.pt (Alberto Silva)
Subject: Documentation of the WWW Library


Hello,

I'll start to orient some academic projects at the Technical Institut of
Lisbon (Portugal) around the WWW area.  We intende to make a simple
version of WinMosaic, as well as a gateway to a hypermedia browser ...

As a start point I think it's important to understand the standard WWW
library.  I would like to know if there is some documentatio  that explain 
the use of these library ?  Even documentation with some extracts of
code...


Thanks in advance.

-- Alberto Silva


---------------------------------------------------------------
Alberto M. Rodrigues da Silva
INESC - R. Alves Redol, 9-4o Sala 436 - 1000 Lisboa - Portugal
Tel	+(351) 1 3100 000 (std)   +(351) 1 3100 305 (direct line) 
Fax	+(351) 1 52 58 43         e-mail:    alb@inesc.pt 
---------------------------------------------------------------



From michael.shiplett@umich.edu  Fri Feb 25 12:18:52 1994 --100
Message-Id: <199402221725.MAA07492@totalrecall.rs.itd.umich.edu>
Date: Fri, 25 Feb 1994 12:18:52 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Authentication and Form Submittal

Hello,

  A project I'm working on needs a way to allow users to submit forms
in a secure fashion. The forms themselves may be widely distributed
and copied. The problem is when a user attempts to submit a form,
whence comes the authentication information required for the
submittal?

  The RIPEM & PEM/PGP authentication protocols for accessing restriced
files rightly place all of the information in the protocol. For form
submittal where the form would determine the authentication needed, it
seems sensible to place at least some information in the HTML source
itself, perhaps either in <HEAD> or <FORM>.

  Assuming the form uses a method of post, should the authentication
information be tied to the CGI program and *not* to the server itself?
It is, after all, the CGI program which is handling the processing of
the form.

  Has anyone worked on these issues?

michael



From connolly@hal.com  Fri Feb 25 12:23:37 1994 --100
Message-Id: <9402221822.AA10361@ulua.hal.com>
Date: Fri, 25 Feb 1994 12:23:37 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Comments on MIME/SGML 

In message <9402211509.AA01936@ptpc00.cern.ch>, Tim Berners-Lee writes:
>Dan,
>
>>    1. We make the following correspondence between the terms of the SGML
>>    standard and the MIME RFC:
>    
>
>...
>>        SGML SYSTEM identifier => MIME Content-ID
>
>	I proposed in an earlier note that the SGML SYSTEM identifier
>	should be a URI and that a special form of URI (cid:) should
>	be used to specify a content identifier within a MIME
>	multipart message.
>	
>
>	SGM SYSTEM identifier  => URI
>	
>	
>	This allows SGML to reference anything in the universal
>	syntax, inclusing now URLs and later URNs, and
>	other parts of a MIME object using the cid: form.
>	I think this is important, as external references
>	are important too, and one will need to mix them.

So far we've only been talking about MIME and SGML. Neither MIME
tools nor SGML tools currently grok URI's. I see no reason to
introduce that much new technology just to allow a MIME message to
hold multiple SGML entities.

Besides: MIME already specifies a way to reference bodies outside
the message: message/external-body. e.g.:

	Content-Type: multipart/x-sgml; boundary="cut-here";
			document=id1

	--cut-here
	Content-ID: id1
	Content-Type: text/x-sgml

	<!DOCTYPE HTML [
	<!ENTITY % htmldtd SYSTEM "id2">
	&htmldtd;
	]>
	<html>...</html>

	--cut-here
	Content-ID: id2
	Content-Type: message/external-body;
		access-type="anon-ftp";
		site="info.cern.ch";
		directory="pub/www/doc";
		name="HTML.dtd"

	Content-Type: text/x-sgml;

	
	--cut-here--

Dan



From wade@cs.utk.edu  Fri Feb 25 10:34:44 1994 --100
Message-Id: <9402181744.AA26536@honk.cs.utk.edu>
Date: Fri, 25 Feb 1994 10:34:44 --100
From: wade@cs.utk.edu (Reed Wade)
Subject: Re: Idea for new form input type 


>How do you imagine this appearing to users? 
[ stuff deleted ]
>
>Dave


I think it could be dangerous to think too much about appearance
at this point. File input is clearly useful for a number of reasons 
so it seems reasonable to make it a special type of form input.
Wedging it into the TEXTAREA input sounds like trouble.


I'd like to suggest 2 more requirements-
 * allow the user to submit an arbitrary number of files
 * preservation of the original file name, when possible

Imagine you want to provide access to a c++ compiler
or something odd like that via www.


-reed

--
wade@cs.utk.edu -- http://netlib2.cs.utk.edu/people/ReedWade.html



From BOLLINGER@galileo.arc.nasa.gov  Fri Feb 25 12:28:12 1994 --100
Message-Id: <940222104427.38c0022f@GAL.ARC.NASA.GOV>
Date: Fri, 25 Feb 1994 12:28:12 --100
From: BOLLINGER@galileo.arc.nasa.gov (Ken Bollinger -Phone 415-604-3184)
Subject: Mosaic 4 DEC Alpha VMS

Hi,

We just recieved a new DEC Alpha with open VMS and a 
POSIX shell. Is there a MOSAIC  program out there that 
will work on this system???

Any help is greatly appreciated!!

Ken



From connolly@hal.com  Fri Feb 25 12:32:20 1994 --100
Message-Id: <9402221918.AA10375@ulua.hal.com>
Date: Fri, 25 Feb 1994 12:32:20 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Virtual Pages 

In message <9402190101.AA17555@hpfcma.fc.hp.com>, Dave Hollander writes:
>
>The attached posting reminded me of another document design and maintenance
>issue. The current model for creating and accessing HTML documents
>forces one to build rather small documents that are heavly linked.
>This can be difficult to maintain particularlly in a production
>environment because the unit of display (html files) have no relationship
>to the unit of submission/processing/maintenance.

I've been thinking about this lately too... how the parts make the
whole and such... You mentioned that an "HTML document" is a "unit of
display". Actually, it was designed as a "unit of transfer" (shoot...
can't find the source of that quote). We have precedent for several
other entity/object/thingys.  Here's my personal terminology for these
things:

* Entity -- unit of data storage/transfer. A sequence of bytes
	with an associated formal interpretation.
	Examples: an SGML document is often broken into entities
	for maintenance reasons. (an entity may be used several
	times in a document, so the author assigns it a name and
	references the name each time rather than storing copies).

	WWW nodes are currently broken into the HTML source plus
	separate entities for graphics, sound, etc.

* Node (Page???) -- unit of display. With the advent of the <IMG SRC=...>
	element, there is no longer a 1-1 relationship between
	transfer and display objects. I expect this situation to
	get more complex...

	The page isn't necessarily something the information producer and
	consumer need agree on.  The producer may edit a document in the form
	of 26 different pages, but the consumer may want to see an outline
	form with the H1 and H2's from all 26 of the authors pages.

* Element -- unit of "information". "Element" is the root of a class
	hierarchy containing Documents, Messages, etc. An element
	has a type, and depending on the type, may have some
	attributes, and some content.

* Document -- unit of composition. A kind of Element.

* Message -- unit of communication. A kind of document.
	A message has an explicit author, 
	audience, and date of "publication". RFC822 messages
	additionally have a globally unique identifier.
	

The current WWW architecture (with the exception of the IMG
element...) seems roughly equivalent to the gopher model, where the
disk file is the ultimate definiton of the entity and the node. We
have these constraints that (1) HTML nodes are completely independent
-- they must contain all their context, authorship info, etc. (2)
folks should be able to maintain HTML files with a text editor, (3)
the server should be able to ship that file over the wire verbatim
without processing it, and (4) the client should be able to format and
display it in real time. This doesn't seem scalable to me. Constraints
(2) through (4) seem somewhat reasonable. It's number (1) that I'd
like to do something about.

I'd like to see an architecture where an author can compose a document
consisting of a set of nodes with common features -- perhaps a style
sheet, common navigation features ("back", "forward", "up", "top",
"index", etc.) -- without having to store the features in each disk
file. I've heard of some folks using the C preprocessor as a solution
to this problem! Ackk! Thptptptp!

Take the GNN web for and example -- I bet it's a nightmare to
maintain!. It seems that the GNN editors should be able to compose one
SGML document containing lots of little HTML entity files. Then an
SGML parser could validate the ID's and IDREFS of all the
intra-document links, as well as the structure of the document.

Hmmm... I have to noodle on this one for a little while.

Dan






From strata@fenchurch.MIT.EDU  Fri Feb 25 12:37:19 1994 --100
Message-Id: <CMM.0.90.0.761948393.strata@fenchurch>
Date: Fri, 25 Feb 1994 12:37:19 --100
From: strata@fenchurch.MIT.EDU (M. Strata Rose)
Subject: MacBinary question


Forgive me if this has been discussed in detail, I've been wading through
the messages every week or so and trying to keep on top of things.  I just
put up the NCSA httpd on a Sun for a consulting client, the latest available
one from ftp.ncsa.uiuc.edu (/pub/Mosaic/http/current or somesuch).

We want to put up multimedia files which were created with Macromind's
Director package.  They have been saved as Player files, ie they are
self contained and do not need Director to be present on the client Mac
to run.  I put up both .bin MacBinary files and .hqx files but
cannot retrieve either successfully with Mac Mosaic 1.0.1.  A document is
created on the desktop if I specify "load to disk" and click on the MacBinary
version, but the document has no appropriate resource fork and merely asks me
if I want to open it with TeachText.  Clicking on the .hqx file displays it
as ASCII text, and if I specify "load to disk" saves it as ASCII and again
asks me if I want to open TeachText on it.

Is there something specific that I need to be doing either to the server
or the Mac Mosaic client to get this to work?  I am a little unclear even
after reading the online docs as to whether I need to explicitly create some
mapping in one of the .conf files between ".bin" and "Mac Binary"-- I made
sure it was in the client's profile, but perhaps the server needs to tell the
client as well?

Thanks for any assistance,
_Strata


M. Strata Rose
Unix & Network Consultant, SysAdmin & Internet Information 
Virtual City Network (tm)
strata@virtual.net | strata@hybrid.com | strata@fenchurch.mit.edu



From connolly@hal.com  Fri Feb 25 12:41:36 1994 --100
Message-Id: <9402222221.AA10417@ulua.hal.com>
Date: Fri, 25 Feb 1994 12:41:36 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: How many parsers does it take...?


.. to build a WWW client?

* one for SGML (well, enought SGML to parse HTML...)
* one for MIME (well, headers anyway... there doesn't seem to be any
	support for MIME multipart body stuff.)
* one for each kind of URL that you support, i.e
	one for file:, one for gopher:, one for wais: ...)
* one for each authorization method you support (basic, pubkey, ...)

Now, my question is: why?

I've done enough arguing against SGML syntax to find out that there is
a sincere commitment to SGML as an interchange format in the WWW
community. Nuff said.

The motivation for MIME comes from the successful and pervasive
deployment of the internet mail and news applications. Much of the
data sent around the net uses RFC822 format, and MIME is the accepted
way to put multipart/multimedia stuff in RFC822 format.

Now what motivates separate parsers for each kind of URL and access
method? Why is libWWW riddled with blurbs of code that copies data
from a structure to a string (be careful to escape all the right
chars!) and passes the string to another routine that parses the data
back out of the string (unescaping...) into another structure? I don't
see sufficient motivation for this strategy.

Contrast it with the WAIS strategy of adopting the Common Lisp
print/read strategy: there's one printer, one reader, one supported
internal structure, and other structures can be supported without all
sorts of parsing and escaping.

I've long argued against the current URL syntax in favor of using the
SGML parser, but SGML is unnecessarily verbose for the task. I think
the Common Lisp syntax is just right.

Consider the introduction of a new URL scheme "alternative" used to
point a client to several copies of a resource an allow the client to
choose the "closest." e.g:

	alternative:escaped-url1,escaped-url2,escaped-url3,...

We're faced with inventing a new syntax and a whole new set of parsers
(one for libWWW, one for perl, one for elisp...) for this scheme. On
the other hand, suppose we used Common Lisp syntax:

	(:alternative url1 url2 ulr3)

for example:

	(:alternative (:local-file "/austin2/users/connolly/home.html"
				 "austin2.hal.com")
			(:http "austin2.hal.com:8001"
				"/~connolly/home.html"))

We could also do things like support cannonical forms and alternate
forms so that ftp://info.cern.ch/pub/www/src/foo.html could be
written in any of the following ways:

	(:ftp (:site "info.cern.ch") (:dir "/pub/www/src") (:name "foo.html"))
	(:ftp "foo.html" "/pub/www/src" "info.cern.ch")
	(:ftp "/pub/www/src/foo.html" "info.cern.ch")
	(:ftp (:path "pub" "www" "src" "foo.html") "info.cern.ch")

The nice thing about supporting Common Lisp style printing/parsing is
that the structure represents a superset of MIME and SGML
printing/parsing!

One could clearly implement the MIME parser as a special kind of Lisp
parser which, on seeing:

	From: "Daniel W. Connolly" <connolly@hal.com>
	To: www-talk@info.cern.ch
	Subject: example
	Content-Type: multipart/mixed; boundary="cut-here"
	
	--cut-here
	Content-Type: text/html

	<html> ... </html>

	--cut-here
	Content-Type: image/gif
	Content-Transfer-Encoding: base64

	234k23j4oij234lkj234lkj

	--cut-here

would return the same thing the Common Lisp parser would return on
seeing:

(:part
 (:head
  (From (:mbox "connolly@hal.com" "Daniel W. Connolly"))
  (To (:mbox "www-talk@info.cern.ch"))
  (Subject "example")
  (Content-Type (multipart mixed (:boundary "cut-here")))
 )
 (:body
  (:part
   (:head
    (Content-Type (text html))
   )
   "<html> ... </html>"
  )
  (:part
   (:head
    (Content-Type (image gif))
    (Content-Transfer-Encoding base64)
   )
   (:any 10034 "10034 decoded bytes from 234k23j4oij234lkj234lkj")
  )
 )
)

And the SGML parser would act on:

	<HTML><HEAD><BASE HREF="http://host/dir/file.html">
		<TITLE>Example</TITLE>
		</HEAD>
		<BODY>Example of &lt;SGML&gt; stuff</BODY>
	</HTML>

just as the Lisp parser would act on:

	(HTML ()
	 (HEAD ()
	  (BASE ((HREF (:http (:host "host")
				 (:path "dir" "file.html")))
		))
	  (TITLE () "Example") )
	 (BODY ()
	  "Example of " "<" "SGML" ">" " stuff")
	)
		

The libWWW code is already migrating from the style of:

	char *tmp = escape_url_in_attr(url);
	char *tmp2 = escape_text_in_cdata(text);
	sprintf(buffer, "<A HREF=\"%s\>%s</A>", tmp, tmp2);
	free(tmp); free(tmp2);
	SGML_parse(HText, buffer);

to the style of:

	StartTag(HText, "A",
		 "HREF", url_string,
		 NULL);
	Data(HText, text);
	EndTag(HText, "A");

The current primitives:

	obj.startTag(tag_name, attrs...);
	obj.data(string);
	obj.entity(name);
	obj.endTag(tag_name);

aren't bad, but they're awkward for things like MIME and WAIS WSRC
files. I suggest the new base class:

class LispStructured{
public:
	void atom(const Atom* atom);
	void string(const char *null_terminated_string);
	void bytes(size_t length, unsigned char *length_bytes);
	void start(const Atom* tag);
	void end();
}



From dsr@hplb.hpl.hp.com  Fri Feb 25 12:45:48 1994 --100
Message-Id: <9402231032.AA00531@manuel.hpl.hp.com>
Date: Fri, 25 Feb 1994 12:45:48 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Virtual Pages

Dave Hollander writes:

> The approach used in the SDL is to have a document be a collection
> of virtual pages where a virtual page is much like today's HTML document.
> However, the virtual page collection can be managed as a single entity,
> and sytles sheets or other doc set information only needs to be 
> instanced once.

> In addition, it provides a comfortable place to place an index, table
> of contents, glossary, etc.

For existing browsers, HTML nodes are standalone. I have been thinking about
this for some time as is evident from the various drafts for HTML+. You can
use the LINK element in the document head to define relationships between
nodes, for instance: index, table of contents, glossary, etc. You can also
define parent->child relationships and use these to imply LINK elements in
child nodes. This is also applicable to hypertext paths. I have recently
added the SRC attribute to LINK in a bid to allow authors to use images
for links which are intended appear in a document specific toolbar. I hope
to provide a practical demonstration of these ideas at the WWW Conference
in May. Here are some of the relationship types that have been proposed:

    o   REL="Subdocument"   (<A> and <LINK> elements ?)

Used by a parent document to specify child nodes. This relation can be used
to propagate properties of the parent node to their children, e.g. Style
Sheets and Contents "pages". (use REV attribute for child->parent links).

    o   REL="Contents"      (<LINK> element)

Used to specify a node which acts as a document contents "page". May be
inherited from a parent document as described above.

    o   REL="StyleSheet"    (<LINK> element)

Used to specify an associated style sheet. May be inherited from a parent
document as described above.

    o   REL="Path"          (<A> element)

Used in a node specifying a hypertext path to insert a path defined in
another node.

    o   REL="Node"          (<A> element)

Used in a node specifying a hypertext path to specify a node on that path.

    o   REL="Previous"      (<LINK> element)

Used in a node to specify the previous node on a hypertext path. This is
implied when a separate node is acting as a hypertext path

    o   REL="Next"          (<LINK> element)

Used in a node to specify the next node on a hypertext path. This is implied
when a separate node is acting as a hypertext path

    o   REL="Bookmark"      (<LINK> element)

This is used to specify interesting bookmarks. These are labelled via the
TITLE attribute or as images using the SRC attribute.
        
Dave Raggett



From dmh@hpfcma.fc.hp.com  Fri Feb 25 12:52:47 1994 --100
Message-Id: <9402231628.AA08432@hpfcma.fc.hp.com>
Date: Fri, 25 Feb 1994 12:52:47 --100
From: dmh@hpfcma.fc.hp.com (Dave Hollander)
Subject: Re: Virtual Pages 


Hi,

I am still not sure that I like the idea of so many files; I will have to
think about that for a while. I do agree that we need to have a defined
unit (I'll use node) that, in this case, is the unit of transport and 
display. I reference to Dan's concerns for context, it is at the 
node level that we must make sure that the context variables are
either well defined or non-existent. 

However, I am not sure that the physical storage model has to be the
same as the node model. There could be an assumption that the server
will do something to break apart node sets and deliver them one
at a time.

What parameters of a physical storage model are relevant to the web?
I think that this model should cater to the information providers as
much as possible and require that the server/browser developers 
provide the bridge between their needs and the "surfer".

Anyway, some specific comments follow...


> Dave Raggett writes:
> 
> For existing browsers, HTML nodes are standalone. I have been thinking about
> this for some time as is evident from the various drafts for HTML+. You can
> use the LINK element in the document head to define relationships between
> nodes, for instance: index, table of contents, glossary, etc. You can also
> define parent->child relationships and use these to imply LINK elements in
> child nodes. This is also applicable to hypertext paths. I have recently
> added the SRC attribute to LINK in a bid to allow authors to use images
> for links which are intended appear in a document specific toolbar. 

I would like to see this src attribute (and shape?) available as a 
"button" (I believe that hytime calls/used to call this an endterm) 
to allow a graphics to be consistently associated with a link. 

This assumes that the "A" element will be extended to include an IDREF to 
a link element; I believe has been recommended before to help with
link maintenance.

BTW...the version of the HTML+ dtd available via the web
(tp://15.254.100.100/pub/htmlplus.dtd.txt) does not have the src
attribute.



Also, if there is to be some "significant" values for the rel attributes
then that attribute should be an enumerated list. If technology
prevents a list, then at least document the values within the
DTD. Otherwise, no two browsers will behave the same way and the
feature becomes useless for transportable documents.


Dave Hollander




From dsr@hplb.hpl.hp.com  Fri Feb 25 12:56:51 1994 --100
Message-Id: <9402231910.AA01167@manuel.hpl.hp.com>
Date: Fri, 25 Feb 1994 12:56:51 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Virtual Pages

Dave Hollander writes:

> However, I am not sure that the physical storage model has to be the
> same as the node model. There could be an assumption that the server
> will do something to break apart node sets and deliver them one
> at a time.

Right now the # suffix is used for links to parts of documents, and
browsers adjust the display position to show the referenced location.
Servers could choose to return just the referenced part e.g. when
the requested ID identifies a DIVn element rather than say a paragraph.
This would offer a relatively clean approach to keeping nodes in a
single file. It requires that current browsers are adapted to search
for ID attributes rather than just the NAME attribute on the <A> element.

>> I have recently added the SRC attribute to LINK in a bid to allow authors
>> to use images for links which are intended appear in a document specific
>> toolbar. 

> I would like to see this src attribute (and shape?) available as a 
> "button" (I believe that hytime calls/used to call this an endterm) 
> to allow a graphics to be consistently associated with a link. 

> This assumes that the "A" element will be extended to include an IDREF to 
> a link element; I believe has been recommended before to help with
> link maintenance.

Currently documents use HREF and SRC attributes to specify documents
and inline images respectively as URLs. Alternative attributes are needed
if we wish to support HyTime CLINKs - this needs to be discussed at
the WWW Conference in May.

> BTW...the version of the HTML+ dtd available via the web
> (tp://15.254.100.100/pub/htmlplus.dtd.txt) does not have the src
> attribute.

I have updated this and added parameter entities to specify the set
of relationship types. The set needs further discussion as little use
has yet been made of these.

Dave Raggett



From dcmartin@library.ucsf.edu  Fri Feb 25 13:01:16 1994 --100
Message-Id: <199402232111.AA26824@library.ucsf.edu >
Date: Fri, 25 Feb 1994 13:01:16 --100
From: dcmartin@library.ucsf.edu (David C. Martin)
Subject: Online Britannica

FYI

dcm
------- Forwarded Message
From: Bruce Miller at UCSDLIBRARY
Date: 2/23/94 7:41AM
Subject: FYI: Online Britannica

The Library is participating in the beta test of the Online Britannica, the 
networked version of the Encyclopaedia Britannica.  The Online Britannica will 
be an information resource on InfoPath and is also available via direct access 
to the UCSD network.  Currently access is available only through workstations 
that can support the Mosaic client interface (for Macintosh, PCs with Windows, 
and X terminals).  Line mode access (for terminals and PCs that emulate 
terminals) will be supported shortly.  

Development is being done by the EB Advanced Technology Group in La Jolla.  
Professor Rik Belew in Computer Science and Engineering is part of the 
development team and is using the Online Britannica as a research testbed.  
Part of his research will result in direct links between the bibliographic 
citations in the Britannica and the bibliographic records in ROGER.  

Ronnie Coates is coordinating public implementation for the Library.  Get in 
touch with her if you have any questions.  Training for staff and public 
instruction will be developed shortly.  Library Systems Department and Academic 
Computing Services Office of Network Operations staff are providing technical 
support to the Advanced Technology Group.  ONO is responsible for campus 
support of client software (which includes Mosaic) and can handle questions 
regarding access to and use of Mosaic.

-- Bruce Miller



From mohta@necom830.cc.titech.ac.jp  Fri Feb 25 13:05:42 1994 --100
Message-Id: <9402240347.AA08016@necom830.cc.titech.ac.jp>
Date: Fri, 25 Feb 1994 13:05:42 --100
From: mohta@necom830.cc.titech.ac.jp (Masataka Ohta)
Subject: Re: Selecting Languages

>   The documents (on the web) will be written in many languages. I
> think selecting an appropriate language is a very important issue.

There was similar proposal in ietf-822 ML on MIME.

>   We are now testing "selecting languages" by the "Accept-Language"
> with http server. I think It can be performed by using mime-type, too.

The proposal used "Content-Language:" header.

>   Suppose a person want to read all information in English.  If
> default of that pages is written in Japanese, he needs to search
> anchors to the English pages or English version of that pages ( or
> learn reading Japanese :-)).
>   Furthermore, I think auto-switching is crucial for autopilot type
> clients(e.g., knowbot).

The problem is how to express language.

There is ISO 639 two letter langage code (not contry code) but it does
not contain a lot of languages.

So, whether it should be extended or not and how can it be extended
is, I think, the largest issue.

Do you want to distinguish Queen's and American English?

						Masataka Ohta



From atotic@ncsa.uiuc.edu  Fri Feb 25 13:09:52 1994 --100
Message-Id: <9402240404.AA20682@void.ncsa.uiuc.edu>
Date: Fri, 25 Feb 1994 13:09:52 --100
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: subscribe

subscribe



From wei@sting.Berkeley.EDU  Fri Feb 25 13:13:49 1994 --100
Message-Id: <9402240410.AA09585@sting.Berkeley.EDU>
Date: Fri, 25 Feb 1994 13:13:49 --100
From: wei@sting.Berkeley.EDU (Pei Y. Wei)
Subject: ViolaWWW beta release is available

Hi--

The new ViolaWWW is now available for ftp'ing. It's beta and feedback
is very welcomed. The README file follows...


===========================================================================
ViolaWWW, Version 3.0 Beta                              Feb 23 1994
==========================

ViolaWWW is an extensible World Wide Web hypermedia browser for XWindows.

Based on and drawing from the Viola scripting language and toolkit,
ViolaWWW provides a way to build relatively complex hypermedia applications
that are beyond the provisions of the current HTML standard.


Notable features in the new ViolaWWW
------------------------------------

* HTML+ support: container paragraphs; input forms; tables.
  (Note: not yet exact and complete compliance with the currently
   evolving HTML+ standard)

* In addition to the currently defined HTML+, there're a few other
  extensions, such as for columning and document insertion (client side).

* Embeddable in-document and in-toolbar programmable viola objects.
  A document can embed mini viola applications (ie: a chess board),
  or can cause mini apps to be placed in the toolbar.

* Motif front-end. The X11 (non Motif) version is also available.

* Single binary for easy installation, unlike the old ViolaWWW which
  required setup of various viola application files.


Availability
------------

Source and binary can be found in ftp://ora.com/pub/www/viola.
Sparc binary is supplied.

I'd appreciate hearing about compilation success stories (and patches)
for platforms other than SunOS and Ultrix.


Compiling and running ViolaWWW
------------------------------

To compile, run the 'BUILD' script (or 'BUILD.decstation').

This generate two binaries: viola/src/viola/viola and viola/src/vw/vw.

Right now, you should use the binary 'vw' as it is the ViolaWWW with
the Motif GUI, which is currently the more polished front-end.
But do let us know if you'd like support on the non Motif version.

Note: Viola requires X11R5, and the Motif version of ViolaWWW requires
a Motif toolkit library. The libwww(2.14) that comes with ViolaWWW is
modified, so until the changes are integrated into CERN's, don't use
the libwww that didn't come with viola.


Contact
-------

You can send mail to viola@ora.com. Feedbacks, bug reports, patches,
constructive criticisms, etc, are always welcome.


Acknowledgements
----------------

In particular, many thanks to: Tim Bernners-Lee for inventing the WWW;
Tim and the CERN crew for the libwww; All the contributors on www-talk etc;
Jon Blow (ORA) for the lexical analyzer used in viola; Scott Silvey (ORA)
for creating the Motif front-end; Terry Allen (ORA) for lots of testings
and help with SGML; Dale Dougherty and Tim O'Reilly for supporting the
work on viola at ORA.


                                Pei Y. Wei (wei@ora.com)
                                O'Reilly & Associates, Inc.




From m.koster@nexor.co.uk  Fri Feb 25 13:18:03 1994 --100
Message-Id: <9402240935.AA03187@dxmint.cern.ch>
Date: Fri, 25 Feb 1994 13:18:03 --100
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Important: Spiders, Robots and Web Wanderers


Hi all,

Don't worry, this is not a debate about "are robots good or bad".

Recognising that robots exist and will never go away, I have
setup a page devoted to gathering as much info about active 
robots as possible: http://web.nexor.co.uk/mak/doc/robots/robots.html
(Please use this exact URL for all accesses).

It contains codes of practice for robot writers, a list of all known
robots in use, and most importantly a proposed standard that will
allow WWW server maintainers to indicate if they want robots to access
their server, and if so which parts.

This proposed standard doesn't require any server/client/protocol
changes, and can provide a partial solution to problems caused by
robots. I am inviting comments on it, but I do hope we can keep the
discussion focused, and not degenerate in a "robots are good/bad"
discussion that won't be resolved.

Robots are one of the few aspects of the web that cause operational
problems and cause people grief. At the same time they do provide very
useful services. This standard should minimise the problems and may
well maximise the benefits, so I think we need to sort this out as
soon as possible. The major robot writers are in favour of this idea,
so I don't see any fundamental problems.

-- Martijn

PS: I do hope this gets out; www-talk as been empty for days...
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From m.koster@nexor.co.uk  Fri Feb 25 13:22:02 1994 --100
Message-Id: <9402240952.AA06601@dxmint.cern.ch>
Date: Fri, 25 Feb 1994 13:22:02 --100
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Referer


Just to say a public Thank You to Lou Montulli for implementing the
Referer field in the latest Lynx.

I have changed my logging to list Referer (and "From") and I have
changed my logging analyser to maintain an inverse map of pages that
refer to my server, which lets me build up a better picture of my
users.

But more importantly, today it has allowed me to track down a
potentially annoying problem (where people accessed the wrong server)
with a simple grep through my logs. This sort of thing has cost me
hours to find before, and now it cost only seconds.

I hope other User-agents (oh dear, I'm starting to _speak_ HTTP now :-()
implementors will follow his example and don't ignore these simple
basic details in favour of the latest sexy user-interface features.

Looking at the User-agent field is interesting too actually. I've 
found 65 different versions of browsers in use, including ones
I've never heard of. 

In case you're interested, these files are available on 
http://web.nexor.co.uk/reports?*

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From dsr@hplb.hpl.hp.com  Fri Feb 25 13:26:57 1994 --100
Message-Id: <9402241148.AA03243@manuel.hpl.hp.com>
Date: Fri, 25 Feb 1994 13:26:57 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Virtual Pages

Peter Flyn writes:

> (I don't like the idea of hardcoding <P> numbers, but let it stand for
> the moment). What is going to be crucial for many applications is to
> have not only the stuff numbered locally, eg

> 3. Marketing aspects

> 3.1 text text text

> 3.2 ...

> but numbered _in context_, so that if this particular <div2> occurs
> within a <div1> numbered "4", it will appear

> 4.3 Marketing aspects

> 4.3.1 text text text

> 4.3.2 ...

> or at least, to have this structural-numerical framework retrievable,
> maybe in a structure window like some SGML editors do.

> Has anyone any pointers as to how we might achieve this?

The obvious approach is to use an element in the document head to
initialize the starting numbers. This could also be used to specify
that paragraphs or maybe just headers are to be numbered. That way
you wouldn't have to place an attribute on each <P> element.

   e.g. <NUMBER FROM="4.7" WHICH="H1 H2 H3 H4 H5 H6 P">

Where the which attribute is a namelist of tag names. The from
attribute also implies a context for DIVs in which the current node
fits as part of a larger document. The NUMBER element could be implicit
when following Subdocument links.

Dave Raggett



From dolesa@smtp-gw.spawar.navy.mil  Fri Feb 25 13:32:06 1994 --100
Message-Id: <9401247621.AA762109685@smtp-gw.spawar.navy.mil>
Date: Fri, 25 Feb 1994 13:32:06 --100
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: Archie & http, & PC Sound files


     I have http 1.1 running and have applied c-archie-1.4.1-FIXED.tar.Z 
     and AA-1.2.tar.Z.  Mosaic Windows 2.0 crashes when I try to call the 
     AA.html document.  Is everyone else experiencing this also?  Using 
     Windows 1.0, I can get the simple input, vs form, and it works fine 
     (when I upgraded to the FIXED version of c-archie...)
     
     Also, I have been working on a way to play sounds back on the PC 
     without a sound card... (Good luck, huh?)  Anyways, I have a pc driver 
     that will allow me to play back .wav & .au sound files thru the built 
     in speaker.  When I convert a file and call it from Mosaic, I have it 
     so it will launch sound recorder, but the sound file isn't loaded when 
     the app pops up.  When I download the .wav file to disk, I am able to 
     manually load and play the sound file.  Why won't it just load into 
     the application and play (Or does anyone know of an App that will...)?
     
     Help appreciated.
     
                                Andre'



From djkahle@MIT.EDU  Fri Feb 25 13:40:51 1994 --100
Message-Id: <9402241810.AA16972@marinara.MIT.EDU>
Date: Fri, 25 Feb 1994 13:40:51 --100
From: djkahle@MIT.EDU (djkahle@MIT.EDU)
Subject: Library Form


Would anyone with some programming experience be willing to help the
GNA Library develop a mosaic form (or any form type) that would enable
students to submit library reference questions directly to the appropriate GNA
librarians? 






From wei@xcf.Berkeley.EDU  Fri Feb 25 13:45:38 1994 --100
Message-Id: <9402241938.AA12001@xcf.Berkeley.EDU>
Date: Fri, 25 Feb 1994 13:45:38 --100
From: wei@xcf.Berkeley.EDU (Pei Y. Wei)
Subject: ViolaWWW beta release is available

Hi--

The new ViolaWWW is now available for ftp'ing. It's beta and feedback
is very welcomed. The README file follows...


===========================================================================
ViolaWWW, Version 3.0 Beta                              Feb 23 1994
==========================

ViolaWWW is an extensible World Wide Web hypermedia browser for XWindows.

Based on and drawing from the Viola scripting language and toolkit,
ViolaWWW provides a way to build relatively complex hypermedia applications
that are beyond the provisions of the current HTML standard.


Notable features in the new ViolaWWW
------------------------------------

* HTML+ support: container paragraphs; input forms; tables.
  (Note: not yet exact and complete compliance with the currently
   evolving HTML+ standard)

* In addition to the currently defined HTML+, there're a few other
  extensions, such as for columning and document insertion (client side).

* Embeddable in-document and in-toolbar programmable viola objects.
  A document can embed mini viola applications (ie: a chess board),
  or can cause mini apps to be placed in the toolbar.

* Motif front-end. The X11 (non Motif) version is also available.

* Single binary for easy installation, unlike the old ViolaWWW which


Availability
------------

Source and binary can be found in ftp://ora.com/pub/www/viola.
Sparc binary is supplied.

I'd appreciate hearing about compilation success stories (and patches)
for platforms other than SunOS and Ultrix.


Compiling and running ViolaWWW
------------------------------

To compile, run the 'BUILD' script (or 'BUILD.decstation').

This generate two binaries: viola/src/viola/viola and viola/src/vw/vw.

Right now, you should use the binary 'vw' as it is the ViolaWWW with
the Motif GUI, which is currently the more polished front-end.
But do let us know if you'd like support on the non Motif version.

Note: Viola requires X11R5, and the Motif version of ViolaWWW requires
a Motif toolkit library. The libwww(2.14) that comes with ViolaWWW is
modified, so until the changes are integrated into CERN's, don't use
the libwww that didn't come with viola.


Contact
-------

You can send mail to viola@ora.com. Feedbacks, bug reports, patches,
constructive criticisms, etc, are always welcome.


Acknowledgements
----------------

In particular, many thanks to: Tim Bernners-Lee for inventing the WWW;
Tim and the CERN crew for the libwww; All the contributors on www-talk etc;
Jon Blow (ORA) for the lexical analyzer used in viola; Scott Silvey (ORA)
for creating the Motif front-end; Terry Allen (ORA) for lots of testings
and help with SGML; Dale Dougherty and Tim O'Reilly for supporting the
work on viola at ORA.


                                Pei Y. Wei (wei@ora.com)
                                O'Reilly & Associates, Inc.




From BACIEWJ@albany.albany.edu  Fri Feb 25 13:54:25 1994 --100
Message-Id: <9402242042.AA11917@dxmint.cern.ch>
Date: Fri, 25 Feb 1994 13:54:25 --100
From: BACIEWJ@albany.albany.edu (joe baciewicz)
Subject: Vm client

Hello
 I am one of the lucky people who reside on an IBM 3084 running vm. We cannot
telnet anywhere (no vt100 emmulation) so we need a vm client for www. I
ftp'd the vm binaries but they die because I believe we do not run XA mode.
Does anyone have a WWW client that runs in standard vm mode, I would really
appreciate any ideas or help. Is there a vm genius who can also tell me why
tnvt100 bombs big time when I try to use it to telnet to info.cern.ch?

Joseph Baciewicz                                 Niskayuna School District
Internet    baciewj@uacsc2.albany.edu            1626 Balltown Road
Computer Science                                 Niskayuna, NY, 12309



From kaehms@sedbsvr.se.ssd.lmsc.lockheed.com  Fri Feb 25 13:58:44 1994 --100
Message-Id: <9402242101.AA14947@eagle.is.lmsc.lockheed.com>
Date: Fri, 25 Feb 1994 13:58:44 --100
From: kaehms@sedbsvr.se.ssd.lmsc.lockheed.com (Bob Kaehms)
Subject: Mosaic update?

yooo, NCSA...
Quiet in here... hows the updates for the PC and MAC coming?  Will the forms
be out shortly? helloooooo...




From john@math.nwu.edu  Fri Feb 25 11:45:08 1994 --100
Message-Id: <9402202107.AA29940@hopf.math.nwu.edu>
Date: Fri, 25 Feb 1994 11:45:08 --100
From: john@math.nwu.edu (John Franks)
Subject: Re: complaint about CGI

According to Ari Luotonen:
> > The question is, what do we do about it? Many people want me to ``fix''
> > httpd so that it sends any unknown headers back to the client. This isn't so
> > bad, and would be backward compatible, but I don't want people to depend on
> > it without formally changing the spec
> 
> I'm all for it.

I am happy with either way as long as we have a clear cut spec that is
well publicized. It is not enough for us to agree among ourselves or
agree on www-talk. If we agree to change, someone (presumably Rob) has
to document the changes.  I think it is unacceptable if people have to
look at the source code of a server to understand the spec.

> 
> > (John, Ari, what did you guys do?).
> 
> Originally I sent back all the headers, but took that off since the
> spec marked this as illegal.
> 

I followed the spec and take only Content-type from the script.

> 
> BTW, Rob: Content-Encoding should be Content-Transfer-Encoding :-(
> 

I agree with this!  But it isn't only Rob's problem. I originally used
Content-Transfer-Encoding until I found that Mosaic won't accept that,
only Content-encoding.  I am also still campaigning for browsers which
handle decoding to announce that fact in an Accept-Encoding (or
whatever the correct name is) header.  Gif and au files may be pretty
well compressed already but for Postscript files, for example,
compression is a *big* win.  Postscript is likely to be a standard way
to distribute math journal articles and it would be very nice to
compress on the fly if the browser can handle it (or decompress if it
can't).


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu




From appel@cih.hcuge.ch  Fri Feb 25 15:48:49 1994 --100
Message-Id: <1676*/S=appel/OU=cih/O=hcuge/PRMD=switch/ADMD=arcom/C=ch/@MHS>
Date: Fri, 25 Feb 1994 15:48:49 --100
From: appel@cih.hcuge.ch (Ron D. Appel)
Subject: RE: versatile proxy code

Would anybody be kind enough to tell me where I can find some description on 
what is a proxy?

Thanks

-------------------------------------------------------------------------
| Ron D. Appel                             | Tel.:   (+41 22) 372 6264  |
| Hopital Cantonal Universitaire de Geneve | Fax.:   (+41 22) 372 6198  |
| Centre d'Informatique Hospitaliere       | e-mail: appel@cih.hcuge.ch |
| 24, rue Micheli-du-Crest                 |   (S=appel;OU=cih;O=hcuge; |
| CH-1211 Geneve 14                        |    P=switch;A=arcom;C=ch)  |
| Switzerland                              |                            |
|                                                                       |
|  Try the ExPASy Molecular Biology Server at http://expasy.hcuge.ch/   |
-------------------------------------------------------------------------



From junga@informatik.tu-muenchen.de  Fri Feb 25 16:13:14 1994 --100
Message-Id: <1994Feb25.122544.23658@Informatik.TU-Muenchen.DE>
Date: Fri, 25 Feb 1994 16:13:14 --100
From: junga@informatik.tu-muenchen.de (Achim Jung)
Subject: Trouble with w3get on HP: gethostname not available


Hi!

I tried running the script w3get on a HP9000/720. But I can`t get it run,
because it`s using a systemcall which isn`t available on HP-UX :-(

Undefined subroutine "ftp'SYS_gethostname" called at ftplib.pl line 36.

Has anybody got a workaround for this? (sorry, but I don`t speak perl)

Ciao, Achim
-------------------------------------------------------------------
Achim Jung        IRC: Flops        junga@informatik.tu-muenchen.de
WWW: http://www.informatik.tu-muenchen.de/personen/junga/junga.html




From nikos@cbl.leeds.ac.uk  Fri Feb 25 16:20:19 1994 --100
Message-Id: <3559.9402251244@cblelca.cbl.leeds.ac.uk>
Date: Fri, 25 Feb 1994 16:20:19 --100
From: nikos@cbl.leeds.ac.uk (nikos@cbl.leeds.ac.uk)
Subject: Re: Virtual Pages


Dave Hollander writes:

>I am still not sure that I like the idea of so many files; I will have to
>think about that for a while. I do agree that we need to have a defined
>unit (I'll use node) that, in this case, is the unit of transport and 
>display. I reference to Dan's concerns for context, it is at the 
>node level that we must make sure that the context variables are
>either well defined or non-existent. 
>
>However, I am not sure that the physical storage model has to be the
>same as the node model. There could be an assumption that the server
>will do something to break apart node sets and deliver them one
>at a time.

This view of separating the physical storage from the node model
is also supported by the experience of using conversion packages.
One reason they are attractive is that source documents are created and 
maintained as single entities containing enough information for an
automated process to extract the nodes and link them in a sensible
and uniform way. 

The same could be done for large HTML documents if there were 
appropriate tags to specify the node structure from within 
one or more physical files.

Such tags would be useful for HTML editors and conversion packages
because they could all use the same  back-end filter (or rely on a gateway,
or a server, or a client) to decide how to create (or serve or display)
each node. 

Nikos.


--
Nikos Drakos			
Computer Based Learning Unit   	nikos@cbl.leeds.ac.uk
University of Leeds		http://cbl.leeds.ac.uk/nikos/personal.html




From decoux@moulon.inra.fr  Fri Feb 25 16:26:52 1994 --100
Message-Id: <9402251348.AA02191@moulon.moulon.inra.fr>
Date: Fri, 25 Feb 1994 16:26:52 --100
From: decoux@moulon.inra.fr (ts)
Subject: Authentication and Form Submittal


>   Assuming the form uses a method of post, should the authentication
> information be tied to the CGI program and *not* to the server itself?
> It is, after all, the CGI program which is handling the processing of
> the form.
> 
>   Has anyone worked on these issues?

 When I update an Oracle database, server make authentication and must call
the script with the process uid of the authenticated user.

 Script use the process uid to open the database and check the real access
to the database.


Guy Decoux






From imd1707@ggr.co.uk  Fri Feb 25 12:48:57 1994 --100
Message-Id: <Pine.3.89.9402221642.A12516-0100000@uk0x04>
Date: Fri, 25 Feb 1994 12:48:57 --100
From: imd1707@ggr.co.uk (Ian Dunkin)
Subject: Re: Proxy Servers (and SOCKS)

On Wed, 16 Feb 1994 11:37:28 Tim Berners-Lee <timbl@ptpc00.cern.ch> wrote:

> > Each client application will have to decide when to proxy and when not to.

    [...]

> Yes.  I think that a simple and common case will be that
> anything within a certain single domain will be local access.

The SOCKS proxy mechanism has already addressed this one, but its
solution uses a configuration _file_, which allows more elbow room for
fine adjustment than environment variables -- where I can see that your
space can quickly get cluttered..

SOCKS allows you to define which networks (specified by numeric network
addresses and mask combinations) should be accessed directly, and which
networks should be reached via particular proxy servers (you can have
more than one proxy, if you wish, each serving particular network
areas).

Why bother with this fine control?   

Well: SOCKS comes at this specifically from the side of providing
services through _Firewalls_. 

Think of a typical, real Firewall configuration, where a bastion host on
a restricted `DMZ' segment between the internal networks and the
Internet aims to provide proxy service for internal clients.  It can
instigate connections outwards on behalf of client systems, but it
cannot instigate connection inwards, into the internal net. 

This is what SOCKS was designed for.  It already provides all the
service transport and access control stuff.  It needs the fine control
over which connections go via a proxy on the bastion system for this
reason: it's not simply that it's less _efficient_ to send connections
to internal hosts via a proxy, unnecessarily.  Rather, if a connection
does get passed unnecessarily to the proxy it will _not be possible_ for
it to connect inwards to the local host at all.  The connection will
fail. 

This is why I think that SOCKS and the clever Montulli/Luotonen/Altis
scheme are complementary, and not competetive: the M/L/A scheme seems to
come more from the direction of providing _connectivity_ for particular
WWW services, not exclusively for use with Firewalls. 

SOCKS has been getting established in use for providing WWW access through
firewalls.  As you'll know, X Mosaic now ships with hooks to link in the
SOCKS library; Aleks Totic has mentioned on the `mmosaic-fire' mailing
list that Mac Mosaic is to do the same; and SOCKSized versions of Lynx,
and even of the CERN httpd have been described on the `socks' lists.  All
of these involve hooks patched into the build of versions of the libwww
code.  Is there any interest at CERN in hooking SOCKS into the `master'
WWW library, to centralise all this effort and do away with the
duplication? 

    I.

--
Ian Dunkin <imd1707@ggr.co.uk>
--



From altis@ibeam.jf.intel.com  Fri Feb 25 13:50:22 1994 --100
Message-Id: <m0pZmJa-00042uC@ibeam.intel.com>
Date: Fri, 25 Feb 1994 13:50:22 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: support of default document names

Currently, there is no standard way to determine a default name to use for
a document retrieved on the Web. Clients prompt the user for a filename or
make up some temporary name that doesn't correspond to the original item.
This makes saving a document more difficult than it needs to be. In
addition, servers send the Last-modified information for a document, so why
not the document name as well. The "path" part of an URL is supposed to be
opaque to the client, though we could obviously use the element after the
last slash as a default name, this certainly won't work once we have URNs,
which will be opaque. The simple solution would be to follow the MIME
Content-Type addition of "name=" in the HTTP server response, so that
instead of a response like:
        Content-type: image/gif
a server would return:
        Content-type: image/gif; name="test.gif"

This works for multi-part MIME messages, and also allows the server to send
other Content-type information back to the client such as those described
by Tony Sanders at <http://www.bsdi.com/HTTP:TNG/MIME-ClientProfile.html>.

ka





From sanders@BSDI.COM  Fri Feb 25 17:08:11 1994 --100
Message-Id: <199402251602.KAA06321@austin.BSDI.COM>
Date: Fri, 25 Feb 1994 17:08:11 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Virtual Pages 

Dave Hollander writes:
> I am still not sure that I like the idea of so many files; I will have to

Nobody says you have to have a lot of files.  If you want to store all
the data in one file, then you just need something on the server end that
will extract the requested page and return it.  This is what a filesystem
does for you, but nothing is preventing you from doing it the other way.

--sanders



From stripes@uunet.uu.net  Fri Feb 25 14:03:25 1994 --100
Message-Id: <9402242141.AAwerm05236@rodan.UU.NET>
Date: Fri, 25 Feb 1994 14:03:25 --100
From: stripes@uunet.uu.net (Josh Osborne)
Subject: Re: Selecting Languages

[...]
>Do you want to distinguish Queen's and American English?

Not really.  However if we can, I really want the browsers to take
a _list_ of acceptable languages (that way I can specify both Englishes).



From kims@ncsa.uiuc.edu  Fri Feb 25 17:24:58 1994 --100
Message-Id: <9402251621.AA25381@void.ncsa.uiuc.edu>
Date: Fri, 25 Feb 1994 17:24:58 --100
From: kims@ncsa.uiuc.edu (Kim Stephenson)
Subject: Re: Mosaic update?

>yooo, NCSA...
>Quiet in here... hows the updates for the PC and MAC coming?  Will the forms
>be out shortly? helloooooo...

The Mac team has begun work on NCSA Mosaic 2.0.  Things are coming along
quite nicely so far..... but as there is quite a bit of work to do it's going to
be a bit before we can release.   No definate dates yet.

Stay tuned.

Kim Stephenson

Software Development Group
National Center for Supercomputing Applications





From dsr@hplb.hpl.hp.com  Fri Feb 25 18:46:48 1994 --100
Message-Id: <9402251740.AA06763@manuel.hpl.hp.com>
Date: Fri, 25 Feb 1994 18:46:48 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Virtual Pages

Nikos Drakos writes:

> The same could be done for large HTML documents if there were 
> appropriate tags to specify the node structure from within 
> one or more physical files.

> Such tags would be useful for HTML editors and conversion packages
> because they could all use the same  back-end filter (or rely on a gateway,
> or a server, or a client) to decide how to create (or serve or display)
> each node. 

HTML+ includes two relevant tags:

   DIV1 to DIV6         these define divisions starting from each header
                        and may be inferred from the headers alone

   LINK                 used to define inter-node relationships

You can use the DIVs to divide up a large document into divisions
which can then be extracted and built into smaller nodes. The LINK
elements are then useful to specify the relationships between the
resultant nodes.

Dave Raggett



From dsr@hplb.hpl.hp.com  Fri Feb 25 15:58:36 1994 --100
Message-Id: <9402251121.AA04926@manuel.hpl.hp.com>
Date: Fri, 25 Feb 1994 15:58:36 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Idea for new form input type

>> One way of combining the two ideas is for the TEXTAREA widget to show
>> a file menu in addition to the vertical and horizontal scrollbars.
>> Surely this more general approach would be better than a simple file
>> name widget?

> Are you really comfortable with dragging an executable or image file
> into a TEXTAREA window.  The conception of dropping binary data into
> a text window seems very odd.  Not to mention the fact that the
> implementation would be very difficult.  What do you do if someone
> has typed in data and then drops in a binary file?  Also, all current
> implementations of TEXTAREA would have to be completely rewritten.
> (you can't simply use a standard text widget anymore)

I think that we will need to generalise forms to accept a whole
variety of input and the right way to go is to make the widgets
smarter rather than forcing users to carefully select between a
set of fields to find the right one for the current type of object.
Right now you can enter good ol' text, but pretty soon when all
mailers and usenet viewers support multimedia, users will want
the same out of www forms too.

This *will* mean writing new widgets since current operating systems
weren't designed with this kind of thing in mind - but then thats
progress for you ... :-)

I hope to see you at the WWW Conference and look forward to a good
debate on next steps for forms then.

Cheers,

Dave Raggett



From dsr@hplb.hpl.hp.com  Fri Feb 25 15:53:42 1994 --100
Message-Id: <9402251107.AA04912@manuel.hpl.hp.com>
Date: Fri, 25 Feb 1994 15:53:42 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Idea for new form input type

> This approach would not address the problem of non text files very well.
> Should a CGI script have to be prepared to handle a MS Word document as
> well as ASCII text from a text area field?

> Should the form allow the specification of a desired/required MIME content
> type for the file to be named?

> Should there be a mechanism for specifying the MIME content type of the
> file to be sent?

Yes this is essential and provides the means for encapsulating form contents
as a multipart MIME message. CGI scripts will be able to then process the
data accordingly. MIME messages can have nested parts, so the TEXTAREA field
would map to a nested multipart message when multiple objects have been
"dropped" into that field.

This idea and the Lou's simpler file name field needs to be discussed at the
WWW Conference in May along with other ideas for extending forms.

Dave Raggett



From luotonen@ptsun00.cern.ch  Fri Feb 25 16:07:48 1994 --100
Message-Id: <9402251158.AA01059@ptsun03.cern.ch>
Date: Fri, 25 Feb 1994 16:07:48 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: Rules file for CERN server question


> I am using CERN's WWW server on a VMS 5.5-2 system with MULTINET 3.2C.
> I am trying to serve documents located in different directories (and disks)
> but I have not been able to figure out the exact syntax for my rules file.
> Or, maybe, what I am trying to do cannot be done.
> Here is the rules file I am using. I have no problem http serving docs under
> the /www/* directory tree. Anything outside this tree, is not getting served.
> 
> # httpd.conf
> # Rules file for my WWW server
> pass	/		file:/diska/www/welcome.html
> pass	/*		file:/diska/www/*
> map     /ftp/*		file:/diska/www/ftp/*   
> map	/star/*		file:/diskb/star/*
> map	/opal/*		file:/diskc/opal/*
> pass	file:/diska/www/ftp/*	<-- this works ok
> pass	file:/diskb/star/*	<-- does not work
> pass	file:/diskc/opal/*	<-- does not work
> fail	*
> htbin	/diska/www/htbin

Rules are translated from top to bottom; first matching Pass, Fail, Exec
or HTBin [HTBin is an obsolite form of Exec, but VMS version doesn't have
Exec yet] will terminate rule translations.

In your case you have

	pass /* file:/diska/www/*

which will map _everything_ to diska, and pass it, so further rules
are never reached.

Your config file should read:

# httpd.conf
# Rules file for my WWW server
htbin	/diska/www/htbin
pass	/		file:/diska/www/welcome.html
pass	/ftp/*		file:/diska/www/ftp/*   
pass	/star/*		file:/diskb/star/*
pass	/opal/*		file:/diskc/opal/*
pass	/*		file:/diska/www/*
fail	*

It's important to have pass /* ... as the last one.

Cheers,
--
Ari Luotonen		 |
World-Wide Web Project	 |
CERN			 | phone: +41 22 767 8583
CH - 1211 Geneve 23	 | email: luotonen@dxcern.cern.ch



From dsr@hplb.hpl.hp.com  Fri Feb 25 16:03:10 1994 --100
Message-Id: <9402251134.AA04942@manuel.hpl.hp.com>
Date: Fri, 25 Feb 1994 16:03:10 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Indented <MENU>s

Marc writes:

>> Then why don't you just implement one of the many style sheet
>> proposals that are on the table.  This would pretty much solve the
>> problem if done correctly.

> So then I get to tell people, "Well, you get to learn *this* language
> to write your document, and then you get to learn *that* language for
> actually making your document look like you want it to."  Oh, they'll
> love that.

> Marc's viewpoint: style sheets are an artificial construct inflicted
> on us because of the whole non-presentation philosophy we've been
> using (more or less, but enough to keep this particular set of
> problems alive), which I argue is wholly inappropriate for document
> delivery front-ends and is crippling our system.  Why not strip out a
> level of complexity (and user headeaches) by having our front ends
> simply handle a layout format suitable from the ground up for
> front-end display of documents?  Publishers can still use SGML out the
> wazoo on the back end if they want; if they don't and all they want is
> documents that look the way they want them to look, we don't inflict
> it on them.

I believe there is room for style sheets as a means for giving publishers
tight control over appearence, especially when printing documents out.
However, I am also looking at ways of including style attributes in
the document head and other elements to give authors a simpler means
of differentiating their documents. This would cover:

    o   background color and texture (watermarked paper)

    o   text color, font family and relative sizes

    o   vertical and horizontal spacing for each element

    o   use of images for corporate logo's and navigation
        buttons in a document specific toolbar separate
        from the document window area

What Marc is forgetting to mention is that most markup formats were
designed with paper in mind and assume fixed page sizes. We are
offering users much greater control with resizable windows, and
preference selections for document appearence. The most important
factor though is the far greater degree of platform independence
that is achieved through a logical representation than is achieved
with existing formats like Postscript and RTF. We will be able to
offer equivalent control over fonts via URN based shareable resources.

Dave Raggett





From FisherM@is3.indy.tce.com  Fri Feb 25 21:25:05 1994 --100
Message-Id: <2D6E8438@MSMAIL.INDY.TCE.COM>
Date: Fri, 25 Feb 1994 21:25:05 --100
From: FisherM@is3.indy.tce.com (Fisher Mark)
Subject: Re: Idea for new form input type


I really like this MIME-based form input type.  Implementation would save me 
from years of future maintenance on a program I am developing now to allow 
users to submit reference documents to a WWW-based repository.  This is 
definitely a cleaner way to handle submission of whole files/documents.

I would think as far as the new widget goes, it would:
1. Initialize a MIME multipart object;
2. Put typed-in text as plain text; and
3. Put dropped files as encoded contents;
building up each part as it goes along.  Because (as I understand X :() each 
part would correspond to a separate message or set of messages to the widget 
(files would be "dropped-object" messages while text would be "keyboard 
character" messages (oversimplification)), the widget should just be able to 
sort this out as it processes along.  Note that I don't say this will be 
easy ab initio (before anyone has written the first widget) -- just that it 
will be possible...
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From altis@ibeam.jf.intel.com  Fri Feb 25 21:31:02 1994 --100
Message-Id: <m0pa93m-00041aC@ibeam.intel.com>
Date: Fri, 25 Feb 1994 21:31:02 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: Proxy Servers (and SOCKS)

At 12:48 PM 2/25/94 +0000, Ian Dunkin wrote:
>On Wed, 16 Feb 1994 11:37:28 Tim Berners-Lee <timbl@ptpc00.cern.ch> wrote:
>
>> > Each client application will have to decide when to proxy and when not to.
>
>    [...]
>
>> Yes.  I think that a simple and common case will be that
>> anything within a certain single domain will be local access.
>
>The SOCKS proxy mechanism has already addressed this one, but its
>solution uses a configuration _file_, which allows more elbow room for
>fine adjustment than environment variables -- where I can see that your
>space can quickly get cluttered..

The current proxy reliance does have some setup weak points, but I would
prefer that the setup not become as complex as SOCKS. The two issues to
address are:
1. Being able to specify one or more proxy servers to use. While the
environment variable is simple, it doesn't allow for the case of a proxy
server going down or for any kind of load balancing across proxy servers. A
configuration file or environment variable listing multiple servers
available for use would be more appropriate. A configuration list might
associate particular proxy servers to be used with particular domains. This
would allow a single configuration list to be used across an entire
organization.
2. Clients should not have to proxy all requests for a particular protocol,
though some sites may choose to do this. The simple case as Tim mentioned
is for "local" requests (within your domain) not to be proxied. This can be
done via a simple string compare by the WWW client. This case can actually
be handled by an environment variable setting. The more complex case may
require a SOCKS style configuration list. There is also the possibility of
wanting to go through a separate proxy for "local" requests to take
advantage of caching by your local proxy. This would imply a "protocol -
domain - proxy server" kind of mapping.

Most sites that require the use of a proxy would not need anything more
than the ability to specify a proxy for a particular protocol, plus a
domain or domains to not proxy. I need specific examples if this doesn't
work for your site so we can understand your needs.

>This is why I think that SOCKS and the clever Montulli/Luotonen/Altis
>scheme are complementary, and not competetive: the M/L/A scheme seems to
>come more from the direction of providing _connectivity_ for particular
>WWW services, not exclusively for use with Firewalls.

Actually, the proxy scheme is intended to be a complete replacement for
SOCKS in the case of Web clients (reliance on HTTP), while SOCKS can
continue to work for telnet and special ftp clients. There are numerous
reasons for the SOCKS replacement, but I don't want to drag all of the
reasons out on this list.

There has been a lot of confusion about the proxy, which is mostly my fault
for printing the t-shirts :-) before doing the documentation (old
programmer habit). I knew the idea was sound, but I didn't think this
initial beta would be so widely used so quickly. Much credit goes to Ari,
Lou, etc. that it all works so well. I will be working on preliminary
documentation of the proxy this weekend and will announce the URL location
next week.

ka





From imd1707@ggr.co.uk  Fri Feb 25 23:04:16 1994 --100
Message-Id: <Pine.3.89.9402252107.A16477-0100000@uk0x04>
Date: Fri, 25 Feb 1994 23:04:16 --100
From: imd1707@ggr.co.uk (Ian Dunkin)
Subject: Re: Proxy Servers (and SOCKS)

On Fri, 25 Feb 1994, Kevin Altis wrote:

> Most sites that require the use of a proxy would not need anything more
> than the ability to specify a proxy for a particular protocol, plus a
> domain or domains to not proxy. I need specific examples if this doesn't
> work for your site so we can understand your needs.

Kevin,  Thanks for your reply (and the new mechanism!)

If it allows for domains (plural: we use internal servers across several
different domains) they yes, it would fit _my_ site's needs. 

> Actually, the proxy scheme is intended to be a complete replacement for
> SOCKS in the case of Web clients (reliance on HTTP), while SOCKS can
> continue to work for telnet and special ftp clients. There are numerous
> reasons for the SOCKS replacement, but I don't want to drag all of the
> reasons out on this list.

I'd be genuinely interested in them.  You see, for me an advantage of
using SOCKS for all services is that it does give a unity of access
control and configuration.  Adding further mechanisms for particular
protocols adds overheads in this respect and requires additional software
on a firewall host, where one wants as little as possible.  Let people
choose SOCKS if they want.

Mind you, they can: After quick experiment, I found that if I SOCKSize a
CERN httpd, and direct the debug Win-Mosaic's proxy requests at that, I
can use my existing SOCKS transport well enough.  (Nasty, eh?  :-)

    I.

(Tentative question: Have you thought of the `Firewalls' mailing list for
canvassing further, non-WWW opinion?)



From satie3@netcom.com  Fri Feb 25 23:38:27 1994 --100
Message-Id: <199402252235.OAA15605@netcom9.netcom.com>
Date: Fri, 25 Feb 1994 23:38:27 --100
From: satie3@netcom.com (David Parker)
Subject: Re: Idea for new form input type

>Dave,
>>Lou,
>>
>>> In order to support the posting of large files using form based
>>> machanisms I suggest that a new input type "include-file" (or
>..
>>> The reasons this will be useful are as follows:
>>> * large text files can be sent without having to cut and paste
>>>   them into a textarea window as they are now.
>>> * no memory limits on the size of the file.  (currently all input
>>>   data is held in memory)
>>> * arbitrary binary files can be sent
>>
>>Seems good to me, I guess smart browser could also support drag 'n drop.
>>Tim Berners-Lee suggested a while back now, that you should be able to
>>paste (or drag 'n drop) arbitary data into a TEXTAREA field with the
>>browser being responsible for managing the encapsulation when sending
>>it to the server, and also how to present the pasted data to the user.
>>The browser could default to showing the file name and size if it doesn't
>>have any other way of displaying the file/object in the widget.
>>
>>One way of combining the two ideas is for the TEXTAREA widget to show
>>a file menu in addition to the vertical and horizontal scrollbars.
>>Surely this more general approach would be better than a simple file
>>name widget?
>>
>>Dave
>
>This approach would not address the problem of non text files very well.
>Should a CGI script have to be prepared to handle a MS Word document as
>well as ASCII text from a text area field?
>
>Should the form allow the specification of a desired/required MIME content
>type for the file to be named?
>
>Should there be a mechanism for specifying the MIME content type of the
>file to be sent?
====
Rob:

I agree that there is a urgent need for an 'include-file' feature.  As an 
example, I have been beating my head to find a way to reference and 
download self-extracting files.  I _do not_ want to open it or filter it
in any way - only save it locally from a file server.  At this time, I know
of no way to do it.  I think include-file feature would cover this.

This does not have to 'smart.' It could be as simple as a reference to the 
file like the existing URL.

One possible script for this could look like, <A HREF=includefile://...> and
<A HREF=includehttp://...>

There should NOT be a MIME content type for the file: it is too complicated
and defeats the nature of the transfer one is trying to achieve (anonymous
file types).

Regards,

David Parker                                                    408/285-7718
Database Specialist                                       satie3@netccom.com
Tandem Computers. Inc.                             Parker_David_F@tandem.com



From mkrause@maestro.mitre.org  Sat Feb 26 00:01:51 1994 --100
Message-Id: <9402251759.ZM6109@maestro>
Date: Sat, 26 Feb 1994 00:01:51 --100
From: mkrause@maestro.mitre.org (Mark Krause)
Subject: WAIS -> WWW Gateway and Titles

Has anyone developed a WAIS to WWW gateway that will present a
user-friendly title (i.e. the text between <TITLE> and </TITLE>)
instead of the sometimes cryptic URL that is currently presented
to the user?

-- 
Mark A. Krause                  mkrause@mitre.org
The MITRE Corporation           Mail Stop W273
7525 Colshire Drive             (703)883-7642 (Voice)
McLean, VA 22102                (703)883-6809 (Fax)



From phillips@cs.ubc.ca  Sat Feb 26 00:26:33 1994 --100
Message-Id: <7569*phillips@cs.ubc.ca>
Date: Sat, 26 Feb 1994 00:26:33 --100
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: CGI stuff

[ A bunch of www-talk messages just came in; was there a general problem? ]

Guy Decoux says:
>Apparently, CERN Daemon, NCSA Daemon and Plexus don't give the same
>"PATH_INFO" when there is a "+" in the pathname.
>
>Who are right ?
[example with GET /cgi-bin/a/essai[+%3B+]?a deleted ]

Oh dear, I'm afraid the answer is none of them.  In this case,
PATH_INFO should be:

	/essai[+%3B+]

The server should not un-escape PATH_INFO.  Otherwise, things
like GET /cgi-bin/a/path%2Finfo do a very wrong thing.  I just wrote
on this in more detail, see http://www.cs.ubc.ca/spider/phillips/CGI-muse
I think some bug fixing needs to be done, but we should clarify these
details in the spec, first.


Rob McCool said:

>One of the most popular (in fact, the only one I've gotten) complaints about
>CGI is the fact that the non-nph script's output is stripped of everything
>except Location: and Content-type:.

I also concur with the solution of just letting the headers through.
Happily, that change can go along with escaping fixups.

For CGI/1.1 we may want to consider npa (no parse anything) scripts which
get fed everything the client sends and talk directly back to the client.
It's a punt, but it'll at least let script authors do anything
(authentication, experimental things, etc.) until a new version of
CGI catches up with new HTTP/1.0 protocol features.

			-- George



From phillips@cs.ubc.ca  Sat Feb 26 00:41:56 1994 --100
Message-Id: <7571*phillips@cs.ubc.ca>
Date: Sat, 26 Feb 1994 00:41:56 --100
From: phillips@cs.ubc.ca (George Phillips)
Subject: support of default document names

altis@ibeam.jf.intel.com said:
>Currently, there is no standard way to determine a default name to use for
>a document retrieved on the Web. Clients prompt the user for a filename or
>make up some temporary name that doesn't correspond to the original item.
..
>which will be opaque. The simple solution would be to follow the MIME
>Content-Type addition of "name=" in the HTTP server response, so that
>instead of a response like:

As you said, using the base name of the URL path works reasonably for
URLs.  lynx does that, but Mosaic for X doesn't.  Until more browsers
do this simple thing, I can't see us getting far with "name=" (which
is a fine idea).

I made an attempt at making Mosaic use the basename, but the !Y#^%@&
Motif file dialog doesn't seem to have any clue about default save
names.  Maybe some Motif wizard could kludge it in, but the best
alternative I came up with was a "save as foo.gif" button.  Haven't
got around to implementing it.  I can certainly see why X Mosaic
doesn't do "this simple thing" :-)

Alternately, one could add a "Document-Name: " header.  Same difference,
but maybe an extra header is more MIMEish.



From jonm@ncsa.uiuc.edu  Sat Feb 26 00:47:18 1994 --100
Message-Id: <9402252343.AA03958@void.ncsa.uiuc.edu>
Date: Sat, 26 Feb 1994 00:47:18 --100
From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
Subject: Re: Mosaic update?

At 02:00 PM 2/25/94 --100, Bob Kaehms wrote:
>yooo, NCSA...

yooo, lockheed...

>Quiet in here... hows the updates for the PC and MAC coming?  Will the forms
>be out shortly? helloooooo...

hellooo?  Mosaic for Windows 2.0a1 has been out for a couple of weeks with
initial form support.  Alpha 2 will be coming out in the next couple of
days with the rest of forms and many many bug fixes and some simple UI
enhancements. 

BTW, as a commercial company, just how much has lockheed donated to the
WWW/Mosaic effort that gives you the right to the be quite so sarcastic?
I personally feel that it is amazing how fast (~9 months) 1.5 people have 
turned out windows mosaic...especially since we all spend half our time 
answering mail/etc...

-Jon

---


Jon E. Mittelhauser (jonm@ncsa.uiuc.edu)
Research Programmer, NCSA                          (NCSA Mosaic for MS Windows)
More info <a href="http://www.ncsa.uiuc.edu/SDG/People/jonm/jonm.html">here</a>




From kaehms@sedbsvr.se.ssd.lmsc.lockheed.com  Sat Feb 26 01:06:47 1994 --100
Message-Id: <9402260002.AA03896@eagle.is.lmsc.lockheed.com>
Date: Sat, 26 Feb 1994 01:06:47 --100
From: kaehms@sedbsvr.se.ssd.lmsc.lockheed.com (Bob Kaehms)
Subject: Re: Mosaic update? (fwd)

> BTW, as a commercial company, just how much has lockheed donated to the
> WWW/Mosaic effort that gives you the right to the be quite so sarcastic?
I guess since Jon's complaining, I should appologize to the entire group.
No sarcasm intended.  Just yelling out to a group that had been quiet for 2
days. (sheesh, it was only 2 lines, and if you only knew of the the grass roots
efforts I've been involved with to try and get you guys funding... hopefully 
others in other "commercial" organizations are doing similar work)




From kaehms@sedbsvr.se.ssd.lmsc.lockheed.com  Sat Feb 26 01:33:51 1994 --100
Message-Id: <9402260029.AA04998@eagle.is.lmsc.lockheed.com>
Date: Sat, 26 Feb 1994 01:33:51 --100
From: kaehms@sedbsvr.se.ssd.lmsc.lockheed.com (Bob Kaehms)
Subject: WAIS -> WWW Gateway and Titles (fwd)

> 
> Has anyone developed a WAIS to WWW gateway that will present a
> user-friendly title (i.e. the text between <TITLE> and </TITLE>)
I've got one.  You use PATH_INFO to set things like the Title.  Right now
I have a problem with letting it out on the net, and am trying to figure
out a way to get around the legal hassles.  Am thinking of writing a little
article for some technical pub where I can just release the code.  Corporations
encourage publications, discourages sharing software.  Anyone else out there
have this problem, and or solutions?





From altis@ibeam.jf.intel.com  Sat Feb 26 01:40:59 1994 --100
Message-Id: <m0paD22-00041aC@ibeam.intel.com>
Date: Sat, 26 Feb 1994 01:40:59 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: support of default document names

At  3:39 PM 2/25/94 -0800, George Phillips wrote:
It is sad about Motif and the save as dialog, but then that's Motif. Other
window managers, plus Windows, Mac, and the numerous other browsers
appearing don't have that problem with saving. One would hope we don't ever
cripple part of the Web architecture just because you can't implement a
feature cleanly under Unix, Motif, etc. :)

>Alternately, one could add a "Document-Name: " header.  Same difference,
>but maybe an extra header is more MIMEish.

Since the name="" is already used with MIME Content-type: headers, there's
no sense in inventing a new metainformation field. Also, I don't think the
extra metainformation field would work with a multipart MIME message, but I
do know that Content-type: is always going to be there for each piece of a
multipart MIME message, so the name="" method works regardless.

I would also expect that if you were to mail an HTML document to me from
your browser, the browser would put name="" info. on the Content-type:
header so my mail program would have a clue. When your browser mailed a
multipart MIME message with an TEXT, HTML, and PostScript version of the
document, each Content-type: would be labeled appropriately.

ka





From altis@ibeam.jf.intel.com  Sat Feb 26 01:51:44 1994 --100
Message-Id: <m0paDCy-00041aC@ibeam.intel.com>
Date: Sat, 26 Feb 1994 01:51:44 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: Mosaic update?

At 12:47 AM 2/26/94 +0000, Jon E. Mittelhauser wrote:
>I personally feel that it is amazing how fast (~9 months) 1.5 people have
>turned out windows mosaic...especially since we all spend half our time
>answering mail/etc...

Yeah. This time last year, the concept of a WWW browser for Windows was a
hershey bar in the back pocket of Thomas Bruce :) Now, we have two browsers
for Windows. I'm impressed. Your MIS department (or ours for that matter)
if they even excepted the job would probably be in the late design stages
about now, without a single mangy line of code to show. A few weeks ago, I
nagged the NCSA folks about supporting the proxy code, which luckily, Lou
supplied, and to my amazement, they actually added it before I could nag
them again.

As always, I stand in awe of the browser and server writers among us. I for
one, owe you a lot of beers, but you have to come to Portland, Oregon to
get it. ;-)

ka





From kevinh@eit.COM  Sat Feb 26 05:25:33 1994 --100
Message-Id: <199402260423.UAA16180@kmac.eit.com>
Date: Sat, 26 Feb 1994 05:25:33 --100
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re:  WAIS -> WWW Gateway and Titles

> From: mkrause@maestro.mitre.org (Mark Krause)
> 
> Has anyone developed a WAIS to WWW gateway that will present a
> user-friendly title (i.e. the text between <TITLE> and </TITLE>)
> instead of the sometimes cryptic URL that is currently presented
> to the user?

	I have; you can see the results at http://www.eit.com/.
Unfortunately I haven't released the code just yet (I want to check out
the new freeWAIS), and I want to add some new features (currently you
can specify different indexes/ports in the URL for the WAIS server,
so you can search multiple databases through it).
	Also, I believe you can get a similar gateway that
Ari Luotonen at CERN wrote - it's somewhere on http://www.cern.ch/.
	Aloha,

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://www.eit.com/)
Hypermedia Industrial Designer * Duty now for the future!



From ain@ee.kyungpook.ac.kr  Mon Feb 28 10:17:26 1994 --100
Message-Id: <9402270455.AA17500@ee.kyungpook.ac.kr>
Date: Mon, 28 Feb 1994 10:17:26 --100
From: ain@ee.kyungpook.ac.kr (ain@ee.kyungpook.ac.kr)
Subject: Mosaic


Would you let me receive the information on NCSA Mosaic ?




From hitoaki@mahler.NTT.JP  Mon Feb 28 10:23:10 1994 --100
Message-Id: <9402271335.AA08708@mahler.ntt.jp>
Date: Mon, 28 Feb 1994 10:23:10 --100
From: hitoaki@mahler.NTT.JP (Hitoaki Sakamoto)
Subject: Re: Selecting Languages 


Hi Masataka,

  I haven't seen you for a long time.... :-)

>> The problem is how to express language.

  I see.

>> There is ISO 639 two letter langage code (not contry code) but it does
>> not contain a lot of languages.
>>
>> So, whether it should be extended or not and how can it be extended
>> is, I think, the largest issue.

  Let me see. But ISO 639 is resonable for the time.  Do you 
think about it? 

>> Do you want to distinguish Queen's and American English?

  It is diffcult for me to make exact distinctions between
both languages. ;-< 

-hitoaki



From hitoaki@mahler.NTT.JP  Mon Feb 28 10:27:31 1994 --100
Message-Id: <9402271355.AA08785@mahler.ntt.jp>
Date: Mon, 28 Feb 1994 10:27:31 --100
From: hitoaki@mahler.NTT.JP (Hitoaki Sakamoto)
Subject: Re: Selecting Languages 


>> [...]
>> >Do you want to distinguish Queen's and American English?
>> 
>> Not really.  However if we can, I really want the browsers to take
>> a _list_ of acceptable languages (that way I can specify both Englishes).

  Our home page(URL: http://www.ntt.jp/) was written by English and
Japanese. If you prefer English at our home page, then you can read
English all the time in the our server.

  And please look at the typical example for this problem. "Radio
JAPAN: broadcasting information" 
(URL: http://www.ntt.jp/japan/NHK/broad/intro/) has 22 languages, 
so you can see 15 languages...

-----
Hitoaki Sakamoto (hitoaki@mahler.ntt.jp)
Communication Switching Systems Laboratories.
Nippon Telegraph and Telephone Corporation.



From mohta@necom830.cc.titech.ac.jp  Mon Feb 28 10:32:09 1994 --100
Message-Id: <9402271644.AA19754@necom830.cc.titech.ac.jp>
Date: Mon, 28 Feb 1994 10:32:09 --100
From: mohta@necom830.cc.titech.ac.jp (Masataka Ohta)
Subject: Re: Selecting Languages

> Hi Masataka,
> 
>   I haven't seen you for a long time.... :-)

For more than a year. Will you go Seattle?

> >> So, whether it should be extended or not and how can it be extended
> >> is, I think, the largest issue.
> 
>   Let me see. But ISO 639 is resonable for the time.  Do you 
> think about it? 

It depends on applications.

ISO 639 is enough for the coarsest approximation, and can be used
for a while until extension mechanism is developed based on
real needs and experiences, I think.

> >> Do you want to distinguish Queen's and American English?
> 
>   It is diffcult for me to make exact distinctions between
> both languages. ;-< 

The difference is minor and ISO 639 nor its proposed extension does not
distinguish them.

But, UNIX command spell distinguishes them, at least.

						Masataka Ohta



From P.Lister@cranfield.ac.uk  Mon Feb 28 10:41:01 1994 --100
Message-Id: <9402280921.AA02097@xdm039.ccc.cranfield.ac.uk>
Date: Mon, 28 Feb 1994 10:41:01 --100
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Re: using PGP/PEM? using Kerberos?

> Also, I'm looking for experienced Kerberos site administrators who are
> willing to try out a Kerberized version of Mosaic/httpd. No, they're not
> done yet, but they will be relatively soon. As with the PGP/PEM stuff, this
> should be considered experimental.

Me, me, me, me! Actually I'm using Plexus, but I could probably bring myself 
to try httpd if it had Kerberos. Anyone Kerberised Plexus yet?

Peter Lister                             Email: p.lister@cranfield.ac.uk
Computer Centre, Cranfield University    Voice: +44 234 754200 ext 2828
Cranfield, Bedfordshire MK43 0AL UK        Fax: +44 234 750875
--- Another great idea from the man who brought you beer milkshakes. ---




From wmperry@indiana.edu  Mon Feb 28 10:36:37 1994 --100
Message-Id: <m0pax1U-000011C@monolith>
Date: Mon, 28 Feb 1994 10:36:37 --100
From: wmperry@indiana.edu (William M. Perry)
Subject: Proposal for standard icons/symbols

>Some time ago there was discussion on this list about defining a set of
>standard icons for things like Gopher types, "home" buttons, etc.  The
>discussion didn't reach a conclusion. Below is a proposal.  Reactions
>please!

  I think this is a great start!  I just implemented this in my emacs
browser this evening :)  For those of you with an image-capable browser,
check out http://cs.indiana.edu/elisp/w3/icons.html - this is a list of 47
entities and the corresponding graphics from bert's ISO-bitmaps and
Hughes-icons files, plus a few drawn from scratch or taken from the faces
archive at cs.indiana.edu.

  How about this for a list:

&archive;, &audio;, &binary.document;, &binhex.document;, &caution;,
&clock;, &compressed.document;, &disk.drive;, &diskette;, &display;,
&document;, &fax;, &filing.cabinet;, &film;, &fixed.disk;, &folder;,
&form;, &ftp;, &glossary;, &home;, &image;, &index;, &keyboard;, &mail;,
&mail.in;, &mail.out;, &map;, &mouse;, &network;, &next;, &notebook;,
&parent;, &previous;, &printer;, &scheduler;, &stop;, &summary;, &symlink;,
&telephone;, &telnet;, &text.document;, &tn3270;, &toc;, &trash;,
&unknown.document;, &uuencoded.document;, &workstation;

-Bill P.



From rodw@cbl.leeds.ac.uk  Mon Feb 28 13:56:19 1994 --100
Message-Id: <9868.9402281253@cblelcc.cbl.leeds.ac.uk>
Date: Mon, 28 Feb 1994 13:56:19 --100
From: rodw@cbl.leeds.ac.uk (rodw@cbl.leeds.ac.uk)
Subject: [Q] tk ismap editor

Hello,

I remember seeing a mention of a simple editor written with tcl/tk to 
create ismaps.  I can't seem to find a pointer to it online so does
anyone know where its available for ftp or the email of the author.

Thanks for your help,

Rod
----
Roderick Williams          R.J.Williams@cbl.leeds.ac.uk



From dolesa@smtp-gw.spawar.navy.mil  Mon Feb 28 14:11:18 1994 --100
Message-Id: <9401287624.AA762451043@smtp-gw.spawar.navy.mil>
Date: Mon, 28 Feb 1994 14:11:18 --100
From: dolesa@smtp-gw.spawar.navy.mil (dolesa@smtp-gw.spawar.navy.mil)
Subject: Re: MacBinary question


     
I have the same problem with Quicktime movies.  I can't even save the file
to disk (under A/UX...) or read directly from A/UX to Mac Mosaic.  The file
loses it's resource fork.  I know that in the past I had a problem with
GIF's being transmitted from NCSA http, and found that the A/UX language
forces me to "split" the file in order to strip the GIF header.  Possibly
your problem is similar.  I haven't been able to figure out Quicktime movies
under NCSA Mosaic, but they work fine from my MacHTTP server.  I think
discussion concerning Mac resources, and the way that the files are handled
by NCSA http is in order.  (Preferrably under A/UX, but not limited to...)
(P.S., I create MM dir movies also,... I haven't gotten to the question you
presented, thanks for beating me to it!  ;-)

                                Andre'

______________________________ Reply Separator _________________________________
Subject: MacBinary question
Author:  strata@fenchurch.MIT.EDU at SMTP-GW
Date:    2/25/94 9:01 AM


Forgive me if this has been discussed in detail, I've been wading through 
the messages every week or so and trying to keep on top of things.  I just 
put up the NCSA httpd on a Sun for a consulting client, the latest available 
one from ftp.ncsa.uiuc.edu (/pub/Mosaic/http/current or somesuch).
     
We want to put up multimedia files which were created with Macromind's 
Director package.  They have been saved as Player files, ie they are 
self contained and do not need Director to be present on the client Mac 
to run.  I put up both .bin MacBinary files and .hqx files but
cannot retrieve either successfully with Mac Mosaic 1.0.1.  A document is 
created on the desktop if I specify "load to disk" and click on the MacBinary 
version, but the document has no appropriate resource fork and merely asks me 
if I want to open it with TeachText.  Clicking on the .hqx file displays it as 
ASCII text, and if I specify "load to disk" saves it as ASCII and again asks 
me if I want to open TeachText on it.
     
Is there something specific that I need to be doing either to the server 
or the Mac Mosaic client to get this to work?  I am a little unclear even
after reading the online docs as to whether I need to explicitly create some 
mapping in one of the .conf files between ".bin" and "Mac Binary"-- I made 
sure it was in the client's profile, but perhaps the server needs to tell the 
client as well?
     
Thanks for any assistance,
_Strata
     
     
M. Strata Rose
Unix & Network Consultant, SysAdmin & Internet Information 
Virtual City Network (tm)
strata@virtual.net | strata@hybrid.com | strata@fenchurch.mit.edu
     



From bianco@giant.larc.nasa.gov  Mon Feb 28 14:15:59 1994 --100
Message-Id: <199402281305.NAA10712@MiSTy.larc.nasa.gov.larc.nasa.gov>
Date: Mon, 28 Feb 1994 14:15:59 --100
From: bianco@giant.larc.nasa.gov (David Bianco)
Subject: WAIS -> WWW Gateway and Titles

Mark Krause writes:
 > Has anyone developed a WAIS to WWW gateway that will present a
 > user-friendly title (i.e. the text between <TITLE> and </TITLE>)
 > instead of the sometimes cryptic URL that is currently presented
 > to the user?
 > 
 > -- 
 > Mark A. Krause                  mkrause@mitre.org
 > The MITRE Corporation           Mail Stop W273
 > 7525 Colshire Drive             (703)883-7642 (Voice)
 > McLean, VA 22102                (703)883-6809 (Fax)
 > 

Hmm... no gateway, but I do have a version of freeWAIS which
recognizes the HTML type and picks the <TITLE></TITLE> pair out
exactly as you suggest.  I've got patches to freeWAIS-0.202 if anyone
wants 'em...

	David



From sthomas@ciesin.org  Mon Feb 28 16:06:09 1994 --100
Message-Id: <9402281502.AA08561@tigger.ciesin.org>
Date: Mon, 28 Feb 1994 16:06:09 --100
From: sthomas@ciesin.org (Sharlyn Thomas)
Subject: sub

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Sharlyn D. Dendy-Thomas               				        %%%%%
EDUC'N: UM - Computer Science Department %% GRADUAT'G SENIOR w/ 3.2GPA :-) %%
OCCU'N: **Information Cooperative Programmer @ Consortium for International%%
            Earth Science Information Network (CIESIN) --pronounced season %%
	**Technical Support II @ Environmental Research Institute of       %%
	    Michigan (ERIM)				   		   %%
EMAIL: <sharlyn.thomas@ciesin.org> or <sthomas@erim.org>  		   %%
VMAIL: (517) 797-2646						        %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ENJOY!! ;-)  %%%%%%%%%%%%%%%%%%%%%%%%%%%
			      % % % % % % % %	
					











From atotic@ncsa.uiuc.edu  Mon Feb 28 16:33:20 1994 --100
Message-Id: <9402281510.AA28997@void.ncsa.uiuc.edu>
Date: Mon, 28 Feb 1994 16:33:20 --100
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: MacBinary question

> talk about how to serve Mac files through HTTP....
> under NCSA Mosaic, but they work fine from my MacHTTP server.  I think
> discussion concerning Mac resources, and the way that the files are handled
> by NCSA http is in order.  (Preferrably under A/UX, but not limited to...)
> (P.S., I create MM dir movies also,... I haven't gotten to the question you
> presented, thanks for beating me to it!  ;-)

Serving Mac files from http server presents a problem, since you cannot 
serve the resource fork. If you try to use MacBinary, you will not be
able to launch the player application, because macMosaic will launch
MacCompress by default, that will uncompress your file, but will not 
automatically launch the viewer for the uncompressed file. So, only files
that consist of data fork only can be served and launched immediately.
Good test to determine if this is the case is to transfer your file to
FTP server in binary mode, transfer it back to your Mac, restore its
creator/file type and see if it would play.

Aleks








From kaehms@sedbsvr.se.ssd.lmsc.lockheed.com  Mon Feb 28 16:41:14 1994 --100
Message-Id: <9402281532.AA08721@eagle.is.lmsc.lockheed.com>
Date: Mon, 28 Feb 1994 16:41:14 --100
From: kaehms@sedbsvr.se.ssd.lmsc.lockheed.com (Bob Kaehms)
Subject: Re: Mosaic update? (fwd)

> supplied, and to my amazement, they actually added it before I could nag
> them again.
> 
> As always, I stand in awe of the browser and server writers among us. I for
> one, owe you a lot of beers, but you have to come to Portland, Oregon to
> get it. ;-)

Jon,

This was bugging me all weekend, mostly because the 2 liner wasn't meant to
be sarcastic.  The appreciation really is there, and I am think about ways
to formally and informally get you guys some funding. I had left a phone
message with Jae, even before getting chewed out by you. Figuring out 
bueaurocratic solutions to this can be trying however, so I will be personally
sending you $10.00 so you don't have to come all  the way to California to
collect.  That doesn't mean I wouldn't buy you a beer also if you did drop
by.

Why am I postng this to WWW-talk?  Maybe others will be inspired to do 
similar things in lieu of more formal channels...

-bob (speaking for himself, and not his employer)




From davis@DRI.cornell.edu  Mon Feb 28 17:02:41 1994 --100
Message-Id: <199402281558.AA03278@willow.tc.cornell.edu>
Date: Mon, 28 Feb 1994 17:02:41 --100
From: davis@DRI.cornell.edu (Jim Davis)
Subject: Re: [Q] tk ismap editor


ftp://dri.cornell.edu/pub/davis/ismap-editor.tcl

See also ismap-format in that same dir

It does not support NCSA format ismap files, though.
I will do that "someday".  



From luotonen@ptsun00.cern.ch  Mon Feb 28 16:26:36 1994 --100
Message-Id: <9402281507.AA06599@ptsun03.cern.ch>
Date: Mon, 28 Feb 1994 16:26:36 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: CERN httpd 2.16beta released


CERN httpd 2.16beta released, source:

	ftp://info.cern.ch/pub/www/src/cern_httpd.tar.Z

This package includes EVERYTHING, so don't ftp the libwww.  Also, DO
NOT use the libwww that comes with it for anything else, libwww 2.15
is NOT officially released yet.

Precompiled binaries are supplied for:

  Sun4: 	     ftp://info.cern.ch/pub/www/bin/sun4/httpd_2.16beta.Z
  Solaris:	     ftp://info.cern.ch/pub/www/bin/solaris/httpd_2.16beta.Z
  HP:		     ftp://info.cern.ch/pub/www/bin/snake/httpd_2.16beta.Z
  NeXT:		     ftp://info.cern.ch/pub/www/bin/next/httpd_2.16beta.Z
  Fat NeXT-386:	     ftp://info.cern.ch/pub/www/bin/next-386/httpd_2.16beta.Z
  DecStation Ultrix: ftp://info.cern.ch/pub/www/bin/decstation/httpd_2.16beta.Z
  DEC OSF/1:	     ftp://info.cern.ch/pub/www/bin/dec-osf1/httpd_2.16beta.Z

For other platforms I'm happy to receive diffs and bins.  This release
is mainly for proxy caching, but a lot of other new features and fixes
have been included.  I call this beta because of caching -- everything
else seems to be stable (I'm not saying caching isn't but just to be
careful).

                       CERN HTTPD 2.16BETA RELEASE NOTES
 
     * If you are upgrading from 2.15beta, you need to make no changes.
     * If you are upgrading from 2.14, there is one single thing that
       needs to be done:
 
 
        Rename your old /htbin scripts to end in .pp suffix!
 
Firewall Gateway (Proxy) Additions, Fixes
 
     * ftp with binary files work
     * x-compress and x-gzip work correctly over proxy
     * Firewalling now works through arbitrary number of proxies;
       http_proxy, ftp_proxy, gopher_proxy and wais_proxy configuration
       directives cause proxy to connect to the outside world through
       another proxy. Environment variables with the same names have same
       effects, but config file is user-friendlier for this.
     * Now sends all the headers sent by client.
     * Proxy log file now gives byte count.
     * Proxy log file now gives correct status code also on error.
 
Firewall Gateway (Proxy) Caching
 
     * CacheRoot directive specifies cache root directory, and turns on
       proxy caching. Cache root directory must be dedicated to httpd -
       all files in there are subject to garbage collection.
     * Cache size (in megabytes) is specified by CacheSize directive;
       cache size should be several megabytes, 50-100MB should give good
       results. Cache may, however, temporarily grow a few megabytes
       bigger than specified. Also, space taken up by directories is not
       calculated in the current version.
     * http, ftp, gopher with GET method get cached.
     * However, not caching:
          + HTTP0 responses (you never know if it failed; also confused
            HTTP1 servers sometimes output garbage in front of HTTP1
            headers).
          + Protected documents (request had Authorization: field).
          + Queries - they have too often side-effects. (POST should be
            always used with forms, and all script responses should
            have Expires: header when necessary. Until then, we don't
            cache them.)
     * Expiry date is extracted:
          + From Expires: header.
          + If not present Last-Modified: is used to approximate expires.
            If a file hasn't changed in five months the chances are it
            won't change during the next week. On the other hand, if a
            file has changed yesterday, it will probably change again
            pretty soon. I know this is heuristic but until all the
            servers give Expires: this works much better than not using
            it, so no flames about it.
          + If Last-Modified: not given use the time given by
            CacheDefaultExpiry directive, default 7 days.
     * Format of cache files and directory structure under cache root is
       subject to change if necessary. No application should yet rely on
       any certain cache format. Eventually I can see clients accessing
       cache files directly, bypassing proxy server.
     * Caching system understands both time formats, also the one output
       by old NCSA httpds.
     * Cache files get locked during transfer. Lock files time out if
       something goes wrong. Timeout can be set by CacheLockTimeOut
       directive (default 20 minutes). During the lock is in effect,
       further requests to the same file get retrieved from the remote
       host.
     * Garbage collection directives:
          +
          + GcMemoryUsage to advice gc about how radical to be in memory
            use (more memory => smarter gc).
          + GcTimeInterval, how often to do gc.
          + GcReqInterval, after how many requests to do gc.
          + (gc is also automatically started if cache size limit is
            reached.)
          + CacheLimit_1, size in KB until which files are equally
            valuable despite their size (200K).
          + CacheLimit_2, size in KB after which files get discarded
            because they are too big (4MB).
          + CacheClean, remove all files older than this (default 21
            days).
          + CacheUnused, remove all files that have not been used in this
            long time (default 14 days).
     * Garbage collector always removes all expired, too long unused, and
       too old files.
     * If cache size limit is reached some files need to be sacrified;
       the current algorithm takes into account:
          + Time remaining to unconditional removal; if it expires
            tomorrow it might as well be removed today.
          + Time last accessed; if it hasn't been accessed in 5 days, it
            probably won't be accessed anymore before it expires.
          + Size; huge files get removed move easily.
          + Time it took to load it from the remote host; files that were
            time-consuming to transfer have much higher value. This
            compensates the size factor. Load delay is the single most
            significant value.
          + Time it has already been in cache; ancient files get removed
            more easily than fresh ones.
 
Other New Features
 
     * Error log file.
     * Referer: field ends up in error log when a request fails.
     * UserId and GroupId to set default uid and gid (used instead of
       nobody and nogroup).
     * Timeout for input and output; default time to wait for a request
       is 2 minutes, and to send response 20 minutes. Timeout causes a
       note to error log, and terminates child (no more hanging httpds).
       Note: the one zombie is normal; don't report to me about it, I
       may do something about it some day, or maybe I won't. Zombie
       doesn't take up any other system resources except the one process
       table entry.
     * Suffixes are no longer case-sensitive by default; this may be
       changed via the SuffixCaseSense configuration directive.
     * Lou Montulli's news and proxy diffs added to the library.
     * Most command line options now also available as configuration
       directives:
          + DirAccess
          + DirReadme
          + AccessLog
          + ErrorLog
          + LogFormat
          + LogTime
     * -vv command line option for Very Verbose trace output. Outputs
       also request headers as they came in. Otherwise like -v flag.
 
Enhancements, Fixes
 
     * NPH-scripts now work from automatically backgrounded standalone
       server.
     * Fixed the many problems with Content-Transfer-Encoding:
          + Mosaic uses Content-Encoding, although spec says
            Content-Transfer-Encoding; I now output both
          + Content-Transfer-Encoding sometimes didn't show up although
            it should have, fixed.
          + Content-Transfer-Encoding didn't come up correctly with ftp,
            fixed.
     * Strange escaping fixed with directory indexing (legal characters
       got escaped randomly by a gcc-compiled version).
     * Timezone bug around midnight with the new logfile format fixed.
       (New logfile format is not yet default, use -newlog command line
       option, or LogFormat directive in configuration file.)
     * Dashes for non-existent status codes and byte counts now show up
       correctly in the log.
     * Forking code once again enhanced - fixed a possible hanging
       situation.
     * Log time fixed to be the time of incoming request, not the time of
       request served.
     * Zombies now correctly waited away on HP (this was in fact fixed
       already in 2.15beta binaries distributed after February 17th -
       note, that this bug had no effect on any other platforms ).
     * Directory listings no longer have Content-Length: (because it was
       wrong).
     * Now understands also the old Accept: syntax, with spaces as
       separators between actual content-type and its parameters. This
       will eventually be taken out.
     * htadm now uses the same file creation mask as in the original
       password file.
 
Code is Purify'd, and I truly wonder how anybody can live without that
marvellous piece of software.  My productivity has doubled after I
started using it.  Well done, Pure Software!

-- Cheers, Ari --



From jonm@ncsa.uiuc.edu  Mon Feb 28 18:29:48 1994 --100
Message-Id: <9402281725.AA01824@void.ncsa.uiuc.edu>
Date: Mon, 28 Feb 1994 18:29:48 --100
From: jonm@ncsa.uiuc.edu (Jon E. Mittelhauser)
Subject: Re: Mosaic update? (fwd)

At 04:42 PM 2/28/94 --100, Bob Kaehms wrote:

>This was bugging me all weekend, mostly because the 2 liner wasn't meant to
>be sarcastic.  The appreciation really is there, and I am think about ways
>to formally and informally get you guys some funding.

Just for the official record, I sent mail to Bob over the weekend 
apologizing for my reply.  I *obviously* had misinterpreted the
original message and overreacted.  It had much more to do with
the situation here than anything else... :^)

>I had left a phone
>message with Jae, even before getting chewed out by you. 

:^( Sorry.... :^)

>Figuring out 
>bueaurocratic solutions to this can be trying however, so I will be personally
>sending you $10.00 so you don't have to come all  the way to California to
>collect.  That doesn't mean I wouldn't buy you a beer also if you did drop
>by.

Hold the beer for me instead.  I'm probably outta here in May and there
is a decent chance that Sunny California will be my destination... :^)

-Jon





From montulli@stat1.cc.ukans.edu  Mon Feb 28 21:08:54 1994 --100
Message-Id: <9402282004.AA40079@stat1.cc.ukans.edu>
Date: Mon, 28 Feb 1994 21:08:54 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Tables in HTML+

> 
> >> I disagree. Why force users to enter unnecessary end tags when the content
> >> model is clear and unambigous! The DTD allows authors to omit end tags for
> >> the following elements:
> 
> > Inferring where end tags should be is not hard in valid HTML. The problem
> > arises when trying to cope with invalid variations, that include things
> > such as <P> used as carriage return, free-floating LI. The <P> element is
> > the biggest problem, since its appearance is quite random.
> 
> Please tell me more about why this is difficult.
> 
> My parser treats a free-floating <LI> by ungetting this token and
> calling the routine to parse a UL with the "implied token" flag set
> to true, thereby setting the attributes for UL to their default values.
> A random <P> element terminates the current paragraph container and
> starts a new one, as you would expect. Note that the parser does treat
> <P> and <LI> as containers unlike HTML. Maybe you would find it easier
> if you made that transition.
> 
<li> are a problem since it has no real meaning outside of a list
structure.  I think they should just be ignored if there is no open
list structure.

<p>, on the other hand, does have some defacto meaning almost anywhere.
The current use of <p> (not the spec. definition) is to put two returns
in and stay in the current style.  In other words <p> means two <br>'s.
Therefore if a <p> is encountered in a list then add two returns,
the same with headers, paragraphs, etc.  This isn't all that hard to
parse.  There is some confusion as to what to do with extra line breaks.
Xmosaic collapses extra vertical whitespace, while in Lynx I leave it
in so that the user can get vertical whitespace if they want it.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *      Let's face it, I'm a nerd, why else would I have a sig file?      *
  **************************************************************************



From ellson@hotsand.att.com  Tue Mar  1 06:13:54 1994 --100
Message-Id: <9403010506.AA01603@hotsand.dacsand>
Date: Tue, 1 Mar 1994 06:13:54 --100
From: ellson@hotsand.att.com (ellson@hotsand.att.com)
Subject: Re:  CERN httpd 2.16beta released

> CERN httpd 2.16beta released, source:

Ari, or somebody,

OK, It runs for me as a basic http daemon.

Now could you please point me at an httpd.conf file 
or docs showing how I can enable the proxy and caching features?

John Ellson
AT&T Bell Labs




From phillips@cs.ubc.ca  Tue Mar  1 08:48:16 1994 --100
Message-Id: <7620*phillips@cs.ubc.ca>
Date: Tue, 1 Mar 1994 08:48:16 --100
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: CGI stuff

Rob said:
>I say nay on an initial proposal of a npa- script that gets the headers on
>stdin, exactly as the client sent them. I also think that the initial line
>(GET /blah HTTP/1.0) can already be reconstructed from information the
>script has (that is, in a server that doesn't un-escape PATH_INFO) and isn't
>necessary. The same goes for SERVER_ADDRESS, it should at the very least be
>SERVER_URL or SERVER_URL_PREFIX and I don't see why it is necessary, since
>we may be abstracting the URL representation for the protocol but we already
>know the protocol: HTTP. If you're not using HTTP, then REQUEST_PROTOCOL
>should not say HTTP.

The nice part about CGI is that most scripts don't have to care about
SERVER_PROTOCOL (unless they need POST information, are nph- scripts
or wish to output self-referencing URLs).  This is a good thing.  CGI
could be used with an entirely differnent access protocol like gopher:
or x-exec:.  With proper use of SERVER_URL_PREFIX, most of your CGI
script could be dropped right into this new server without modification.
Otherwise, you'll end up with code like:

	if ($rp eq 'HTTP/1.0' || $rp eq 'HTTP/1.0') print "http://$srv:$port/";
	if ($rp eq 'gopher') print "gopher://$srv:$port";
	if ($rp eq 'x-exec') print "x-exec://$srv";

instead of:

	print $SERVER_URL_PREFIX;

and even then the mapping of current CGI variables onto the different
protocol schemes may not be entirely appropriate.  All in all I'd say
that CGI should have had SERVER_URL_PREFIX instead of SERVER_NAME
and SERVER_PORT from the beginning since the former is much more suited
to self-referencing URLs.

>What I would consider is a new env. variable called REQUEST_EXTRA_HEADERS
>which would contain the text of all the headers the server didn't
>understand. I don't support giving all of them to the script (notably
>Authorization) since in the case of authorization, I don't want every script
>to have the users' passwords in near-plaintext. I also don't support it
>because then we'd have two copies of Mosaic's 700 byte accept headers and
>on some systems, we're that much closer to the dreaded ARG_MAX.

I don't understand what you mean by 2 copies.  Wouldn't the Accept: headers
live in HTTP_ACCEPT and not be duplicated in REQUEST_EXTRA_HEADERS?
At any rate, I think this argues in favour of "npa-".  No worries about
environment variable space or what should or should not be passed to
the script while giving CGI/1.1 writers the power to do anything.  I
agree that passing the first line is not necessary, just the headers
on down.

Did you have any particular reason why you don't want "npa-"?  Seems like
it would fall out of the NCSA httpd code quite nicely if you don't pass
the first "method url" line.  I certainly think script writes would
like to see "npa-".



From robm@ncsa.uiuc.edu  Tue Mar  1 09:25:32 1994 --100
Message-Id: <9403010822.AA14970@void.ncsa.uiuc.edu>
Date: Tue, 1 Mar 1994 09:25:32 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI stuff

/*
 * Re: CGI stuff  by George Phillips
 *    written on Mar  1,  8:49am.
 *
 * The nice part about CGI is that most scripts don't have to care about
 * SERVER_PROTOCOL (unless they need POST information, are nph- scripts
 * or wish to output self-referencing URLs).  This is a good thing.  CGI
 * could be used with an entirely differnent access protocol like gopher:
 * or x-exec:.  With proper use of SERVER_URL_PREFIX, most of your CGI
 * script could be dropped right into this new server without modification.
 * Otherwise, you'll end up with code like:
 * 
 * 	if ($rp eq 'HTTP/1.0' || $rp eq 'HTTP/1.0') print "http://$srv:$port/";
 * 	if ($rp eq 'gopher') print "gopher://$srv:$port";
 * 	if ($rp eq 'x-exec') print "x-exec://$srv";
 * 
 * instead of:
 * 
 * 	print $SERVER_URL_PREFIX;
 * 
 * and even then the mapping of current CGI variables onto the different
 * protocol schemes may not be entirely appropriate.  All in all I'd say
 * that CGI should have had SERVER_URL_PREFIX instead of SERVER_NAME
 * and SERVER_PORT from the beginning since the former is much more suited
 * to self-referencing URLs.

Hmmm, the problem I was considering was that I believed gopher could not
support arbitrary types returned by a script. From what I just read of the
GN documentation, it seems that GN supports it. In the interests of script
simplicity perhaps we should make a variable such as PROTOCOL_URL_PREFIX
which contains http:// or gopher:// etc.

 * I don't understand what you mean by 2 copies.  Wouldn't the Accept: headers
 * live in HTTP_ACCEPT and not be duplicated in REQUEST_EXTRA_HEADERS?
 
What I mean is that I don't support placing ALL of the headers in a
variable, I would support only putting the unknown ones there. Sorry that
wasn't clear.

 * At any rate, I think this argues in favour of "npa-".  No worries about
 * environment variable space or what should or should not be passed to
 * the script while giving CGI/1.1 writers the power to do anything.  I
 * agree that passing the first line is not necessary, just the headers
 * on down.
 *
 * Did you have any particular reason why you don't want "npa-"?  Seems like
 * it would fall out of the NCSA httpd code quite nicely if you don't pass
 * the first "method url" line.  I certainly think script writes would
 * like to see "npa-".
 */

Actually, currently it doesn't if you consider user authentication,
especially PGP/PEM. I don't know how it fits into the other servers.

It's curious that you would like to see protocol independence (in URL
returns), but want to introduce a completely HTTP-dependent feature into
CGI/1.1. How does npa- fit into gopher CGI execution? (John?)

My main problem is that I feel that this doesn't fit well into CGI, and
would better fit into a different protocol for actual server modules
instead. One of the things I'm working on for httpd 2.0 is extensive
modularity: a well documented API as well as an external interface for
installing custom methods for doing access control, virtual->physical
translation, etc. This is where I believe such functionality belongs, since
it is cleaner (both documents and scripts could have custom access control),
at it doesn't further pollute an already HTTP-biased specification.

Comments? John, Ari, Tony?
--Rob



From robm@ncsa.uiuc.edu  Tue Mar  1 09:43:00 1994 --100
Message-Id: <9403010839.AA15022@void.ncsa.uiuc.edu>
Date: Tue, 1 Mar 1994 09:43:00 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: CGI/1.1 draft


Here are my proposals for changes to CGI, to make its version 1.1:

1. Any headers output by non-nph scripts which the server doesn't understand
   may be passed to the client.

2. Add a new env. variable called HTTP_EXTRA_HEADERS which contains all
   headers sent by the client which the server didn't understand. This does
   not include Authorized, Accept, Content-type, or Content-length as these
   are already elsewhere in the CGI variable space. The server may perform
   collapsing of these lines, i.e. it may consolidate multiple occurrences
   of these lines as it already does with HTTP_ACCEPT.

---
Discussion:

1: This is has already been discussed and largely agreed upon as a Good
   Idea. It's backward compatible with CGI/1.0 behavior, and fits nicely in.

2: This will allow stupid pet tricks like using From:, but more importantly,
   allows us to not need to update the CGI spec every time a new header is
   introduced.

Finally, note that I don't mention whether PATH_INFO should be unescaped or
not. My first impression is that it should remain escaped, in order to avoid
ambiguities like the decoding of foo="1%3d2". Problem is, all of the current
implementations are ``broken'', and therefore such a change technically
isn't backward compatible. So perhaps we should update the spec. to reflect
the implementations. Comments?

--Rob



From robm@ncsa.uiuc.edu  Tue Mar  1 11:32:35 1994 --100
Message-Id: <9403011029.AA16135@void.ncsa.uiuc.edu>
Date: Tue, 1 Mar 1994 11:32:35 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI/1.1 draft


I wrote:

/*
 * CGI/1.1 draft  by Rob McCool
 *    written on Mar  1,  2:39am.
 *
 * 
 * Here are my proposals for changes to CGI, to make its version 1.1:
 * 
 * 1. Any headers output by non-nph scripts which the server doesn't understand
 *    may be passed to the client.
 * 
 * 2. Add a new env. variable called HTTP_EXTRA_HEADERS which contains all
 *    headers sent by the client which the server didn't understand. This does
 *    not include Authorized, Accept, Content-type, or Content-length as these
 *    are already elsewhere in the CGI variable space. The server may perform
 *    collapsing of these lines, i.e. it may consolidate multiple occurrences
 *    of these lines as it already does with HTTP_ACCEPT.

To clarify, this is the headers sent by the client which _CGI_ didn't
understand, not what the server didn't understand. Currently, this means
everything except Authorization, Accept, Content-type and Content-length.

 * ---
 * Discussion:
 * 
 * 1: This is has already been discussed and largely agreed upon as a Good
 *    Idea. It's backward compatible with CGI/1.0 behavior, and fits nicely in.
 * 
 * 2: This will allow stupid pet tricks like using From:, but more importantly,
 *    allows us to not need to update the CGI spec every time a new header is
 *    introduced.
 * 
 * Finally, note that I don't mention whether PATH_INFO should be unescaped or
 * not. My first impression is that it should remain escaped, in order to avoid
 * ambiguities like the decoding of foo="1%3d2". Problem is, all of the current
 * implementations are ``broken'', and therefore such a change technically
 * isn't backward compatible. So perhaps we should update the spec. to reflect
 * the implementations. Comments?
 */

To clarify on this point, I just went back and looked at the spec. again.
The current implementations are in fact up to spec., as the specification
has always read that PATH_INFO should be unescaped (dating back to Dec. 13
last year). The notable exception is NCSA httpd 1.x which mistakenly
converts + signs to spaces in file names.

The question is then whether we should have PATH_INFO in fact be escaped, or
unescaped. At any rate, changing this aspect of the specification will break
things, so I'm leery of it.

--Rob



From Baard.Haafjeld@nta.no  Tue Mar  1 13:52:50 1994 --100
Message-Id: <199403011249.AA10954@bang.nta.no>
Date: Tue, 1 Mar 1994 13:52:50 --100
From: Baard.Haafjeld@nta.no (Baard.Haafjeld@nta.no)
Subject: Re: CGI/1.1 draft

I would like to suggest (re)introduction of two environment variables
that I miss when I'm writing CGI scripts. I'm sorry if I reintroduce
matters already discussed, I have not been reading www-talk regularily
up to now.

These suggested variables basically aims at avoid hardcoding directory 
configuration information into the scripts themselves, thus making them
a lot more portable.

SCRIPT_DATA_DIR: I keep missing some preconfigured place a script can go
	look for configuration data, search indices, write logs etc. 
	Currently my
	scripts tend to start with 'data_dir=/some/where' etc. It would
	be a lot more portable if the servers could provide one common
	configured directory for such stuff, so I could say 
	'data_dir=$SCRIPT_DATA_DIR/myscriptname/...'

DOCUMENT_ROOT: I miss the DOCUMENT_ROOT environment variable that is
	available to NCSA http /htbin scripts. I realize you have
	probably discussed this and it was removed as 'too http'ish' :)
	However I have scripts where I KNOW I'm operating in http
	environment and want to update html documents etc from my
	scripts. I then need to know where the http server's tree
	starts. My current trick is extracting this from PATH_INFO and
	PATH_TRANSLATED, but this does not work if there is no
	path-info(at least not with NCSA http). So I'm back to
	hardcoding paths in the top if my scrips. Bad, bad.
	Even if this variable may not make sense for gopher etc, I still
	think it should be present for those scripts who KNOW they are
	in a http environment.. which at the moment are most of them?

 				       |
Baard Haafjeld			       | When you give a wolf a poodle cut, you 
Norwegian Telecom Research	       | don't get a show dog but a pissed wolf.
SMTP-mail: Baard.Haafjeld@tf.tele.no   |                        -Robert Asprin




From john@math.nwu.edu  Tue Mar  1 16:35:06 1994 --100
Message-Id: <9403011531.AA03235@hopf.math.nwu.edu>
Date: Tue, 1 Mar 1994 16:35:06 --100
From: john@math.nwu.edu (John Franks)
Subject: Re: CGI stuff

According to Rob McCool:
> 
> Hmmm, the problem I was considering was that I believed gopher could not
> support arbitrary types returned by a script. From what I just read of the
> GN documentation, it seems that GN supports it. 

GN supports it for HTTP clients only.  It is a nice idea, but after
considerable thought I think that use of CGI by gopher servers is
unrealistic for both practical and political reasons.  The CGI
protocol could be (pretty much is) protocol independent, but the
writers of CGI scripts are mostly HTTP users and expect an HTTP client
to receive the output of their script.  And it wouldn't really be fair
to ask them to do otherwise.  I have looked a fair number of CGI
scripts and very few of them would work with a gopher server.  For
many (e.g. ismap applications) there is no way to get them to work
with gopher.  Also gopher server maintainers are unlikely to adopt CGI
as a standard since many aspects of the protocol seem quite unnatural
in a gopher context.

In short, I think it doesn't make sense to try to bend CGI in a way that
would presumably make it better for the gopher protocol.  The real value
of CGI is that it should work with all HTTP servers.


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu






From john@math.nwu.edu  Tue Mar  1 16:43:44 1994 --100
Message-Id: <9403011540.AA03251@hopf.math.nwu.edu>
Date: Tue, 1 Mar 1994 16:43:44 --100
From: john@math.nwu.edu (John Franks)
Subject: Re: CGI/1.1 draft

According to Rob McCool:
> 
> To clarify on this point, I just went back and looked at the spec. again.
> The current implementations are in fact up to spec., as the specification
> has always read that PATH_INFO should be unescaped (dating back to Dec. 13
> last year). The notable exception is NCSA httpd 1.x which mistakenly
> converts + signs to spaces in file names.
> 
> The question is then whether we should have PATH_INFO in fact be escaped, or
> unescaped. At any rate, changing this aspect of the specification will break
> things, so I'm leery of it.
> 

I guess I would favor keeping current practice, but I would not object
violently a change in this area.  I do want to make sure that the spec
is very specific on this point though.  If PATH_INFO is unescaped we
must say so explicitly and say what this means, i.e. does unescaping
include '+' -> space translation or just the %hex decoding.  My 
understanding when I did this was that unescaping included both and that's
the way my server works.  If this is wrong then I would like to fix
it.  "URL decoding" might be a better term than "unescaping".

I like the idea of a data root directory environment variable.  At 
user request I have already implemented this as a non-standard feature
in GN. 


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu






From dmh@hpfcma.fc.hp.com  Tue Mar  1 16:57:15 1994 --100
Message-Id: <9403011553.AA25150@hpfcma.fc.hp.com>
Date: Tue, 1 Mar 1994 16:57:15 --100
From: dmh@hpfcma.fc.hp.com (Dave Hollander)
Subject: dmh@ce.hp.com


Subscribe





From rbntjc@RohmHaas.Com  Tue Mar  1 17:35:52 1994 --100
Message-Id: <9403011632.AA20199@atlas.br.RohmHaas.Com>
Date: Tue, 1 Mar 1994 17:35:52 --100
From: rbntjc@RohmHaas.Com (Mr. Tom Cozzolino)
Subject: Getsites - where did it go?


Does anyone know where the latest version of getsites.c is?
Pages at CERN point to http://www.hcc.hawaii.edu/files/getsites
but that is no longer available.

Any help is appreciated.
   
+=================================================+
|     Thomas J. Cozzolino - Rohm and Haas Co.     |
|     Internet:    tcozz@rohmhaas.com             |
|     Phone/Fax: (215) 781-4087/4112              |
|                                                 |
|         Internet Access for Everyone..          | 
|             - Isn't it Time?                    |
|                                                 |
|       Opinions expressed are my own, not        |
|   necessarily those of Rohm and Haas Company    |
+=================================================+



From kevinh@eit.COM  Tue Mar  1 18:50:29 1994 --100
Message-Id: <9403011747.AA17877@eit.COM>
Date: Tue, 1 Mar 1994 18:50:29 --100
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re:  Getsites - where did it go?

rbntjc@RohmHaas.Com (Mr. Tom Cozzolino) writes:

> Does anyone know where the latest version of getsites.c is?
> Pages at CERN point to http://www.hcc.hawaii.edu/files/getsites
> but that is no longer available.

	It's been renamed getstats and is at:

	http://www.eit.com/software/getstats/getstats.html

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://www.eit.com/)
Hypermedia Industrial Designer * Duty now for the future!



From luotonen@ptsun00.cern.ch  Tue Mar  1 21:53:40 1994 --100
Message-Id: <9403012051.AA09411@ptsun03.cern.ch>
Date: Tue, 1 Mar 1994 21:53:40 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: CGI/1.1 draft


> I like the idea of a data root directory environment variable.  At 
> user request I have already implemented this as a non-standard feature
> in GN. 

Then what would I do?  cern_httpd doesn't have a single document
root, and there is no reason why it should.  I would like to see
DOCUMENT_ROOT as a server-specific feature rather than dictated
by the spec.

-- Cheers, Ari --




From john@math.nwu.edu  Tue Mar  1 22:38:13 1994 --100
Message-Id: <9403012135.AA03825@hopf.math.nwu.edu>
Date: Tue, 1 Mar 1994 22:38:13 --100
From: john@math.nwu.edu (John Franks)
Subject: Re: CGI/1.1 draft

According to Ari Luotonen:
> 
> > I like the idea of a data root directory environment variable.  At 
> > user request I have already implemented this as a non-standard feature
> > in GN. 
> 
> Then what would I do?  cern_httpd doesn't have a single document
> root, and there is no reason why it should.  I would like to see
> DOCUMENT_ROOT as a server-specific feature rather than dictated
> by the spec.
> 

Well, there is no reason that the value of the DOCUMENT_ROOT can't
depend on the URL, so you could accomodate multiple roots.  But
I don't have strong feelings on this.  I would be happy with having
it be server-specific.

You should be aware however, that (as someone described recently in 
a www-talk post) script writers are taking PATH_TRANSLATED and
trying to delete PATH_INFO from the end to obtain a document root.
They are doing this because their scripts need to access more than
one asscociated file.  How will this work with cern_httpd?

John



From rst@ai.mit.edu  Tue Mar  1 22:43:57 1994 --100
Message-Id: <9403012141.AA09475@volterra>
Date: Tue, 1 Mar 1994 22:43:57 --100
From: rst@ai.mit.edu (Robert S. Thau)
Subject: CGI/1.1 draft

   Date: Tue, 1 Mar 1994 21:55:42 --100
   From: luotonen@ptsun00.cern.ch (Ari Luotonen)


   > I like the idea of a data root directory environment variable.  At 
   > user request I have already implemented this as a non-standard feature
   > in GN. 

   Then what would I do?  cern_httpd doesn't have a single document
   root, and there is no reason why it should.  I would like to see
   DOCUMENT_ROOT as a server-specific feature rather than dictated
   by the spec.

   -- Cheers, Ari --


Hmmm... most of my scripts which manipulate external files store those
files in some known location relative to the script itself (e.g., the
seminar-schedule printer used by several links from

  http://www.ai.mit.edu/events/events-list.html

which works off email archives which are stored in the same directory as
the script).  These scripts would be perfectly happy with a variable which
contained their own (translated) pathname; they could then find the files
they needed relative to that.

In fact, that script presently begins with the lines...

  #! /usr/local/bin/perl

  do '/com/doc/web-support/cgi-hacks.pl'; # Perl CGI assist routines...
  chdir '/com/doc/web/events/seminars';   # Directory containing this script...
  ...

(my DOCUMENT_ROOT being /com/doc/web), which is awkward because I'd have to
change this line if I ever wanted to install the script and its support
files as a package somewhere else.  If a TRANSLATED_SCRIPT_NAME variable
were available, I could do this instead:

  #! /usr/local/bin/perl

  do '/com/doc/web-support/cgi-hacks.pl';
  ($script_dir = $ENV{'TRANSLATED_SCRIPT_NAME'}) =~ s+/[^/]*$++;
  chdir $script_dir;
  ...

(where the gubbish on the second line strips the last path component off of
the TRANSLATED_SCRIPT_NAME to get the enclosing directory); this would work
no matter where the package lived.

This variable would at least have a sensible definition no matter how the
server actually does translations (in particular, whether the notion of a
single DOCUMENT_ROOT makes sense or not).  However, some scripts may need
something more flexible.  Comments?

rst



From luotonen@ptsun00.cern.ch  Tue Mar  1 22:51:14 1994 --100
Message-Id: <9403012148.AA09452@ptsun03.cern.ch>
Date: Tue, 1 Mar 1994 22:51:14 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: CGI/1.1 draft


> You should be aware however, that (as someone described recently in 
> a www-talk post) script writers are taking PATH_TRANSLATED and
> trying to delete PATH_INFO from the end to obtain a document root.
> They are doing this because their scripts need to access more than
> one asscociated file.  How will this work with cern_httpd?

The concept of document root is simply unknown to cern_httpd.
You just map a part of URL space to physical file system, and
the URL hierarchy is in no way bound to the physical file system
hierarchy.  You can omit physical directories from between, you
can map many "URL directories" to a physical directory, and in
a pathologic case what seems to be a subdirectory could actually
be a parent directory.

This is all theory, and nobody really uses the rules like that,
but its possible.  In other words having a document root, a single
one or many of them, just doesn't necessarily make any sense.

-- Cheers, Ari --




From kevinh  Tue Mar  1 18:18:54 1994 PST
Message-Id: <9403020218.AA24319@eit.COM>
Date: Tue, 1 Mar 94 18:18:54 PST
From: kevinh (Kevin 'Kev' Hughes)
Subject: Confidential directory



	To put confidential EIT-only web things in, I've made
/usr/local/www/confidential.

	-- Kev



From robm@ncsa.uiuc.edu  Sun Mar  6 16:42:11 1994 --100
Message-Id: <9403020848.AA05007@void.ncsa.uiuc.edu>
Date: Sun, 6 Mar 1994 16:42:11 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI/1.1 draft



I don't think making DOCUMENT_ROOT available in general is such a good idea
because NCSA httpd doesn't just use DocumentRoot to translate names.
Arguably, most setups are like that, but nonetheless you are not guaranteed
that something comes from DocumentRoot (there are also Alias and ScriptAlias
directives). Doing uri->filesystem translation is simply not something we
can make available

Something I have implemented in 1.2 is that upon CGI execution, the server
changes its working directory to the directory of the currently executing
script. This would allow you to keep your config files in a subdirectory of
the script's directory, if you wanted, but the problem with that is that
people might be able to read them (I have also implemented CGI execution in
non-ScriptAlias directories). Someone mentioned making SCRIPT_TRANSLATED
available, in this setup, a script could get its real name from pwd and
SCRIPT_NAME or argv[0]. However, the spec. does not require the working
directory to be anything on script execution.

One idea I have found interesting is the idea of making some kind of
configuration directory available. Something like CGI_CONFIG which would be
set to a central location of CGI configuration files may be nice, but then
people will all have to have write access to this directory. So I'm not sure
if this would help much either. This could be set to SERVER_ROOT/conf for
NCSA httpd, or just allow users to configure it. The point would be to
create a central repository for CGI config. files which would be available
on any server.

--Rob



From robm@ncsa.uiuc.edu  Sun Mar  6 16:47:11 1994 --100
Message-Id: <9403020852.AA05081@void.ncsa.uiuc.edu>
Date: Sun, 6 Mar 1994 16:47:11 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI/1.1 draft

/*
 * Re: CGI/1.1 draft  by John Franks
 *    written on Mar  1,  9:40am.
 *
 * I guess I would favor keeping current practice, but I would not object
 * violently a change in this area.  I do want to make sure that the spec
 * is very specific on this point though.  If PATH_INFO is unescaped we
 * must say so explicitly and say what this means, i.e. does unescaping
 * include '+' -> space translation or just the %hex decoding.  My 
 * understanding when I did this was that unescaping included both and that's
 * the way my server works.  If this is wrong then I would like to fix
 * it.  "URL decoding" might be a better term than "unescaping".
 */

A misconception I had was that + and ' ' were interchangable. Someone has
told me that in fact, no, the + is only applicable within a query string,
not within a URI. Since the spec. specifically says URL decoding and the URL
spec says no + to ' ' mapping in a URI, PATH_INFO should only have %
decoding done on it. I will be changing NCSA httpd's behavior to be % only
in PATH_INFO (but + and % in QUERY_STRING).

--Rob






From Baard.Haafjeld@nta.no  Sun Mar  6 16:51:20 1994 --100
Message-Id: <199403020919.AA11638@bang.nta.no>
Date: Sun, 6 Mar 1994 16:51:20 --100
From: Baard.Haafjeld@nta.no (Baard.Haafjeld@nta.no)
Subject: Re: CGI/1.1 draft

	From: luotonen@ptsun00.cern.ch (Ari Luotonen)
	
	> I like the idea of a data root directory environment variable.  At 
	> user request I have already implemented this as a non-standard feature
	> in GN. 
	
	Then what would I do?  cern_httpd doesn't have a single document
	root, and there is no reason why it should.  I would like to see
	DOCUMENT_ROOT as a server-specific feature rather than dictated
	by the spec.
	
	-- Cheers, Ari --
	
Seems to me the concept of server-specific features is contrary to the
idea of CGI itself. I perceive CGI to be a (de-facto) standard to
write portable server scripts. If you put useful functionality off as
'server-specific' people would use it and their scripts would not be
CGI-conformant, thus lessening the value of CGI itself. I can thus find
arguments in favor of NOT allowing server-specific stuff at all.

Portability to me is the ability to take a script from one machine,
move it to another and it works without any modifications. A weak spot
in the current CGI is finding the location of the scripts local data
and the document tree in a portable way. Thus my suggestions of 
SCRIPT_DATA_DIR and DOCUMENT_ROOT variables.

As to DOCUMENT_ROOT and the virtual/physical mapping: I agree with Ari
there _need_ not be any direct mapping from URL to file system. However,
I guess most(all?) http server uses the filesystem as the basic mechanism
to do the URL->file mapping, modified by alias-mechanisms etc as needed.
Even if we can't have a 100% solution, a 90% solution would make life a
lot easier. The script author will just have to put into his
documentation what URL's the script accesses as files and the server
administrator will have to make sure they have a direct file mapping.
 				       |
Baard Haafjeld			       | When you give a wolf a poodle cut, you 
Norwegian Telecom Research	       | don't get a show dog but a pissed wolf.
SMTP-mail: Baard.Haafjeld@tf.tele.no   |                        -Robert Asprin




From kurlanda@informatik.uni-frankfurt.de  Sun Mar  6 16:55:20 1994 --100
Message-Id: <9403021408.AA15498@hysteria.rbi.informatik.uni-frankfurt.de>
Date: Sun, 6 Mar 1994 16:55:20 --100
From: kurlanda@informatik.uni-frankfurt.de (kurlanda@informatik.uni-frankfurt.de)
Subject: Q: How to expire / Prevent caching of certain sites

Hi,

can someone please point me to a source where I can learn how two
supply EXPIRE or related stuff in my html-documents?

Beside this, I have a problem with caching. I use the proxy cern httpd2.16b,
lynx2.2 and Mosaic2.2. The server I maintain is the "entry"-server
for *.uni-frankfurt.de. The server provides links to some faculties
and institutions of uni-frankfurt, which have own servers. In addition
it serves our own pages (cs-department). Naturally, I won't have it caching
the httpd-requests that go to *.uni-frankfurt.de, because this isn't necessary
and would obsolete the other servers.

I didn't found how to prevent httpd2.16b from caching certain sites. Is there
a way to prevent Mosaic and Lynx from using my server as a proxy for
such requests ? 

Regards
-- 
Jens Kurlanda 	  (Raum:014b)			J.W.Goethe Universitaet Frakfurt
Tel:069 798 8378				Robert-Mayer-Strasse 11-15
Email: kurlanda@rbi.informatik.uni-frankfurt.de D-60054 Frankfurt (Germany)

			



From bjoerns@stud.cs.uit.no  Sun Mar  6 16:59:33 1994 --100
Message-Id: <199403021725.SAA06528@tklab3.cs.uit.no>
Date: Sun, 6 Mar 1994 16:59:33 --100
From: bjoerns@stud.cs.uit.no (Bjoern Stabell)
Subject: Questions about HTML conventions

Hi,


I have some questions regarding HTML conventions which I thought
I'd air here.


First, where should meta information for documents be?

Already, some information are in the MIME-headers (like
Content-type and Last-modified) but the HEAD-part of HTML
documents have some potential.

There is a trend today, it is even a recommended practice, to put
meta info like 'last updated', 'creation date', 'creator',
'modified by' and 'next document' in the body of documents.  At
least Last-modified is transmitted by many http daemons in the
MIME headers and so the browser should be able to present this,
putting it in the body of the documents is superflous.

At least, it shouldn't be necessary to present a 'back to
prevoius page' button at the bottom of every page.  It's not that
hard to maintain a stack of previously visited URLs in the
browser.  Also, having such stuff in the document makes them less
usable in other environments (like if we incorporate a general
<INCLUDE> scheme in HTML+).


Secondly, how should <TITLE> and <H1> be used?

IMO, many use <H1> as a title just because their browser presents
the <TITLE> very anonymously - i.e., a <H1> is presented in a
larger font and thus stands more out.


Hm, I swear I had some other things on my mind too, but they
must've taken cover for the moment. :)


Bye,
-- 
Bjoern Stabell
(bjoerns@staff.cs.uit.no)



From jtilton@jupiter.willamette.edu  Sun Mar  6 17:03:58 1994 --100
Message-Id: <Pine.3.88.9403021043.A20740-0100000@jupiter>
Date: Sun, 6 Mar 1994 17:03:58 --100
From: jtilton@jupiter.willamette.edu (James)
Subject: HTML+ Current Specifications?

Hi -- I'm curious as to what the current state of the HTML+ spec is, and 
where I should look to find the most up to date DTD and documentation.

Thanks!

							-et



From kama@eng.auth.gr  Sun Mar  6 17:12:15 1994 --100
Message-Id: <9403021913.AA02378@vergina.eng.auth.gr>
Date: Sun, 6 Mar 1994 17:12:15 --100
From: kama@eng.auth.gr (Kostas Amarantidis)
Subject: 

ADD Kostas Amarantidis



From michael.shiplett@umich.edu  Sun Mar  6 17:16:30 1994 --100
Message-Id: <199403031339.IAA22005@totalrecall.rs.itd.umich.edu>
Date: Sun, 6 Mar 1994 17:16:30 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Insecure WWW Access Authorization Protocol?

  I had a colleague look over a proposed Kerberos-based HTTP
authentication protocol I had closely based on the PEM/PGP & RIPEM
exchanges. He pointed out that a man-in-the-middle attack could allow
an evil entity to masquerade as the server since the exchange goes
from cleartext to encrypted.

    Alice wishes to access Bob's server.
    Mallot is in position to intercept all of Alice's requests.
    Alice sends a cleartext request for a restricted file from Bob.
    Mallot intercepts the request.
    Mallot sends Alice a response containing information for
      authenticating to Mallot's server.
    Alice sends a request encrypted for Mallot's server.
    Mallot decrypts the request..
    Mallot sends valid mutual authenticator to Alice.
    Mallot has successfully spoofed Bob's server.

  Unless Alice knows how to authenticate to Bob prior to initiating
the transaction, Mallot will be able to subvert Alice's request. The
basic flaw is Alice relies on the "401 Unauthorized" response for
authentication information, e.g., public key, Kerberos principal, etc.

  I think this attack would work against any authentication
protocol following the WWW Access Authorization protocol examples.

michael



From yezdi@media.mit.edu  Sun Mar  6 17:20:31 1994 --100
Message-Id: <9403041517.AA05079@media.mit.edu>
Date: Sun, 6 Mar 1994 17:20:31 --100
From: yezdi@media.mit.edu (Yezdi Lashkari)
Subject: Agents for the Web ?? 


  Hello everyone. Our group at the Media Laboratory builds
  autonomous interface agents for various classes of 
  applications that use machine learning and AI techniques
  to learn their users patterns and take actions autonomously
  on their behalf after a point. We've successfully built agents
  for a variety of applications such as netnews, email etc.
  
  We are seriously exploring the possibility of an agent for
  WWW/Mosaic. The problem with the WWW is that new exciting
  information comes into existence almost every day all over
  the world. Some of it may be potentially interesting to 
  one. What we'd like is to embed an agent in one's favourite
  WWW browser that learns what types of documents its user finds
  interesting, contacts other users' agents over the net and 
  asks them to recommend new stuff they've discovered. Over time
  agents learn which agents to trust and which not to, for different
  classes of documents. Agents may also recommend peers or forward 
  URLs proactively to other agents that they feel may be interested.
  
  Note that mapping the web is not the solution as
   
  - the map is probably out of date pretty soon
  - resource limitations probably prevent it being comphrehensive
  - browsing through a massive database of www stats is really not
    a solution to our problem
  
  What we want is a distributed form of collaborative filtering for
  all the dynamic information out there on the Web. In addition we
  plan to combine this with a form of social filtering for groups
  of peers and URLs which is very effective for non text documents.
  
  Does anyone know of any work in this area ? I'd really like pointers
  to any work that people know about vis-a-vis the Web/Mosaic and for
  this sort of task. I also welcome comments/critiques on the proposed
  approach's feasibility/performance/scalability etc.
  
  thanks in advance
  
  Yezdi
 




From stumpf@informatik.tu-muenchen.de  Sun Mar  6 17:25:10 1994 --100
Message-Id: <stumpf.762871127@hphalle0>
Date: Sun, 6 Mar 1994 17:25:10 --100
From: stumpf@informatik.tu-muenchen.de (Markus Stumpf)
Subject: Re: complaint about CGI

Could we also have some support for the HTTP response the server
will send?

It would really be nice to be able to have a CGI script generate
e.g. a FORBIDDEN message.
All I managed to produce so far is to send back some HTML document
that says forbidden but the server still tells OK.

When trying to implement e.g. proxy gateways via CGI scripts
this is really needed I think or otherwise we can't have a
transparent handling of the data.

Or is it already there and I didn't notice that?

	\Maex



From martin@rare.nl  Sun Mar  6 17:29:31 1994 --100
Message-Id: <199403061356.AA07526@erasmus.rare.nl>
Date: Sun, 6 Mar 1994 17:29:31 --100
From: martin@rare.nl (John Martin)
Subject: Framemaker -> HTML

Hi,

Can anyone comment on the HTML conversion capabilities of Framemaker to HTML
(on a Mac) please.

Alternatively if you could suggest an editor for use on a Mac, that would be
useful also.

Please send replies to me amd I will summarise to the list.

John



From dcrocker@mordor.stanford.edu  Sun Mar  6 18:54:01 1994 --100
Message-Id: <199403061750.JAA26435@Mordor.Stanford.EDU>
Date: Sun, 6 Mar 1994 18:54:01 --100
From: dcrocker@mordor.stanford.edu (Dave Crocker)
Subject: Re: Agents for the Web ??

At  5:21 PM 3/6/94 +0000, Yezdi Lashkari wrote:
>
>  What we want is a distributed form of collaborative filtering for
>  all the dynamic information out there on the Web. In addition we
>  plan to combine this with a form of social filtering for groups
   ...
>  Does anyone know of any work in this area ?

I believe you have two different requirements.  The first is for
an infratructure of distributed agent technology.  That is, the ability
to have programs talk to each other (and maybe "visit" each other)
around the Internet.  The second requirement is specific to the use
of such an infrastructure, for doing 'collaborative' or 'social' filtering
or the like.

With regard to the former, there has been development of a derivative
of TCL, called safe-tcl, to allow tcl code to visit remote locations without
danger to the executing host.  While this has been done in the context
of email, it will apply to other situations, and the Web seems a perfect
candidate.  (Since safe-tcl has been folded into Mime, folding it into
the Web ought to be quite straightforward.)

You can join the safe-tcl discussion by sending mail to:

        safe-tcl-request@CS.UTK.EDU

Openly-available specifications and software which implements safe-tcl can
be obtained from:

        ftp.ics.uci.edu:/mrose/safe-tcl

It was developed by Marshall Rose and Nathaniel Borenstein.

Dave Crocker





From phillips@cs.ubc.ca  Sun Mar  6 20:33:39 1994 --100
Message-Id: <7686*phillips@cs.ubc.ca>
Date: Sun, 6 Mar 1994 20:33:39 --100
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: CGI/1.1 draft

Rob said:
>Here are my proposals for changes to CGI, to make its version 1.1:
..
>2. Add a new env. variable called HTTP_EXTRA_HEADERS which contains all
>   headers sent by the client which the server didn't understand. This does
>   not include Authorized, Accept, Content-type, or Content-length as these
>   are already elsewhere in the CGI variable space. The server may perform
>   collapsing of these lines, i.e. it may consolidate multiple occurrences
>   of these lines as it already does with HTTP_ACCEPT.

While Accept:, Content-Type: and Content-Length: are literally in
the CGI variable space, only a munged version of Authorized: is
there.  How about changing that to "does not include Authorized:
if server is doing the authentication on that script, otherwise
is does."  For a script with simple authorization requirements,
the server can do the work.  When the script gateways into something
where server authentication is inappropriate or impossible
(like an Oracle database), the writer can use an "nph-" script and
do the authentication herself.

>Finally, note that I don't mention whether PATH_INFO should be unescaped or
>not. My first impression is that it should remain escaped, in order to avoid
>ambiguities like the decoding of foo="1%3d2". Problem is, all of the current
>implementations are ``broken'', and therefore such a change technically
>isn't backward compatible. So perhaps we should update the spec. to reflect
>the implementations. Comments?

Please, please leave PATH_INFO escaped.  It was a mistake to do the
unescaping in the server; let's fix it.  Sure, it's not strictly
backwards compatible, but I seriously doubt many scripts relied upon
the old behaviour.  Besides "%3d", there's also "%00" which a CGI
script really loses on.



From phillips@cs.ubc.ca  Sun Mar  6 20:55:15 1994 --100
Message-Id: <7687*phillips@cs.ubc.ca>
Date: Sun, 6 Mar 1994 20:55:15 --100
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: CGI stuff

Rob said:
>Hmmm, the problem I was considering was that I believed gopher could not
>support arbitrary types returned by a script. From what I just read of the
>GN documentation, it seems that GN supports it. In the interests of script
>simplicity perhaps we should make a variable such as PROTOCOL_URL_PREFIX
>which contains http:// or gopher:// etc.

Well, it's a semi-foward looking thing.  Right now, we only have
HTTP commonly using CGI.  I also have my x-exec: browser extension
which probably isn't in use on a large scale.  If other access
protocols get used, PROTOCOL_URL_PREFIX will be a big win in using
those dusty CGI scripts.  Should you add the variable, I'd recommend
having it include the nearly redundant host:port info since there's
no guarantee that $PROTOCOL_URL_PREFIX$SERVER_HOST:$SERVER_PORT is
the right thing to say.

>It's curious that you would like to see protocol independence (in URL
>returns), but want to introduce a completely HTTP-dependent feature into
>CGI/1.1. How does npa- fit into gopher CGI execution? (John?)

I see CGI in two ways.  First, as an abstract environment where
programs can take URLs and spit out MIME objects.  I think this
is a good thing to have.  Second, CGI is to HTTP as inetd is to TCP/IP.
It is also useful as a vehicle to do everything an HTTP server
can do without having to write your own server.  Dropping in
CGI scripts that do things your server can't do is what CGI
is all about.  Why make any limitations on those capabilites by
not passing all or the headers or insisting that some of them
be server-only like Authorization:?

Maybe you don't agree, but I think that at least explains my
apparently contradictory suggestions.  Anyhow, if all the rest of
the headers are in REQUEST_EXTRA_HEADERS, that's as good as "npa-"
and is fine by me.



From fielding@simplon.ICS.UCI.EDU  Mon Mar  7 10:10:06 1994 --100
Message-Id: <9403070105.aa20985@paris.ics.uci.edu>
Date: Mon, 7 Mar 1994 10:10:06 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Questions about HTML conventions 

Bjoern writes:

> First, where should meta information for documents be?
> 
> Already, some information are in the MIME-headers (like
> Content-type and Last-modified) but the HEAD-part of HTML
> documents have some potential.

Yes, although that assumes that the document is in HTML.  This
is why the META element was added to the HTML+ specification.
Some further discussion of metainformation can be found in

   http://www.ics.uci.edu/WebSoft/MOMspider/#SpecChange


> Secondly, how should <TITLE> and <H1> be used?
> 
> IMO, many use <H1> as a title just because their browser presents
> the <TITLE> very anonymously - i.e., a <H1> is presented in a
> larger font and thus stands more out.

Some people repeat the title in an H1 block, some don't.
Many of my documents include an inline gif (of our University mascot)
within the H1 block.  Thus, I think TITLE's and H1's should be
considered independent of each other.

It would be nice if browsers had a command to display the documents's header
information (say, in a transient pop-up window, or some such).  Having the
Title and URL displayed is a necessity, but there are many other potential
metainfo which are best left hidden unless the reader explicitly requests
to see them.  Otherwise, the browsing process gets too cluttered.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From fielding@simplon.ICS.UCI.EDU  Mon Mar  7 10:21:51 1994 --100
Message-Id: <9403070117.aa21425@paris.ics.uci.edu>
Date: Mon, 7 Mar 1994 10:21:51 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: CGI/1.1 draft 

Rob writes:

> One idea I have found interesting is the idea of making some kind of
> configuration directory available. Something like CGI_CONFIG which would be
> set to a central location of CGI configuration files may be nice, but then
> people will all have to have write access to this directory. So I'm not sure
> if this would help much either. This could be set to SERVER_ROOT/conf for
> NCSA httpd, or just allow users to configure it. The point would be to
> create a central repository for CGI config. files which would be available
> on any server.

Given the option, I would prefer that script config files (including
imagemap's) have their own directory other than the SERVER_ROOT/conf dir.
I need to make special precautions against rabid scripts and I find it
much easier to keep organized (and avoid overlooking something) if all the
script-specific stuff is in a separate location.  Also, it makes it slightly
easier to install new versions of the server.

Thus, I like the idea but would prefer the location SERVER_ROOT/cgi-conf


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From dsr@hplb.hpl.hp.com  Mon Mar  7 12:20:35 1994 --100
Message-Id: <9403071112.AA28694@manuel.hpl.hp.com>
Date: Mon, 7 Mar 1994 12:20:35 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Insecure WWW Access Authorization Protocol?

Michael Shiplett writes:

> I had a colleague look over a proposed Kerberos-based HTTP
> authentication protocol I had closely based on the PEM/PGP & RIPEM
> exchanges. He pointed out that a man-in-the-middle attack could allow
> an evil entity to masquerade as the server since the exchange goes
> from cleartext to encrypted.

>    Alice wishes to access Bob's server.
>    Mallot is in position to intercept all of Alice's requests.
>    Alice sends a cleartext request for a restricted file from Bob.
>    Mallot intercepts the request.
>    Mallot sends Alice a response containing information for
>      authenticating to Mallot's server.
>    Alice sends a request encrypted for Mallot's server.
>    Mallot decrypts the request..
>    Mallot sends valid mutual authenticator to Alice.
>    Mallot has successfully spoofed Bob's server.

>  Unless Alice knows how to authenticate to Bob prior to initiating
> the transaction, Mallot will be able to subvert Alice's request. The
> basic flaw is Alice relies on the "401 Unauthorized" response for
> authentication information, e.g., public key, Kerberos principal, etc.

>  I think this attack would work against any authentication
> protocol following the WWW Access Authorization protocol examples.

A trusted third party is necessary to allow Alice to authenticate Bob
and Bob to authenticate Alice. It still makes sense for Bob and Alice
to send their respective public keys in their messages, but you need
an alternative means of checking that these keys are correct.

RFC1422 describes the certificate-based key management needed for this.
Alice and Bob include in their messages issuer certificates declaring
their public keys, which were sealed by a trusted third party whose public
key is known to both Alice and Bob. Mallot cannot spoof these seals.

This gives Alice confidence in Bob's avowed public key and vice versa.
Alice can now use Bob's public key to check that the response is really
from Bob and not from Mallot. Mallot can't spoof Bob's originator
certificate since Mallot doesn't know Bob's private key. Note that
including both issuer and originator certificates in HTTP requests
and replies avoids the need for out of band messages.

The format for these is defined in RFC1422 and illustrated in RFC1421.
The message body should be sent in binary using the direct output of the
symmetric encryption algorithm, since there is little point in incurring
overheads needed to circumvent the limitations of email routers.

So my advice is to use tried and tested public key management. Export
controls are a minor irritation for software distribution since it forces
US sites to unbundle the security code from the browsers and servers.
Outside the US this is not a problem as all the necessary components are
available as unrestricted freeware from a number of anonymous ftp sites.
There is, I presume, no restriction on *importing* encryption software
into the US?

Dave Raggett



From P.Lister@cranfield.ac.uk  Mon Mar  7 12:29:35 1994 --100
Message-Id: <9403071124.AA02501@xdm039.ccc.cranfield.ac.uk>
Date: Mon, 7 Mar 1994 12:29:35 --100
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Re: Insecure WWW Access Authorization Protocol?

>   I had a colleague look over a proposed Kerberos-based HTTP
> authentication protocol I had closely based on the PEM/PGP & RIPEM
> exchanges. He pointed out that a man-in-the-middle attack could allow
> an evil entity to masquerade as the server since the exchange goes
> from cleartext to encrypted.

Uh? What's that got to do with it? Most Kerberos communications are in clear 
except for the actual encrypted tickets.

>     Alice wishes to access Bob's server.
OK, first of all she (alice@SOMEWHERE) gets a ticket for http.bob@SOMEWHERE
>     Mallot is in position to intercept all of Alice's requests.
Kerberos assume *everyone* can hear *everything*
>     Alice sends a cleartext request for a restricted file from Bob.
Alice can't very well do otherwise without a shared key (Kerberos having no 
public-key encyrption). But Alice precedes the request with a ticket that she 
has obtained from the Kerberos server.
>     Mallot intercepts the request.
Kerberos assumes he (?) will.
>     Mallot sends Alice a response containing information for
>       authenticating to Mallot's server.
Mallot can try, but it won't do him any good since only Bob could have 
decrypted Alice's ticket, and Alice will only accept a reply based on a ticket 
that has been decrypted and modified in the correct way - which only Bob can 
do.
>     Alice sends a request encrypted for Mallot's server.
If Alice has any sense, she waits for Mallot to send a mutual authentication 
reply, which could only have come from Bob.
>     Mallot decrypts the request..
Mallot can only decrypt things he has keys for. He doesn't know either Alice 
or Bob's key, so what the hell can he do? Anyway, by this time Alice has 
dropped the connection since she didn't get a correct reply.
>     Mallot sends valid mutual authenticator to Alice.
Which she ignores.
>     Mallot has successfully spoofed Bob's server.
No he hasn't

>   Unless Alice knows how to authenticate to Bob prior to initiating
> the transaction, Mallot will be able to subvert Alice's request. The
> basic flaw is Alice relies on the "401 Unauthorized" response for
> authentication information, e.g., public key, Kerberos principal, etc.

Kerberos does indeed assume that a client obtains a ticket from the Kerberos 
server *before* starting the first connection is made. I'm not a real expert 
on the internals of either HTTP or Kerberos, but I am a Kerberos site admin, 
and have Kerberised an application, and I assure you that for any normal 
Kerberos client/server connection, the server really must convince the client 
that it holds the service key in question before the client sends anything. 
OK, clients *can* be naively trusting if they want, but there's no point, is 
there?

>   I think this attack would work against any authentication
> protocol following the WWW Access Authorization protocol examples.

But not Kerberos mutual authentication. Would anyone really trust an 
authentication system that *didn't* authenticate server to client?

Where are the protocol examples, anyway? I think I need to have a look.

Peter Lister                             Email: p.lister@cranfield.ac.uk
Computer Centre, Cranfield University    Voice: +44 234 754200 ext 2828
Cranfield, Bedfordshire MK43 0AL UK        Fax: +44 234 750875
--- Go stick your head in a pig.  (R) Sirius Cybernetics Corporation ---




From P.Lister@cranfield.ac.uk  Mon Mar  7 12:51:35 1994 --100
Message-Id: <9403071147.AA02606@xdm039.ccc.cranfield.ac.uk>
Date: Mon, 7 Mar 1994 12:51:35 --100
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Re: Insecure WWW Access Authorization Protocol?

> So my advice is to use tried and tested public key management. Export

But I want tried and tested Kerberos, Dave!! Many sites use Kerberos now, and 
it's much more important for me to be able to authenticate my local users (who 
already have Kerberos tickets when they login), rather than off-site users. 
While I wouldn't stop local users from using PGP/PEM if they want, I don't 
want to have to put another authentication system in place just to 
authenticate WWW when I have a perfectly good Kerberos set up already.

Peter Lister                             Email: p.lister@cranfield.ac.uk
Computer Centre, Cranfield University    Voice: +44 234 754200 ext 2828
Cranfield, Bedfordshire MK43 0AL UK        Fax: +44 234 750875
--- Go stick your head in a pig.  (R) Sirius Cybernetics Corporation ---




From michael.shiplett@umich.edu  Mon Mar  7 15:22:37 1994 --100
Message-Id: <199403071417.JAB19518@totalrecall.rs.itd.umich.edu>
Date: Mon, 7 Mar 1994 15:22:37 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: Insecure WWW Access Authorization Protocol? 


"pl" == Peter Lister, Cranfield Computer Centre <P.Lister@cranfield.ac.uk> writes:

>> I had a colleague look over a proposed Kerberos-based HTTP
>> authentication protocol I had closely based on the PEM/PGP & RIPEM
>> exchanges. He pointed out that a man-in-the-middle attack could allow
>> an evil entity to masquerade as the server since the exchange goes
>> from cleartext to encrypted.

pl> Uh? What's that got to do with it? Most Kerberos communications
pl> are in clear except for the actual encrypted tickets.
  Most but not all--e.g., kerberized telnet with the encryption
``on.'' I intend to use Kerberos in order to provide mutual
authentication and to use the session key to encrypt things like a
form submittal or a document request.

>> Alice wishes to access Bob's server.
pl> OK, first of all she (alice@SOMEWHERE) gets a ticket for
pl> http.bob@SOMEWHERE
  This is the crux of the problem--How does Alice know for what
principal to get a ticket before initiating a transaction with Bob?
Following the WWW Access Authorization protocol, all of the initial
exchange--***including the server's identity***--is in cleartext. If
Alice knows what ticket she needs a priori, then Kerberos and other
authentication methods become trivial.

  The problem is how to convey the information needed for
authentication. Using a Kerberos environment as an example, if I want
to initiate a kerberized telnet session to the machine cyber.foo.com,
the ktelnet protocol says to get a ticket for 'rcmd.cyber@FOO.COM'
(I'm not sure about the realm, but the name & instance are
correct). Anyway, the point is that the protocol specifies the
``server'' identity. In HTTP, it is the server itself which gives this
information.

  After speaking with another colleague, we came up with a method of
identifying the server based on the URL. In the following, Alice is
the client, attempting to get http://bob.foo.com/docs/protected.html.

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

Client:
GET /docs/protected.html HTTP/1.0
UserAgent: Mosaic/X 2.2


Server:
HTTP/1.0 401 Unauthorized
WWW-Authenticate: KerberosV4 principal=``name.instance@realm''
Server: NCSA/1.1


Client:
GET / HTTP/1.0
Authorized: KerberosV4
            ticket=``uuencoded authenticator and ticket''

[NOTE: Alice ignores any non-protocol ``WWW-Authenticate''
       information. In this case, she gets a ticket for & creates an
       authenticator for k4-http.bob_foo_com@FOO.COM.]

<the real request, encrypted with session key>


Server:
HTTP/1.0 200 OK
WWW-Authentication: KerberosV4
                    ticket=``uuencoded mutual authenticator response''

<the reply, encrypted with session key>

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

  I'm not familiar enough with HTTP to know what information the
browser is ``allowed'' to get from the URL, but the browser needs to
get the authentication identity from someplace other than the
server. The advantage of using the URL is that it is already there and
should contain all the necessary authencation identity information.

  Use of the URL means that the client, of course, needs to get the
URL from a trusted source (widely available documentation, an HTML
document acquired through a secure channel, etc.).

  Also, the server needs to keep around replay detection information.

pl> Where are the protocol examples, anyway? I think I need to have a
pl> look.
  http://info.cern.ch/hypertext/WWW/AccessAuthorization/ProtocolExamples.html


michael



From wa@mcc.com  Insecure WWW Access Authorization Protocol?Date: Sun, 6 Mar 1994 17:15:34 --100
Message-Id: <9403071648.AA29773@coyote.mcc.com>
Date: Insecure WWW Access Authorization Protocol?Date: Sun, 6 Mar 1994 17:15:34 --100
From: wa@mcc.com (Wayne Allen)
Subject: 


   Reply-To: michael.shiplett@umich.edu
   Originator: www-talk@info.cern.ch
   Sender: www-talk@www0.cern.ch
   Precedence: bulk
   From: michael shiplett <michael.shiplett@umich.edu>
   X-Listprocessor-Version: 6.0c -- ListProcessor by Anastasios Kotsikonas
   Content-Length: 1212

     I had a colleague look over a proposed Kerberos-based HTTP
   authentication protocol I had closely based on the PEM/PGP & RIPEM
   exchanges. He pointed out that a man-in-the-middle attack could allow
   an evil entity to masquerade as the server since the exchange goes
   from cleartext to encrypted.

       Alice wishes to access Bob's server.

       Mallot is in position to intercept all of Alice's requests.
       Alice sends a cleartext request for a restricted file from Bob.
       Mallot intercepts the request.
       Mallot sends Alice a response containing information for
	 authenticating to Mallot's server.

You seem to be implying that Mallot's information can cause Alice's
client to attempt authentication by contacting Mallot's (evil)
Kerberos server to get a spoofed ticket for Mallot's server, since
Alice's realm server obviously won't deliver one.

In the case of Kerberos, Alice already knows the address of the
service she wants (Bob's address), and does not need any additional
information except the service (principle) name and the name of the
realm the server is *registered* in (*not*, as is commonly thought,
the realm in which Alice must authenticate). If this realm is
different from her own, Alice asks her *own* realm server for
cross-athentication between her realm and the server's target realm.
If there is a pre-existing agreement between the realms (either in V4
or V5 style), authentication can proceed, otherwise not.

Mallot cannot re-direct Alice's ticket request in any case. If
Mallot's fake Kerberos server does not have *Alice's* secret service
key, it cannot generate a valid ticket that Alice's client can
decrypt.

Unless Mallot has conspired with Alice's realm administrator allow cross-
authentication with his rogue realm, it's not possible for Mallot to
spoof Alice in this way.


Cheers,
--
 wa | Wayne Allen, EINet - wa@einet.net
    | MCC/ISD, 3500 West Balcones Center Dr, Austin, Tx 78759




From connolly@hal.com  Mon Mar  7 19:02:13 1994 --100
Message-Id: <9403071733.AA04599@ulua.hal.com>
Date: Mon, 7 Mar 1994 19:02:13 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: TODO: Machine-readable server announcements


This is a MIME message. If your software presents this
paragraph to you, it's not MIME compliant. See RFC1341 for details.

Well -- another explanation is that the www-talk list processor
seems to eath MIME-Version headers. Ack! Thtptptp!

--8<

There are a lot of mail messages and news articles of the
form

	"We're proud to announce a WWW server for
	avid bird-watchers at:
		http:www.bird-watchers.com
	"

I expect what follows is that lots of folks copy the URL
into their home page or their server's root page...

Isn't this an obvious candidate for automation?

This is another issue that the WAIS folks seem to have thought
about... their software comes with directions on how to send
a .src file to quake.think.com such that it automatically becomes
part of the directory-of-servers database.

Granted, the directory-of-servers becomes one huge flat database,
but WAIS is quite a good solution to that problem.

Anyway... I propose that folks who want to announce access to
an internet resource through mail or news use MIME to make
their announcement machine-readable. For example:


--8<
Content-Type: message/rfc822

To: www-announce@info.cern.ch
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="cut-here"

This is a MIME message. If your software presents this
paragraph to you, it's not MIME compliant. See RFC1341 for details.

--cut-here

We're pleased to announce our server...	
[blah blah blah in plain text]

--cut-here
Content-Type: text/x-html

<HTML><HEAD>
<TITLE>Bird Watching WWW Server Announcement</TITLE>
</HEAD>
<BODY>
<H1>Attention Internet Bird-Watchers!</H1>

We've collected lots of nifty info and made it availble on
<A HREF="http://www.bird-watching.com/">our server.</A>

Enjoy!

--cut-here--

--8<

This would allow folks to subscribe to www-announce (and/or
comp.infosystems.announce) and build knowbots to attack the
resource discovery problem.

We could even build a sort of _distributed_ database of
internet resources by having resource owners periodically
post announcements to comp.infosystems.directory-of-servers.

This could function as the "transponder" layer that
Weider and Deutsch dreamed up in
<ftp://ds.internic.net/internet-drafts/draft-ietf-iiir-vision-01.txt>

They drew a picture of internet publishing where
	* an author develops a resource
	* the author asks a publisher to publish the resource
	* the publisher allocats a URN from its namespace
	* the publisher installs a "transponder" that maps the URN
	  to some set of URLs (I guess this is a new TCP/IP service?)
	* folks can register interest in certain types of resources
	* the transponder notifies the folks of the new resource
	* those folks give the URN from the notification back to the
	  transponder in exchange for URLs
	* the folks access the resource

I think the vision can be realized using NNTP and local database mechanisms.
For example:

	* an author develops a resource
	* the author/publisher allocates a URN, for example, by generating a
	  message-id; something like: <x-message-id:1994.Feb.07lkjldj@foo.com>
	* the author/publisher installs a cron job that posts a
	  machine-readable announcement (be it an application/x-wais-source,
	  a text/x-html doc, an application/x-gopher-list, whatever)
	  to comp.infosystems.announce on a periodic basis
	* other sites install news-knobots that archive
	  comp.infosystems.announce (and process expiration... like FAQ
	  distribution) and provide local database searching
	  (e.g. a local WAIS database, or Fulcrum or Verity...)

The feature where users register interest in certain kinds of documents
(or in changes to certain documents, etc. ...) should be addressed in
a scalable fashion. A distributed notification system should be implemented
using a subscription-based strategy (look at Athena Zephyr and USENET NEWS
for successful examples). It's inefficient to maintain a mapping of each
of the N resources to each of the M consumers.

So, for example, we could establish a new branch of the USENET news
hierarchy for announcements about URN's (hmmm... or better yet, a whole
bunch of new leaves where appropriate; e.g. rather than
	urn.comp.sources for announcements about source archives, use
	comp.sources.urn )

The nifty part about this is that the question,
"What resources related to bird-watching have been
announced in the last week?" is already represetable in NNTP:
(I can't recall the exact syntax, but it's something like:)

	group rec.bird-watching.urn
	newnews 19940301000000Z


Comments?

Dan

--8<--





From drlewi1@srv.pacbell.com  Mon Mar  7 19:14:01 1994 --100
Message-Id: <9403071752.AA06595@zeus.srv.PacBell.COM>
Date: Mon, 7 Mar 1994 19:14:01 --100
From: drlewi1@srv.pacbell.com (Dave Lewis)
Subject: San Fran. Bay Area March 24th SIGWEB Meeting Announcement

******  March SIGWEB Meeting Announcement  ******

The March SIGWEB meeting will be held at Stanford University, on March
24th from 2 to 5 pm.  

Featured Speaker:  	Dr. Jay M. Tenenbaum, 
			Founder and CEO
			Enterprise Integration Technologies,

Title of Talk:
  			      CommerceNet:
	     Spontaneous Electronic Commerce on the Internet

Imagine an electronic marketplace where buyers and sellers from thousands of  
companies across Northern California come together to transact business with 
each other -- and with customers and suppliers from around the world.

Imagine

- Using online directories, brokers and referral services to locate a
critical, but hard to find part;

- Browsing through multimedia catalogs and calling up video product demos;

- Having instant access to competitive pricing from a wide variety of
online suppliers -- and then placing your order by making a selection
on your computer screen;

- Marketing your products or services to customers around the world by simply 
posting information on your own computer;

- Delivering customized, full-color catalogs to customers anywhere in
the world for a fraction of the cost of distributing printed material;

- Solving a design problem, in real time, by collaborating with
development partners at remote sites.

You've just imagined CommerceNet, Northern California's electronic
marketplace. It will be here soon, and it will revolutionize the way
business is done. 

We will present CommerceNet from the perspective of both users and
information providers, and describe the services that we are
developing to make the Internet suitable for business use.

			 -------------------

The remainder of the afternoon will be filled out as follows:

SIGWEB Business and Announcements of Related Events/Activities (brief)
	
SIGWEB Member Input Survey
	The SIGWEB volunteer corps is preparing a survey to collect member input
	to aid development of the SIGWEB organization and its member services.  
	Email and WWW-Forms versions will also be available.

Birds of a Feather/Discussion Groups
	In response to SIGWEB member feedback, there will be ample time
	for informal introductions and discussions with others working and/or 
	interested in various topics of interest to SIGWEB members,
	such as:
		Networked Information Delivery and Retrieval
		Gopher, WAIS, WWW, Mosaic, Cello, Z39.50, SGML, HTML(+), HTTP 
		What's Hot in URLs, Gopher Sites, WAIS Sources

	Several "Birds of a Feather" discussion group's will be organized.  
	Some topics will be suggested but others may be created by attendees.

Refreshments will be served.

Questions concerning this meeting may be directed either toward Dave Lewis -
meeting coordinator (drlewi1@pacbell.com, 510-823-2831), or Chris McRae - 
SIGWEB President, (mcrae@ora.com, 415-242-9623)

--------------------------------------------------------------------------------

March SIGWEB Meeting
Date: Thursday, March 24th
Time: 2:00 - 5:00 p.m.
Location:
	Building 420 (Jordan Hall), Room 40
	Stanford University
	Stanford, CA

Directions to Stanford Campus and Meeting Room:

(There is a Stanford University Campus Map at:
	http://www.stanford.edu/bitmaps/stanford-map.jpg)

>From Highway 101 Northbound:
	Exit at Embarcadaro Road/Oregon Expressway
	    - follow off-ramp signs to Embarcadaro Rd West/Stanford U. exit
		(it's the third turn-off once on the off-ramp)
	Continue with "From Embacadaro Road Westbound" below

>From Highway 101 Southbound:
	Exit at Embarcadaro Road - head west

>From Embarcadaro Road  Westbound:
	Follow Embarcadaro; under The Alma, across El Camino, onto campus.
	Park in a metered space (e.g. strait to The Oval, or Tressider Union lot)
	use a campus map to find Jordan Hall

>From 280 North or South:
	Take Page Mill Rd. exit, head east (toward bay)
	Take left at Foothill Expressway (at bottom of long downhill)
	At Campus (2nd light), turn right.
	Park in a metered space (e.g. Tressider Union lot)
	using a campus map to find Jordan Hall

	 







From elevinso@Accurate.COM  Tue Mar  8 01:57:24 1994 --100
Message-Id: <9403072226.AA02733@Accurate.COM>
Date: Tue, 8 Mar 1994 01:57:24 --100
From: elevinso@Accurate.COM (Ed Levinson)
Subject: Re: Comments on MIME/SGML 

Tim,

Your suggestion to use URIs in the Content-IDs is quite appropriate. 
I did not use your
suggestion, which I acknowledge you made earlier and to which I did
not respond, for two reasons.  First, it is important for me to have
the entire document self contained; second, I am unable to point to
URIs through an Internet standard, unfortunately we haven't gotten
there yet.

Is there any reason, however, that someone cannot use a URI in the
Content-ID.  What language would you suggest that would open up the
usage to your proposal.  From my point of view, URIs are useful.  Another
way I think they can be used is as a Message/External-body.  That way
is more general and URIs are available beyond SGML.  Is there a proposal
for doing that?

Thanks.../Ed

repl: bad addresses:
	distribution.@dimacs.rutgers.edu; (see end of body) -- no colon found to terminate route (;)
> 
> 
> Dan,
> 
> >    1. We make the following correspondence between the terms of the SGML
> >    standard and the MIME RFC:
>     
> 
> ....
> >        SGML SYSTEM identifier => MIME Content-ID
> 
> 	I proposed in an earlier note that the SGML SYSTEM identifier
> 	should be a URI and that a special form of URI (cid:) should
> 	be used to specify a content identifier within a MIME
> 	multipart message.
> 	
> 
> 	SGM SYSTEM identifier  => URI
> 	
> 	
> 	This allows SGML to reference anything in the universal
> 	syntax, inclusing now URLs and later URNs, and
> 	other parts of a MIME object using the cid: form.
> 	I think this is important, as external references
> 	are important too, and one will need to mix them.
> 	
> 	Tim BL
> 	
> 
> %%% overflow headers %%%
> Cc: Multiple Recipients of List <ietf-822@dimacs.rutgers.edu>,
>         Dan Connolly -- HTML draft spec author <connolly@hal.com>,
>         Tim Berners-Lee -- WWW Project Lead <timbl@www0.cern.ch>,
>         Terry Allen GNN representative <terry@ora.com>,
>         Dave Raggett -- HTML+ draft spec author <dsr@hplb.hpl.hp.com>,
>         Dave Hollander -- SDL designer <dmh@hpfcma.fc.hp.com>,
>         Elliot Kimber -- HyTime advocate <kimber@passage.com>,
>         Peter Flynn -- HTML educator <pflynn@curia.ucc.ie>,
>         Rob McCool -- NCSA representative <robm@ncsa.uiuc.edu>,
>         Erik Naggum -- SGML advocate <erik@naggum.no>,
>         WWW Talk List -- Archive Mechanism <www-talk@www0.cern.ch>
> %%% end overflow headers %%%



From connolly@hal.com  Tue Mar  8 02:05:08 1994 --100
Message-Id: <9403080054.AA04800@ulua.hal.com>
Date: Tue, 8 Mar 1994 02:05:08 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Questions about HTML conventions 

In message <199403021725.SAA06528@tklab3.cs.uit.no>, Bjoern Stabell writes:
>
>First, where should meta information for documents be?
>
>Already, some information are in the MIME-headers (like
>Content-type and Last-modified) but the HEAD-part of HTML
>documents have some potential.

Ahh... very good question. I've been thinking about the same
thing lately. It seems to me that the HTML head/body structure should
be analagous to -- rather than subordinate to -- the RFC822 head/body
structure.

For example, I hope that one day the two following examples will be
treated identically by WWW clients:

ex 1:
	To: Multiple recipients of list <www-talk@info.cern.ch>
	Subject: Bird Watching
	Date: Sun, 06 Mar 1994 16:56:53
	Mime-Version: 1.0
	Content-Type: text/x-html

	<H1>Connolly on Bird Watching</H1>
	[blah blah blah]

ex 2:
	<HTML><HEAD>
	<TO>Multiple recipients of list <mailaddr mbox="www-talk"
					domain="info.cern.ch"></TO>
	<Subject>Bird Watching</Subject> <!-- or <TITLE> -->
	<Date ZULU="19940306165663">Sun, 06 Mar 1994 16:56:53</Date>
	</HEAD>
	<BODY>
	<H1>Connolly on Bird Watching</H1>
	[blah blah blah]
	</BODY>

>There is a trend today, it is even a recommended practice, to put
>meta info like 'last updated', 'creation date', 'creator',
>'modified by' and 'next document' in the body of documents.  At
>least Last-modified is transmitted by many http daemons in the
>MIME headers and so the browser should be able to present this,
>putting it in the body of the documents is superflous.

Agreed. Meta-information should be separate from document content. But
the whole idea behind hyportext is that the distinction is somewhat
blurry. For example, the server might be sending a cached HTML
conversion of a LaTeX document that was last modified 1/1/94. Should
the Last-Modified be the date of the converted HTML or the original
LaTeX file?

Actually, I'd like to see both. I'd like to see the server express in
machine readable form "I'm sending you foo.html, which was generated
on 19930701 by fred@foo.com from foo.html (which was last modified
19930405)." This requires an extension to the linking formalism of
HTML. For example:

	Subject: Connolly On BirdWatching
	Mime-Version: 1.0
	Content-Type: multipart/x-sgml; boundary="cut-here";
		dtd="ftp://info.cern.ch/new_html_dtd.sgml"

	--cut-here
	<!DOCTYPE HTML SYSTEM "ftp://info.cern.ch/new_html_dtd.sgml"
	<!ENTITY bodyContent SYSTEM "cid:19940305.lkjsdf@hal.com">
	>
	<HTML><HEAD>
	<resource id="r1">
	doc-id:hal.com/connolly/bird-watching</resource>
	<resource id="r2" octets="10754">
	http://www.hal.com/users/connolly/bird-watching.html</resource>
	<resource id="r3" octets="9432" notation="text/x-latex"
		 last-modified"19940301">
	ftp://ftp.hal.com/pub/connolly/bird-watching.tex</resource>
	<translation anchroles="orignal translation" degradation=".75"
		expiration="19940310"
		linkends="r3 r2">
	<locator anchroles="name location" expiration="19941001"
		linkends="r1 r3">
	<copy anchroles="orignal copy"
		linkends="r2 bodyContent">
	</HEAD>
	<BODY conref="bodyContent">
	</HTML>

	--cut-here
	Content-Type: text/x-html

	<H1>Bird Watching</H1>
	<H2>Abstract</H2>
	[... HTML conversion of bird-watching.tex]

	--cut-here--

>At least, it shouldn't be necessary to present a 'back to
>prevoius page' button at the bottom of every page.  It's not that
>hard to maintain a stack of previously visited URLs in the
>browser.  Also, having such stuff in the document makes them less
>usable in other environments (like if we incorporate a general
><INCLUDE> scheme in HTML+).

Amen. Another astute observation. The trick here is that (1) we're
pretty much using an SGML DTD to specify HTML (2) SGML DTDs are by
nature not very extensible, and (3) not everybody will want to use the
same schema for next/previous/up/down type stuff.

I expect what we need is for documents to be transmitted with all the
up/down/etc. stuff in pretty much every node, but labelled in such a
way that the browser can recognized the <next> tag or whatever and
display it as a button in a familiar place, rather than a link in the
text flow.

>Secondly, how should <TITLE> and <H1> be used?

Title is meta-information, and H1 is content. At least that's the
design. Yes it's redundant, but that's not bad, if you ask me.

Dan

------- End of Forwarded Message




From masinter@parc.xerox.com  Tue Mar  8 10:09:00 1994 --100
Message-Id: <94Mar8.010153pst.2732@golden.parc.xerox.com>
Date: Tue, 8 Mar 1994 10:09:00 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Questions about HTML conventions 

This is a response to Bjoern Stabell's post, which started with:
>First, where should meta information for documents be?
and Dan Connolly's response.

It is useful to separate different kinds of `meta-information'. Among
other things, there are:

* information about a specific encoding of a document
   e.g., size in bytes, number of pages, version of the
	representation-language, number of colors used, etc.

* information about the content of the document
   e.g., title, author, abstract, language it is written in

* information about the context of the document in its current
  location,
   e.g., the cost to access it, the number of times it's
   been accessed before, who stored it, how long it's been
   cached, who it was mailed to, who has read access to it.

Here is a simple rule:
  *  for things that can change even if the documents don't,
     don't bury them inside the things you store.

That makes sense if you don't want have to rewrite the file every time
you want to add another person to the access control list, etc.

Certainly, things like `back to previous page' probably don't fit into
this category. Certainly not the `next article' button at the bottom,
which probably has to change if you delete the next article.

If the clients can tell between intrinsic and extrinsic properties, it
doesn't matter as much how the data gets *transported*. If you think
that HTML is a transport mechanism, then you can just put the data
inside the HTML with appropriate tags. But if you're using HTML as a
storage format, too, then you want to be careful not to put some kinds
of metainformation inside the document files themselves.







 



From P.Lister@cranfield.ac.uk  Tue Mar  8 13:31:57 1994 --100
Message-Id: <9403081225.AA02507@xdm039.ccc.cranfield.ac.uk>
Date: Tue, 8 Mar 1994 13:31:57 --100
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Re: Insecure WWW Access Authorization Protocol?

My apologies for trying to teach michael.shiplett@umich.edu or anyone else to 
suck eggs about Kerberos. Kerberos admins regularly have to explain to the 
inexperienced that, no, they haven't discovered a new loophole, so I tend to 
assume that all claims of loopholes are based on misunderstanding. The problem 
definitely lies with the HTTP Authentication protocol as stated, not with 
Kerberos. Mea culpa. Here follow my rambings on how I now understand the 
problem, and my favoured solutions.

In a nutshell, the problem is the HTTP server saying to the client "I'm foo, 
you must provide me with authentication for service foo". A malevolent server 
can persuade the client that it is genuine, and plant false information or 
obtain secret information in a query. The (reasonable) assumption of Kerberos 
is that the client knows with which prinicpal it must mutually authenticate 
*before* making the connection. Since the only information known beforehand is 
the URL, we must map the URL to a Kerberos principal. 

> [NOTE: Alice ignores any non-protocol ``WWW-Authenticate''
>        information. In this case, she gets a ticket for & creates an
>        authenticator for k4-http.bob_foo_com@FOO.COM.]

I think we're agreed that only the server name is relevant (we are 
authenticating to the server, not the document). Current Kerberos practise 
seems to be to use dots in the instance, which can be touch confusing, since 
the instance is generally separated from the name with a dot, but it's 
liveable with, and I'd be happier with dots than underscores. I also reckon 
that "k4-" is redundant. So my preference in your example would be 
"http.bob.foo.com@FOO.COM", where the instance is "bob.foo.com".

>   I'm not familiar enough with HTTP to know what information the
> browser is ``allowed'' to get from the URL, but the browser needs to
> get the authentication identity from someplace other than the
> server. The advantage of using the URL is that it is already there and
> should contain all the necessary authencation identity information.

Hmm. Interesting point. The principal consists of three parts

The principal name is unique to the service. I suggest we use "http", the URL 
type, but it doesn't matter as long as everyone agrees and we stick to one. 
Protocol versions are irrelevant. Logically, the scheme extends to other URL 
types, e.g. gopher,ftp. Anyone know what the URL/URN people feel about this?

Typical Kerberos apps (with exceptions, e.g Zephyr) obtain the server name via 
a Hesiod service location entry (which we assume can be trusted), so the 
client sees the actual machine name, not just the alias (unlike a WWW client, 
unless it asks the dns for the reverse transalation of the IP address). The 
instance is conventionally the name of the server machine, either the short 
form or fully qualified domain name. At Cranfield, POP uses pop.xdm001; AFS 
uses afs.pegasus.cranfield.ac.uk (the fully qualified cell name). My 
preference is to use the URL server name verbatim as the instance, e.g. 
http://www.cranfield.ac.uk/ => http.www.cranfield.ac.uk. This does mean that 
if the same alias applies to several machines, their servers must all have the 
identical srvtab. No big deal, though it makes changing the service key harder 
in traditional Kerberos if it has to be done simultaneously on multiple 
machines.

We have to hand the server name in the URL to Kerberos and leave it to Do The 
Right Thing to work out the realm. Typically, a local (hence trusted) file 
e.g. krb.realms (or Transarc's CellServDB) maps remote domain names to realm 
(e.g. cranfield.ac.uk => PEGASUS.CRANFIELD.AC.UK, as is the case at 
Cranfield). For the client to get a ticket, either the local server must be 
capable of cross-realm authentication, or the user must have a principal in 
the remote realm, but Kerberos should sort that out - all the client needs to 
do is prompt for username/password when there's no ticket granting ticket yet 
- probably the case for all realms other than the local realm, and those it 
cross-authenticates with.

So I reckon the only info we need from the URL is just the server name, and 
I'm sure we're allowed to extract that.

>   Use of the URL means that the client, of course, needs to get the
> URL from a trusted source (widely available documentation, an HTML
> document acquired through a secure channel, etc.).

Would a trusted URN -> URL translations service satisfy this? Have the URN 
guys considered this?

Peter Lister                             Email: p.lister@cranfield.ac.uk
Computer Centre, Cranfield University    Voice: +44 234 754200 ext 2828
Cranfield, Bedfordshire MK43 0AL UK        Fax: +44 234 750875
--- Go stick your head in a pig.  (R) Sirius Cybernetics Corporation ---




From michael.shiplett@umich.edu  Tue Mar  8 14:47:55 1994 --100
Message-Id: <199403081338.IAA18921@totalrecall.rs.itd.umich.edu>
Date: Tue, 8 Mar 1994 14:47:55 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: Insecure WWW Access Authorization Protocol? 

  Even though the following uses Kerberos for much of the discussion,
the mapping of URLs to authentication identities is a generic issue
which needs to be resolved for all authentication methods.

"pl" == Peter Lister, Cranfield Computer Centre
	<P.Lister@cranfield.ac.uk> writes:

pl> In a nutshell, the problem is the HTTP server saying to the client
pl> "I'm foo, you must provide me with authentication for service
pl> foo". A malevolent server can persuade the client that it is
pl> genuine, and plant false information or obtain secret information
pl> in a query. The (reasonable) assumption of Kerberos is that the
pl> client knows with which prinicpal it must mutually authenticate
pl> *before* making the connection. Since the only information known
pl> beforehand is the URL, we must map the URL to a Kerberos
pl> principal.
  I think you've here stated the problem more clearly than I was able
to in my previous posts. This is indeed the situation I tried to
describe.

pl> Current Kerberos practise seems to be to use dots in the instance,
pl> which can be touch confusing, since the instance is generally
pl> separated from the name with a dot, but it's liveable with, and
pl> I'd be happier with dots than underscores. I also reckon that
pl> "k4-" is redundant. So my preference in your example would be
pl> "http.bob.foo.com@FOO.COM", where the instance is "bob.foo.com".
  As we use short hostnames for instances. I did not know if dots were
permitted in instances, hence the underscores. I comment below on my
use of ``k4-.''

>> I'm not familiar enough with HTTP to know what information the
>> browser is ``allowed'' to get from the URL, but the browser needs
>> to get the authentication identity from someplace other than the
>> server. The advantage of using the URL is that it is already there
>> and should contain all the necessary authencation identity
>> information.

pl> Hmm. Interesting point. The principal consists of three parts

pl> The principal name is unique to the service. I suggest we use
pl> "http", the URL type, but it doesn't matter as long as everyone
pl> agrees and we stick to one.  Protocol versions are
pl> irrelevant. Logically, the scheme extends to other URL types,
pl> e.g. gopher,ftp. Anyone know what the URL/URN people feel about
pl> this?
  The different protocols to which I refer are the authentication
protocols--k4, pgp, k5, etc.--not the connection methods--ftp, gopher,
http. I propose that the two together would, for Kerberos, be the
principal's name, e.g., k5-gopher.bob.foo.com@FOO.COM,
k4-http.bob.foo.com@FOO.COM. This would allow each connection method
to determine the authentication protocol.

pl> The instance is conventionally the name of the server machine,
pl> either the short form or fully qualified domain name. At
pl> Cranfield, POP uses pop.xdm001; AFS uses
pl> afs.pegasus.cranfield.ac.uk (the fully qualified cell name). My
pl> preference is to use the URL server name verbatim as the instance,
pl> e.g.  http://www.cranfield.ac.uk/ =>
pl> http.www.cranfield.ac.uk.
  I think one would, as you suggest, have to use the full server name.
For large sites, just the hostname sans domain name will not be
sufficient, e.g., Bob in Accounting should use
k4-http.bob.acct.foo.com@FOO.COM as opposed to k4-http.bob@FOO.COM
since the latter would collide with the machine for Bob in Sales,
bob.sales.foo.com.

pl> This does mean that if the same alias applies to several machines,
pl> their servers must all have the identical srvtab. No big deal,
pl> though it makes changing the service key harder in traditional
pl> Kerberos if it has to be done simultaneously on multiple machines.
  Since this is currently a problem for Kerberos in general, using
HTTP with Kerberos would not introduce this as a new problem.

pl> We have to hand the server name in the URL to Kerberos and leave
pl> it to Do The Right Thing to work out the realm. Typically, a local
pl> (hence trusted) file e.g. krb.realms (or Transarc's CellServDB)
pl> maps remote domain names to realm (e.g. cranfield.ac.uk =>
pl> PEGASUS.CRANFIELD.AC.UK, as is the case at Cranfield).
  I guess it was wishful thinking on my part to forget about the local
realm/cell config file.

pl> So I reckon the only info we need from the URL is just the server
pl> name, and I'm sure we're allowed to extract that.
  
>> Use of the URL means that the client, of course, needs to get the
>> URL from a trusted source (widely available documentation, an HTML
>> document acquired through a secure channel, etc.).

pl> Would a trusted URN -> URL translations service satisfy this? Have
pl> the URN guys considered this?
  I'm not familiar with the workings of URNs--I think the HTTP spec I
have lists them as "to be specified."


michael



From michael.shiplett@umich.edu  Tue Mar  8 14:47:55 1994 --100
Message-Id: <199403081338.IAA18921@totalrecall.rs.itd.umich.edu>
Date: Tue, 8 Mar 1994 14:47:55 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: Insecure WWW Access Authorization Protocol? 

  Even though the following uses Kerberos for much of the discussion,
the mapping of URLs to authentication identities is a generic issue
which needs to be resolved for all authentication methods.

"pl" == Peter Lister, Cranfield Computer Centre
	<P.Lister@cranfield.ac.uk> writes:

pl> In a nutshell, the problem is the HTTP server saying to the client
pl> "I'm foo, you must provide me with authentication for service
pl> foo". A malevolent server can persuade the client that it is
pl> genuine, and plant false information or obtain secret information
pl> in a query. The (reasonable) assumption of Kerberos is that the
pl> client knows with which prinicpal it must mutually authenticate
pl> *before* making the connection. Since the only information known
pl> beforehand is the URL, we must map the URL to a Kerberos
pl> principal.
  I think you've here stated the problem more clearly than I was able
to in my previous posts. This is indeed the situation I tried to
describe.

pl> Current Kerberos practise seems to be to use dots in the instance,
pl> which can be touch confusing, since the instance is generally
pl> separated from the name with a dot, but it's liveable with, and
pl> I'd be happier with dots than underscores. I also reckon that
pl> "k4-" is redundant. So my preference in your example would be
pl> "http.bob.foo.com@FOO.COM", where the instance is "bob.foo.com".
  As we use short hostnames for instances. I did not know if dots were
permitted in instances, hence the underscores. I comment below on my
use of ``k4-.''

>> I'm not familiar enough with HTTP to know what information the
>> browser is ``allowed'' to get from the URL, but the browser needs
>> to get the authentication identity from someplace other than the
>> server. The advantage of using the URL is that it is already there
>> and should contain all the necessary authencation identity
>> information.

pl> Hmm. Interesting point. The principal consists of three parts

pl> The principal name is unique to the service. I suggest we use
pl> "http", the URL type, but it doesn't matter as long as everyone
pl> agrees and we stick to one.  Protocol versions are
pl> irrelevant. Logically, the scheme extends to other URL types,
pl> e.g. gopher,ftp. Anyone know what the URL/URN people feel about
pl> this?
  The different protocols to which I refer are the authentication
protocols--k4, pgp, k5, etc.--not the connection methods--ftp, gopher,
http. I propose that the two together would, for Kerberos, be the
principal's name, e.g., k5-gopher.bob.foo.com@FOO.COM,
k4-http.bob.foo.com@FOO.COM. This would allow each connection method
to determine the authentication protocol.

pl> The instance is conventionally the name of the server machine,
pl> either the short form or fully qualified domain name. At
pl> Cranfield, POP uses pop.xdm001; AFS uses
pl> afs.pegasus.cranfield.ac.uk (the fully qualified cell name). My
pl> preference is to use the URL server name verbatim as the instance,
pl> e.g.  http://www.cranfield.ac.uk/ =>
pl> http.www.cranfield.ac.uk.
  I think one would, as you suggest, have to use the full server name.
For large sites, just the hostname sans domain name will not be
sufficient, e.g., Bob in Accounting should use
k4-http.bob.acct.foo.com@FOO.COM as opposed to k4-http.bob@FOO.COM
since the latter would collide with the machine for Bob in Sales,
bob.sales.foo.com.

pl> This does mean that if the same alias applies to several machines,
pl> their servers must all have the identical srvtab. No big deal,
pl> though it makes changing the service key harder in traditional
pl> Kerberos if it has to be done simultaneously on multiple machines.
  Since this is currently a problem for Kerberos in general, using
HTTP with Kerberos would not introduce this as a new problem.

pl> We have to hand the server name in the URL to Kerberos and leave
pl> it to Do The Right Thing to work out the realm. Typically, a local
pl> (hence trusted) file e.g. krb.realms (or Transarc's CellServDB)
pl> maps remote domain names to realm (e.g. cranfield.ac.uk =>
pl> PEGASUS.CRANFIELD.AC.UK, as is the case at Cranfield).
  I guess it was wishful thinking on my part to forget about the local
realm/cell config file.

pl> So I reckon the only info we need from the URL is just the server
pl> name, and I'm sure we're allowed to extract that.
  
>> Use of the URL means that the client, of course, needs to get the
>> URL from a trusted source (widely available documentation, an HTML
>> document acquired through a secure channel, etc.).

pl> Would a trusted URN -> URL translations service satisfy this? Have
pl> the URN guys considered this?
  I'm not familiar with the workings of URNs--I think the HTTP spec I
have lists them as "to be specified."


michael



From P.Lister@cranfield.ac.uk  Tue Mar  8 16:14:28 1994 --100
Message-Id: <9403081507.AA04312@xdm039.ccc.cranfield.ac.uk>
Date: Tue, 8 Mar 1994 16:14:28 --100
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Re: Insecure WWW Access Authorization Protocol?

>   Even though the following uses Kerberos for much of the discussion,
> the mapping of URLs to authentication identities is a generic issue
> which needs to be resolved for all authentication methods.

Hear hear.

>   The different protocols to which I refer are the authentication
> protocols--k4, pgp, k5, etc.--not the connection methods--ftp, gopher,
> http. I propose that the two together would, for Kerberos, be the
> principal's name, e.g., k5-gopher.bob.foo.com@FOO.COM,
> k4-http.bob.foo.com@FOO.COM. This would allow each connection method
> to determine the authentication protocol.

We know which authentication protocol we're using, the HTTP response sez 
"WWW-Authenticate: KerberosV4". Adding "k4-" to a Kerberos principal name 
doesn't tell anyone anything useful. It may confuse people into believing that 
the principal only works with the "right" authentication protocol, which is 
untrue - a Kerberos 5 speaking HTTP server can probably also understand 
Kerberos 4, and should use a single principal for both. I really don't 
understand why you want this.

BTW, I'm now lead to wonder what happens when a server is happy to accept any 
one of multiple different authentication protocols, e.g. Kerberos[45] and PGP?

Peter Lister                             Email: p.lister@cranfield.ac.uk
Computer Centre, Cranfield University    Voice: +44 234 754200 ext 2828
Cranfield, Bedfordshire MK43 0AL UK        Fax: +44 234 750875
--- Go stick your head in a pig.  (R) Sirius Cybernetics Corporation ---




From wa@mcc.com  Tue Mar  8 17:00:34 1994 --100
Message-Id: <9403081554.AA16170@coyote.mcc.com>
Date: Tue, 8 Mar 1994 17:00:34 --100
From: wa@mcc.com (Wayne Allen)
Subject: Insecure WWW Access Authorization Protocol?


All of this Kerberos discussion is perpetrating a lot of
misconceptions. There's no need to stick service name or realm
information in urls, or worry about instance names and all that.

Here's the big picture.  A client is registered in a Kerberos realm.
For that realm, there are one or more Kerberos servers which provide
service tickets to the registered clients. Likewise, a service is
registered in a Kerberos realm. The realm servers know how to encrypt
ticket data for all of the registered clients and servers. This is
because each client and service has a secret key, which the realm
servers know about.

A client identifies a service it wishes to use. The minimum
information needed for this identity is a host name or IP address, a
port number, and a service name. It does *not* matter whether the
client knows the service name beforehand, or, as in the case of the
HTTP protocol, learns it *after* connecting to the service (the first
time).

Given no other information, the client attempts to obtain a ticket for
the service by making a request to it's Kerberos realm server. If the
service is correctly registered in that same realm, a ticket will be
granted and athentication will proceed.

If the service is registered in a *different* Kerberos realm than that
of the client, the client must provide that realm name with the ticket
request it makes to it's own realm. If the client's and server's
realms have a pre-existing cross-authentication agreement,
athentication can proceed. In the case of the HTTP protocol, the
client learns the realm of the service *after* connection. Again, this
is perfectly ok.

Here's the point. It does not matter *when* or *from whom* the client
learns the service name or registration realm of the server. A rogue
server cannot spoof a client by providing misleading information of
this kind.  If the service is correctly registered in the client's realm
or in a valid cross-authenticating realm, authentication will succeed,
otherwise the client cannot obtain a valid ticket for the service, and
will recieve an "unknown service" error.  The Kerberos protocol and
client-code take care of things like reverse look-up and all that.  I
know all this is confusing, but that's just because it's complicated
;-)

The major security problems with Kerberos have to do with the humans
and human procedures used in adminstering the realms, not the code and
protocol.

Cheers,
--
 wa | Wayne Allen, EINet - wa@einet.net
    | MCC/ISD, 3500 West Balcones Center Dr, Austin, Tx 78759






From P.Lister@cranfield.ac.uk  Tue Mar  8 18:30:09 1994 --100
Message-Id: <9403081725.AA04857@xdm039.ccc.cranfield.ac.uk>
Date: Tue, 8 Mar 1994 18:30:09 --100
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Re: Insecure WWW Access Authorization Protocol?

> All of this Kerberos discussion is perpetrating a lot of
> misconceptions. There's no need to stick service name or realm
> information in urls, or worry about instance names and all that.

I have never suggested "sticking service name or realm information in urls"; I 
have agreed with others that the Kerberos principal should be derived from the 
URL's server name, not the Authentication field return by an untrusted HTTP 
server. I shall continue to worry until the protcol adequately addresses this 
problem, thanks all the same.

> Here's the big picture.  A client is registered in a Kerberos realm.
> For that realm, there are one or more Kerberos servers which provide
> service tickets to the registered clients. Likewise, a service is
> registered in a Kerberos realm. The realm servers know how to encrypt
> ticket data for all of the registered clients and servers. This is
> because each client and service has a secret key, which the realm
> servers know about.

Neither "clients" nor "servers" are registered with the Kerberos server, 
principal/instances are. Kerberos servers happily provide encrypted tickets to 
anyone who asks, safe in the knowledge that only a client in possesion of the 
correct key can decrypt the ticket. It doesn't have to be "registered" at all. 
Whether a principal/instance is a client or server depends solely on who asks 
for a ticket to authenticate to whom.

BTW I use "client" to mean running instance of a client program such as a WWW 
browser, not the system on which it's running. Likewise a server is a running 
instance of a program which responds to requests from the net.

> A client identifies a service it wishes to use. The minimum
> information needed for this identity is a host name or IP address, a
> port number, and a service name. It does *not* matter whether the
> client knows the service name beforehand, or, as in the case of the
> HTTP protocol, learns it *after* connecting to the service (the first
> time).

The service name is http. That's a given. All the client knows is a URL. From 
that it knows the server's hostname and port number, it's already tried to get 
the document and been told it needs to authenticate via Kerberos. Think about 
your last sentence. How did the client get this far without knowing the 
service name?

> Given no other information, the client attempts to obtain a ticket for
> the service by making a request to it's Kerberos realm server. If the
> service is correctly registered in that same realm, a ticket will be
> granted and athentication will proceed.

OK. How does the client know

a) which Kerberos server to ask (it needs to know the realm beforehand)
b) which instance of the principal to ask for (since a single Kerberos realm 
can quite legitimately have multiple HTTP servers)

..given that it can't trust the server yet? All it knows is the URL, so we 
have to agree on how to get this info out of the URL, OK? That's what we're 
saying.

> If the service is registered in a *different* Kerberos realm than that
> of the client, the client must provide that realm name with the ticket
> request it makes to it's own realm. If the client's and server's
> realms have a pre-existing cross-authentication agreement,
> athentication can proceed. In the case of the HTTP protocol, the
> client learns the realm of the service *after* connection. Again, this
> is perfectly ok.

No it isn't. Look, at this point the client *CAN'T TRUST THE SERVER*, yes? It 
might be talking to the right server, it could be talking to a spoofer which 
tells the client to authenticate to the spoofer's principal. How does the 
client know? In normal Kerberos interaction, the service's principal is known 
beforehand because the application has conventions defining to which principal 
the client should authenticate.

If you phone up a small company which you suspect might be crooked, or where 
there is no accepted way of checking their bona fides, and the guy on the 
other end said "Phone this number, and they'll tell you we're genuine", you 
have no way of telling whether the number in question is the conman's partner, 
or a very satisfied and upright member of the community who will testify to 
years of excellent service. If the guys says "Contact this bank, they'll vouch 
for us", and names a respected institution which you then approach 
independently via the well-known information contained in the phone book, 
you're as sure as you can be.

> Here's the point. It does not matter *when* or *from whom* the client
> learns the service name or registration realm of the server. A rogue
> server cannot spoof a client by providing misleading information of
> this kind.  If the service is correctly registered in the client's realm
> or in a valid cross-authenticating realm, authentication will succeed,
> otherwise the client cannot obtain a valid ticket for the service, and
> will recieve an "unknown service" error.  The Kerberos protocol and
> client-code take care of things like reverse look-up and all that.  I
> know all this is confusing, but that's just because it's complicated
> ;-)

But (sigh) services and clients are not registered by the Kerberos server!! 
And how does it learn the server name and realm (and instance), if not from 
the URL? The whole point is that a spoof HTTP server provides a reference to 
any principal the spoofer knows about (so any student at Cranfield, MIT, CMU 
etc has this power). A naive client happily authenicates. It doesn't have to 
be the *right* server principal if the client is stupid enough to believe 
everything it's told from an untrusted source, because the stupid client 
doesn't know what the right server *is*.

I don't know what you imagine that Kerberos client code does, but it 
definitely does *not* check that the reverse IP address translation matches 
the principal name, which is what you seemm to imply. And, anyway, the 
authentication has succeeded, so, Kerberos wouldn't notice anything wrong, 
anyway!

I think this this is confusing because you have a misconception about how 
Kerberos works. Don't worry. I've been managing a Kerberos site for 3 year, 
and it took me about 18 months to handle it, and I still don't regard myself 
as an expert.

> The major security problems with Kerberos have to do with the humans
> and human procedures used in adminstering the realms, not the code and
> protocol.

Here, at least, we are in agreement. But if you read the preceding exchanges, 
you will have noticed that no-one is accusing Kerberos of being at fault; the 
problem is down to the conventions used by HTTP to determine it needs to do 
with Kerberos. These must be sorted out *before* any exchanges take place on 
the wire.

I you have anything further to say, let's take this off-line. I'm sure we're 
boring the pants off www-talk.

Peter Lister                             Email: p.lister@cranfield.ac.uk
Computer Centre, Cranfield University    Voice: +44 234 754200 ext 2828
Cranfield, Bedfordshire MK43 0AL UK        Fax: +44 234 750875
--- Go stick your head in a pig.  (R) Sirius Cybernetics Corporation ---




From sanders@BSDI.COM  Tue Mar  8 20:02:45 1994 --100
Message-Id: <199403081857.MAA27726@austin.BSDI.COM>
Date: Tue, 8 Mar 1994 20:02:45 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Insecure WWW Access Authorization Protocol? 

michael shiplett writes:
> pl> beforehand is the URL, we must map the URL to a Kerberos
> pl> principal.
You cannot trust the URL anymore than you can trust the server reply.

--sanders



From michael.shiplett@umich.edu  Tue Mar  8 22:00:46 1994 --100
Message-Id: <199403082056.PAA27600@totalrecall.rs.itd.umich.edu>
Date: Tue, 8 Mar 1994 22:00:46 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: Insecure WWW Access Authorization Protocol? 

"ts" == Tony Sanders <sanders@BSDI.COM> writes:

ts> michael shiplett writes:

pl> beforehand is the URL, we must map the URL to a Kerberos
pl> principal.

ts> You cannot trust the URL anymore than you can trust the server
ts> reply.
  The URL is as trustworth as the source of the URL--whether the
source is in or out of band.

Examples:

a) A university or organization publication (e.g., a computing guide,
   faculty and staff newsletter, etc.) recommends that users without
   their own home pages default to
   http://www.our.domain.here/homepage.html.

b) A friend tells you about a great new service and suggests you
   that you try it.

  If you don't trust *any* URL, you may as forget about running a web
browser.

michael



From earhart+@CMU.EDU  Tue Mar  8 22:30:31 1994 --100
Message-Id: <ohTCowm00WC7JC6ds8@andrew.cmu.edu>
Date: Tue, 8 Mar 1994 22:30:31 --100
From: earhart+@CMU.EDU (Rob Earhart)
Subject: Re: Insecure WWW Access Authorization Protocol?

"Peter Lister, Cranfield Computer Centre" <P.Lister@cranfield.ac.uk> writes:
> In a nutshell, the problem is the HTTP server saying to the client "I'm foo, 
> you must provide me with authentication for service foo". A malevolent server 
> can persuade the client that it is genuine, and plant false information or 
> obtain secret information in a query. The (reasonable) assumption of Kerberos 
> is that the client knows with which prinicpal it must mutually authenticate 
> *before* making the connection. Since the only information known beforehand is 
> the URL, we must map the URL to a Kerberos principal. 

  Wrong.

  The evil HTTP server says to the client, "I'm foo, you must provide me
with authentication for service foo."

  The client says, "Okay, no problem, here's my session key for talking
with service foo, wrapped up with foo's key."

  The evil HTTP server says to foo, "I'm the client, here's our session
key wrapped in your secret key, along with some other data."

  Foo says, "The IP address in that packet you just sent me isn't the
IP address that it came from; I don't believe you.  EOF"

  And even in environments that don't check IP addresses... Foo says,
"Okay, just to make sure you are who you say you are, wrap the md5 key
of every request with that session key that you think you know."

  (And if foo doesn't do mutual authentication, foo is being very very
insecure; this isn't a problem that HTTP can do anything about).

  At this point, the evil HTTP server is at a loss, because it never
actually had the session key, just the ticket containing the session key
wrapped with foo's secret key.  It looses.  Yes, it can monitor
unencrypted information; this is why you encrypt sensitive data.  It can
replay requests, but this doesn't give it any more information than it
originally saw passing over the wire.

  The evil HTTP server is nothing more or less than a network
sniffer, sitting between the client and foo.  Kerberos is already
designed to defeat this.  It doesn't matter that it's the server telling
you who to authenticate to; if it doesn't have the secret key of the
service it's asking for, it can't do anything with it.

  )Rob



From sanders@BSDI.COM  Tue Mar  8 23:10:50 1994 --100
Message-Id: <199403082207.QAA28832@austin.BSDI.COM>
Date: Tue, 8 Mar 1994 23:10:50 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Insecure WWW Access Authorization Protocol? 

michael shiplett writes:
> "ts" == Tony Sanders <sanders@BSDI.COM> writes:
>   The URL is as trustworth as the source of the URL--whether the
> source is in or out of band.
If you cannot trust the server reply to get the realm information from
then why do you think you can trust the URL?  You have exactly the
same problems as when you started.

--sanders



From michael.shiplett@umich.edu  Tue Mar  8 23:16:17 1994 --100
Message-Id: <199403082209.RAB04855@totalrecall.rs.itd.umich.edu>
Date: Tue, 8 Mar 1994 23:16:17 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: Insecure WWW Access Authorization Protocol? 

"re" == Rob Earhart <earhart+@CMU.EDU> writes:

"Peter Lister, Cranfield Computer Centre" <P.Lister@cranfield.ac.uk> writes:

pl> In a nutshell, the problem is the HTTP server saying to the client
pl> "I'm foo, you must provide me with authentication for service
pl> foo". A malevolent server can persuade the client that it is
pl> genuine, and plant false information or obtain secret information
pl> in a query. The (reasonable) assumption of Kerberos is that the
pl> client knows with which prinicpal it must mutually authenticate
pl> *before* making the connection. Since the only information known
pl> beforehand is the URL, we must map the URL to a Kerberos
pl> principal.

re>   Wrong.

re>   The evil HTTP server says to the client, "I'm foo, you must
re> provide me with authentication for service foo."

re>   The client says, "Okay, no problem, here's my session key for
re> talking with service foo, wrapped up with foo's key."

re>   The evil HTTP server says to foo, "I'm the client, here's our
re> session key wrapped in your secret key, along with some other
re> data."

  You are missing the point, Rob. Reread Peter's last two
sentences. The evil HTTP server is not trying to pose as foo or even
act as an intermediary between the client and foo.

  Let's see if this transaction sequence helps to clear things up:

  With the current HTTP Access Authentication this is what happens
when Alice attempts to retrieve a given URL, let's say:
	       http://bob.foo.com/restricted/file.html.
  Let's also assume that to authenticate to Bob's server, one must
authenticate to the service http.bob.foo.com@FOO.COM.

 1) Alice sends a cleartext request to Bob for /restricted/file.html.

 2) Mallet intercepts the request and sends to Alice:
	401 Unauthorized
	Authenticate: KerberosV4 "http.mallet.evil.com@EVIL.COM"
    This is the critical step, because Mallet is able to control to
    which service Alice should authenticate herself.

 3) Alice, who has a naive, trusting browser which doesn't attempt to
    do any matching between a URL and the alleged server
    authentication identity, goes ahead and sends the authenticator
    for "http.mallet.evil.com@EVIL.COM along with an encrypted request
    for http://bob.foo.com/restricted/file.html.

 4) Mallet can decrypt the authenticator and checksum, process Alice's
    request and ship back Mallet's version of file.html--which could
    very well be an order form containing a field for a credit card
    number.

 5) Alice mutually authenticates Mallet's reply--which looks valid to
    her, since her browser foolishly accepted the identity passed in
    the cleartext 401 Unauthorized messsage. Alice fills out the form
    and POSTS an encrypted version to Mallet's CGI program.

 6) Mallet takes a Carribean cruise, courtesy of Alice's credit card
    number.


  The problem is that Mallet is able to intercept Alice's initial,
cleartext request and feed back a bad server identity. Unless Alice is
able to know in advance what the server identity should be (possibly
from the URL), Alice has *no* way of knowing whether the server
identity she receives is Bob's or Mallet's.

  I'd like to point out (again), that this is not a problem endemic to
Kerberos. It is just as easy for Mallet to step in and present a valid
X.509 certificate causing Alice to use Mallet's public key and not
Bob's. The security protocol is not breached--the browser is given
incorrect identification information.

michael



From michael.shiplett@umich.edu  Tue Mar  8 23:21:01 1994 --100
Message-Id: <199403082215.RAB05451@totalrecall.rs.itd.umich.edu>
Date: Tue, 8 Mar 1994 23:21:01 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: Insecure WWW Access Authorization Protocol? 

"ts" == Tony Sanders <sanders@BSDI.COM> writes:

ts> michael shiplett writes:
>> "ts" == Tony Sanders <sanders@BSDI.COM> writes:
>> The URL is as trustworth as the source of the URL--whether the
>> source is in or out of band.
ts> If you cannot trust the server reply to get the realm information from
ts> then why do you think you can trust the URL?  You have exactly the
ts> same problems as when you started.

  Currently there is no mapping between the URL and the server's
identity--this is correct. I am proposing that the client authenticate
to an identity based on the URL. In this proposal I am not trusting
the server to give *any* information regarding its identity. As long
as the URL for a given service is trusted, authentication to said
service can proceed in a secure manner. Everything proceeds from a URL
not from the initial connection to the service.

michael




From sarr@citi.umich.edu  Tue Mar  8 23:33:36 1994 --100
Message-Id: <9403082230.AA11829@dxmint.cern.ch>
Date: Tue, 8 Mar 1994 23:33:36 --100
From: sarr@citi.umich.edu (Sarr Blumson)
Subject: Re: Insecure WWW Access Authorization Protocol? 


Tony Sanders says:
  michael shiplett writes:
  > "ts" == Tony Sanders <sanders@BSDI.COM> writes:
  >   The URL is as trustworth as the source of the URL--whether the
  > source is in or out of band.
  If you cannot trust the server reply to get the realm information from
  then why do you think you can trust the URL?  You have exactly the
  same problems as when you started.
  
I think we're munging two different things here.  Michael is willing to 
trust whoever gave him the URL pointing to, for example, 
citi.umich.edu, and believes that citi.umich.edu has what he wants.  
What he wants the security mechanisms to do for him is guarantee that 
the server he ends up talking to really _is_ citi.umich.edu, and not 
some imposter who has attacked the intervening cable.

I believe this is important because Michael is thinking about using 
forms to put confidential data into his server, so spoofing the server 
is more than just a denial of service.

--------
Sarr Blumson                         sarr@citi.umich.edu
voice: +1 313 764 0253               FAX: +1 313 763 4434
CITI, University of Michigan, 519 W William, Ann Arbor, MI 48103-4943




From earhart+@CMU.EDU  Tue Mar  8 23:49:46 1994 --100
Message-Id: <shTDy6a00WC7NC6fdy@andrew.cmu.edu>
Date: Tue, 8 Mar 1994 23:49:46 --100
From: earhart+@CMU.EDU (Rob Earhart)
Subject: Re: Insecure WWW Access Authorization Protocol?

michael shiplett <michael.shiplett@umich.edu> writes:
>   With the current HTTP Access Authentication this is what happens
> when Alice attempts to retrieve a given URL, let's say:
>                http://bob.foo.com/restricted/file.html.
>   Let's also assume that to authenticate to Bob's server, one must
> authenticate to the service http.bob.foo.com@FOO.COM.
> 
>  1) Alice sends a cleartext request to Bob for /restricted/file.html.
> 
>  2) Mallet intercepts the request and sends to Alice:
>         401 Unauthorized
>         Authenticate: KerberosV4 "http.mallet.evil.com@EVIL.COM"
>     This is the critical step, because Mallet is able to control to
>     which service Alice should authenticate herself.
> 
>  3) Alice, who has a naive, trusting browser which doesn't attempt to
>     do any matching between a URL and the alleged server
>     authentication identity, goes ahead and sends the authenticator
>     for "http.mallet.evil.com@EVIL.COM along with an encrypted request
>     for http://bob.foo.com/restricted/file.html.

  Heh.  Don't exchange realm keys with EVIL.COM unless you trust them
to not do such things. :-)

  Okay... and as long as we're assuming that Alice's browser is dumb
enough to not tell Alice who it's authenticating to, let's also not have
it tell Alice the URL (after all, URL's are nasty protocol things that
should be kept away from the user, right?).

  Then Mallet intercepts the request, and sends to Alice:
	301 Moved
	URI: http://mallet.evil.com:/restricted/file.html

  So Alice's oh-so-helpful browser jumps over there, and Mallet sends back:
	401 Unauthorized
	Authenticate: KerberosV4 "http.mallet.evil.com@EVIL.COM"

  At which point, it doesn't matter whether or not you're doing URL
checking; you still lose.

  Before sending a document that requires security to a server, the
client needs to tell the user what form of authentication it's using,
and whom it's authenticating itself to.  If you really feel the need to
show the user the URL, and do URL checking, fine; I'd rather just give
the user the information directly, and allow the server to use any name
it wants for authentication.

  )Rob



From terry@ora.com  Wed Mar  9 00:04:42 1994 --100
Message-Id: <199403082259.AA21845@rock.west.ora.com>
Date: Wed, 9 Mar 1994 00:04:42 --100
From: terry@ora.com (Terry Allen)
Subject: RE Machine-readable server announcements

I'm quite interested in Dan's SGML/MIME thread, but when
I reread his proposal for making server announcement
knowbot-friendly, I think I missed something.  Dan,
is there anything about the format of the message that
describes its content as a server announcement?
Or would any message sent to the server-announcement 
newsgroup that contained a URL be (mis)interepreted
as a server announcement?

Regards,

-- 
Terry Allen  (terry@ora.com)
Editor, Digital Media Group
O'Reilly & Associates, Inc.
Sebastopol, Calif., 95472



From connolly@hal.com  Wed Mar  9 01:10:46 1994 --100
Message-Id: <9403090000.AA05122@ulua.hal.com>
Date: Wed, 9 Mar 1994 01:10:46 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: RE Machine-readable server announcements 

In message <199403082259.AA21845@rock.west.ora.com>, Terry Allen writes:
>I'm quite interested in Dan's SGML/MIME thread, but when
>I reread his proposal for making server announcement
>knowbot-friendly, I think I missed something.  Dan,
>is there anything about the format of the message that
>describes its content as a server announcement?

Only the Newsgroups, Mime-Version, and Content-Type headers.

>Or would any message sent to the server-announcement 
>newsgroup that contained a URL be (mis)interepreted
>as a server announcement?

I expect any message sent to the server-announcement newsgroup would
be interpreted as a server announcement... is there some reason not
to? The question is: who does the "interpreting." Plain text messages
are fine for interpretation by humans, but not so great for automated
newsreaders.

The feature I'm after is a reliable way to extract URLs from these
announcements. Currently, folks post plain text in any of the
following forms:

	See <A HREF="http://foo.com/index.html">our server</A>

	Have a look at the server on foo.com

	Point your browser at <http://foo.com/index/html> and go!

etc. And while there are plenty of heuristic methods that would find
90% of the urls posted today, that sort of thing doesn't scale well.

The object of the game is that (1) we settle on a format or a small
number of formats and register them with the IANA as MIME
content-types (this may already be the case for wais-sources... HTML
is headed that direction). (2) folks use those formats to distribute
announcements (and label them as such using MIME headers), and
finally, (3) other folks have well-defined ways to extract resource
pointers from announcements. They may choose to (4) stick the
announcement in a fulltext indexed database for local resource
discovery.

As for the SGML/MIME stuff... I'm also interested in expressing lots
of other sentiments in a machine-readable way. Such things as:

	"This data is also mirrored at the following sites..."
	"The latest version is always available from ..."
		(caching, replication)
	"The document is available as text, postscript..."
		(format conversion/negotiation)
	"This text was written by Daniel W. Connolly on March 8, 1994"
		(digital signatures)
	"The following is a quote from document X, as of March 1, 1994..."
		(verifiable links)
	"Only folks that have a license to this data can read it"
		(authentication, authorization)

In order to evolve the set of sentiments we can express in
machine-readable fashion, I think it's critical to develop a system
where we distribute more self-describing SGML documents, i.e.
documents that contain (at least a pointer to) their DTD.

For example, there's no handy way to validate an HTML document, since
most of them have an instance with no hint of a prologue. This is
largely due to some bad decisions I made a year or so ago... I was
naive enough to expect that we'd all agree on the same DTD. Not in
this lifetime :-{

HTML serves the needs of simple situations like campus-wide
information systems pretty well. But imagine preparing a hypertext
legal briefing: you'd want to be SURE that the document you link to
don't change out from under you (or at least that you can tell if it
does...). You might be willing to pay to get access to documents... you
might pay more for better indexing... you might pay a hypertext
librarian to organize the documents you have access to with respect to
a particular vertical market...

You might think this is far-fetched, but there are already seeds of
electronically distributed research journals using WWW and other
Internet tools. From there, it will bleed into entertainment, news
media, etc.

Dan



From terry@ora.com  Wed Mar  9 01:47:57 1994 --100
Message-Id: <199403090042.AA24371@rock.west.ora.com>
Date: Wed, 9 Mar 1994 01:47:57 --100
From: terry@ora.com (Terry Allen)
Subject: RE server announcements

| >I think I missed something.  Dan,
| >is there anything about the format of the message that
| >describes its content as a server announcement?
| 
| Only the Newsgroups, Mime-Version, and Content-Type headers.

Thanks, that was what I was after.  Which of the Content-Type
lines in your example is relevant here, message/rfc822,
multipart/alternative; boundary="cut-here", or text/x-html?

| >Or would any message sent to the server-announcement 
| >newsgroup that contained a URL be (mis)interepreted
| >as a server announcement?
| I expect any message sent to the server-announcement newsgroup would
| be interpreted as a server announcement... is there some reason not
| to? The question is: who does the "interpreting." Plain text messages
| are fine for interpretation by humans, but not so great for automated
| newsreaders.
| The feature I'm after is a reliable way to extract URLs from these
| announcements. 
[ . . .]
| The object of the game is that (1) we settle on a format or a small
| number of formats and register them with the IANA as MIME
| content-types (this may already be the case for wais-sources... HTML
| is headed that direction). (2) folks use those formats to distribute
| announcements (and label them as such using MIME headers), and
| finally, (3) other folks have well-defined ways to extract resource
| pointers from announcements. They may choose to (4) stick the
| announcement in a fulltext indexed database for local resource
| discovery.
| As for the SGML/MIME stuff... I'm also interested in expressing lots
| of other sentiments in a machine-readable way. Such things as:
| 	"This data is also mirrored at the following sites..."
| 	"The latest version is always available from ..."
| 		(caching, replication)
| 	"The document is available as text, postscript..."
| 		(format conversion/negotiation)
| 	"This text was written by Daniel W. Connolly on March 8, 1994"
| 		(digital signatures)
| 	"The following is a quote from document X, as of March 1, 1994..."
| 		(verifiable links)
| 	"Only folks that have a license to this data can read it"
| 		(authentication, authorization)
[ . . .]
| For example, there's no handy way to validate an HTML document, since
| most of them have an instance with no hint of a prologue. This is
| largely due to some bad decisions I made a year or so ago... I was
| naive enough to expect that we'd all agree on the same DTD. Not in
| this lifetime :-{

You have to assume the DTD is the official HTML DTD, not some local
variant; this is what the browsers assume anyway.  The issue has 
been muddied because the HTML DTD initially distributed didn't 
work well, leading to local fixes, and new stuff from HTML+ has leaked
into browser functionality, necessitating local updates.
Users want to use the full display ability of the
browsers they use, and browser developers haven't waited
on an official revision.

For the case of multiple DTDs, a prologue is necessary, though.
if the DTD is public (and useable) you can refer to it through a
Formal Public Identifier in the DOCTYPE declaration.  Seems to
me that the offical HTML DTD should be considered the default DTD if 
none is specified in the instance. 
		
| HTML serves the needs of simple situations like campus-wide
| information systems pretty well. But imagine preparing a hypertext
| legal briefing: you'd want to be SURE that the document you link to
| don't change out from under you (or at least that you can tell if it
| does...). You might be willing to pay to get access to documents... you
| might pay more for better indexing... you might pay a hypertext
| librarian to organize the documents you have access to with respect to
| a particular vertical market...
| You might think this is far-fetched, but there are already seeds of
| electronically distributed research journals using WWW and other
| Internet tools. From there, it will bleed into entertainment, news
| media, etc.

Browsers that read arbitrary DTDs are on their way.  It seems to me
that what you are pursuing (rightly) is a well defined set of info
that should be accomodated by any DTD that claims to be useful
for hypertext, along with another well defined set of info that
is to be supplied in the MIME wrapper when the SGML instance
is served.  The first part might resolve into some set of 
"architectural forms," that is, attributes with #FIXED values
that can be used in, even retrofitted to, any DTD that actually
has the appropriate info (such as an <AUTHOR> tag).

Do I read you correctly?

Regards,

-- 
Terry Allen  (terry@ora.com)
Editor, Digital Media Group
O'Reilly & Associates, Inc.
Sebastopol, Calif., 95472



From connolly@hal.com  Wed Mar  9 02:49:00 1994 --100
Message-Id: <9403090139.AA05230@ulua.hal.com>
Date: Wed, 9 Mar 1994 02:49:00 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: RE server announcements 

In message <199403090042.AA24371@rock.west.ora.com>, Terry Allen writes:
>| >I think I missed something.  Dan,
>| >is there anything about the format of the message that
>| >describes its content as a server announcement?
>| 
>| Only the Newsgroups, Mime-Version, and Content-Type headers.
>
>Thanks, that was what I was after.  Which of the Content-Type
>lines in your example is relevant here, message/rfc822,
>multipart/alternative; boundary="cut-here", or text/x-html?

Well, the message/rfc822 was just a (machine readable:) way
to stick the example message in the middle of my comments.

The multipart/alternative type says to a MIME user agent "Here are
several versions of the same info. Pick the richest one you can
display." So dumb (i.e. minimally MIME-capable) newsreaders will show
the text/plain version. Knowbots and smart newsreaders will recognize
text/x-html and parse it for URLs.

>
>You have to assume the DTD is the official HTML DTD, not some local
>variant;

What official HTML DTD? The IETF draft version that just expired? Last
I checked, it had syntax errors that prevented it from parsing.

> this is what the browsers assume anyway.

Some approximation of it anyway...

>  The issue has 
>been muddied because the HTML DTD initially distributed didn't 
>work well, leading to local fixes, and new stuff from HTML+ has leaked
>into browser functionality, necessitating local updates.
>Users want to use the full display ability of the
>browsers they use, and browser developers haven't waited
>on an official revision.

Like I said... there's no handy way to validate an HTML document...
:-) The only practical way is to try it out on all the browsers you
expect your consumers to use. It's just a sad fact that HTML doesn't
have a working formal definition.

>Browsers that read arbitrary DTDs are on their way.  It seems to me
>that what you are pursuing (rightly) is a well defined set of info
>that should be accomodated by any DTD that claims to be useful
>for hypertext, along with another well defined set of info that
>is to be supplied in the MIME wrapper when the SGML instance
>is served.  The first part might resolve into some set of 
>"architectural forms," that is, attributes with #FIXED values
>that can be used in, even retrofitted to, any DTD that actually
>has the appropriate info (such as an <AUTHOR> tag).
>
>Do I read you correctly?

I think so. It's all a question of quality versus functionality,
specification versus deployment, namespaces and syntaxes ...

Also... I don't see MIME and SGML as exclusive in functionality:
they're both encodings for structured information. And if you want to
look at maturity, USENET has got to be the worlds most mature
computer-supported-collaborative-work application.

MIME has got some syntactic nasties, but they're motivated by trying
to get GIF files through VMS mail gateways.... The real value of MIME
is the collective experience from the widespread deployment of
internet mail and news. If you know you're not sending your info
through such beasties, you want to use as little of the MIME syntax as
you can... unless you're designing a protocol on-the-fly like HTTP.

SGML has got even worse syntactic nasties, and we won't go into why.
The real value of SGML is the ability to say "this document conforms
to this structure..." But the SGML standard is so foggy on semantics,
it's barely useful for anything else. (Don't tell me it says "nothing"
about semantics: it has all sorts of inuendos about characters being
processed "before" other characters, invoking programs to "interpret"
notations, things being "ignored", "significant", etc.)

The real problem with SGML is the namespace issue: there's no
subclassing mechanism. Look at the loops HyTime goes through to map
some pretty straigh-forward notions onto SGML.

The important step is to define the semantics we're interested in, and
then find a representation in some widely supported formats (MIME and
SGML are handy...). For example: how do we identify documents such
that we can test them for equality? (why? for caching purposes)

I'm starting to ramble... More later.

Dan



From connolly@hal.com  Wed Mar  9 06:25:43 1994 --100
Message-Id: <9403090520.AA05463@ulua.hal.com>
Date: Wed, 9 Mar 1994 06:25:43 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Contemporary work on Security Issues


With all the noise going around about security, authorization, etc. in
HTTP, I wonder if HTTP should borrow from CAT etc. -- I've been
reading over some of these RFC's, and they seem very appropriate:

1511  I    J. Linn, "Common Authentication Technology Overview", 09/10/1993.  
           (Pages=2) (Format=.txt) 

1510  PS   J. Kohl, B. Neuman, "The Kerberos Network Authentication Service  
           (V5)", 09/10/1993. (Pages=112) (Format=.txt) 

1509  PS   J. Wray, "Generic Security Service API : C-bindings", 09/10/1993.  
           (Pages=48) (Format=.txt) 

1508  PS   J. Linn, "Generic Security Service Application Program Interface", 
           09/10/1993. (Pages=49) (Format=.txt) 

1507  E    C. Kaufman, "DASS - Distributed Authentication Security Service", 
           09/10/1993. (Pages=119) (Format=.txt) 

Dan



From dsr@hplb.hpl.hp.com  Wed Mar  9 12:48:23 1994 --100
Message-Id: <9403091140.AA04920@manuel.hpl.hp.com>
Date: Wed, 9 Mar 1994 12:48:23 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: RE Machine-readable server announcements 

Dan writes:

> For example, there's no handy way to validate an HTML document, since
> most of them have an instance with no hint of a prologue. This is
> largely due to some bad decisions I made a year or so ago... I was
> naive enough to expect that we'd all agree on the same DTD. Not in
> this lifetime :-{

It is good practice to include the DOCTYPE element to name the DTD,
but one still needs to register the name so we all can agree on which
DTD it references.

Is there an additional means for specifying the DTD via a URL?

> HTML serves the needs of simple situations like campus-wide
> information systems pretty well. But imagine preparing a hypertext
> legal briefing: you'd want to be SURE that the document you link to
> don't change out from under you (or at least that you can tell if it
> does...).

This suggests that hypertext links should include some means of
authenticating the destination document. I will ammend the HTML+
DTD to include an attribute for a digital signature.

Many thanks for the idea.

Dave Raggett



From P.NEWBY@east-anglia.ac.uk  Wed Mar  9 16:45:47 1994 --100
Message-Id: <0097B2EA.9E401400.24350@cpcmg.uea.ac.uk>
Date: Wed, 9 Mar 1994 16:45:47 --100
From: P.NEWBY@east-anglia.ac.uk (P.NEWBY)
Subject: Searching a file.

Platform: NCSA httpd on DEC Alpha OSF.

How do I implement a simple search ?

The server holds a large file of the LISTSERV mailing lists, with one
mailing list per line. I want to:

  - ask the user for a keyword
  - extract from the file all lines containing this keyword
  - present the results to the user.

(I'm no expert in either WWW or unix - yet).

Pat Newby, Computing Centre, University of East Anglia, Norwich, UK.
(p.newby@uea.ac.uk)



From mjv@uminho.pt  Wed Mar  9 17:25:29 1994 --100
Message-Id: <9403091619.AA29052@zeca.uminho.pt>
Date: Wed, 9 Mar 1994 17:25:29 --100
From: mjv@uminho.pt (mjv@uminho.pt)
Subject: any information



Hello all,

	Here at University of Minho, we intend to partially automate
the process of evaluating teaching activities in the University. The
idea is to use the WWW and GOPHER information servers we maintain, in
order to represent the relevant information.

Some of the problems we must face are:

	o choosing a structure for convenenientely retrieving the information.
	o managing the data-reception templates (what format to use,
		what tools to support it).
	o How to process the information.
	o Producing convenient outputs from the data.
	o MOST IMPORTANT, dealing with matters of authority (access
	  control):
	  how to allow only certain people to change the information
	  they are responsible for, that is, how to give them 
	  access to a certain limited portion of the information
	  space, so that they can modify it.

If you have any information on this (or comments), please mail me 
directly to: mjv@uminho.pt


This message is being cross-posted to several lists. My apologies for any
inconvenience.

                              Thanks in advance
                                 
				Maria Joao Viamonte.





-- 
Maria Joao Viamonte         | EMail: mjv@uminho.pt
M.Sc Student                | X.500: c=PT@o=Universidade do Mino
Departamento de Informatica |            @ou=Departamento de Informatica
Universidade do Minho       |            @cn=Joao Viamonte
Campus de Gualtar           | WWW URL: http://www.uminho.pt/~mjv/myPage.html
4700 Braga                  | Voice: +351 (53) 604 475         
Portugal                    | Fax: +351 (53) 612 954         



From neuss@igd.fhg.de  Wed Mar  9 17:32:58 1994 --100
Message-Id: <9403091624.AA13188@wildturkey.igd.fhg.de>
Date: Wed, 9 Mar 1994 17:32:58 --100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: Re: Searching a file.

> How do I implement a simple search ?
> 

> The server holds a large file of the LISTSERV mailing lists, with one
> mailing list per line. I want to:
> 

>   - ask the user for a keyword
>   - extract from the file all lines containing this keyword
>   - present the results to the user.
This is fairly easy. Take a look at the examples in the cgi-bin directory,
especially the "cal" command. Copy it to a file like "keysearch" and modify
it in such a way that it does a "grep" on the listing file and returns
the result embedded in <PRE> </PRE>.
Now you can do keyword searches by accessing "/htbin/keysearch".
You can also create a forms interface which makes it easier for users to
use your keyword searching mechanism.

For more complicated search tasks, I recommend our public domain ICE
search extension package. I will soon release a new version which is
CGI compliant (sorry Rob and Ari.. you were right. It needs to have
a cgi-bin interface, not a server hack).

Contact me by email if you need further help - but please wait until next
week, since I am extremely busy preparing for the german CeBit computer fair.

Cheerio,
Chris
---
"I ride a tandem with the random.." 

Christian Neuss   # Fraunhofer Institute for Computer Graphics
Wilhelminenstr.7  #  64283 Darmstadt # Germany
e-mail: neuss@igd.fhg.de  finger: neuss@wildturkey.igd.fhg.de



From Peter.Sylvester@inria.fr  Wed Mar  9 18:18:00 1994 --100
Message-Id: <199403091713.AA24237@tavel.inria.fr>
Date: Wed, 9 Mar 1994 18:18:00 --100
From: Peter.Sylvester@inria.fr (Peter Sylvester)
Subject: Re: Searching a file.

> The server holds a large file of the LISTSERV mailing lists, with one
> mailing list per line. I want to:
> 
>   - ask the user for a keyword
>   - extract from the file all lines containing this keyword
>   - present the results to the user.
> 
Using WAIS seems to be a very good approach to provide
indexing for "documents".  



From pdc@dcs.edinburgh.ac.uk  Wed Mar  9 20:26:31 1994 --100
Message-Id: <2552.9403091646@colonsay.dcs.ed.ac.uk>
Date: Wed, 9 Mar 1994 20:26:31 --100
From: pdc@dcs.edinburgh.ac.uk (Paul Crowley)
Subject: Re: Insecure WWW Access Authorization Protocol?

Isn't this the name problem in another guise?  Any protocol that tries
to go any higher level than "I trust the person with this public key" or
"I trust anyone that this Kerberos server will authenticate as being
HTTP server 1729" will have to define what it means by that higher-level
concept in a machine-readable way.

For example, supposing I want to dial out for pizza from the
ExtraMushrooms Pizza Company.  If all I know about ExtraMushrooms is
their name, then there's absolutely nothing I can do to contact them
that can't be spoofed by EvilMallet's credit card grabber, unless
there's some trusted third party I can grab authentication information
from.  That might be a trusted nameserver that I know the Kerberos ID
for, or a URL from a trusted friend embedded in an authenticated
document.

Authenticating something is often easier than working out what you're
trying to authenticate.
  __                                  _____
\/ o\ Paul Crowley   pdc@dcs.ed.ac.uk \\ //
/\__/ Trust me. I know what I'm doing. \X/

Disclaimer: I've only just caught the tail end of this discussion, so I
don't know what it's really about.  I'm not a cryptologist, but I play
one in my so-called spare time.  I don't know HTTP all that well.



From sanders@BSDI.COM  Wed Mar  9 21:11:22 1994 --100
Message-Id: <199403092003.OAA03170@austin.BSDI.COM>
Date: Wed, 9 Mar 1994 21:11:22 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Insecure WWW Access Authorization Protocol? 

michael shiplett <michael.shiplett@umich.edu> writes:
>  2) Mallet intercepts the request and sends to Alice:
>         401 Unauthorized
>         Authenticate: KerberosV4 "http.mallet.evil.com@EVIL.COM"
>     This is the critical step, because Mallet is able to control to
>     which service Alice should authenticate herself.

Would someone please explain to me why a user would enter a password
at a prompt that read: "http.mallet.evil.com@EVIL.COM" (or anything
thing else they didn't recoginze?

Wouldn't they have to have a password for that realm and don't you
think they might think "gee, I don't have a password for that
realm".

What's missing from this picture?

--sanders



From connolly@hal.com  Wed Mar  9 21:20:10 1994 --100
Message-Id: <9403092007.AA05704@ulua.hal.com>
Date: Wed, 9 Mar 1994 21:20:10 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Comments on HTTP spec

------- =_aaaaaaaaaa0
Content-Type: text/plain; charset="us-ascii"
Content-ID: <5699.763243624.2@ulua>
Content-Description: HTTP comments as text

                                                          Comments on HTTP spec
Mime Terms

   The HTTP spec[1] makes many references to MIME and related terms, but it
   seems to use the terms loosely and incorrectly in many cases:
   
   Some useful terms are:
   
  Body Part               Headers, a blank line, and a body
                         
   Body                   a sequence of octets obeying a content transfer
                         encoding implied by context (such as the
                         Content-Transfer-Encoding header in the enclosing body
                         part).
                         
   Perhaps a better way to explain it is:
   

text/plain      akindof Body    (just a plain text data stream)
image/gif       akindof Body    (ordinary GIF data stream)
message/rfc822  akindof Body    (data stream defined by RFC822)
message/rfc822  akindof Body Part (wierd, huh?)
Body Part       hasa    Head
Body Part       hasa    Body
Head            hasa    Content-Transfer-Encoding
Head            hasa    Content-Type
multipart/mixed isa     Body
multipart/mixed hasa    sequence of Body Parts

   From  HTTP: The Request[2]:
   

        FullRequest       =     Method UR ProtocolVersion CrLf
                                [*<HTRQ Header>]
                                [<CrLf> <data>]

        <Method>          =     <InitialAlpha>

        ProtocolVersion   =     HTTP/1.0

        uri               =     <as defined in URL spec>

        <HTRQ Header>     =     <Fieldname> : <Value> <CrLf>

        <data>            =      MIME-conforming-message

   What is "MIME-conforming-message"? The term itself suggests that it means an
   body of type message/rfc822, but the usage in the grammar says it's just a
   generic body. Using the grammar of the MIME spec, RFC1521[3], this should be
   written:
   

        FullRequest       =     Method UR ProtocolVersion CrLf
                                [body-part]

        <Method>          =     <InitialAlpha>

        ProtocolVersion   =     HTTP/1.0

        uri               =     <as defined in URL spec>

   where body-part should be defined similarly to the way it's done in MIME,
   i.e. some variation on the following:
   

   body-part := <"message" as defined in RFC 822,
             with all header fields optional, and with the
             specified delimiter not occurring anywhere in
             the message body, either on a line by itself
             or as a substring anywhere.  Note that the
             semantics of a part differ from the semantics
             of a message, as described in the text.>

Accept Field

   Hmmm... in the Accept: field: is it comma or semicolot separated? Contrast:
   
       This field contains a semicolon-separated list of representation sc
     hemes ( Content-Type metainformation values) which will be accepted i
     n the response to this request.
     
   with this example:
   

Accept: text/plain, text/html
Accept: text/x-dvi; q=.8; mxb=100000; mxt=5.0, text/x-c

   and then there's this example:
   

Accept:  *.*, q=0.1
Accept:  audio/*, q=0.2
Accept:  audio/basic q=1

Default Content-Transfer-Encoding

   From Comments on HTTP spec[4]
   
       The data (if any) sent with an HTTP request or reply is in a format
      and encoding defined by the object header fields, the default being
     "plain/text" type with "8bit" encoding. Note that while all the other
      information in the request (just as in the reply) is in ISO Latin1 w
     ith lines delimited by Carriage Return/Line Feed pairs, the data may
     contain 8-bit binary data.
     
   The 8bit encoding means characters can be in the range 0-255, but the
   characters are still arranged as lines of text not to exceed 76 characters,
   delimited by CRLF pairs.
   
   The binary encoding meands an arbitrary stream of octets. I expect this is
   what the HTTP designers had in mind for the default
   content-transfer-encoding... Hmmm... or perhaps not... perhaps sending
   arbitrary binary data is somewhat at odds with historical usage of HTTP, and
   so binary data should be labelled as such. This could use some clarification
   in any case.
   
The libWWW implementation

   I browsed around the libWWW implementation, and I'm somewhat confused:
   
   First of all what is "www/mime"? It appears to work like message/rfc822, but
   then stuff labelled message/rfc822 seems to be handed off to MetaMail!!!
   
   From what I can tell, there are two classes of HTTP transactions:
   
   The "0.9" style transaction could be described by the following Modula-like
   interface:
   

INTERFACE HTTP0_9;
        TYPE HTML = TEXT;
        TYPE SearchWords = REF ARRAY OF TEXT;
        TYPE Request = RECORD
                resource : TEXT;
                search : SearchWords = NIL;
             END;
        TYPE Response = RECORD
                structured : HTML;
                plain : TEXT = NIL;
             END;
END HTTP0_9.

   For example,
   

HTTP                                            Modula
Client:
        GET /foo/bar.html               req = Request{resource="/foo/bar.html"}
;
Server:
        <PLAINTEXT>                     resp := Response{
        Four score and seven years ago          structured="<PLAINTEXT>",
        today...                                plain="Four score and seven..."
};

   or...
   

HTTP                                            Modula
Client:
        GET /foo?a+b                    words := NEW(ARRAY[3] OF TEXT);
                                        words[0] := "a"; words[1] := "b";
                                        req = Request{resource="/foo",
                                                         search = words);
Server:
        <H1>Search Results
        </H1>                           r := Response{structured="<H1>Search...
"};
        <A HREF="x1">r1>/A>

   The "1.0" style transaction is more involved... it looks like:
   

INTERFACE MIME;
        TYPE Body = TEXT;

        TYPE BaseType = { text, audio, image, video,
                         application, message, multipart, extension };
        TYPE SubType = { plain, enriched, basic, gif, jpeg, mpeg,
                        octet_stream, postscript,
                        rfc822, partial, external_body,
                        alternative, mixed, parallel,
                        extension };

        TYPE ContentType = OBJECT
                base : BaseType;
                xbase : TEXT;
                sub : SubType;
                xsub: TEXT;
                parameters : REF ARRAY OF RECORD
                        name : TEXT;
                        value : TEXT;
                END;

        TYPE BodyPart = OBJECT
                        headers = REF ARRAY OF RECORD
                                name : TEXT;
                                value : TEXT;
                        END;
                        body : Body;
                METHODS
                        contentType() : ContentType;
                        decode() : TEXT; (* undoes Content-Transfer-Encoding *)
                END;
END MIME.

INTERFACE HTTP1_0;
        IMPORT MIME;

        TYPE SearchWords = REF ARRAY OF TEXT;

        TYPE Method = {'GET'};

        TYPE Request = RECORD
                method : Method;
                resource : TEXT;
                search : SearchWords;
                aux : MIME.BodyPart;

        TYPE Version = [0..999];
        TYPE StatusCode = [0..999];
        TYPE Line = TEXT; (* with no CR or LF chars *)

        TYPE Response = RECORD
                version : Version;
                code : StatusCode;
                reason : Line;
                object : MIME.BodyPart;
             END;
END.

   Whew... that was a fun excercise. I'm not sure what the point of it was,
   except to attempt to specify HTTP at a more abstract level than sequences of
   characters.
   
   A minimal HTTP client must understand the text/plain and text/html content
   types, plus the message/rfc822 head/body syntax. Unlike a minimally
   conforming MIME user agent, it is not required to understand any encodings
   (base64 and quoted-printable are required for MIME UA's) or multipart
   content types (MIME UAs must understand the boundary syntax). Fair enough...
   it matches historical usage.
   
   It seems that there are several of servers out there that will take mail
   messages/news articles and serve them up as HTML. This makes sense if
   they're going to add hyperlinking markup. On the other hand, it makes a lot
   of sense for smart WWW clients to understand RFC822/MIME syntax in all its
   glory (with the built-in ability to recognize references, etc.), given the
   amount of data available in this format (from NTTP servers, WAIS servers of
   mail archives, etc.... unfortunately, I think a lot of WAIS servers label
   news articles as TEXT, rather than message/rfc822)
   
   What's the direction in this area? I for one would like to see HTML and
   RFC822/MIME as alternate representations for roughly the same structure of
   information. More on that later...
   
  REFERENCES
  

1521[5]  DS   N. Borenstein, N. Freed, "MIME  (Multipurpose Internet Mail
           Extensions) Part One:  Mechanisms for Specifying and Describing the
           Format of Internet Message Bodies", 09/23/1993. (Pages=81)
           (Format=.txt, .ps) (Obsoletes RFC1341) (Updated by RFC1590)

 HTTP: A protocol for networked information[6]

   

------- =_aaaaaaaaaa0
Content-Type: text/x-html; charset="us-ascii"
Content-ID: <5699.763243624.3@ulua>
Content-Description: HTTP comments as html
Content-Transfer-Encoding: quoted-printable

<HEAD>
<TITLE>Comments on HTTP spec</TITLE>
</HEAD>
<body>

<H2>Mime Terms</H2>

<A HREF=3D"#HTTP">The HTTP spec</A> makes many references to MIME and
related terms, but it seems to use the terms loosely and incorrectly
in many cases: <P>

Some useful terms are:
<DL>
<DT> Body Part
<DD> Headers, a blank line, and a body
<DT> Body
<DD> a sequence of octets obeying a content transfer encoding implied
by context (such as the Content-Transfer-Encoding header in the
enclosing body part).
</DL>

Perhaps a better way to explain it is:

<PRE>
text/plain	akindof	Body	(just a plain text data stream)
image/gif	akindof	Body	(ordinary GIF data stream)
message/rfc822	akindof	Body	(data stream defined by RFC822)
message/rfc822	akindof	Body Part (wierd, huh?)
Body Part	hasa	Head
Body Part	hasa	Body
Head		hasa	Content-Transfer-Encoding
Head		hasa	Content-Type
multipart/mixed	isa	Body
multipart/mixed	hasa	sequence of Body Parts
</PRE>


=46rom <A HREF=3D"http://info.cern.ch/hypertext/WWW/Protocols/HTTP/Request=
.html">
HTTP: The Request</A>:

<BLOCKQUOTE>
<PRE>
        FullRequest       =3D     Method UR ProtocolVersion CrLf
                                [*&lt;HTRQ Header&gt;]
                                [&lt;CrLf&gt; &lt;data&gt;]

        &lt;Method&gt;          =3D     &lt;InitialAlpha&gt;

        ProtocolVersion   =3D     HTTP/1.0

        uri               =3D     &lt;as defined in URL spec&gt;

        &lt;HTRQ Header&gt;     =3D     &lt;Fieldname&gt; : &lt;Value&gt; =
&lt;CrLf&gt;

        &lt;data&gt;            =3D      MIME-conforming-message        =

</PRE>
</BLOCKQUOTE>

What is "MIME-conforming-message"? The term itself suggests that it
means an body of type message/rfc822, but the usage in the grammar
says it's just a generic body. Using the grammar of the MIME spec,
<A HREF=3D"#rfc1521">RFC1521</A>, this should be written:

<PRE>
        FullRequest       =3D     Method UR ProtocolVersion CrLf
				[body-part]

        &lt;Method&gt;          =3D     &lt;InitialAlpha&gt;

        ProtocolVersion   =3D     HTTP/1.0

        uri               =3D     &lt;as defined in URL spec&gt;
</PRE>

where <CODE>body-part</CODE> should be defined similarly to the way
it's done in MIME, i.e. some variation on the following:

<PRE>
   body-part :=3D &lt;"message" as defined in RFC 822,
             with all header fields optional, and with the
             specified delimiter not occurring anywhere in
             the message body, either on a line by itself
             or as a substring anywhere.  Note that the
             semantics of a part differ from the semantics
             of a message, as described in the text.&gt;

</PRE>

<H2>Accept Field</H2>

Hmmm... in the Accept: field: is it comma or semicolot separated?
Contrast:

<BLOCKQUOTE>
This field contains a semicolon-separated list of representation schemes (=
 Content-Type metainformation values)
which will be accepted in the response to this request.
</BLOCKQUOTE>

with this example:

<BLOCKQUOTE>
<PRE>
Accept: text/plain, text/html
Accept: text/x-dvi; q=3D.8; mxb=3D100000; mxt=3D5.0, text/x-c
</PRE>
</BLOCKQUOTE>

and then there's this example:

<BLOCKQUOTE>
<PRE>
Accept:  *.*, q=3D0.1
Accept:  audio/*, q=3D0.2
Accept:  audio/basic q=3D1
</PRE>
</BLOCKQUOTE>

<H2>Default Content-Transfer-Encoding</H2>

=46rom Comments on <A
HREF=3D"http://info.cern.ch/hypertext/WWW/Protocols/HTTP/Body.html">HTTP
spec</A>

<BLOCKQUOTE>
The data (if any) sent with an HTTP request or reply is in a format
and encoding defined by the object header fields, the default being
"plain/text" type with "8bit" encoding. Note that while all the other
information in the request (just as in the reply) is in ISO Latin1
with lines delimited by Carriage Return/Line Feed pairs, the data may
contain 8-bit binary data.
</BLOCKQUOTE>

The 8bit encoding means characters can be in the range 0-255, but the
characters are still arranged as lines of text not to exceed 76
characters, delimited by CRLF pairs. <P>

The binary encoding meands an arbitrary stream of octets. I expect
this is what the HTTP designers had in mind for the default
content-transfer-encoding... Hmmm... or perhaps not... perhaps sending
arbitrary binary data is somewhat at odds with historical usage of
HTTP, and so binary data should be labelled as such. This could use
some clarification in any case.<P>

<H2>The libWWW implementation</H2>

I browsed around the libWWW implementation, and I'm somewhat confused:
<P>

First of all what is "www/mime"? It appears to work like
message/rfc822, but then stuff labelled message/rfc822 seems to be
handed off to MetaMail!!! <P>

=46rom what I can tell, there are two classes of HTTP transactions: <P>

The "0.9" style transaction could be described by the following
Modula-like interface:

<PRE>
INTERFACE HTTP0_9;
	TYPE HTML =3D TEXT;
	TYPE SearchWords =3D REF ARRAY OF TEXT;
	TYPE Request =3D RECORD
		resource : TEXT;
		search : SearchWords =3D NIL;
	     END;
	TYPE Response =3D RECORD
		structured : HTML;
		plain : TEXT =3D NIL;
	     END;
END HTTP0_9.
</PRE>

For example,

<PRE>
HTTP						Modula
Client:
	GET /foo/bar.html		req =3D Request{resource=3D"/foo/bar.html"};
Server:
	&lt;PLAINTEXT&gt;			resp :=3D Response{
	Four score and seven years ago		structured=3D"&lt;PLAINTEXT&gt;",
	today...				plain=3D"Four score and seven..."};
</PRE>

or...

<PRE>
HTTP						Modula
Client:
	GET /foo?a+b			words :=3D NEW(ARRAY[3] OF TEXT);
					words[0] :=3D "a"; words[1] :=3D "b";
					req =3D Request{resource=3D"/foo",
							 search =3D words);
Server:
	&lt;H1&gt;Search Results
	&lt;/H1&gt;				r :=3D Response{structured=3D"&lt;H1&gt;Search..."};
	&lt;A HREF=3D"x1"&gt;r1&gt;/A&gt;
</PRE>

The "1.0" style transaction is more involved... it looks like:

<PRE>
INTERFACE MIME;
	TYPE Body =3D TEXT;

	TYPE BaseType =3D { text, audio, image, video,
			 application, message, multipart, extension };
	TYPE SubType =3D { plain, enriched, basic, gif, jpeg, mpeg,
			octet_stream, postscript,
			rfc822, partial, external_body,
			alternative, mixed, parallel,
			extension };

	TYPE ContentType =3D OBJECT
		base : BaseType;
		xbase : TEXT;
		sub : SubType;
		xsub: TEXT;
		parameters : REF ARRAY OF RECORD
			name : TEXT;
			value : TEXT;
		END;

	TYPE BodyPart =3D OBJECT
			headers =3D REF ARRAY OF RECORD
				name : TEXT;
				value : TEXT;
			END;
			body : Body;
		METHODS
			contentType() : ContentType;
			decode() : TEXT; (* undoes Content-Transfer-Encoding *)
		END;
END MIME.

INTERFACE HTTP1_0;
	IMPORT MIME;

	TYPE SearchWords =3D REF ARRAY OF TEXT;

	TYPE Method =3D {'GET'};

	TYPE Request =3D RECORD
		method : Method;
		resource : TEXT;
		search : SearchWords;
		aux : MIME.BodyPart;

	TYPE Version =3D [0..999];
	TYPE StatusCode =3D [0..999];
	TYPE Line =3D TEXT; (* with no CR or LF chars *)

	TYPE Response =3D RECORD
		version : Version;
		code : StatusCode;
		reason : Line;		=

		object : MIME.BodyPart;
	     END;
END.
</PRE>

Whew... that was a fun excercise. I'm not sure what the point of it
was, except to attempt to specify HTTP at a more abstract level than
sequences of characters. <P>

A minimal HTTP client must understand the text/plain and text/html
content types, plus the message/rfc822 head/body syntax. Unlike a
minimally conforming MIME user agent, it is not required to understand
any encodings (base64 and quoted-printable are required for MIME UA's)
or multipart content types (MIME UAs must understand the boundary
syntax). Fair enough... it matches historical usage.<P>

It seems that there are several of servers out there that will take
mail messages/news articles and serve them up as HTML. This makes
sense if they're going to add hyperlinking markup. On the other hand,
it makes a lot of sense for smart WWW clients to understand
RFC822/MIME syntax in all its glory (with the built-in ability to
recognize references, etc.), given the amount of data available in
this format (from NTTP servers, WAIS servers of mail archives, etc....
unfortunately, I think a lot of WAIS servers label news articles as
TEXT, rather than message/rfc822)<P>

What's the direction in this area? I for one would like to see HTML
and RFC822/MIME as alternate representations for roughly the same
structure of information. More on that later...<P>

<H3>References</H3>
<PRE>
<A NAME=3D"rfc1521"
HREF=3D"ftp://ds.internic.net/rfc/rfc1521.txt">
1521</A>  DS   N. Borenstein, N. Freed, "MIME  (Multipurpose Internet Mail=
  =

           Extensions) Part One:  Mechanisms for Specifying and Describing=
 the =

           Format of Internet Message Bodies", 09/23/1993. (Pages=3D81) =

           (Format=3D.txt, .ps) (Obsoletes RFC1341) (Updated by RFC1590) =


<A NAME=3D"HTTP"
HREF=3D"http://info.cern.ch/hypertext/WWW/Protocols/HTTP/HTTP2.html">
HTTP: A protocol for networked information</A>
</PRE>

------- =_aaaaaaaaaa0--



From michael.shiplett@umich.edu  Wed Mar  9 21:56:48 1994 --100
Message-Id: <199403092053.PAB01199@totalrecall.rs.itd.umich.edu>
Date: Wed, 9 Mar 1994 21:56:48 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: Insecure WWW Access Authorization Protocol? 

"ts" == Tony Sanders <sanders@BSDI.COM> writes:

ts> michael shiplett <michael.shiplett@umich.edu> writes:
>> 2) Mallet intercepts the request and sends to Alice:
>> 401 Unauthorized
>> Authenticate: KerberosV4 "http.mallet.evil.com@EVIL.COM"
>> This is the critical step, because Mallet is able to control to
>> which service Alice should authenticate herself.

ts> Would someone please explain to me why a user would enter a
ts> password at a prompt that read: "http.mallet.evil.com@EVIL.COM"
ts> (or anything thing else they didn't recoginze?
  Because it is not given that the user enters a password or actively
participates in the authentication sequence. For example, with
Kerberos, as long as the user has valid ticket-granting ticket,
Kerberos will handle getting the information needed to authenticate to
the http server--without requiring the user to actively provide *any*
information. With public key cryptography when the private key is
encrypted with a symmetric algorithm, many programs allow the user to
enter the private key password once per session instead of asking for
it each and every time the private key is needed.

ts> Wouldn't they have to have a password for that realm and don't you
ts> think they might think "gee, I don't have a password for that
ts> realm".
  The evil server could just as easily be in the user's realm. It is
not obvious nor necessary that the user is directly involved in
authenticating.

michael



From fielding@simplon.ICS.UCI.EDU  Wed Mar  9 23:20:07 1994 --100
Message-Id: <9403091415.aa05301@paris.ics.uci.edu>
Date: Wed, 9 Mar 1994 23:20:07 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Conditional GET Proposal for HTTP Caching

Hello all,

I made a proposal in January about providing a conditional GET
capability in HTTP.  It involves accepting a Last-modified header
in an HTTP request and sending the document only if the Last-modified
date of the document is more recent than the one in the request.

The full proposal is now available at 

      http://www.ics.uci.edu/WebSoft/caching.html

I was wondering whether any of the server authors are planning on
including this feature in future versions?


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From SCHAEFER@sandse.ssd.lmsc.lockheed.com  Thu Mar 10 01:15:29 1994 --100
Message-Id: <9403100007.AA25571@eagle.is.lmsc.lockheed.com>
Date: Thu, 10 Mar 1994 01:15:29 --100
From: SCHAEFER@sandse.ssd.lmsc.lockheed.com (Jay Schaefer )
Subject: Viewing  Tables and Math Equations

Hi,

Has anyone used an html viewer (Mosaic ?) to view tables and 
equations?  I have need to display this type of information in the very 
near future.

Thanks,  Jay


----------------------------------------------------------------------
Jay Schaefer
O/73-12 B/564
Research Specialist/Group Lead
Systems Engineering Software Tools Group
SSD Systems Engineering

Lockheed Missiles and Space Company
1111 Lockheed Way
Sunnyvale, CA  94089-3504

Phone : work  (408) 756-6955
        fax   (408) 743-0863
        home  (510) 795-2785

Email:  jay_schaefer@lmsc.lockheed.com
----------------------------------------------------------------------



From Jon.Tetzchner@nta.no  Thu Mar 10 09:50:23 1994 --100
Message-Id: <199403100845.AA02566@hal.nta.no>
Date: Thu, 10 Mar 1994 09:50:23 --100
From: Jon.Tetzchner@nta.no (Jon von Tetzchner Stephenson)
Subject: Re:  [martin@rare.nl: Framemaker -> HTML] Framemaker -> HTML

	From ivars Mon Mar  7 09:06:00 1994
	Received: from haydn.nta.no by hal.nta.no with SMTP id AA28355
	  (5.65c/IDA-1.4.4 for <jons>); Mon, 7 Mar 1994 09:05:59 +0100
	Message-Id: <199403070805.AA28355@hal.nta.no>
	Received: by haydn.nta.no (4.1/SMI-4.1)
		id AA13170; Mon, 7 Mar 94 09:05:57 +0100
	To: jons
	Subject: [martin@rare.nl: Framemaker -> HTML]
	Date: Sun, 6 Mar 1994 17:29:31 --100
	Reply-To: martin@rare.nl
	Originator: www-talk@info.cern.ch
	Sender: www-talk@www0.cern.ch
	Precedence: bulk
	From: martin@rare.nl (John Martin)
	To: Multiple recipients of list <www-talk@www0.cern.ch>
	Subject: Framemaker -> HTML
	X-Listprocessor-Version: 6.0c -- ListProcessor by Anastasios Kotsikonas
	Content-Length: 259
	Status: RO

	Hi,

	Can anyone comment on the HTML conversion capabilities of Framemaker to HTML
	(on a Mac) please.

	Alternatively if you could suggest an editor for use on a Mac, that would be
	useful also.

	Please send replies to me amd I will summarise to the list.

	John


Hi,

I suggest you take a look at my FrameMaker to HTML converter at
bang.nta.no in the pub directory.

Regards,

Jon.



From fielding@simplon.ICS.UCI.EDU  Thu Mar 10 10:10:44 1994 --100
Message-Id: <9403100106.aa10764@paris.ics.uci.edu>
Date: Thu, 10 Mar 1994 10:10:44 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Announcing wwwstat-0.3 -- an access log summary program 


This message is to announce the availability of wwwstat Version 0.3 --
a program for analyzing NCSA httpd_1.1 or 1.0 server access logs and printing
an HTML-formatted summary report.  The program is written in Perl
and, once customized for your site, should work on any UNIX-based system
with Perl 4.019 or better.  The program is in the public domain (i.e. FREE).

As an example of what wwwstat can do for you, look
    <A HREF="http://www.ics.uci.edu/Admin/wwwstats.html"> here </A>
to see UC Irvine's Department of Information and Computer Science
WWW server statistics.

For more information and access to the wwwstat-0.3 distribution,
point your World-Wide Web client at

    <A HREF="http://www.ics.uci.edu/WebSoft/wwwstat/"> wwwstat-0.3 </A>.

For those of you without offsite http access but with ftp access, wwwstat is
also available via anonymous ftp at:

    <ftp://liege.ics.uci.edu/pub/arcadia/wwwstat/wwwstat-0.3.tar.Z>

One of the nicest things about wwwstat is that it does not make any changes
to or write any files in the server directories. Thus, this program can be
safely run by any user with read access to the httpd server's access_log and
srm.conf files. This allows people to do specialized summaries of just the
things they are interested in.

Version 0.3 now provides a plethora of options for creating customized
reports and for making it easier for webmasters to maintain their server.
See below for a further description of the options. It does not yet support
the proposed new log format -- that will be version 1.0.


What's new in this version:

   wwwstat [-helLoOuUvxz] [-f logfile] [-s srmfile] [-i pathname] [-d date]
                          [-a IP_address] [-n archive_name]       [-t time]


   Version 0.3                                       March 9, 1994
      Added links for last server summary, table-of-contents, 
        and a reference to the standard distribution site because
        similar things looked good in Kevin Hughes' getstats output.
      Automatically determines URL of previous month's summary.
      Now allows extra spaces on Alias directive lines in srm.conf.
      Now recognizes Redirect directives and estimates size of message.
      No longer counts automatically redirected directory names twice --
        it estimates size of redirect message and counts that instead.
      Now uses normal printf's instead of perl forms.
      Reversed order of printed fields to allow for long names and the
        ability to read its own output (see the -i option below).
      Updated the country-codes file to reflect latest standards/spelling.
      Added the following options (phew!):
      Display Options:
        -h  Help -- just display the usage message and quit.
        -e  Display all invalid log entries on STDERR;
              -- this is great for finding trashed log entries for cleaning.
        -l  Do    display full IP address of clients in my domain.
        -L  Don't display full IP address of clients in my domain.
        -o  Do    display full IP address of clients from other domains.
        -O  Don't display full IP address of clients from other domains.
        -u  Do    display IP address from unresolved domain names.
        -U  Don't display IP address from unresolved domain names.
        -v  Verbose display (to STDERR) of each log entry processed;
              -- useful, but not recommended for long logs.
        -x  Display all requests of nonexistant files to STDERR;
              -- this is great for finding misadvertized or moved URLs.
      Input Options:
        -f  Read from the following access_log file instead of the default;
              -- allows you to read archived (or test) logfiles.
        -z  Use zcat to uncompress the log file while reading [requires -f];
              -- allows you to read compressed logfile archives;
                 use "gzip -9" to get factor of 10 reduction in file sizes.
        -s  Get the server directives from the following srm.conf file;
              -- allows you to archive the configuration along with the log.
        -i  Include the following file (assumed to be a prior wwwstat output);
              -- incredibly great, allows you to keep partial summary
                 periods in wwwstat output files and purge the logfile.
                 Inventive admins can find many uses for this, such as being
                 used by scripts to provide fast, up-to-the-minute stats.
      Search Options (include in summary only those log entries):
        -a  Containing the following "substring" in the IP address.
        -d  Containing the following "substring" in the date.
        -t  Containing the following "substring" in the time.
        -n  Containing the following "substring" in the archive (URL) name.
              -- allows you to restrict logfile summaries to an area
                 of particular interest; great for custom author summaries;
                 Search strings are matched as substrings, prefix (if string
                 starts with a caret "^"), or suffix (if string ends with "$").
                 Note that strings containing $ must be enclosed in single
                 quotes for most shell command lines.



Obviously, versions of this program would also be nice for the Plexus
and CERN servers.  However, I found that much of the logic for finding
file names was just too specific to the NCSA server to justify all the
other work of making this general.  Feel free to do so yourself.
In particular, wwwstat has been carefully designed to accurately estimate
the files and bytes transmitted per request by following as closely as
possible the logic used by the NCSA server in handling requests.

This work has been sponsored in part by the Defense Advanced Research Projects
Agency under Grant Number MDA972-91-J-1010.  This software does not
necessarily reflect the position or policy of the U.S. Government and no
official endorsement should be inferred.  Their support is appreciated.

If you have any suggestions, bug reports, fixes, or enhancements,
send them to me at <fielding@ics.uci.edu>.  Also, I would like to ask anyone
who uses wwwstat on a regular basis to please send me an e-mail message which
indicates how and where it is being used (i.e. to publish stats, perform
research, assist in server maintenance, and/or just allow HTML authors to
see how much their work is appreciated) and also, if it is public information,
a URL to your site. This is, of course, only voluntary and I don't want
anyone to divulge private information, but please understand that such
information allows free-software authors like me to justify the time and
effort needed to build quality tools.

Have fun,


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From luotonen@ptsun00.cern.ch  Thu Mar 10 10:52:10 1994 --100
Message-Id: <9403100948.AA06521@ptsun03.cern.ch>
Date: Thu, 10 Mar 1994 10:52:10 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: Conditional GET Proposal for HTTP Caching


> I made a proposal in January about providing a conditional GET
> capability in HTTP.  It involves accepting a Last-modified header
> in an HTTP request and sending the document only if the Last-modified
> date of the document is more recent than the one in the request.
> 
> The full proposal is now available at 
> 
>       http://www.ics.uci.edu/WebSoft/caching.html
> 
> I was wondering whether any of the server authors are planning on
> including this feature in future versions?

CERN team thinks this is a neat idea, and is all for it.  It
doesn't break any existing servers, and makes it possible to
have much faster caching.

But let's not call it Last-Modified, let's call it Newer-Than
or somethig like that.  Once we are in agreement about the name
I'll put it into cern_httpd.

Cheers,
--
Ari Luotonen		 |
World-Wide Web Project	 |
CERN			 | phone: +41 22 767 8583
CH - 1211 Geneve 23	 | email: luotonen@dxcern.cern.ch



From Paul.Wain@brunel.ac.uk  Thu Mar 10 11:09:54 1994 --100
Message-Id: <27910.9403101004@thor.brunel.ac.uk>
Date: Thu, 10 Mar 1994 11:09:54 --100
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Re: Conditional GET Proposal for HTTP Caching

@ > I made a proposal in January about providing a conditional GET
@ > capability in HTTP.  It involves accepting a Last-modified header
@ > in an HTTP request and sending the document only if the Last-modified
@ > date of the document is more recent than the one in the request.
@ 
@ [...]
@
@ But let's not call it Last-Modified, let's call it Newer-Than
@ or somethig like that.

What about ``Last-Seen''

That fits in best with the idea that is being premoted here surely?

Paul

.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From fielding@simplon.ICS.UCI.EDU  Thu Mar 10 11:15:07 1994 --100
Message-Id: <9403100205.aa13438@paris.ics.uci.edu>
Date: Thu, 10 Mar 1994 11:15:07 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Conditional GET Proposal for HTTP Caching 

> But let's not call it Last-Modified, let's call it Newer-Than
> or somethig like that.  Once we are in agreement about the name
> I'll put it into cern_httpd.

Yeah, I guess a name change would make it more explicit.
How about   If-Modified-Since:  ?


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)



From dsr@hplb.hpl.hp.com  Thu Mar 10 11:28:34 1994 --100
Message-Id: <9403101021.AA09484@manuel.hpl.hp.com>
Date: Thu, 10 Mar 1994 11:28:34 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Conditional GET Proposal for HTTP Caching

> I made a proposal in January about providing a conditional GET
> capability in HTTP.  It involves accepting a Last-modified header
> in an HTTP request and sending the document only if the Last-modified
> date of the document is more recent than the one in the request.

Please Please lets get this rolling. My browser uses a shared cache
based on a UDP protocol to contact the cache server and NFS to share
files. Roy's suggestion if adopted widely would be really helpful.

Dave Raggett



From luotonen@ptsun00.cern.ch  Thu Mar 10 11:38:48 1994 --100
Message-Id: <9403101035.AA06542@ptsun03.cern.ch>
Date: Thu, 10 Mar 1994 11:38:48 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: Conditional GET Proposal for HTTP Caching


> Yeah, I guess a name change would make it more explicit.
> How about   If-Modified-Since:  ?

PERFECT!!

-- Cheers, Ari --




From terry@ora.com  Thu Mar 10 16:32:48 1994 --100
Message-Id: <199403101525.AA04768@rock.west.ora.com>
Date: Thu, 10 Mar 1994 16:32:48 --100
From: terry@ora.com (Terry Allen)
Subject: Server announcements: forms?

Back on server announcements, Dan proposed a combination of MIME
typing and sending-the-announcement-to-the-right-place as a way
of making these announcements easier to automate.

This places a lot of responsibility on the person making up the
announcement message.  Could we do better by proposing a Form
to be filled out?  (Think about complicated announcements, in
which several URLs are given for various parts of the resource.)

I suppose I'm suggesting, indirectly, forms that complain when
you've filled them out incorrectly . . .

Regards,

-- 
Terry Allen  (terry@ora.com)
Editor, Digital Media Group
O'Reilly & Associates, Inc.
Sebastopol, Calif., 95472



From luotonen@ptsun00.cern.ch  Thu Mar 10 18:17:46 1994 --100
Message-Id: <9403101712.AA07792@ptsun03.cern.ch>
Date: Thu, 10 Mar 1994 18:17:46 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: If-Modified-Since and 304


Ok,

I added If-Modified-Since: and 304 Use Local Copy to HTTP spec.
I also added 204 No Response that seemed to be missing from there.

If-Modified-Since: date
 
   This request header is used with GET method to make it conditional: if
   the requested document has not changed since the time specified in
   this field the document will not be sent, but instead a Use Local Copy
   304 reply.
 
   Format of this field is the same as for Date:.
 

USE LOCAL COPY 304
 
   If the client has done a conditional GET and access is allowed, but
   the document has not been modified since the date and time specified
   in If-Modified-Since field, server responds with 304 status code, and
   does not send the document back to the client.
 
   This is to allow fast update of caches.
 
 
NO RESPONSE 204
 
   Server has received the request but there is no information to send
   back, and the client should stay in the same document view. This is
   mainly to allow input for scripts without changing the document at the
   same time.


-- Cheers, Ari --




From joel@binkley.cs.mcgill.ca  Thu Mar 10 19:15:34 1994 --100
Message-Id: <9403101810.AA23968@binkley.cs.mcgill.ca>
Date: Thu, 10 Mar 1994 19:15:34 --100
From: joel@binkley.cs.mcgill.ca (Joel MALARD)
Subject: Language preference in www servers



Could anyone provide some pointers or hints on how to incorporate
language preferences in http servers and www clients. We are building
in Montreal a Freenet whose function is to accept documents in various
languages, at least french and english.

Joel Malard. - Tech Comm. of Reseau Electronique du Montreal Metropolitain.
joel@cs.mcgill.ca



From masinter@parc.xerox.com  Thu Mar 10 19:22:28 1994 --100
Message-Id: <94Mar10.101243pst.2732@golden.parc.xerox.com>
Date: Thu, 10 Mar 1994 19:22:28 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: last-seen

a) If the server has a file with an EARLIER date, do you want that too?
b) is the date in GMT?



From ellson@hotsand.att.com  Thu Mar 10 20:13:50 1994 --100
Message-Id: <9403101907.AA03991@hotsand.dacsand>
Date: Thu, 10 Mar 1994 20:13:50 --100
From: ellson@hotsand.att.com (ellson@hotsand.att.com)
Subject: Re:  last-seen

> a) If the server has a file with an EARLIER date, do you want that too?

No.

> b) is the date in GMT?

Don't care.

IMHO The date stamp that is sent back should be the exact string that was
in the "Last-modified" field when the document was first received
by the cache.  If the string is different in any way then the server
should assume that the receiving cache has the wrong document and re-serve it.

The receiving cache should not attempt to understand the
semantics of the "Last-modified" date.  It should be treated as a
tag provided by the server that the server can match against when it
is sent back in the "If-Modified-Since" field.

Since only the original server makes any semantic interpretation of
the string, it doesn't matter if the timestamp is GMT, or local.  

John Ellson
AT&T Bell Labs



From altis@ibeam.jf.intel.com  Thu Mar 10 20:26:01 1994 --100
Message-Id: <m0peqH9-00041wC@ibeam.intel.com>
Date: Thu, 10 Mar 1994 20:26:01 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: If-Modified-Since and 304

At  6:16 PM 3/10/94 +0000, Ari Luotonen wrote:
>Ok,
>
>I added If-Modified-Since: and 304 Use Local Copy to HTTP spec.
>I also added 204 No Response that seemed to be missing from there.
>
>If-Modified-Since: date

"date" should be the same format as "Date:" and "Last-modified:". See
examples below:

Date: Thursday, 10-Mar-94 19:20:00 GMT
Last-modified: Thursday, 10-Feb-94 22:23:32 GMT

Note, use of GMT, not local time!

ka





From bert@let.rug.nl  Thu Mar 10 20:42:17 1994 --100
Message-Id: <9403101937.AA11612@freya.let.rug.nl>
Date: Thu, 10 Mar 1994 20:42:17 --100
From: bert@let.rug.nl (Bert Bos)
Subject: Re: last-seen

 |> b) is the date in GMT?
 |
 |Don't care.
[...]
 |The receiving cache should not attempt to understand the
 |semantics of the "Last-modified" date.  It should be treated as a
 |tag provided by the server that the server can match against when it
 |is sent back in the "If-Modified-Since" field.

But presumably the requesting program only asks for the document if
the version it has is expired. That means it must have interpreted the
date & time. Isn't it better then to let the receiving cache generate
a valid date & time from its internal representation, instead of
keeping the original date string around?


-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|



From Gary.Adams@east.sun.com  Thu Mar 10 20:48:35 1994 --100
Message-Id: <9403101936.AA07801@zeppo.East.Sun.COM>
Date: Thu, 10 Mar 1994 20:48:35 --100
From: Gary.Adams@east.sun.com (Gary Adams - Sun Microsystems Labs BOS)
Subject: Re: If-Modified-Since and 304

If the value of If-modified-since field really is a date value
then it should be possible to perform time dependent operations on 
it. I don't think people really want to accomplish revision control
via this mechanism (e.g. Fetch the Jan 10 version of the What's New file).
If the primary purpose of the value is to serve as an opaque server cookie
then it ought to be spec'ed that way.

$.02



From heffron@falstaff.css.beckman.com  Thu Mar 10 20:55:38 1994 --100
Message-Id: <9403101945.AA09226@dxmint.cern.ch>
Date: Thu, 10 Mar 1994 20:55:38 --100
From: heffron@falstaff.css.beckman.com (Matt Heffron)
Subject: last-seen & server ability to indicate "Never cache"?

Larry Masinter <masinter@parc.xerox.com> asks:

>a) If the server has a file with an EARLIER date, do you want that too?

>From Ari's description of "If-Modified-Since: date" I'd say NO. 
return the 304 status code

>b) is the date in GMT?

It better be, or include the offset from GMT.

---
If we want to do caching well, we also need some way to indicate
that a document should never be cached, e.g. the output from some
scripts will ALWAYS be new documents.  There's no point
cluttering the cache with things that will never be served again.

Matt Heffron

PS.  Where is the latest HTTP spec available from?

--
Matt Heffron                      heffron@falstaff.css.beckman.com
Beckman Instruments, Inc.         voice: (714) 961-3128
2500 N. Harbor Blvd. MS X-10, Fullerton, CA 92634-3100
I don't speak for Beckman Instruments unless they say so.



From jay@eit.COM  Thu Mar 10 21:02:32 1994 --100
Message-Id: <199403101955.TAA06762@hnear>
Date: Thu, 10 Mar 1994 21:02:32 --100
From: jay@eit.COM (Jay Glicksman)
Subject: WWW "versus" gopher

At the current rate of growth, WWW use on the Internet will pass
gopher use (in both packet count and byte count) sometime during this
current month (or next month for sure). The raw statistics are below.

For a graphic view of the relative growth of various Internet services
see

http://www.eit.com/techinfo/nsfnet/nsfnet.html

Congratulations to the WWW community for developing a service so
widely used.

	Jay Glicksman


                    NSFNET Traffic Distribution Highlights

                             February 1994

 Service Name       Port  Rank Packet Count % Pkts  Rank   Byte Count  % Byts
 ============       ====  ==== ============ ======  ==== ============= ======
 gopher               70     9   1472386850  2.455     6  396066059800  3.470
 www                  80    11   1105399700  1.843     7  347503518500  3.044

                             January 1994

 Service Name       Port  Rank Packet Count % Pkts  Rank   Byte Count  % Byts
 ============       ====  ==== ============ ======  ==== ============= ======
 gopher               70     9   1383479450  2.502     5  374680912550  3.640
 www                  80    11    822317950  1.487     8  269129084100  2.614

                             December 1993

 Service Name       Port  Rank Packet Count % Pkts  Rank   Byte Count  % Byts
 ============       ====  ==== ============ ======  ==== ============= ======
 gopher               70     9   1148324150  2.191     6  309690082950  3.041
 www                  80    11    685153950  1.307     8  225443122450  2.213

                             November 1993

 Service Name       Port  Rank Packet Count % Pkts  Rank   Byte Count  % Byts
 ============       ====  ==== ============ ======  ==== ============= ======
 gopher               70     8   1121316450  2.104     6  291132949450  2.902
 www                  80    12    512805100  0.962     8  172339899050  1.718



From luotonen@ptsun00.cern.ch  Thu Mar 10 21:35:57 1994 --100
Message-Id: <9403102028.AA07878@ptsun03.cern.ch>
Date: Thu, 10 Mar 1994 21:35:57 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: last-seen & server ability to indicate "Never cache"?


First, if we have If-Modified-Since: header, we naturally
don't return an even older version -- only if we have a newer
one.

Second, yes, date is in GMT.  All HTTP times are in GMT, and
I made a reference saying that the format is the same as for
Date: field and assumed that would be clear.

So what John Ellson said about giving exactly the same string
back in If-Modified-Since header as we got from Last-Modified
is valid, and in that sense we don't have to care about the
time zone, but since (for other reasons) Last-Modified is spec'd
to be GMT so is If-Modified-Since.


> If we want to do caching well, we also need some way to indicate
> that a document should never be cached, e.g. the output from some
> scripts will ALWAYS be new documents.

Use Expires: with the current time.


> PS.  Where is the latest HTTP spec available from?

http://info.cern.ch/hypertext/WWW/Protocols/HTTP/HTTP2.html

-- Cheers, Ari --




From altis@ibeam.jf.intel.com  Thu Mar 10 21:54:20 1994 --100
Message-Id: <m0perXv-00042PC@ibeam.intel.com>
Date: Thu, 10 Mar 1994 21:54:20 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: WWW "versus" gopher

At  9:01 PM 3/10/94 +0000, Jay Glicksman wrote:
>At the current rate of growth, WWW use on the Internet will pass
>gopher use (in both packet count and byte count) sometime during this
>current month (or next month for sure). The raw statistics are below.

This is great.

On the other hand, I can't understand the staggering amount of telnet
traffic. More than email, almost as much as ftp data, most Internet users
must still be on character oriented displays?! I guess if the telnet
numbers start dropping, then we can assume that people are making CGI front
ends to databases (like the Library of Congress) normally accessed via a
telnet session.

ka





From wiltzius@students.wisc.edu  Thu Mar 10 22:29:07 1994 --100
Message-Id: <199403102121.PAA14868@audumla.students.wisc.edu>
Date: Thu, 10 Mar 1994 22:29:07 --100
From: wiltzius@students.wisc.edu (Jeffrey David Wiltzius)
Subject: info

How do I unsubscribe from this mailing list.  I've tried to do it a couple
of times but I just can't figure it out.

Thanks,


Jeffrey D. Wiltzius





From bloemer@tnt.uni-hannover.de  Thu Mar 10 23:58:19 1994 --100
Message-Id: <9403102250.AA03703@helios.tnt.uni-hannover.de>
Date: Thu, 10 Mar 1994 23:58:19 --100
From: bloemer@tnt.uni-hannover.de (Arnold Bloemer)
Subject: Who is exhibiting on CeBIT 94 fair?

I am looking for exhibitors on CeBIT 94 fair in Hannover who are
demonstrating advanced internet technology like MBone and World-Wide
Web.

During the time of CeBIT fair I will work for the Deutsche Messe AG as
a scientific adviser. One of my jobs is to look around for interesting
new products and write articles with a stronger technical background.
These articles are handed out to the jounalists and shall give them
some hints on where it is worth to look.

I would like to make a special report on Internet services.  If you are
presenting such a service or know who does, please send me a short
notice on where to meet you on CeBIT fair.

Arnold



From rik@rdt.monash.edu.au  Fri Mar 11 00:37:21 1994 --100
Message-Id: <199403102327.JAA16355@alquist.rdt.monash.edu.au>
Date: Fri, 11 Mar 1994 00:37:21 --100
From: rik@rdt.monash.edu.au (Rik Harris)
Subject: Re: If-Modified-Since and 304 

> Date: Thursday, 10-Mar-94 19:20:00 GMT
> Last-modified: Thursday, 10-Feb-94 22:23:32 GMT

Why can't new standards use 4-digit years?  Is character-for-character
compliance with outdated standards really necessary?  Why is there a
reluctance to move away from 2-digit years?

(personally, I'd even go a step further than that and use ISO dates
like 1994-03-10, but that's not quite as human friendly, but great for
sorting :-)

rik (depressed person trying to do automated searching/sorting, etc).
--
Rik Harris - rik.harris@vifp.monash.edu.au              || Systems Programmer
+61 3 560-3265 (AH) +61 3 90-53227 (BH)                 || and Administrator
Fac. of Computing & Info.Tech., Monash Uni, Australia   || Vic. Institute of
http://www.vifp.monash.edu.au/people/rik.html           || Forensic Pathology



From montulli@stat1.cc.ukans.edu  Fri Mar 11 01:00:35 1994 --100
Message-Id: <9403102352.AA17388@stat1.cc.ukans.edu>
Date: Fri, 11 Mar 1994 01:00:35 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: WWW "versus" gopher

> 
> At  9:01 PM 3/10/94 +0000, Jay Glicksman wrote:
> >At the current rate of growth, WWW use on the Internet will pass
> >gopher use (in both packet count and byte count) sometime during this
> >current month (or next month for sure). The raw statistics are below.
> 
> This is great.
> 
> On the other hand, I can't understand the staggering amount of telnet
> traffic. More than email, almost as much as ftp data, most Internet users
> must still be on character oriented displays?! I guess if the telnet
> numbers start dropping, then we can assume that people are making CGI front
> ends to databases (like the Library of Congress) normally accessed via a
> telnet session.
> 
Those are all the Lynx users. :)

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *      Let's face it, I'm a nerd, why else would I have a sig file?      *
  **************************************************************************



From stumpf@informatik.tu-muenchen.de  Fri Mar 11 01:44:35 1994 --100
Message-Id: <94Mar11.013926mesz.311468@hprbg5.informatik.tu-muenchen.de>
Date: Fri, 11 Mar 1994 01:44:35 --100
From: stumpf@informatik.tu-muenchen.de (Markus Stumpf)
Subject: Multipart Messages in HTTP

I have just read in 
   http://info.cern.ch/hypertext/WWW/Protocols/HTTP/Body.html
about the possibility for servers.

If I understand it right, the server does not need to close the connection
after the document is delivered, if it is possible to "detect the end
of the document" by the presence of a Content-Length: field.

Are there any possibilities for clients to send multipart-requests to
the server?
As with all the icons and little images the expensive part in fetching
a document is the connection setup.

If a client and a server "could hold the line" until either
1) the server must close the connection, as he can't predict the
   length of the document (some data generating script or the like)
2) the client has all it want's to get for now (e.g. all the inlined
   images to the document).
3) timout or failure occurs.

I checked, but I didn't find anything and as it looks the current
practice of the servers is to close the connection anyway.

Would it be a protocol viloation if the client would send another
e.g. GET after all the data of the former GET was read?

	\Maex
-- 
______________________________________________________________________________
 Markus Stumpf                        Markus.Stumpf@Informatik.TU-Muenchen.DE 
                                http://www.informatik.tu-muenchen.de/~stumpf/



From janssen@parc.xerox.com  Fri Mar 11 03:10:34 1994 --100
Message-Id: <whTx8LMB0KGWA2acxR@holmes.parc.xerox.com>
Date: Fri, 11 Mar 1994 03:10:34 --100
From: janssen@parc.xerox.com (Bill Janssen)
Subject: Re: WWW "versus" gopher

Gosh, this doesn't seem so surprising, as "www" includes all of gopher,
http, ftp, etc.  What would be more interesting would be if, say, http
use surpassed gopher.  But looking at your message (port 80), it would
seem that this is really what you meant to say.

Bill



From felicia@sprecher.cs.uwm.edu  Fri Mar 11 06:04:09 1994 --100
Message-Id: <199403110500.XAA17753@sprecher.cs.uwm.edu>
Date: Fri, 11 Mar 1994 06:04:09 --100
From: felicia@sprecher.cs.uwm.edu (Chu-Peng Cheong)
Subject: HELP - Need to startup a HTTP daemon.

I have converted a set of documents to HTML format, and am trying to
setup a server to test remote access to these files.  These files are
now owned and kept under my account.
However, I have not been successful in starting up the httpd server
under my persomal account (which is not super user).

Would someone please point out what is wrong with this setup?  
Any pointer would be helpful!

**** The error I got:

<BODY><H1>Error 400</H1>

Invalid request "" (unknown method)</BODY>

[1]    Exit -54             httpd -r http_rule.conf

**** The command I run to start the daemon:

	httpd -r http_rule.conf &

**** And, the http_rule.conf file is:

map 	/	/usr/ai/felicia/lynx2-2/html2
map 	/*	file:/usr/ai/felicia/lynx2-2/html2/*
pass	file:/usr/ai/felicia/lynx2-2/html2/*
fail 	*


----------------------------------------------------------------------------
Felicia Cheong




From Paul.Wain@brunel.ac.uk  Fri Mar 11 09:45:28 1994 --100
Message-Id: <13372.9403110841@thor.brunel.ac.uk>
Date: Fri, 11 Mar 1994 09:45:28 --100
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Re: WWW "versus" gopher

Quick question,

@  Service Name       Port  Rank Packet Count % Pkts  Rank   Byte Count  % Byts
@  ============       ====  ==== ============ ======  ==== ============= ======
@  gopher               70     9   1472386850  2.455     6  396066059800  3.470
@  www                  80    11   1105399700  1.843     7  347503518500  3.044

Looking at these stats, I would think that the values could be a bit low
for the WWW packet count. The reason I ask is, what about all those
servers out here that exist on other ports, such as, say:

801, 8000, 8080, 4040, etc etc.

Could some rough factoring be added in for them? I just ask since even
it 90% of people used port 80 and only 10% dont (looking at my hotlist
most appear to be on another port :) then that would give new www
figures of:

@  Service Name       Port  Rank Packet Count % Pkts  Rank   Byte Count  % Byts
@  ============       ====  ==== ============ ======  ==== ============= ======
@  gopher               70     9   1472386850  2.455     6  396066059800  3.470
>> www                  80etc xx   1215939670  x.xxx     x  382253870355  x.xxx

Which although it keeps the WWW is "7"th place, means that by now it has
already overtaken gopher....

Just a quick hack out at some maths there. Let me know what you all
think :)

Paul

.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
+-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
+-------------------------------------------------------------------------+
|               http://http1.brunel.ac.uk:8080/paul/	                  |
|               http://htt21.brunel.ac.uk:8080/paul/                      |
|               http://http1.brunel.ac.uk:4040/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From paulp@is.internic.net  Fri Mar 11 10:55:26 1994 --100
Message-Id: <Pine.3.89.9403110111.A23124-0100000@is.internic.net>
Date: Fri, 11 Mar 1994 10:55:26 --100
From: paulp@is.internic.net (Paul Phillips)
Subject: Re: WWW "versus" gopher

  Maybe we should all read "Lying with Statistics." :-) Wouldn't one 
expect as many gopher servers to run on a non-standard port as http 
servers? Inflating the WWW numbers to account for this while leaving the 
gopher numbers untouched generates numbers for the sake of numbers.  The 
Web is growing fine on its own...

|-------------------------------------------------------------------------|
| Paul Phillips                  | EMAIL: paulp@is.internic.net           |
| InterNIC Information Services  |   WWW: http://www.internic.net/~paulp/ |
| Reference Desk Staff           | PHONE: 619-455-4626 FAX: 619-455-4640  |
|-------------------------------------------------------------------------|
 

On Fri, 11 Mar 1994, Paul S. Wain wrote:

> Quick question,
> 
> @  Service Name       Port  Rank Packet Count % Pkts  Rank   Byte Count  % Byts
> @  ============       ====  ==== ============ ======  ==== ============= ======
> @  gopher               70     9   1472386850  2.455     6  396066059800  3.470
> @  www                  80    11   1105399700  1.843     7  347503518500  3.044
> 
> Looking at these stats, I would think that the values could be a bit low
> for the WWW packet count. The reason I ask is, what about all those
> servers out here that exist on other ports, such as, say:
> 
> 801, 8000, 8080, 4040, etc etc.
> 
> Could some rough factoring be added in for them? I just ask since even
> it 90% of people used port 80 and only 10% dont (looking at my hotlist
> most appear to be on another port :) then that would give new www
> figures of:
> 
> @  Service Name       Port  Rank Packet Count % Pkts  Rank   Byte Count  % Byts
> @  ============       ====  ==== ============ ======  ==== ============= ======
> @  gopher               70     9   1472386850  2.455     6  396066059800  3.470
> >> www                  80etc xx   1215939670  x.xxx     x  382253870355  x.xxx
> 
> Which although it keeps the WWW is "7"th place, means that by now it has
> already overtaken gopher....
> 
> Just a quick hack out at some maths there. Let me know what you all
> think :)
> 
> Paul
> 
> .-------------------------------------------------------------------------.
> |       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
> +-------------------------------------------------------------------------+
> | Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
> |   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
> +-------------------------------------------------------------------------+
> |               http://http1.brunel.ac.uk:8080/paul/	                  |
> |               http://htt21.brunel.ac.uk:8080/paul/                      |
> |               http://http1.brunel.ac.uk:4040/~ccsrpsw/                  |
> `-------------------------------------------------------------------------'
> 



From P.Lister@cranfield.ac.uk  Fri Mar 11 13:54:40 1994 --100
Message-Id: <9403111241.AA02582@xdm039.ccc.cranfield.ac.uk>
Date: Fri, 11 Mar 1994 13:54:40 --100
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Authentication *before* GET?

Apart from the current "can Kerberos be spoofed" thread both on and off 
www-talk, I notice from "WWW Access Authorization protocol examples" that 
Authentication info is exchanged only *after* a GET request has resulted in an 
Authorisation header.

>  When a browser gets an Unauthorized status code or otherwise figures
>  authorization is needed, it sends authorization information along with the
>   next request to the server. 
>
>
>          GET whatever HTRQ/V1.0
>          Authorization: Basic KDENfKdifwekFD23nf==
>          ...

Michael Shiplett noted that the first part of the exchange (GET ...) is in 
clear; while I don't that this causes a problem for spoofing, it does of 
course mean that that anyone can read "whatever". If instead of "GET whatever" 
the method is "PUT <credit card number>", it's pointless 
authenticating/encrypting afterwards. I would like HTTP to be able to 
authenticate with Kerberos right from the start; client and server can use 
encrypted comms based on the Kerberos session key which they both now know.

OK, this begs the question - how does the browser know that Kerberos should be 
used for this request? Kerberos is intended to be the first exchange between 
client and server over an untrusted link. In a solely Kerberised environment, 
this is no problem. Where several authentication protocols (of which the most 
basic is "none") are mixed, browsers which know only a URL talking to an 
untrusted server need to know whether they should demand Kerberos 
authentication to prove the server's identity. Obviously, this extends to the 
question "of the several authentication mechanisms I know, which is 
appropriate?".

So may I throw the following idea into the air and stand well back while 
everyone else savages it.

When a browser makes a request - especially in the case of a fill-out form - 
the first action is not GET/PUT, etc but something like

[SERVER sez]  Can-authenticate: KerberosIV,None
[CLIENT sez]  WWW-Authenticate: KerberosIV <client's ticket for server>
[SERVER sez]  WWW-Authenticate: KerberosIV <servers mutual auth reply>
              Key-info: KerberosIV-session-key
[CLIENT and SERVER then perform a normal HTTP exchange via an encrypted 
channel]

The point is that the browser establishes that it can trust the server 
immediately. As soon as it knows that it can trust the server, it displays a 
big green tick "Yes, the documents you're reading are genuine" (along with a 
note on which authentication mechanism was used). If it can't, a big red 
cross; you *can't trust* this. Similarly, when a form is submitted, the client 
will say "There is no authentication, do you really want to submit this?". And 
similarly, A client should be able to force encryption (under the user's 
control), and prompt for confirmation if this can't be done. On the server 
side, files and script can use ACLs to control whether only authenticated 
users should access, Access Control Lists to specify limited groups of 
readers, and also an "eyes only" flag where the server will insist on using 
encryption and refuse to send the output where this is not possible.

Zephyrgrams are tagged on a similar basis - the fact that a message is 
unauthenticated doesn't mean it isn't useful, simply that there's no evidence 
that you can trust it. After all, if the parish magazine comes through my 
letterbox, I don't worry that it isn't security printed, as it's highly 
unlikely that anyone wants to forge it; I would be suspicious of a letter 
purporting to be from my bank manager which was obviously churned out on a 
low-quality photocopier, especially if it asked me for personal information.

Where multiple documents are obtained via one call, then the whole call (not 
just each request) can be trusted.  Also, since I believe that HTTP can 
identify that multiple calls are one session, client and server can continue 
to use encrypted communications with that session key, and do not need to 
reauthenticate. Where it is established that the server can use PGP for 
documents, it uses PGP (conceivably, on top of an already Kerberos encrypted 
comms channel!). Where the client doesn't speak Kerberos, it will simply not 
send the WWW-Authenticate: KerberosIV, and everything proceeds exactly as if 
Kerberos doesn't exist.

Kerberos allows a client to trust a server, the *process* that we are talking 
to. As I understand PEM and PGP, they are oriented toward the individual 
*document*, and hence there is no point in authentication processing until 
after the doc has been requested. Lets ensure that HTTP can cope with either 
case.

I would also recommend that "None" is a registered authentication type, which 
can be explicitly left out of a Can-authenticate: list to say to browsers, "I 
will not talk to you without authentication", so that a browser can tell the 
user immediately that they won't get anywhere.




From P.Lister@cranfield.ac.uk  Fri Mar 11 14:16:20 1994 --100
Message-Id: <9403111256.AA02620@xdm039.ccc.cranfield.ac.uk>
Date: Fri, 11 Mar 1994 14:16:20 --100
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Authentication *before* GET?

Sorry, pressed the send button a touch early.

One thing which could be misinterpreted

              Key-info: KerberosIV-session-key

This is intended to convey that the server is telling the client to use the 
Kerberos session key for encryption; *not* that either party should actually 
quote the key over HTTP. This would be very, very silly. Actually, thinking 
about my later comments, I really want a method for preceding each 
request/reply with a header which says that the following text is encrypted 
(or not) and the mechanism used.

Also, while I said that a Can-authenticate header should not default to "None" 
(so that a very secure server can clearly insist on authentication), browsers 
should treat the *absence* of Can-authenticate as "None", to cope with older 
servers.

Flame away. :-)

Peter Lister                             Email: p.lister@cranfield.ac.uk
Computer Centre, Cranfield University    Voice: +44 234 754200 ext 2828
Cranfield, Bedfordshire MK43 0AL UK        Fax: +44 234 750875
--- Go stick your head in a pig.  (R) Sirius Cybernetics Corporation ---




From ellson@hotsand.att.com  Fri Mar 11 14:26:45 1994 --100
Message-Id: <9403111305.AA21357@hotsand.dacsand>
Date: Fri, 11 Mar 1994 14:26:45 --100
From: ellson@hotsand.att.com (ellson@hotsand.att.com)
Subject: Re: last-seen

> From: Bert Bos <bert@let.rug.nl>
> 
>  |> b) is the date in GMT?
>  |
>  |Don't care.
> [...]
>  |The receiving cache should not attempt to understand the
>  |semantics of the "Last-modified" date.  It should be treated as a
>  |tag provided by the server that the server can match against when it
>  |is sent back in the "If-Modified-Since" field.
> 
> But presumably the requesting program only asks for the document if
> the version it has is expired. That means it must have interpreted the
> date & time.

If the document has expired or if the document has no "Expires" header
field.  In either case the "Last modified" date is separate.

> Isn't it better then to let the receiving cache generate
> a valid date & time from its internal representation, instead of
> keeping the original date string around?

Perhaps this is a local implementation issue rather that a protocol
issue?  I agree with Gary Adams that it might be better if the
specification treated the string sent back in the "If-Modified-Since"
field as an "opaque server cookie."  That way, the mechanism for cache
coherency can be independent of time representation.  However, since the
receiving system may use the "Last modified date" for other purposes,
the string isn't always opaque to the receiving system.

John Ellson
AT&T Bell Labs




From fielding@simplon.ICS.UCI.EDU  Fri Mar 11 14:36:22 1994 --100
Message-Id: <9403110531.aa06572@paris.ics.uci.edu>
Date: Fri, 11 Mar 1994 14:36:22 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: If-Modified-Since and 304

Ari's last message was right on the mark regarding the interpretation
and format of the If-Modified-Since: header.  The date must be in HTTP
format (whatever that is) and is always GMT.  Also, the Expires header
is the appropriate place to indicate "don't cache."

John Ellson brought up an interesting point in indicating that the cache
manager could just save the cached document's prior Last-modified "tag"
and pass that back in the If-Modified-Since header.  However, I think that
is an implementation issue for cache managers and should not be considered
part of HTTP.  All that should be required of future servers is that they
be able to parse an HTTP date format and behave according to the spec.

As to the HTTP date format itself, Rik Harris has brought up a topic that
I have talked about before in that the format used by current servers

> Date: Thursday, 10-Mar-94 19:20:00 GMT
> Last-modified: Thursday, 10-Feb-94 22:23:32 GMT

is based on an out-of-date standard (rfc850).  The format should be
changed to this:

  Date: Thu, 10 Mar 1994 19:20:00 GMT
  Last-modified: Thu, 10 Feb 1994 22:23:32 GMT

because it is of fixed length, completely unambiguous for both machines and
humans, and is used by most Internet-based mail and NNTP servers
(with the exception that HTTP only uses GMT for timezone).  However, such
a change will have to be decided upon by the server authors and then placed
in the HTTP2 spec before servers and clients can rely upon it.  For now
(and probably for the next year or so), clients like the one I'm building
will have to be able to parse and understand both formats.

BTW, my server <http://www.ics.uci.edu/> has been using the latter format
since December with no problems -- it is just a one line change in the
NCSA httpd code.  My wwwstat program (written in Perl) also generates that
format and anyone is welcome to use the subroutine in it called wtime.
I'll be working on Perl code to parse both formats next week.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From joe@MIT.EDU  Fri Mar 11 15:11:03 1994 --100
Message-Id: <9403111406.AA18620@theodore-sturgeon.MIT.EDU>
Date: Fri, 11 Mar 1994 15:11:03 --100
From: joe@MIT.EDU (joe@MIT.EDU)
Subject: Management using WWW


I'm trying to use WWW in order to help manage Globewide Network
Academy, (a project to create an online university and the world's
first virtual corporation).  I'd like some public feedback on the web
that I've created.  It's at

http://uu-gna.mit.edu:8001/uu-gna/admin/president/index.html

Also, underneath the link marked "Documents" is a public draft
proposal for holding one session of the upcoming WWW conference
online.  The "memo" right now is extremely sketchy, but I'd like to
use it as an "prototype" for how a group can suggest a project,
develop it into a formal proposal, and then make it happen.  In
particular, I'd like to use hypertext links so that any part of the
memo which is unclear to people first reading it can be made more
clear.




From gvoosten@isosa2.estec.esa.nl  Fri Mar 11 15:33:51 1994 --100
Message-Id: <9403111428.AA05538@isosa2.estec.esa.nl>
Date: Fri, 11 Mar 1994 15:33:51 --100
From: gvoosten@isosa2.estec.esa.nl (Gertjan van Oosten)
Subject: Re: If-Modified-Since and 304

As quoted from Roy T. Fielding:
> I'll be working on Perl code to parse both formats next week.

I already mailed him my perl code to do just that.

Cheers,
-- 
-- Gertjan van Oosten,  West Consulting bv
-- Estec,               gvoosten@isosa1.estec.esa.nl,  +31-1719-85668



From P.Lister@cranfield.ac.uk  Fri Mar 11 14:21:07 1994 --100
Message-Id: <9403111304.AA02681@xdm039.ccc.cranfield.ac.uk>
Date: Fri, 11 Mar 1994 14:21:07 --100
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Authentication and authorisation

Lets also remember that in network terms, Kerberos authenticates a client and 
server to each other, but says nothing about authorisation, i.e. what you do 
or don't do for a person once you know you really are talking to them.

It seems reasonable that a CGI script should be know who it's talking to and 
how they got authenticated. There is nothing to prevent a server performing an 
initial level of access control when it decides whether to the start the 
script (or indeed which script to start), but the script itself be able to 
make it's own decisions, as well as using the info.

Peter Lister                             Email: p.lister@cranfield.ac.uk
Computer Centre, Cranfield University    Voice: +44 234 754200 ext 2828
Cranfield, Bedfordshire MK43 0AL UK        Fax: +44 234 750875
--- Go stick your head in a pig.  (R) Sirius Cybernetics Corporation ---




From brun@ascom.cica.fr  Fri Mar 11 16:27:57 1994 --100
Message-Id: <9403111523.AA18418@ascom.cica.fr>
Date: Fri, 11 Mar 1994 16:27:57 --100
From: brun@ascom.cica.fr (Philippe Brun)
Subject: WWW and SNMP

Hello,
I am developing a gateway between http and snmp. It allows WWW
clients to browse SNMP agents. There is already 
"something" running at http://ascom.cica.fr/asrl.
I would appreciate if you could try it and give me something feed back,
particularly on the way to map SNMP information model (sort of
hierarchy of objects) into hypermedia documents.  

Thanks

Philippe



From yezdi@media.mit.edu  Fri Mar 11 16:51:45 1994 --100
Message-Id: <9403111546.AA25880@bartok.media.mit.edu>
Date: Fri, 11 Mar 1994 16:51:45 --100
From: yezdi@media.mit.edu (Yezdi Lashkari)
Subject: annotations and usenet news

Some time ago I came across a proposal to propagate the 
annotations made by WWW/Mosaic clients via the USENET
mechanism so that potentially everyone would have 
access to these annotations and use them. Could someone
tell me what has happened to this proposal ? 

thanks
Yezdi




From dkulp@gdb.org  Fri Mar 11 17:15:08 1994 --100
Message-Id: <9403111609.AA19449@dev.gdb.org>
Date: Fri, 11 Mar 1994 17:15:08 --100
From: dkulp@gdb.org (David Kulp)
Subject: Re: annotations and usenet news


> From: Yezdi Lashkari <yezdi@media.mit.edu>
> 
> Some time ago I came across a proposal to propagate the 
> annotations made by WWW/Mosaic clients via the USENET
> mechanism so that potentially everyone would have 
> access to these annotations and use them. Could someone
> tell me what has happened to this proposal ? 
> 
> thanks
> Yezdi
> 
> 
I'm very interested in this as well.  There's some old notes on one of the
NCSA servers written by Marc Andreesen which sum up the discussion on how
to revamp the annotations, but the issue seems to have been dropped.

Rob McCool recently mentioned that annotation may be a part of httpd 1.2.
I sure hope so.  We're interested in creating an on-line medical journal
in which the editors could edit by annotation.  Annotation facilities would
be a real bonus.

-David Kulp.



From connolly@hal.com  Fri Mar 11 17:59:29 1994 --100
Message-Id: <9403111648.AA07306@ulua.hal.com>
Date: Fri, 11 Mar 1994 17:59:29 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Whence came 'URC'?


I just saw a blurb on 'Uniform Resource Citations' on the web...

>From "UR* and The Names and Addresses of WWW objects",
aka <http://info.cern.ch/hypertext/WWW/Addressing/Addressing.html>:

URC: Uniform Resource Citation. A set of attribute/value pairs
describing an object. Some of the values may be URIs of various kinds.
Others may include, for example, athorship, publisher, datatype, date,
copyright status and shoe size. Not normally discussed as a short
string, but a set of fields and values with some defined free
formatting.

[I'd also suggest: MD5 signature, Message-ID, etc.]

What discussion forum did this come from?

I think this is the critical facility in all this Open Hypermedia
stuff... URI's are a key part, but they're mostly just distributed
filesystem techniques. URC's are the key to reliable, links that
degrade gracefully.

Uniform Citations (I called them "references" in past postings)
are the key... This is probably where HyTime ideas will be most
applicable and valuable. I certainly hope URC's will be expressible in
HyTime (even if they are usually expressed in some more convenent
syntax...)

Dan



From M.T.Hamilton@lut.ac.uk  Fri Mar 11 18:35:27 1994 --100
Message-Id: <199403111727.RAA19982@lust.mrrl.lut.ac.uk>
Date: Fri, 11 Mar 1994 18:35:27 --100
From: M.T.Hamilton@lut.ac.uk (Martin Hamilton)
Subject: Re: Whence came 'URC'?

Hi Dan, you said:

$ URC: Uniform Resource Citation. A set of attribute/value pairs
$ describing an object. Some of the values may be URIs of various kinds.
$ Others may include, for example, athorship, publisher, datatype, date,
$ copyright status and shoe size. Not normally discussed as a short
$ string, but a set of fields and values with some defined free
$ formatting.
$ 
$ [I'd also suggest: MD5 signature, Message-ID, etc.]
$ 
$ What discussion forum did this come from?

The IETF URI working group - anyone who's interested should
probably check out the archive at

  http://www.acl.lanl.gov/URI/archive/uri-archive.index.html

and

  http://www.gatech.edu/urm.paper

Send mail to uri-request@bunyip.com...

$ I think this is the critical facility in all this Open Hypermedia
$ stuff... URI's are a key part, but they're mostly just distributed
$ filesystem techniques. URC's are the key to reliable, links that
$ degrade gracefully.
$ 
$ Uniform Citations (I called them "references" in past postings)
$ are the key... This is probably where HyTime ideas will be most
$ applicable and valuable. I certainly hope URC's will be expressible in
$ HyTime (even if they are usually expressed in some more convenent
$ syntax...)

Care to elaborate ? :-)

Cheers!

Martin




From joe@MIT.EDU  Sun Mar 13 03:52:58 1994 --100
Message-Id: <9403130242.AA26378@theodore-sturgeon.MIT.EDU>
Date: Sun, 13 Mar 1994 03:52:58 --100
From: joe@MIT.EDU (Joseph Wang)
Subject: Announce tkWWW 0.11 Prerelease 1


A prerelease for tkWWW 0.11 has been uploaded to info.cern.ch from
which it should migrate over to /pub/www/dev soon.  This is a bug fix
version with the following changes.


12 Mar 1994 Fails gracefully if it cannot find xli
            Add image works correctly
            Put prerelease 1 on info.cern.ch

05 Mar 1994 Set variables to record link attributes
            Changed image.tcl to avoid freeze if image viewer cannot be found
            Fixed bug in Personal annotation deletion (reported by
                Marc_Spreizter.Parc@Xerox.com)
            Added scrollbar to whine dialog
            Added lines so that make install installs the proper Tcl files
                in the library directory
            Now works with RCS
 
06 Feb 1994 Added patches by markd@grizzy.com
               Fixes various segementation faults
               Uses sed instead of cpp to create tkWWW

02 Feb 1994 Fixed "Back" and "Home" bug which gave blank pages for those 
               buttons (reported by ellson@hotsand.att.com,
                                    dl@hplyot.obspm.circe.fr and others)
            Fixed edit.tcl so that Ctrl-a and Ctrl-e work 
                        (boning@mtl.mit.edu)

Have fun!!!!!!





From earhart+@CMU.EDU  Tue Mar 15 13:59:45 1994 --100
Message-Id: <chV=fBu00WC71dboEX@andrew.cmu.edu>
Date: Tue, 15 Mar 1994 13:59:45 --100
From: earhart+@CMU.EDU (Rob Earhart)
Subject: Giving up, need help :-)

  Okay... so can someone with a couple seconds of free time tell me
what's wrong with the output coming from
"http://www.contrib.andrew.cmu.edu:8002/" ?  (An experimental server)

  To my eyes, and to a telnet hexdump debug, it's exactly the same as
what's coming from the server process running on port 8001, but neither
Mosaic nor www seem to like it.  They're getting the stuff off 8001 just
fine.

  (yeah, I know it should be probably be using network newlines in the
headers, and using <head> and <body> tags... it's still coming along,
and still experimental.  And the datastream works fine as-is on the
first port...)

  Thanks,
  )Rob



From dmk@allegra.att.com  Tue Mar 15 14:07:20 1994 --100
Message-Id: <9403142001.AB10951@dxmint.cern.ch>
Date: Tue, 15 Mar 1994 14:07:20 --100
From: dmk@allegra.att.com (Dave Kristol)
Subject: http hooks for payment for services?

Please forgive my ignorance if this has been discussed a lot (and
please direct me to an archive or FAQ for same)....

Are there provisions in HTTP to support fees for documents?  I'm
looking for a mechanism that works something like this:

1) Client requests document.
2) Server rejects, requesting payment.  Server informs client of cost.
Server expects some kind of (say) certificate that proves the client
has deposited money in the server's "bank account" somewhere or that
represents some kind of "digital cash".  (I'm using the term VERY
loosely.)
3) If user hasn't done so already (and perhaps even if s/he has),
client requests payment authorization from user (in some unspecified
form).
4) With payment in hand, client re-requests document from server.

Thanks for any pointers.
Dave Kristol



From connolly@hal.com  Tue Mar 15 14:13:50 1994 --100
Message-Id: <9403142053.AA07867@ulua.hal.com>
Date: Tue, 15 Mar 1994 14:13:50 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Testing URIs for equality

------- =_aaaaaaaaaa0
Content-Type: text/plain; charset="us-ascii"
Content-ID: <7861.763677456.1@ulua>

I'm new to this discussion, so I may be missing a few issues, but I
read through the nifty hypertext archive of the list, and I'd like to
make some suggestions that may resolve several outstanding lexical
issues one way or another:

It seems to me that there's a lot of discussion about the syntax of
URI's without regard to what the syntax represents. In order to
resolve the question of what a URI in string form represents, let's
consider the question:

	How does one decide if two URI's are "equal"?

I suggest that any specification which leaves this question unanswered
or unanswerable in any cases is sorely lacking, as many caching
strategies will need to be able to ask "have I been here before?"
independent of the scheme involved.

I suggest that the algorithm for comparing URI's is simple string
comparison [after reducing it to canonical form]. I believe it is
already fairly widely deployed. As we'll see, this leaves no room for
different implementations to use different quoting mechanisms, but it
does allow for different strategies for reducing a data stream to
canonical URI form (for example, removing all whitespace and strip <
and > in mail messages).

Question 1: What's a scheme? The grammar in
<http://info.cern.ch/hypertext/WWW/Addressing/URL/5_BNF.html> is
highly ambiguous. One can trace
	scheme => ialpha => xalphas => xalpha => extra => ':'
so that a scheme can have a ':' character in it. That seems pretty
silly, but in practice it's not much of an issue. Most implementations
disambiguate the same way. But, one can also
trace:
	scheme => ialpha => xalphas => xalpha => escape

So are the following equal?

	x-my-scheme:abc
	x-my-schem%65:abc

[my answer: no. The latter is illegal]

There's also the question of whether

	ftp://ftp.host.com/foo.txt
and
	FTP://ftp.host.com/foo.txt
are equal, but I think that's in the spec somewhere

[my answer: no, they're not equal, though I'm willing to bend on this]


Question 2: Is the "opaque string" limited to a certain set of
characters, or are all 256 octets available? If the character set
is limited, how does one represent octets outside that set?

For example, suppose I want to compose a URI consisting of the
string "<%a/b/c%>" and the scheme "x-my-scheme". Can I write:

	x-my-scheme:<%a/b/c%>

I suggest that if we allow that, we'd break a lot of existing
implementations, and that's a Bad Thing To Do. So perhaps I write:

	x-my-scheme:%3C%25a%2Fb%2Fc%3E

Hmmm... is that equal to the following?

	x-my-scheme:%3C%25%61%2F%62%2F%63%3E

If the answer is yes, then we have violated the premise that
everything after the first ':' is opaque. (which I will do again
below anyway, but for now...)

If the answer is no, then implementations will have to be very careful
to do only "minimal" quoting; i.e. there will have to be at least a
per-scheme agreement on the canonical representation of the opaque
string.

[my answer: no. The latter is illegal, or at least not in canonical form.]

I suggest that (a) the character set is limited, but (b) we agree on a
common quoting mechanism, carefully specified so that it is not
necessary to "unquote" a pair of URI's for comparison. It goes like
this:

We partition the 256 octets into three classes:
	(1) markup characters (exactly the 6 characters ":/@?#%").
	These _must_ be quoted ala "%3A%2F%40%3F%23%25"
	(2) reserved characters, e.g. <, >, and space because historical
	implementations can't support them, { and } because they
	don't survive email transport, octets 0-31 and 127-255 because
	they're not "printable", etc.
	These _must_ be quoted.
	(3) data characters, i.e. whatever's left.
	(e.g. letters, digits, -, _ ., perhaps others. We need to
	specify this set carefully)
	These _must not_ be quoted.

This quoting scheme maps artibrary octet strings into strings over the 
set of data characters plus % in an invertible fashion; i.e. if
	strcmp(quote(s1), quote(s2)) == 0
then we can conclude that
	strcmp(s1, s2) == 0
without going to the trouble to unquote s1 and s2.

For backwards compatibility, some systems may want to check for
canonical form before comparing URI's. For example, a routine might
take:

	URI_compare("  <http://host.com  /%61%62%63>",
		"HTTP://host.com/abc")
and reduce them both to
	"http://host.com/abc" before passing them to strcmp().
but that routine could _not_ change:
	http://host.com/abc%2Fdef
to
	http://host.com/abc/def
before passing it to strcmp() -- markup and reserved characters
_remain quoted_ in canonical form. Otherwise there is no way to
represent those characters opaquely.


Question 3: Is there a well-defined way to determine the host, port,
username, password, path, search string, and fragment-identifier, etc.
from a URI in a scheme-independent manner? In other words, can you
learn _anything_ about a URI if you don't recognize the scheme?

Again, using the premise that contradicting widely-deployed
implementations is Bad, the answer is yes, even though this violates
the "everything after the : is opaque" rule.

Using the following grammar and the above quoting rules, we can achive
both goals of being able to parse the stuff after the : and being able
to encode arbitrary data in an opaque fashion: note that a WORD can
represent _any_ sequence of octets in a way that's opaque to this
grammar. On the other hand, if you want to take advantage of the
hierarchical namespace properties of URL's, you would, e.g. use the
'/' character unescaped.

The purpose of a grammar, to me, is to give implementors an exact,
unambiguous description of the syntax. The grammar in the current spec
is so ambiguous that it does not accomplish this goal. I suggest the
following replacement. I've used yacc syntax rather than the
"BNF-like" syntax so that I can use the machine to check for
ambiguity.

[The resulting grammar is LR(1) with the exception of the
user/password production -- you need to look ahead about three tokens
before you see the '@' that disambiguates. So you can't build a URL
parser that groks user:password@host.com with yacc. Take the line:
      | '//' user passwdOpt '@' host portOpt 
out, and it becomes completely LR(1).]

I've also introduced a lexical token WORD that separates the quoting
issues from the parsing issues. So the terminals of the grammar are:

	':' '@' '//' '/' '?' '#' and WORD


------- =_aaaaaaaaaa0
Content-Type: text/plain; charset="us-ascii"
Content-ID: <7861.763677456.2@ulua>

/* YACC spefication of URI grammar */
/* $Id: url-formal.y,v 1.1 1994/03/14 17:17:33 connolly Exp connolly $ */

%token WORD /* string of data characters and escape squences, i.e.
		DATA = [^:@/?#%]
		HEX = [0-9A-F]
		WORD = ({DATA}|%{HEX}{HEX})+ */ ;

%%
uri : scheme ':' hostpartOpt abspath suffix
	;

hostpartOpt : /* null */
	| '//' user passwdOpt '@' host portOpt
	| '//' host portOpt
	;

passwdOpt : /* null */
	| ':' password
	;

portOpt : /* null */
	| ':' port
	;

abspath	: '/' path
	;

path : WORD '/' path
	| WORD '/'
	| WORD
	;

suffix : '#' fragmentid
	| '?' search
	;

scheme : WORD ;
user : WORD ;
password : WORD ;
host : WORD ;
port : WORD ;
fragmentid : WORD ;
search : WORD ;


------- =_aaaaaaaaaa0
Content-Type: text/plain; charset="us-ascii"
Content-ID: <7861.763677456.3@ulua>

We should perhaps introduce '+' as a markup character so that clients
could parse the individual search words from the following:

	x-my-scheme://host/database?word1+word2


Consider the follwing applications of the above suggestions:

A Macintosh filename 'Vol X:dir1:dir two:xyz.html' available via FTP
could be represented as:

	ftp://mac.host.name/Vol%20X%3Adir1%3Adir%20two%3Axyz.html

or as:

	ftp://mac.host.name/Vol%20X/dir1/dir%20two/xyz.html

A client won't be able to tell that these point to the same file. It
should interpret the first as:

	ftp mac.host.com
	ftp> get "Vol X:dir1:dir two:xyz.html"	[not sure about quoting in ftp]

and the second as:

	ftp mac.host.com
	ftp> cd "Vol X"
	ftp> cd dir1
	ftp> cd "dir 2"
	ftp> get xyz.html

Relative links in the xyz.html file probably won't work if the path is
encoded the first way.

These options are also available for VMS files, WAIS doc-ids, etc.


The point here is to agree on some semantics for URIs that matches
current practice without infringing on future mechanisms. The grammar
I've given is arbirary in some ways: it specifies a scheme-independent
way to parse the scheme, user, password, host, path, search string,
and fragment identifier, but leaves other parsing to scheme-specific
parsing of WORDs (e.g. gopher type, wais size, type, etc.). But it is
unambiguous and workable.

Plus, it leaves some room for extension. For example, the string:

	x-my-scheme://host/keyword:value/keyword:value/keyword:value

is illegal by this grammar. This means that in the future we can
safely extend the grammar to include this syntax with the knowledge
that we will not be changing the meaning of any extant URLs.


Comments?

If this proposal receives a somewhat favorable response, I think
the next thing to do is to augment the above yacc specification
with enough supporting materials to build a reference implementation
of a URI parser and begin to compile a suite of test cases so
that implementors can validate their work. I volunteer to compile
the test suite.


Dan


p.s. What was the motivation for abbreviating Message-Id: down to
mid:? I vote we just use message-id: and content-id: as the scheme
names.

p.p.s. The ~ character is used quite a bit these days in urls, e.g.

	http://host/~user/index.html

Is this legal? The grammar has an unused production for "national"
that mentions the ~ character, as if it's not allowed in URI's.


------- =_aaaaaaaaaa0--



From connolly@hal.com  Tue Mar 15 14:20:32 1994 --100
Message-Id: <9403142350.AA08999@ulua.hal.com>
Date: Tue, 15 Mar 1994 14:20:32 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: ANNOUNCE: draft test suite for URIs

------- =_aaaaaaaaaa0
Content-Type: text/x-html; charset="us-ascii"
Content-ID: <8987.763688890.1@ulua>


<H1>Test Suite for URI's (alpha quality)</H1>

In an effort to flush out the remaining issues in the URI
specification, I'm developing a suite of test cases to
demonstrate various cases and issues. <P>

I've also written two test drivers: one that just calls
HTParse() and prints out the results, and
another based on the yacc-style grammar that I posted
Mon, 14 Mar 1994 14:53:04 -0600 as conten-id 7861.763677456.2@ulua.
<P>

<A HREF="http://www.hal.com/~connolly/dist/url_test-19940315.tar.Z">
URI Test Suite</A>


------- =_aaaaaaaaaa0
Content-Type: message/external-body; access-type="x-http";
	site="www.hal.com"; path="/~connolly/dist/url_test-19940315.tar.Z"

Content-Type: application/octet-stream; type="tar";
	x-conversions="compress"
Content-ID: <8987.763688890.2@ulua>

------- =_aaaaaaaaaa0--



From dsr@hplb.hpl.hp.com  Tue Mar 15 15:48:16 1994 --100
Message-Id: <9403151415.AA08458@dragget.hpl.hp.com>
Date: Tue, 15 Mar 1994 15:48:16 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: http hooks for payment for services?

> Are there provisions in HTTP to support fees for documents? 

Yes.

Error code 402 Payment Required is sent by the server when
appropriate. The header fields associated with this response
indicate acceptable charging schemes. The client can then
retry with a suitable ChargeTo header. This needs to be
supplemented with a suitable authorization scheme.

I am very interested in adapting PEM to suit WWW/HTTP needs
for authentication, privacy (e.g. for credit card details)
and billing. We also should provide means for non-repudiation
of placement and acceptance of orders. RSA have told me that
they are keen to see public key techniques adopted by the
World Wide Web and will waive license fees for non-commercial
i.e. freeware implementations in WWW browsers and servers.

The US export restrictions are not a barrier here, since
source code for public key encryption/decryption is freely
available outside the US at a number of ftp sites. The export
restrictions do prevent you from placing secure browsers on
anonymous ftp sites in the US. This can be circumvented by
using non-US ftp sites for the whole browser or just the
publc key library. There is no law against importing public
key cryptosystems into the US (yet)!

The following is quoted from the EFF:

 The US government plans to proceed on every front to make the
 Clipper Chip encryption scheme a national standard, and to
 discourage the development and sale of alternative powerful
 encryption technologies. If the government succeeds in this
 effort, the resulting blow to individual freedom and privacy
 could be immeasurable.

Lets use the web to defeat Clipper and in the process build a
thriving global market for electronic commerce!

Dave Raggett



From bjoerns@stud.cs.uit.no  Tue Mar 15 15:56:34 1994 --100
Message-Id: <199403151446.PAA02907@tklab3.cs.uit.no>
Date: Tue, 15 Mar 1994 15:56:34 --100
From: bjoerns@stud.cs.uit.no (Bjoern Stabell)
Subject: The future of meta-indices/libraries?

Hi,

The main problem for many users of WWW today is that they cannot
locate the information they are searching for.

Solutions to these problems are meta-indices/libraries which
either manually or automatically collect (meta) information from
information (ftp, www, gopher, wais) sites around the world.

The problem is, these meta-libraries require much work to be kept
up-to-date (and so, they usually aren't very up-to-date at all)
and there are so many of them; resulting in most meta-libraries
keeping a list of other meta-libraries.

Where is this all going?  More and more meta-libraries result in
more and more replication of work and also less quality of the
different meta-libraries.  Today, it seems most sites have their
own (small and incomplete) meta-libraries, which should also be
unneccessary.  Will there be a _the_ meta-library sometime in the
future which are mirrored perhaps in several places?  Or some
other way to locate information?

What is the best solution in the meantime?  (I've thought about
giving the GNA Meta-Library and the W3 library at CERN all the
information in our local 'meta-library' so that they at least
will be a superset of ours.  Then, mirroring/caching the
browsable library-pages locally - the database/searchable index
part cannot be replicated very easily.  If it is agreed upon here
that this sounds like a decent solution, then maybe we should
urge all webmasters around the world to do the same?)


Bye,
-- 
Bjoern Stabell
(bjoerns@staff.cs.uit.no)



From neuss@igd.fhg.de  Tue Mar 15 17:06:34 1994 --100
Message-Id: <9403151601.AA09382@wildturkey.igd.fhg.de>
Date: Tue, 15 Mar 1994 17:06:34 --100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: Re: The future of meta-indices/libraries?

Hiyall,

Bjoern Stabell (bjoerns@staff.cs.uit.no) writes:
> The main problem for many users of WWW today is that they cannot
> locate the information they are searching for.
[munch]

> The problem is, these meta-libraries require much work to be kept
> up-to-date (and so, they usually aren't very up-to-date at all)
> and there are so many of them; resulting in most meta-libraries
> keeping a list of other meta-libraries.

True 'nuff.. what we need is an archie like mechanism which allows
for doing world wide searches on a specific topic. We are currently
working on a CGI interface which allows doing conceptual searches
on WWW archives. If you use a spider program to walk the web, or an
mechanism close to archie, you could do a _world wide_
  "show me all documents dealing with foobar"
and get back a clickable list of world wide URLs!

That'd be kinda cool, wouldn't it.. ;-)

Chris
---
"I ride a tandem with the random.."
Christian Neuss   # Fraunhofer Institute for Computer Graphics
Wilhelminenstr.7  #  64283 Darmstadt # Germany
e-mail: neuss@igd.fhg.de  finger: neuss@wildturkey.igd.fhg.de
 

Chris



From earhart+@CMU.EDU  Tue Mar 15 18:58:19 1994 --100
Message-Id: <QhVTLIi00WC7RdbxoD@andrew.cmu.edu>
Date: Tue, 15 Mar 1994 18:58:19 --100
From: earhart+@CMU.EDU (Rob Earhart)
Subject: Re: Giving up, need help :-)

  Ok, it seems that I wasn't waiting for the entire request to come in
before replying; sigh (I wasn't looking at the headers because it was still
an experiment under development).

  My thanks to the several people who took a few moments of their time
to send me suggestions; I doubt I'd have found it anytime soon.

  )Rob



From peterd@bunyip.com  Tue Mar 15 19:54:10 1994 --100
Message-Id: <9403151831.AA11794@expresso.bunyip.com>
Date: Tue, 15 Mar 1994 19:54:10 --100
From: peterd@bunyip.com (Peter Deutsch)
Subject: Re: The future of meta-indices/libraries?

Hi all,

[ You wrote: ]

> Hiyall,
> 
> Bjoern Stabell (bjoerns@staff.cs.uit.no) writes:
> > The main problem for many users of WWW today is that they cannot
> > locate the information they are searching for.
> [munch]
> 
> > The problem is, these meta-libraries require much work to be kept
> > up-to-date (and so, they usually aren't very up-to-date at all)
> > and there are so many of them; resulting in most meta-libraries
> > keeping a list of other meta-libraries.
> 
> True 'nuff.. what we need is an archie like mechanism which allows
> for doing world wide searches on a specific topic. We are currently
> working on a CGI interface which allows doing conceptual searches
> on WWW archives. If you use a spider program to walk the web, or an
> mechanism close to archie, you could do a _world wide_
>   "show me all documents dealing with foobar"
> and get back a clickable list of world wide URLs!

Actually, we do plan to add this capability to the archie
server system in the very near future.  For WWW there's the
obvious problem of what to index, since there is no real
useful meta-info in the URL itself (how many copies of
"default.html" are there, anyways? :-) so at this point
we'd be happy to be told what to collect and serve.

We had planned to come to the community round about the
end of this month to start the discussion as to what would
be your preferences.  At our end we need to produce only a
simple data-gathering script (modelled on what we already
have for archie's anonFTP) and a parser to allow us to
check that the collected info is valid. The rest will
using the existing archie code for access, data sharing,
database management and so on.

FYI, we now have a WAIS index search engine internal to
the system as well, so we can index and serve template
oriented info as well. We're planning to use this to
gather and serve IAFA templates (among other things) and
we can use this for WWW, if the info available requires
it.  If the WWW community can agree on a template
structure for documents this may be the best way to go.

FYI, we've already extended the system to index Gopher and
are testing the new collection now at a pilot site
provided by NEARNET in Boston (sorry, no general
availability yet, although it's not far away). This test
collection now indexes several hundred sites and is being
added to on a daily basis. Also FYI, the current gopher
index collection is tentatively called "gophind". Although
my partner despises the name, the rest of us here at Bunyip
Central think it's kinda cute. You'll know who won that
battle when we announce it to the entire net!  :-)

As part of this development we've added a direct gopher
frontend onto the information, allowing you to choose
either the anonFTP or gophind collection through
gopher menus. We use the same internal database engine so
you have all the same search choices for gopher menus you
have for archie queries. We also have a WWW frontend
operating, although we're not sure that can be added as a
free upgrade at this point and are looking at the best way
to make this available. We'll keep you posted.

We hope to make the gopher index a part of the next
release currently scheduled for mid-April or so. It will
be offered to all existing archie sites as an additional
collection and they'll each choose individually whether to
offer this as part of their service. We then plan to start
serious work on the WWW index. Feedback and ideas are most
welcome and should be sent to "archie-group@bunyip.com".
Of course, we're also more than happy to see this
discussed on www-talk, as that's where the user community
for this service is to be found. 

> 
> That'd be kinda cool, wouldn't it.. ;-)

I hope so. The only question is what's the best stuff to
gather and index for a first pass. For that we need to
hear from the community, keeping in mind the tradeoff
between disk space and info desired. Can you all define a
simple template (or perhaps use one of the IAFA ones)? Is
the HEAD info enough? Once we know that the rest should be
fairly easy.

					- peterd


-- 
------------------------------------------------------------------------------
  My proposal for funding the Internet is pretty simple. I vote we institute
  an "Information Superhighway" tax, the proceeds of which will be used to
  fund network infrastructure. The way this would work is simple - every time
  someone uses the words "Information Superhighway" or any of its derivatives
  we strike them with a sharp object and make them pay a $10 fee (of course,
  the sharp object is not actually needed to make this scheme work, it's just
  in there because it seems an appropriate thing to do...)
------------------------------------------------------------------------------



From connolly@hal.com  Tue Mar 15 20:32:52 1994 --100
Message-Id: <9403151920.AA09263@ulua.hal.com>
Date: Tue, 15 Mar 1994 20:32:52 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: ANNOUNCE: draft test suite for URIs 


[Srinivas: Sorry to send this to a larger audience without asking, but
it seems relavent to the whole list...]

In message <9403151913.AA13909@ig1.att.att.com>, Srinivas R Sataluri +1 908 949
 7782 writes:
>	Subject: ANNOUNCE: draft test suite for URIs
>	From: "Daniel W. Connolly" <connolly@hal.com>
>	Status: O
>	
>	<A HREF="http://www.hal.com/~connolly/dist/url_test-19940315.tar.Z">
>	URI Test Suite</A>
>Dan:
>
>How am I supposed to use the above URL? I cut and paste into my mosaic
>client and it returned nothing. I then replaced http with ftp and it
>still did not work. Just wondering? Thanks.

Hmmm... I guess the www.hal.com server is serving the compressed tar
file like it was HTML.

Try the "load to local disk" option on Mosaic... I'm not sure what
do with other clients.

Dan




From neuss@igd.fhg.de  Tue Mar 15 20:39:32 1994 --100
Message-Id: <9403151928.AA11985@wildturkey.igd.fhg.de>
Date: Tue, 15 Mar 1994 20:39:32 --100
From: neuss@igd.fhg.de (neuss@igd.fhg.de)
Subject: Re: The future of meta-indices/libraries?

Hi the Web,

Peter Deutsch <peterd@bunyip.com> wrote:
> Actually, we do plan to add this capability to the archie
> server system in the very near future.  For WWW there's the
> obvious problem of what to index, since there is no real
> useful meta-info in the URL itself (how many copies of
> "default.html" are there, anyways? :-) so at this point
> we'd be happy to be told what to collect and serve.

The title comes to mind.. but something with a bit more meaning
would be better. Well, how about using the group statement to mark
up special semantics of HTML text? You could have

   <GROUP role="keywords">
   <H2> Keywords: </H2>
   compression, encryption
   </GROUP>

in the header, or at multiple positions in the text: 


  Now let us talk about <GROUP role="keywords">encoding</GROUP>.

In my humble opinion, extracting these groups is something that should
be done on the server side, e.g. via a CGI script. All the archie software
has to do is call up that script. I'm working on this anyway, it could be
included with the server distribution. Also, please remember that names
can be remapped to other locations then their physical paths - file tree
walking will not do.

Have a good one,
Chris
---
"I ride a tandem with the random.."
Christian Neuss   # Fraunhofer Institute for Computer Graphics
Wilhelminenstr.7  #  64283 Darmstadt # Germany
e-mail: neuss@igd.fhg.de  finger: neuss@wildturkey.igd.fhg.de




From altis@ibeam.jf.intel.com  Tue Mar 15 20:50:01 1994 --100
Message-Id: <m0pgezW-00042vC@ibeam.intel.com>
Date: Tue, 15 Mar 1994 20:50:01 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: The future of meta-indices/libraries?

At  7:54 PM 3/15/94 +0000, Peter Deutsch wrote:
>Actually, we do plan to add this capability to the archie
>server system in the very near future.  For WWW there's the
>obvious problem of what to index, since there is no real
>useful meta-info in the URL itself (how many copies of
>"default.html" are there, anyways? :-) so at this point
>we'd be happy to be told what to collect and serve.

We already have some tools that will build a list of document titles and
associated URLS so you get something that looks like (<tab> is the ASCII
tab character):
What's New With NCSA
Mosaic<tab>http://www.ncsa.uiuc.edu/SDG/Software/Mosaic/Docs/whats-new.html

We can search on a list like this (just the titles or titles and URLs) on
our local server which has turned out to be quite simple and fast. Simply
extending this list to include <A HREF> items makes it extremely easy to
find items on your *local* server or local links to documents on other
servers as long as the link name isn't something like "here." For local
servers this can be extended to include <A NAME> tags. Document authors can
include <A NAME> tags to go along with H1-H6 headers to increase hits.

Following existing practices with FTP, Gopher, etc. these lists can be made
available at the root of a Web server with a special file name. It is not
necessary then for a Web robot to do anything besides pick up the one file.
It would probably be useful to separate local links to local files and
local titles from local links to other servers, but a filter program could
do that along with breaking out ftp, telnet, gopher URLs. Meta-indexes
would have to merge these lists to reduce duplication of course.

I would like to be able to see other meta-information included in these
lists such as document author <LINK REV="made"
HREF="mailto:altis@ibeam.intel.com">, language, whether the document
requires authentication, etc. This additional meta-information would
require the file to include the same information returned by the HTTP
server itself.

Much of the above text may be obvious, but I haven't seen it said on this
list, so comment away.

ka





From john@math.nwu.edu  Tue Mar 15 20:56:16 1994 --100
Message-Id: <9403151942.AA13288@hopf.math.nwu.edu>
Date: Tue, 15 Mar 1994 20:56:16 --100
From: john@math.nwu.edu (John Franks)
Subject: Re: The future of meta-indices/libraries?

According to Peter Deutsch:
> 
> Actually, we do plan to add this capability to the archie
> server system in the very near future.  For WWW there's the
> obvious problem of what to index, since there is no real
> useful meta-info in the URL itself (how many copies of
> "default.html" are there, anyways? :-) so at this point
> we'd be happy to be told what to collect and serve.
> 
..
> 
> FYI, we now have a WAIS index search engine internal to
> the system as well, so we can index and serve template
> oriented info as well. We're planning to use this to
> gather and serve IAFA templates (among other things) and
> we can use this for WWW, if the info available requires
> it.  If the WWW community can agree on a template
> structure for documents this may be the best way to go.
> 
..

> I hope so. The only question is what's the best stuff to
> gather and index for a first pass. For that we need to
> hear from the community, keeping in mind the tradeoff
> between disk space and info desired. Can you all define a
> simple template (or perhaps use one of the IAFA ones)? Is
> the HEAD info enough? Once we know that the rest should be
> fairly easy.
> 

I think the WWW community should have addressed this long ago.  This
is the main area in which we are well behind the gopher community.

In my opinion, one of the most important design criteria should be to
eliminate the need for indexers (of whom there will likely be many) to
walk the entire server tree.  This can be annoying and it the worst
cases disruptive.

A second important criterion would be giving the maintainer control
over what is indexed.

I would argue for a very simple document to be provided to indexers
(or created on the fly for them).  It should contain the following:

1. A creation date and optional expiration date
2. A list of the titles of all documents on the server paired with
   the corresponding URL.
3. An optional short list of keywords relevant to this server.

Perhaps there are other things which would be useful, but it is
primarily the titles one wants to index and it is a good idea to keep
it simple.  I think this should be provided with a standard URL for a
document of type text/plain.  There is no need to get mired down in
changes/additions to any protocols.

As a server writer I would implement this by having my server create
this document on the fly when it is first requested and then cache
it for later use until it expires.  Subsequent requests would get
the cached version until its expiration after which a new version 
would be created and cached.  The maintainer would set the expiration
period and could mark any part (or all) of his tree as not to be 
indexed.  The cached file would be extremely useful for features local
to the server also.  For example, a search of all titles on the server
or WAIS searches which return a menu of *titles* of hits (this is done
now by WWWWais, for example, but it must search each document corresponding
to a hit to extract its title).  Of course, other implementations might
work as well.


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu




From john@math.nwu.edu  Tue Mar 15 21:16:34 1994 --100
Message-Id: <9403152011.AA13352@hopf.math.nwu.edu>
Date: Tue, 15 Mar 1994 21:16:34 --100
From: john@math.nwu.edu (John Franks)
Subject: Re: The future of meta-indices/libraries?

According to Kevin Altis:
> 
> We can search on a list like this (just the titles or titles and URLs) on
> our local server which has turned out to be quite simple and fast. Simply
> extending this list to include <A HREF> items makes it extremely easy to
> find items on your *local* server or local links to documents on other
> servers as long as the link name isn't something like "here." For local
> servers this can be extended to include <A NAME> tags. Document authors can
> include <A NAME> tags to go along with H1-H6 headers to increase hits.

You only want to index *titles* and only of documents on the local server.
If I have an HREF on my server to a document on your server that should
not be in an index associated with my server.  Also we don't want to
index HREFs (or worse NAMEs) that are local to my server.  The reason
is they don't have enough information -- many of them would be "click here"
or "check this out".  If the document referenced is local, it should have
a title and be in the list elsewhere in any case.


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu





From m.koster@nexor.co.uk  Tue Mar 15 21:25:57 1994 --100
Message-Id: <9403152013.AA06666@dxmint.cern.ch>
Date: Tue, 15 Mar 1994 21:25:57 --100
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: The future of meta-indices/libraries? 


altis@ibeam.jf.intel.com wrote:

> At  7:54 PM 3/15/94 +0000, Peter Deutsch wrote:
> >Actually, we do plan to add this capability to the archie
> >server system in the very near future.  For WWW there's the
> >obvious problem of what to index, since there is no real
> >useful meta-info in the URL itself (how many copies of
> >"default.html" are there, anyways? :-) so at this point
> >we'd be happy to be told what to collect and serve.
> 
> Following existing practices with FTP, Gopher, etc. these lists can be made
> available at the root of a Web server with a special file name

> Much of the above text may be obvious, but I haven't seen it said on this
> list, so comment away.

I agree it all sounds very sensible :-) Have a look at ALIWEB, it does
exactly what a number of people have suggested in this thread: It's
Archie-Like Indexing in the WEB. It uses an index file format based
on the IAFA templates, and combines it into a searcheable database.

It has been designed, implemented, and tested since September last
year, and has been running happily for months, with currently 
about 50 Web sites participating. Everbody is invited to participate.

I won't go into too much detail here; All the (recently revamped) 
documentation, and implementation is reacheable from

http://web.nexor.co.uk/aliweb/doc/aliweb.html

Do have a look; I agree with you all this is a problem that needs to
be addressed, and that this is a sensible way to go about doing it.

I must apologise in advance for any problems in getting to this 
material: part of the UK Internet between me and the rest of the world
is playing up severely.

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From m.koster@nexor.co.uk  Tue Mar 15 21:34:06 1994 --100
Message-Id: <9403152025.AA07313@dxmint.cern.ch>
Date: Tue, 15 Mar 1994 21:34:06 --100
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: The future of meta-indices/libraries? 


> I think the WWW community should have addressed this long ago.  This
> is the main area in which we are well behind the gopher community.

I think this is one of the examples of the lack of a Working Group.
It is really easy to discuss problems and come up with solutions,
but even if solutions are proven to work there is no mechanism
for standardising it. As a result all the same problems keep arising,
and people keep coming up with the same solutions.

In this case the problem has been addressed by ALIWEB. Have a look at
http://web.nexor.co.uk/aliweb/doc/aliweb.html
 
> In my opinion, one of the most important design criteria should be to
> eliminate the need for indexers (of whom there will likely be many) to
> walk the entire server tree.  This can be annoying and it the worst
> cases disruptive.

I couldn't agree more. This is why I don't welcome the Robot trend,
and hope to help keep an eye of them by gathering information on the
Robot page (http://web.nexor.co.uk/mak/doc/robots/robots.html)

> A second important criterion would be giving the maintainer control
> over what is indexed.

> I would argue for a very simple document ....

ALIWEB does that.

> As a server writer I would implement this by having my server create
> this document on the fly when it is first requested and then cache
> it for later use until it expires.  Subsequent requests would get
> the cached version until its expiration after which a new version 
> would be created and cached.  The maintainer would set the expiration
> period and could mark any part (or all) of his tree as not to be 
> indexed.  The cached file would be extremely useful for features local
> to the server also.  For example, a search of all titles on the server
> or WAIS searches which return a menu of *titles* of hits (this is done
> now by WWWWais, for example, but it must search each document corresponding
> to a hit to extract its title)

I am not sure what you mean here. I'm not sure it is going to be sensible
to index all titles on a server and search those, even though it sounds
attractive. You do need to retain the context of the titles.

You mention marking part of a tree not to be indexed. Although it is
not quite what you mean, you may find it interesting to learn about a
proposal on the Robots page to introduce a voluntary mechanisms to
exclude part of trees by robots. I agree robots are the wrong solution
to the resource discovery problem, but they are going to be around, and
it makes sense to reduce problems they cause.

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From burchard@geom.umn.edu  Tue Mar 15 21:56:04 1994 --100
Message-Id: <9403152051.AA15604@mobius.geom.umn.edu>
Date: Tue, 15 Mar 1994 21:56:04 --100
From: burchard@geom.umn.edu (burchard@geom.umn.edu)
Subject: Re: The future of meta-indices/libraries? 

bjoerns@stud.cs.uit.no (Bjoern Stabell) writes:
> The problem is, these meta-libraries require much work
> to be kept up-to-date (and so, they usually aren't very
> up-to-date at all) and there are so many of them;
> resulting in most meta-libraries keeping a list of other
> meta-libraries. 


Actually, I'm very impressed with the CUI W3 Catalog.  Rather than  
merely listing other meta-libraries and indices, the CUI catalog  
intelligently incorporates the contents of selected low-noise lists  
(including ALIWEB).  I consistently get informative and relevant  
responses from CUI.

To some extent, I think it's good to have a variety of approaches,  
especially when places like CUI can provide a single interface to  
combine the strengths of the various systems.  But I agree that we  
should standardize on some system by which servers can provide  
meta-information, and I'd say we already have a winner with ALIWEB.

--------------------------------------------------------------------
Paul Burchard	<burchard@geom.umn.edu>
``I'm still learning how to count backwards from infinity...''
--------------------------------------------------------------------



From john@math.nwu.edu  Tue Mar 15 22:37:58 1994 --100
Message-Id: <9403152134.AA13443@hopf.math.nwu.edu>
Date: Tue, 15 Mar 1994 22:37:58 --100
From: john@math.nwu.edu (John Franks)
Subject: Re: The future of meta-indices/libraries?

According to Martijn Koster:
> 
> > In my opinion, one of the most important design criteria should be to
> > eliminate the need for indexers (of whom there will likely be many) to
> > walk the entire server tree.  This can be annoying and it the worst
> > cases disruptive.
> 
> I couldn't agree more. This is why I don't welcome the Robot trend,
> and hope to help keep an eye of them by gathering information on the
> Robot page (http://web.nexor.co.uk/mak/doc/robots/robots.html)
> 
> > A second important criterion would be giving the maintainer control
> > over what is indexed.
> 
> > I would argue for a very simple document ....
> 
> ALIWEB does that.
> 

There are many good things about ALIWEB.  However, my impression from
reading the documents referenced above is that the templates must be
human generated.  I am firmly convinced that any scheme which is not
almost completely automated is doomed fail.  Many maintainers will
simply not create the templates and the ones who do will not keep them
up to date.  I have no doubt that a human writing an ALIWEB form will
do a better job than software, but the unfortunate fact is that most
maintainers will simply not make the effort (often they cannot).

> 
> I'm not sure it is going to be sensible
> to index all titles on a server and search those, even though it sounds
> attractive. You do need to retain the context of the titles.
> 

I think this should be the default.  Of course, the maintainer should
be given as much flexibility as possible in eliminating titles from
the index.  Of course retaining the context is desirable, but the time
for doing this is when the document is created, not when it is indexed.

The bottom line choice is between an index of 50 servers with
carefully hand-crafted templates and an index of 5000 servers with
machine generated templates which are less well constructed but up to
date.  I would certainly opt for the later.  I would also do everything
possible to encourage maintainers to massage their templates to improve
them.

John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu






From altis@ibeam.jf.intel.com  Tue Mar 15 22:49:03 1994 --100
Message-Id: <m0pggsJ-00043AC@ibeam.intel.com>
Date: Tue, 15 Mar 1994 22:49:03 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: The future of meta-indices/libraries?

At  9:16 PM 3/15/94 +0000, John Franks wrote:
>According to Kevin Altis:
>>
>> We can search on a list like this (just the titles or titles and URLs) on
>> our local server which has turned out to be quite simple and fast. Simply
>> extending this list to include <A HREF> items makes it extremely easy to
>> find items on your *local* server or local links to documents on other
>> servers as long as the link name isn't something like "here." For local
>> servers this can be extended to include <A NAME> tags. Document authors can
>> include <A NAME> tags to go along with H1-H6 headers to increase hits.
>
>You only want to index *titles* and only of documents on the local server.
>If I have an HREF on my server to a document on your server that should
>not be in an index associated with my server.  Also we don't want to
>index HREFs (or worse NAMEs) that are local to my server.  The reason
>is they don't have enough information -- many of them would be "click here"
>or "check this out".  If the document referenced is local, it should have
>a title and be in the list elsewhere in any case.

Well, I did suggest separating local references versus local documents.
Being able to quickly search your local server is often as important as
searching the whole web and while searching my local server I certainly
want to pick up local references. My main point however is that *just*
indexing TITLEs is not good enough. For one thing, I would like to see
non-HTML documents within the index, so a reference such as <A
HREF="/some_local_dir/install.txt">installation instructions for widget</A>
would be included. Including <A NAME> entries and getting a lot of "click
here" entries is a potential problem, but again those could be filtered. As
authors use a better style, those <A NAME> references will have more value,
especially if they put them with each header reference, major list, etc. If
local references were allowed, then this might also help make up for the
problem of most server administrators not providing indexes of their
servers.

It is also important that no extra work need to be done in order to
generate decent indexes of the Web server material. If the indexes are
going to contain keywords and other meta-information then that should be
stored as part of a normal HTML document or a meta-information file
associated with the document. We are already experimenting with this within
our group at Intel, but I would welcome a standard meta file format,
extension (.meta?) that servers would recognize for Expires: info. among
other things. I think the Aliweb work is headed in the right direction.

ka





From m.koster@nexor.co.uk  Wed Mar 16 00:07:48 1994 --100
Message-Id: <9403152304.AA06025@dxmint.cern.ch>
Date: Wed, 16 Mar 1994 00:07:48 --100
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: The future of meta-indices/libraries? 


John Franks writes:

> There are many good things about ALIWEB.  However, my impression from
> reading the documents referenced above is that the templates must be
> human generated.  I am firmly convinced that any scheme which is not
> almost completely automated is doomed fail.  Many maintainers will
> simply not create the templates and the ones who do will not keep them
> up to date.  I have no doubt that a human writing an ALIWEB form will
> do a better job than software, but the unfortunate fact is that most
> maintainers will simply not make the effort (often they cannot).

There is no reason index files couldn't be produced by automatic
means.  As far as ALIWEB is concerned it is a text file, who/what
creates it is immaterial.

I also don't think that the manual creation of entries in a document
is prohibitive. After all, these maintainers are putting a lot of
effort in publishing material such as HTML pages, gateways and the
like. Why should they shy away from a small text file? There are very
real advantages to them: their information is not just "in the Web",
but is findable in the web. They don't need to maintain their own 
"lists of interesting places".

Actually it turns out it that the fact that most (all?) people write
the index files by hand is more a feature than a problem. Because
people don't want to manually create massive index files they ony
describe the most important services, which results in a database
with little irrelevant material.

> > I'm not sure it is going to be sensible
> > to index all titles on a server and search those, even though it sounds
> > attractive. You do need to retain the context of the titles.
> 
> I think this should be the default.  Of course, the maintainer should
> be given as much flexibility as possible in eliminating titles from
> the index. 

I personally would prefer to create a sensible index file by hand
myself, but there is nothing in ALIWEB that dictates that this should
be the only/best way, and if you have other methods of making sure
only sensible documents are indexed that is great.

> Of course retaining the context is desirable, but the time for doing
> this is when the document is created, not when it is indexed.

What I meant was that sometimes it makes more sense to only index
one main document for a specific service instead of all associated
pages. For example, I wouldn't want an entire PC Software directory
indexed if my service provides a nice specialised search interface
for this. As said this responsibility is with the maintainer.
 
> The bottom line choice is between an index of 50 servers with
> carefully hand-crafted templates and an index of 5000 servers with
> machine generated templates which are less well constructed but up to
> date.  I would certainly opt for the later. 

Well, maybe. If these 5000 servers all index only the titles of all
their 1000 documents each the resulting database will not be that
useful. Try a Veronica search for Perl: it comes up with > 4000
matches, how am I supposed to find the servers dedicated to Perl?

> I would also do everything possible to encourage maintainers to
> massage their templates to improve them.

Quite. We both agree that what is indexed should be up to the
maintainer. How the index is created is also up to the maintainer.
I hope the maintainers will take their responsibility seriously, and
that these indices will be in a standard format so that they can be
integrated automatically.

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From raisch@internet.com  Wed Mar 16 00:32:26 1994 --100
Message-Id: <Pine.3.85.9403151219.A6592-0100000@hmmm>
Date: Wed, 16 Mar 1994 00:32:26 --100
From: raisch@internet.com (Rob Raisch, The Internet Company)
Subject: Re: The future of meta-indices/libraries? 


Yup.  All absolutely correct.

One nit, though.  This is hardly a problem with WWW alone.

Archie/Veronica fail miserably if you do not actually know beforehand 
what you are looking for.  In other words, if I know the exact book I 
require I can search a list for the title and find the exact location in 
the library where it resides.  But woe is me if I am looking for books on 
a particular topic, the librarian only gives me a blank stare and points 
me to the same list of titles.

Since the only place where this information exists is in the
administration of the repository itself, it does little good to reap book
titles.  Only the administrator of the repository knows what "resources" 
she offers to the network at large.

Currently all resource navigation tools on the gI suffer from this 
problem (gI := global Internet.)  The only effort which I believe comes 
close to being a useful solution to this problem is the "topic oriented" 
gopher lists.  But the failure here is the fact that this information 
must be gathered by a human without any help from the source of the 
information -- the repository.  Sort of like taking down names of 
businesses from the phone book and deciding in which category that 
business might reside.

I strongly feel that the repository must shoulder the role of the
cataloger. But (before all you repository managers balk at the size of the
problem), we really need to take a very hard look at what it is we need to
catalog. 

I believe that the gI is made up of "resources" where that word refers to
collections of information -- not to the information itself.  For example: 

  The Electronic Newsstand is a resource, as are the magazines it 
  encompasses, but the individual articles are not resources.

  Counterpoint's Federal Register is itself, a resource, as are the 
  various gubermint agencies included, but not the individual rules and 
  notices.

It has been in my mind for some time to start something which begins to
categorize the value which the gI represents, but does not specifically
index. (And this is the real question, the difference between an index --
which serves one valuable purpose, and a "table of contents" -- which
serves quite another.)

So...

I propose the GRIP -- Global Resource Identification Project -- where 

-- managers of internet resources (a concept, to be clearly defined) are 
provided with an IAFA-like template of a form along the lines of:

	Name:		Out Magazine
	URL:		gopher://enews.com/1/collected/out
			news://enews.out
			mail:info@enews.com
	Location:	Palo Alto, CA
	Description:	The world's leading magazine of gay and lesbian
			issues.  Published monthly.
	Keywords:	Gay Lesbian Homosexual Queer Politics Entertainment
	Category:	Publications/Magazines
			Politics/Gay and Lesbian Issues
			Culture/Human Sexuality
	Abstract:	{URL which points to a long description}

-- the filled in template is retrieved on a regular basis from the 
resource site and used to create a number of navigation tools:

	Under gopher:

		GRIP/
		 Publications/
		  Magazines/
		   Out Magazine/
		    Information --> the text of the template
		    To connect  --> points to the actual resource

	Also incorporated in similar ways into databases accessable via 
	WAIS/WWW/Whois++ etc.

Guilding Principles:

	- provide a template format which has clearly identified 
	  ranges of complience, eg. Required/Recommended elements

	- maintain some editorial control over the actual categorization
	  mechanism to enhance the value of the service

	- provide the database in as many forms as possible

	- provide the raw data (templates) to allow others to create
	  a rich set of tools to leverage it in different ways

	- distribute the load over as many well connected sites as possible


Offer:
	If we can come up with a document which clearly identifies what is
	and is now a "resource"...

	If we can come up with an "approved" template...

	And if we can come up with an "official" categorizing method...


	The Internet Company will donate programming/MIPS/pipe and Megs 
	to get this off the ground.


Ultimately, this is an effort which should be distributed over the entire 
gI in a reasonable manner.  Portions of the gopherspace so generated 
could be distributed on various altruistically inclined sites, as well as 
the WWW and Whois++ databases.  Dunno 'bout the WAIS stuff.  I suppose 
it's possible to break this out into categories and WAIS up the categories.

I also think that the raw data (templates) retrieved from each site 
should be made available to any and all, to develop a richer set of tools 
to interpret the data.

Ok?  Who wants to play?

--  </rr>  Rob Raisch, The Internet Company









From robm@ncsa.uiuc.edu  Wed Mar 16 02:18:01 1994 --100
Message-Id: <9403160113.AA05399@void.ncsa.uiuc.edu>
Date: Wed, 16 Mar 1994 02:18:01 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: Insecure WWW Access Authorization Protocol?

/*
 * Insecure WWW Access Authorization Protocol?  by michael shiplett
 *    written on Mar  6,  5:16pm.
 *
 *   I had a colleague look over a proposed Kerberos-based HTTP
 * authentication protocol I had closely based on the PEM/PGP & RIPEM
 * exchanges. 
 
 * He pointed out that a man-in-the-middle attack could allow
 * an evil entity to masquerade as the server since the exchange goes
 * from cleartext to encrypted.
 *
 *   Unless Alice knows how to authenticate to Bob prior to initiating
 * the transaction, Mallot will be able to subvert Alice's request. The
 * basic flaw is Alice relies on the "401 Unauthorized" response for
 * authentication information, e.g., public key, Kerberos principal, etc.

 *   I think this attack would work against any authentication
 * protocol following the WWW Access Authorization protocol examples.
 */

Yes, which is why we are changing Mosaic's behavior for PGP/PEM and, in the
future, Kerberos, to not send the request unencrypted first, but to get the
authentication information from the user and allow the user to force Mosaic
to send the request encrypted the first time. 

There are more problems for privacy using 401 than the one you're focusing
on. Namely, if a form has a credit card number in it that you don't want
going out in plaintext, it will go out in plaintext anyway the first time
the browser sends the request.

--Rob

obvious problems for privacy in this model



From robm@ncsa.uiuc.edu  Wed Mar 16 02:24:22 1994 --100
Message-Id: <9403160115.AA05425@void.ncsa.uiuc.edu>
Date: Wed, 16 Mar 1994 02:24:22 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: complaint about CGI

/*
 * Re: complaint about CGI  by Markus Stumpf
 *    written on Mar  6,  5:25pm.
 *
 * Could we also have some support for the HTTP response the server
 * will send?
 * 
 * It would really be nice to be able to have a CGI script generate
 * e.g. a FORBIDDEN message.
 * All I managed to produce so far is to send back some HTML document
 * that says forbidden but the server still tells OK.

How about a header line

Status: xxx mmm

where xxx is the 3 digit HTTP status code, and mmm is the message, such as
Forbidden.

 * When trying to implement e.g. proxy gateways via CGI scripts
 * this is really needed I think or otherwise we can't have a
 * transparent handling of the data.
 * 
 * Or is it already there and I didn't notice that?
 */

It's not.

--Rob



From robm@ncsa.uiuc.edu  Wed Mar 16 02:30:03 1994 --100
Message-Id: <9403160118.AA05467@void.ncsa.uiuc.edu>
Date: Wed, 16 Mar 1994 02:30:03 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI/1.1 draft

/*
 * Re: CGI/1.1 draft  by "Roy T. Fielding"
 *    written on Mar  7,  1:17am.
 *
 * Given the option, I would prefer that script config files (including
 * imagemap's) have their own directory other than the SERVER_ROOT/conf dir.
 * I need to make special precautions against rabid scripts and I find it
 * much easier to keep organized (and avoid overlooking something) if all the
 * script-specific stuff is in a separate location.  Also, it makes it slightly
 * easier to install new versions of the server.
 * 
 * Thus, I like the idea but would prefer the location SERVER_ROOT/cgi-conf
 */

My point is that a given server would allow the admin to configure what
directory it was, but that all HTTP servers would set CGI_CONFDIR to
something that scripts could rely on. If someone set it to
SERVER_ROOT/cgi-conf under NCSA httpd, that's fine.

--Rob



From phillips@cs.ubc.ca  Wed Mar 16 02:46:10 1994 --100
Message-Id: <7781*phillips@cs.ubc.ca>
Date: Wed, 16 Mar 1994 02:46:10 --100
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: complaint about CGI

Rob suggested:
>Status: xxx mmm
>
>where xxx is the 3 digit HTTP status code, and mmm is the message, such as
>Forbidden.

I don't have a problem with this being added, but it isn't strictly
necessary as you can output any code you want if you write an
nph- script.



From robm@ncsa.uiuc.edu  Wed Mar 16 02:52:40 1994 --100
Message-Id: <9403160144.AA05816@void.ncsa.uiuc.edu>
Date: Wed, 16 Mar 1994 02:52:40 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: complaint about CGI

/*
 * Re: complaint about CGI  by George Phillips  (Tel (604)-822-4230)
 *    written on Mar 15,  5:41pm.
 *
 * I don't have a problem with this being added, but it isn't strictly
 * necessary as you can output any code you want if you write an
 * nph- script.
 * 
 */

I know, but many people respond with fear and loathing when I suggest they
use nph- scripts. It's not clear to me why.

--Rob



From joe@MIT.EDU  Wed Mar 16 03:48:49 1994 --100
Message-Id: <9403160244.AA10823@theodore-sturgeon.MIT.EDU>
Date: Wed, 16 Mar 1994 03:48:49 --100
From: joe@MIT.EDU (Joseph Wang)
Subject: Re: The future of meta-indices/libraries


(This message was sent to the www-talk list in response to questions
there)

Keeping the GNA meta-library up to date has been a considerable
problem.  I think that I almost have things to the point in which most
of the indexing can be done by people with no technical knowledge of
how the meta-library works.  The trouble is that I won't have the time
to put the extra effort needed to get it all to work for another few
weeks.

If anyone out there is interested in organizing things in the meta-library,
the technical documentation is in 

http://uu-gna.mit.edu:8001/uu-gna/tech/index.html



From joe@MIT.EDU  Wed Mar 16 04:27:30 1994 --100
Message-Id: <9403160323.AA10948@theodore-sturgeon.MIT.EDU>
Date: Wed, 16 Mar 1994 04:27:30 --100
From: joe@MIT.EDU (Joseph Wang)
Subject: Re: The future of meta-indices/libraries


(This message was sent to www-talk)

I second Rob Raisch's call for a consortium to fix the resource
indexing problem.

I'd be willing to contribute whatever the GNA meta-library has an
effort at solving the resource discovery problem.  It seems that there
are about a dozen different meta-library out there each with its
strength and weaknesses.  

The strengths of the GNA meta-library are:

1. It has a very sophisticated search engine that can do keyword searchs
   on authors, titles, and keywords
2. It's entries are indexed in a hierarchical topic structure rather than
   be a keyword only search.  This makes it possible to be searchs of 
   resources in a general area.
3. (And IMHO most important) it has a concept called "coverage code" or 
   resource breadth.  For example, if you want to be a search of resources
   on "Korea," chances are that you would prefer to scan an entire WWW site
   devoted to Korea rather than an single article on the subject.  With
   archie like searches, there is no way you can tell if the resource you
   have found is an entire library or if it is merely a paragraph in an
   article.

The disadvantages of the meta-library approach is

1. The sophsiticated search engine (i.e. the postgres database) is extremely
   fragile and breaks down about once a week.
2. Because meta-library entries contain coverage code and resource breadth
   information, they must be entered by hand and are therefore EXTREMELY
   out of date (i.e. most entries are from October).

What would be interesting would be to try to combine the ALIWEB and
meta-library approaches.  The ALIWEB index format could be amended to
include a hierarchical topic index and a coverage code.  It would then
be easy to write a PERL robot that would take the indexes and enter
them into the meta-library search engine.

Any thoughts?

Incidentally, I can get accounts on the meta-library machine for
anyone who wants to contribute to the global indexing initiative, and
would be happy to donate the meta-library database system and contents
for such an initiative.





From peterd@bunyip.com  Wed Mar 16 02:59:18 1994 --100
Message-Id: <9403160040.AA12219@expresso.bunyip.com>
Date: Wed, 16 Mar 1994 02:59:18 --100
From: peterd@bunyip.com (Peter Deutsch)
Subject: Re: The future of meta-indices/libraries?

[ You wrote: ]
.  .  .
> I also don't think that the manual creation of entries in a document
> is prohibitive. After all, these maintainers are putting a lot of
> effort in publishing material such as HTML pages, gateways and the
> like. Why should they shy away from a small text file? There are very
> real advantages to them: their information is not just "in the Web",
> but is findable in the web. They don't need to maintain their own 
> "lists of interesting places".

This does create a potential maintenance problem in the
long run, since in my experience the enthusiasm for
creating or maintaining meta-info is inversely
proportional to the time since the document was created.

As one data point, there is a document I wrote several
years ago called "What is archie?". It is everywhere.
It's been indexed in WAIS, menuized in gopher, included in
bibliographies, etc. I wrote this when still at McGill and
it mentioned "archie.mcgill.ca" as "the" archie server.
Even though this machine hasn't been with us for several
years now we _still_ get mail from people mentioning this
doc and asking about the status of archie.mcgill.ca. 

I'd hope we end up with a system where it's a little
easier to correct or recall such documents and the info
about such documents. WWW promises this, but I suspect if
we require people to do such things by hand it will suffer
in consequence. (I know this analogy is a bit weak, since
my problem above is with the content, not the meta-info,
but please cut me some slack... :-)


> Actually it turns out it that the fact that most (all?) people write
> the index files by hand is more a feature than a problem. Because
> people don't want to manually create massive index files they ony
> describe the most important services, which results in a database
> with little irrelevant material.

But one person's irrelevant is another person's useful
data. We've had people mine the current archie collection
just so they could study such things as the proportion of
file types and other information about the data.  We
certainly didn't forsee such applications when we started
so I'd rather not hard-wire in too many assumption about
what people might want or need at this point. We
definitely want to stay flexible.

> > > I'm not sure it is going to be sensible
> > > to index all titles on a server and search those, even though it sounds
> > > attractive. You do need to retain the context of the titles.

In theory I agree, although in practice we may find that
titles alone (machine generated and thus accurate) are
more useful than full templates which are hand-generated
and thus inaccurate. Filenames alone have proved of use
in archie even though in theory descriptive information
would be more useful.


.  .  .
> > The bottom line choice is between an index of 50 servers with
> > carefully hand-crafted templates and an index of 5000 servers with
> > machine generated templates which are less well constructed but up to
> > date.  I would certainly opt for the later. 
> 
> Well, maybe. If these 5000 servers all index only the titles of all
> their 1000 documents each the resulting database will not be that
> useful. Try a Veronica search for Perl: it comes up with > 4000
> matches, how am I supposed to find the servers dedicated to Perl?

Can we aim for both? I see wanting both a cheap,
relatively useful set of machine-generated and accurate
titles plus more descriptive info where available. That's
why we added the WAIS indexing capability and worked on
the IAFA template stuff. All the components are now in
place to do more detailed descriptive info once we figure
out where it is. Meanwhile, and until enough sites agree
to do this, filenames/menu items/titles are not a bad
first approximation and a lot easier to automate.



					- peterd

-- 
------------------------------------------------------------------------------
  My proposal for funding the Internet is pretty simple. I vote we institute
  an "Information Superhighway" tax, the proceeds of which will be used to
  fund network infrastructure. The way this would work is simple - every time
  someone uses the words "Information Superhighway" or any of its derivatives
  we strike them with a sharp object and make them pay a $10 fee (of course,
  the sharp object is not actually needed to make this scheme work, it's just
  in there because it seems an appropriate thing to do...)
------------------------------------------------------------------------------



From decoux@moulon.inra.fr  Wed Mar 16 07:27:58 1994 --100
Message-Id: <9403160625.AA22600@moulon.moulon.inra.fr>
Date: Wed, 16 Mar 1994 07:27:58 --100
From: decoux@moulon.inra.fr (ts)
Subject: complaint about CGI


> Rob suggested:
> >Status: xxx mmm
> >
> >where xxx is the 3 digit HTTP status code, and mmm is the message, such as
> >Forbidden.
> 
> I don't have a problem with this being added, but it isn't strictly
> necessary as you can output any code you want if you write an
> nph- script.
> 
> 

 I write nph-script only to send, sometimes, code different from 200 and
301.

 It's really easy to send only a header line "Status: xxx mmm", rather than
send all header lines ("MIME-Version", "Content-type", "Content-length", ...)


Guy Decoux



From robm@ncsa.uiuc.edu  Wed Mar 16 07:42:22 1994 --100
Message-Id: <9403160638.AA08424@void.ncsa.uiuc.edu>
Date: Wed, 16 Mar 1994 07:42:22 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI/1.1 draft

/*
 * Re: CGI/1.1 draft  by George Phillips
 *    written on Mar  6, 11:30am.
 *
 * While Accept:, Content-Type: and Content-Length: are literally in
 * the CGI variable space, only a munged version of Authorized: is
 * there.  How about changing that to "does not include Authorized:
 * if server is doing the authentication on that script, otherwise
 * is does."  For a script with simple authorization requirements,
 * the server can do the work.  When the script gateways into something
 * where server authentication is inappropriate or impossible
 * (like an Oracle database), the writer can use an "nph-" script and
 * do the authentication herself.

Hmmm. I don't think that would be a problem for NCSA httpd, what about the
others (John, Tony, Ari)?

Also, what about, instead of making it REQUEST_EXTRA_HEADERS, how about
placing each line in a variable? This way, you would have HTTP_FROM for the
from: line, etc. This makes HTTP_ACCEPT just another header. Any header
lines with - would have the - translated to _. Multiple instances would be
concatenated into the variable with commas as separators. Anyone see a
problem with this?

 * Please, please leave PATH_INFO escaped.  It was a mistake to do the
 * unescaping in the server; let's fix it.  Sure, it's not strictly
 * backwards compatible, but I seriously doubt many scripts relied upon
 * the old behaviour.  Besides "%3d", there's also "%00" which a CGI
 * script really loses on.
 */

I don't agree. I think that with dummy inputs available in forms, we can
finally move away from using PATH_INFO to convey state information to
scripts and go back to using them for their intended purpose: To allow
scripts to access the server's virtual->physical translation and access
authorization for auxillary files. If you're using filenames in PATH_INFO
then you don't have to escape the information, and if you have it as dummy
inputs in a form then your data is already escaped anyway.

-------------

Updated proposed changes list:

1. Any headers without special meaning to the server which are output by
non-nph scripts are to be passed back to the client.

1a. A new special header will be added to the script's output. Status: will
convey a 3-digit HTTP status code followed by a reason string. Example:
Status: 403 Forbidden

2. The header lines from the client, if any, are to be placed into the
environment with the prefix HTTP_ and followed by the header name. Any -
characters in the header name should be changed to _ characters. This does
not include Authorization: if the server has already processed the
Authorization: line. 

Example:

Client:
GET / HTTP/1.0
From: marvin@mars.gov
User-agent: DisintegratorWWW 1.1
Accept: text/html
Accept: text/plain

The CGI env. variables for this transaction's headers would be

HTTP_FROM="marvin@mars.gov"
HTTP_USER_AGENT="DisintegratorWWW 1.1"
HTTP_ACCEPT="text/html, text/plain"

Comments welcome
--Rob



From decoux@moulon.inra.fr  Wed Mar 16 08:02:16 1994 --100
Message-Id: <9403160700.AA22641@moulon.moulon.inra.fr>
Date: Wed, 16 Mar 1994 08:02:16 --100
From: decoux@moulon.inra.fr (ts)
Subject: CGI/1.1 draft


> While Accept:, Content-Type: and Content-Length: are literally in
> the CGI variable space, only a munged version of Authorized: is
> there.  How about changing that to "does not include Authorized:
> if server is doing the authentication on that script, otherwise
> is does."  For a script with simple authorization requirements,
> the server can do the work.  When the script gateways into something
> where server authentication is inappropriate or impossible
> (like an Oracle database), the writer can use an "nph-" script and
> do the authentication herself.
> 

 Why server authentication is impossible with an Oracle database ?

 Actually, when I consult an Oracle database server make authentication and
call the script with the process uid of the authenticated user. Script use
only process uid (and no password) to open the database, i.e. (in Perl) :

      &ora_login($database,'/','');

 Script don't make authentication but check only the real access to the
database.

Guy Decoux




From m.koster@nexor.co.uk  Wed Mar 16 10:15:51 1994 --100
Message-Id: <9403160856.AA00736@dxmint.cern.ch>
Date: Wed, 16 Mar 1994 10:15:51 --100
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: The future of meta-indices/libraries? 


Peter Deutsch wrote:

> If we require people to do such things by hand it will suffer in
> consequence.

I agree that we shouldn't require this to be done by hand.
I did say that in the message you quoted.

> ... But one person's irrelevant is another person's useful
> data. We've had people mine the current archie collection
> just so they could study such things as the proportion of
> file types and other information about the data.  We
> certainly didn't forsee such applications when we started
> so I'd rather not hard-wire in too many assumption about
> what people might want or need at this point. We
> definitely want to stay flexible.

Of course. But we do need to make the information mangeable, 
otherwise we might as well give up and let full web robots
to it for us.

> > > > I'm not sure it is going to be sensible
> > > > to index all titles on a server and search those, even though it sounds
> > > > attractive. You do need to retain the context of the titles.
> 
> In theory I agree, although in practice we may find that
> titles alone (machine generated and thus accurate) are
> more useful than full templates which are hand-generated
> and thus inaccurate. Filenames alone have proved of use
> in archie even though in theory descriptive information
> would be more useful.

In Archie you can use directories and tar files to give a context to
information.  It is up to the information provider to decide what to
tar up into one file, and where to put it. If all files in FTP sites
were untarred in a single directory Archie would be unusable.

In the Web this is a bit more difficult. _Titles_ of web documents
aren't hierarchically structured like file paths. I suppose the
hierarchical natue of most URL's may help there, but it's dodgy. There
is no concept of grouping like in a tar file, you can't specify "Index
this document but nothing linked from it". This means that additional
info is needed more badly than in Archie.

> > > The bottom line choice is between an index of 50 servers with
> > > carefully hand-crafted templates and an index of 5000 servers with
> > > machine generated templates which are less well constructed but up to
> > > date.  I would certainly opt for the later. 
> 
> Can we aim for both?
>
> I see wanting both a cheap, relatively useful set of
> machine-generated and accurate titles plus more descriptive info
> where available.

As I said in the previous message, I am completely happy to leave the
decision of what and how to index up to the person providing the
information, as long as it's done in a standard and parseable way.

> why we added the WAIS indexing capability and worked on
> the IAFA template stuff. All the components are now in
> place to do more detailed descriptive info once we figure
> out where it is.

ALIWEB index files should (really) be on http://.../site.idx

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From dsr@hplb.hpl.hp.com  Wed Mar 16 10:56:11 1994 --100
Message-Id: <9403160951.AA10386@dragget.hpl.hp.com>
Date: Wed, 16 Mar 1994 10:56:11 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: The future of meta-indices/libraries?

Christian Neuss writes:

> The title comes to mind.. but something with a bit more meaning
> would be better. Well, how about using the group statement to mark
> up special semantics of HTML text? You could have

>   <GROUP role="keywords">
>   <H2> Keywords: </H2>
>   compression, encryption
>   </GROUP>

> in the header, or at multiple positions in the text:

Before you start inventing new elements, consider the HTML+ META
element. This was added as a general solution to adding meta
information to HTML documents. This scheme allows wobots to scan
this information efficiently using the HTTP HEAD method.

    <!--
     Servers should read the document head to generate HTTP headers
     corresponding to META elements, e.g. if the document contains:

        <meta name="Expires" value="Tue, 04 Dec 1993 21:29:02 GMT">

     The server should include the HTTP date format header field:

        Expires: Tue, 04 Dec 1993 21:29:02 GMT

     Other likely names are "Keywords", "Created", "Owner" (a name)
     and "Reply-To" (an email address)
    -->
    <!ELEMENT META - O EMPTY>
    <!ATTLIST META
        id      ID      #IMPLIED -- to allow meta info  --
        name    CDATA   #IMPLIED -- HTTP header e.g. "Expires" --
        value   CDATA   #IMPLIED -- associated value -->
      



From kevinh@eit.COM  Wed Mar 16 11:57:06 1994 --100
Message-Id: <9403161052.AA13463@eit.COM>
Date: Wed, 16 Mar 1994 11:57:06 --100
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re: The future of meta-indices/libraries?

> From: Peter Deutsch <peterd@bunyip.com>
> 
> > > The bottom line choice is between an index of 50 servers with
> > > carefully hand-crafted templates and an index of 5000 servers with
> > > machine generated templates which are less well constructed but up to
> > > date.  I would certainly opt for the later. 
> > 
> > Well, maybe. If these 5000 servers all index only the titles of all
> > their 1000 documents each the resulting database will not be that
> > useful. Try a Veronica search for Perl: it comes up with > 4000
> > matches, how am I supposed to find the servers dedicated to Perl?
> 
> Can we aim for both? I see wanting both a cheap,
> relatively useful set of machine-generated and accurate
> titles plus more descriptive info where available. That's
> why we added the WAIS indexing capability and worked on
> the IAFA template stuff. All the components are now in
> place to do more detailed descriptive info once we figure
> out where it is. Meanwhile, and until enough sites agree
> to do this, filenames/menu items/titles are not a bad
> first approximation and a lot easier to automate.

	I like what Martjin has done with ALIWEB, and I think templates
are the way to go - using templates, you can have both!
	Entries in a site's template file can be manually created or
massaged; automatically (locally) generated templates can be appended
or put into another index file specifically for automatically-created
template entries.
	Maybe put a field in each entry specifying whether it was manually
or automatically created; that way the end user doing the searching
would know a bit more about the data they've received? Or at least a
"Last-Modified" line, something like that.
	I like this approach myself because this way web robots can be
polite and only gather what the local webmaster wants made available,
without having to traverse trees, pull out titles, search for who knows
what. And using this method, local admins can configure their
template-creating programs to grab only what they feel is important.

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://www.eit.com/)
Hypermedia Industrial Designer * Duty now for the future!



From M.T.Hamilton@lut.ac.uk  Wed Mar 16 12:37:59 1994 --100
Message-Id: <199403161133.LAA05540@lust.mrrl.lut.ac.uk>
Date: Wed, 16 Mar 1994 12:37:59 --100
From: M.T.Hamilton@lut.ac.uk (Martin Hamilton)
Subject: Re: The future of meta-indices/libraries?

Rob Raisch said:

$ So...
$ 
$ I propose the GRIP -- Global Resource Identification Project -- where 
$ 
$ -- managers of internet resources (a concept, to be clearly defined) are 
$ provided with an IAFA-like template of a form along the lines of:
$ 
$ 	Name:		Out Magazine
$ 	URL:		gopher://enews.com/1/collected/out
$ 			news://enews.out
$ 			mail:info@enews.com
$ 	Location:	Palo Alto, CA
$ 	Description:	The world's leading magazine of gay and lesbian
$ 			issues.  Published monthly.
$ 	Keywords:	Gay Lesbian Homosexual Queer Politics Entertainment
$ 	Category:	Publications/Magazines
$ 			Politics/Gay and Lesbian Issues
$ 			Culture/Human Sexuality
$ 	Abstract:	{URL which points to a long description}

Methinks this stuff would sit nicely in the URC (URM? URT?) :-)

(See http://www.gatech.edu/urm.paper for more info)

Rob - was that what you had in mind?

$ -- the filled in template is retrieved on a regular basis from the 
$ resource site and used to create a number of navigation tools:
$ 
$ 	Under gopher:
$ 
$ 		GRIP/
$ 		 Publications/
$ 		  Magazines/
$ 		   Out Magazine/
$ 		    Information --> the text of the template
$ 		    To connect  --> points to the actual resource
$ 
$ 	Also incorporated in similar ways into databases accessable via 
$ 	WAIS/WWW/Whois++ etc.

I've just had one of those rare (for me!) "Eureka!" moments...

Have the resource author embed (at least) categorizing info in their
resource's meta-info (e.g. <head> element in HTML or gopher item
attributes), and robots can pick these up directly.

Next question:  where do the categories come from?!?

How about the Interpedia ? ;-)

Cheers,

Martin




From P.Lister@cranfield.ac.uk  Wed Mar 16 12:56:30 1994 --100
Message-Id: <9403161152.AA02564@xdm039.ccc.cranfield.ac.uk>
Date: Wed, 16 Mar 1994 12:56:30 --100
From: P.Lister@cranfield.ac.uk (Peter Lister, Cranfield Computer Centre)
Subject: Re: The future of meta-indices/libraries?

> 	I like this approach myself because this way web robots can be
> polite and only gather what the local webmaster wants made available,
> without having to traverse trees, pull out titles, search for who knows
> what. And using this method, local admins can configure their
> template-creating programs to grab only what they feel is important.

And it also means that local admins can say "The hierarchy under this URL 
actually drives a script". Which brings me to consider how such indexing 
mechanisms consider e.g. search engines? I think it's useful to ensure that 
Archie knows that this entity is in fact searchable or does things and 
displays the results, not just a file archive?

Peter Lister                             Email: p.lister@cranfield.ac.uk
Computer Centre, Cranfield University    Voice: +44 234 754200 ext 2828
Cranfield, Bedfordshire MK43 0AL UK        Fax: +44 234 750875
--- Go stick your head in a pig.  (R) Sirius Cybernetics Corporation ---




From rxc@nrc.gov  Wed Mar 16 14:39:53 1994 --100
Message-Id: <9403161333.AA01491@nrr2.nrc.gov>
Date: Wed, 16 Mar 1994 14:39:53 --100
From: rxc@nrc.gov (Ralph Caruso)
Subject: WWW servers for Novell

I am a newcomer to this mailing list, and to WWW in general, but
my organization is in the process of setting up a WWW server for
internal use (initially) and eventually for external use.  We have
just started to use unix workstations, and are learning our
way, but we still have a large network of PCs, connected by a
Novell network running IPX (for now).  This network contains a large
amount of data that we would like to make available via a WWW-type
server.  I have looked in the list of tools for WWW, but have not found
any PC-type servers listed there.  Does any one who subscribes to
this list know where such a thing can be found?  Alternatively, does
anyone know whether it is possible for a unix server to reach into
a PC network and serve documents from a PC?  Any and all thoughts on this
are welcome.

Ralph Caruso

US Nuclear Regulatory Commission



From th@rz.fh-wolfenbuettel.de  Wed Mar 16 15:00:03 1994 --100
Message-Id: <9403161353.AA29132@bilbo.rz.fh-wolfenbuettel.de>
Date: Wed, 16 Mar 1994 15:00:03 --100
From: th@rz.fh-wolfenbuettel.de (Thorsten Ludewig)
Subject: Re: WWW servers for Novell

> Subject: WWW servers for Novell
> 
> I am a newcomer to this mailing list, and to WWW in general, but
> my organization is in the process of setting up a WWW server for
> internal use (initially) and eventually for external use.  We have
> just started to use unix workstations, and are learning our
> way, but we still have a large network of PCs, connected by a
> Novell network running IPX (for now).  This network contains a large
> amount of data that we would like to make available via a WWW-type
> server.  I have looked in the list of tools for WWW, but have not found
> any PC-type servers listed there.  Does any one who subscribes to
> this list know where such a thing can be found?  Alternatively, does
> anyone know whether it is possible for a unix server to reach into
> a PC network and serve documents from a PC?  Any and all thoughts on this
> are welcome.
> 
> Ralph Caruso
> 
> US Nuclear Regulatory Commission
> 

Just try to connect the Novell own WWW server for more information.
  <a href="http://www.novell.de/">The Novell European Support Center</a>
  <a href="http://www.novell.com/">The US Novell Support Center</a>

Ciao Thorsten

_______________________________________________________________________________
Thorsten Ludewig, th@rz.fh-wolfenbuettel.de, http://www.fh-wolfenbuettel.de/~th





From browne@cs.utk.edu  Wed Mar 16 16:21:31 1994 --100
Message-Id: <199403161515.KAA21095@pebbles.cs.utk.edu>
Date: Wed, 16 Mar 1994 16:21:31 --100
From: browne@cs.utk.edu (browne@cs.utk.edu)
Subject: Re:  The future of meta-indices/libraries?

I have noticed that a skilled reference librarian doing an
on-line search does not simply type a few keywords into a
global metaindex and expect to get useful results.  And
why not?  On-line databases are subject area specific, for
one thing.  The searcher must first choose a database, either
from prior knowledge or by consulting a directory of databases.
Next, if she is not already familiar with the database,
she studies its documentation to become familiar with its
classification schemes/codes and how to use them.  Once she
has gotten results from an initial query, she widens or
narrows the search according to whether too little or too
much information was returned.  She also uses initial results
as a guide to formulating further queries.  For example, if
a particularly relevant reference is spotted, a useful
technique is to plug descriptor codes returned with the citation
for that reference into a new search.  These techniques
depend on being able to qualify the search in specific ways.
For example, to restrict the search, one might specify that
query keywords be applied only to the keyword and descriptor
fields, rather than to entire abstracts.

So just as no single on-line database can exhaustively
index the entire world of printed publications (those that
attempt to do so succeed only superficially), neither can
a single Internet database index all information available
electronically.  Any database that attempts to do so will
be unwieldy and hopelessly out-of-date.  So the solution
seems to be to divide up the world.  But how to divide it
up?  Some have suggested dividing it up geographically.  I
think this is a bad idea, since I seldom want to restrict
sources of information geographically when doing a search.
Some have suggested indexing separately by type of service
provide -- e.g., one index for anonftp, another for gopher,
another for WWW -- in fact, this is what is already being
done.  But again, I am usually not interested in restricting
my search in this manner.  Instead, I want to retrieve all
relevant material, regardless of its access protocol and
format, although perhaps I have a preferred format, if more than
one is available.  It makes much more sense to me to divide
up the world by subject area, and to have active participants
in different subject areas (e.g., high-energy physics, HPCC,
environmental sciences, education) 
have ownership of subject area databases.

To do effective searches, we will need to be able to search
on particular attributes and to use subject area specific
keywords and descriptors.  The IAFA templates may well be a
workable format for specifying metainformation in the form
of attribute/value pairs, if they can be standardized.
It seems reasonable to use standardized template definitions
across different subject areas, if the templates are
developed with the needs of different groups in mind, although
specialized templates may be required for special cases.  Choices of
classification schemes and descriptor keywords, however,
will need to be made by experts in the individual subject areas.
A single classification would be too unwieldy and difficult
to use, and the same keyword may have different meanings
in different contexts.  An example of a discipline-specific
classification scheme that is currently in use for searching
repositories of mathematical software is the GAMS classification
scheme developed at NIST (URL http://gams.nist.gov/).

Thus, subject area consortiums and on-line communities should have the
responsibility of developing classification schemes, subject
area thesauri, and guidelines for using them.  
Each group should also have the responsibility for constructing
and maintaining the searchable indices for its area.
The global Internet community should be responsible for
standardizing the metainformation format (in cooperation with
the various subject areas), for providing public-domain software to
do the indexing and run the search engines, for providing
user interfaces and client software that provide assistance
to the non-expert user and present as uniform a search interface
as possible, and for providing a global directory to area-specific
databases.  I suggest that groups or individuals, such as Peter
Deutsch at Bunyip or Rob Raisch at Internet Company, who are interested
in developing and providing the generic tools, try to develop
a working relationship with one of the existing or forming
on-line research communities, such as HEPNet or the HPCC
community, and use them as a prototype.

Lastly, consider the approach that nowadays seems popular of
trying to automatically index all network resources with a minimum
of human intervention.  The sad fact of the matter is, the quality
of searching that is possible depends critically on the accuracy,
specificity, and thoroughness of the metainformation provided,
and providing this metainformation necessarily involves human
effort.  Perhaps retrofitting all existing resources with full
metainformation is too daunting a task.  As soon as metainformation
standards can be agreed upon, authors and publishers of new
material must be expected to provide the appropriate metainformation
and to update it as needed.  Otherwise, the "searching for
Internet resources" problem simply will not be solved.  I agree that
keeping metainformation accurate and up-to-date is currently
a problem, but perhaps this problem can be alleviated by
providing appropriate repository management software.  Perhaps
the URN/URC/URT scheme, together with Chris Weider's transponder/agent
idea would be useful here.

************************************************************************
Shirley Browne         Research Associate      107 Ayres Hall
browne@cs.utk.edu      Computer Science Dept.  University of Tennessee
(615) 974-5886         Fax (615) 974-8296      Knoxville, TN 37996-1301
*************************************************************************




From john@math.nwu.edu  Wed Mar 16 16:31:10 1994 --100
Message-Id: <9403161525.AA14485@hopf.math.nwu.edu>
Date: Wed, 16 Mar 1994 16:31:10 --100
From: john@math.nwu.edu (John Franks)
Subject: Re: The future of meta-indices/libraries?


We may be approaching a consensus, at least among the people 
participating in this discussion.  

I think we might be able to agree on the following:

1.  We use the ALIWEB template format with the addition
    of a creation date and an optional expiration date.
    (I know the date will be in the HTTP header -- it should
    be in the document too.)

2.  The template should be accessed with the URL  
    http://hostname/site.idx.

3.  It is up to the maintainer how to create the template (by hand
    or automatically).


Now here are some personal thoughts on which there will likely be
disagreement:

1.  Hand made versus Machine made:

    Hand made templates are better (in general) than machine made
    templates, but the percentage of sites willing or able to produce
    them is very small. (About 1% of current sites participate in
    ALIWEB.  This is what we should expect in general.)  Also as Peter
    Deutsch points out, enthusiasm for making these templates declines
    exponentially with time so hand made ones we usually not be up to
    date.  
    
    For this reason it is vitally important that default automatic support for
    template generation be built into servers. It is not even sufficient
    to have an external script or program to create the template.  It
    must be done automatically by default.  On this point we have yet
    to hear from the two most important players, NCSA and CERN.  Unless
    the creators of these two servers buy into this proposal we are
    probably wasting our time.  The reality is that while our discussion
    may be valuable, standards are set by NCSA and CERN.

2.  HTML+ Meta information

    I think that, by all means, authors should be encouraged to put
    meta information into their html documents.  And automatic
    template creators should be designed to incorporate this
    information if it exists.  But I don't think that a robot doing a
    HEAD request for all the documents on a server is the correct way
    to do indexing.  I particular I don't know how the robot would
    know the URLs of documents on the server all it only used the HEAD
    method.


John Franks 	Dept of Math. Northwestern University
		john@math.nwu.edu




From michael.shiplett@umich.edu  Wed Mar 16 17:15:14 1994 --100
Message-Id: <199403161610.LAA23924@totalrecall.rs.itd.umich.edu>
Date: Wed, 16 Mar 1994 17:15:14 --100
From: michael.shiplett@umich.edu (michael shiplett)
Subject: Re: Insecure WWW Access Authorization Protocol? 

"rm" == Rob McCool <robm@ncsa.uiuc.edu> writes:

rm> Yes, which is why we are changing Mosaic's behavior for PGP/PEM
rm> and, in the future, Kerberos, to not send the request unencrypted
rm> first, but to get the authentication information from the user and
rm> allow the user to force Mosaic to send the request encrypted the
rm> first time.
  Are you looking to make obtaining the authentication information an
automatic procedure based on the server name in URL, e.g, given
http://server.com/..., obtain the public key for server.com using
X.509 certificates to verify the public key; then get the user's
private key somehow? Or will the user always be required to find the
server's public key and to enter information (passphrase, etc.) to get
his private key?

  I don't know the status of the Generic Security Service API
(GSS-API) in the Internet community, but after reading RFC 1509, it
could be a good thing to use the GSS-API in httpd servers/clients
instead of each program reinventing the code.

michael






From dsr@hplb.hpl.hp.com  Wed Mar 16 18:30:11 1994 --100
Message-Id: <9403161725.AA11467@dragget.hpl.hp.com>
Date: Wed, 16 Mar 1994 18:30:11 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: The future of meta-indices/libraries?

> 2.  HTML+ Meta information

>    I think that, by all means, authors should be encouraged to put
>    meta information into their html documents.  And automatic
>    template creators should be designed to incorporate this
>    information if it exists.  But I don't think that a robot doing a
>    HEAD request for all the documents on a server is the correct way
>    to do indexing.  I particular I don't know how the robot would
>    know the URLs of documents on the server all it only used the HEAD
>    method.

I think that a local "robot" is in a good position to create the index.

    o   speed isn't a problem

    o   local staff can use config files to control
        which directories are searched

    o   you can combine manual with automatic entry generation
        for different parts of the local web as needed

Dave Raggett



From phillips@cs.ubc.ca  Wed Mar 16 22:28:21 1994 --100
Message-Id: <7790*phillips@cs.ubc.ca>
Date: Wed, 16 Mar 1994 22:28:21 --100
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: CGI/1.1 draft

Easy stuff first:  I agree with the proposed changes.  Now, about
PATH_INFO...

Rob said:
>I don't agree. I think that with dummy inputs available in forms, we can
>finally move away from using PATH_INFO to convey state information to
>scripts and go back to using them for their intended purpose: To allow
>scripts to access the server's virtual->physical translation and access
>authorization for auxillary files. If you're using filenames in PATH_INFO
>then you don't have to escape the information, and if you have it as dummy
>inputs in a form then your data is already escaped anyway.

I agree that PATH_INFO is not the right place for user input, but
PATH_INFO is something generated by the script for use by the script.
The server shouldn't be touching it.  It shouldn't even have any
idea if % or some other escaping is done on the information there.
As long as there are no bad characters in it, it just doesn't matter.

I certainly don't agree with your idea of the intended purpose of
CGI scripts.  I use them all the time for dynamically translating
data into browser-understandable formats (like HTML).  Input
forms and searches are just one possible use.



From connolly@hal.com  Wed Mar 16 22:50:18 1994 --100
Message-Id: <9403162137.AA10696@ulua.hal.com>
Date: Wed, 16 Mar 1994 22:50:18 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Updated URI test suite; resolving some issues...


I've updated my URI test suite
	<http://www.hal.com/%7Econnolly/dist/url_test-19940316.tar.Z>
to address _lots_ more issues.  While I was at it, I of course had to
tweak the grammar because of things I hadn't thought of.

WHICH CHARACTERS?

While I was at it, I decided on a workable finalization of the set of
data characters. I started with the POSIX portable filename character
set (letters, digits, hyphen, underscore, and period). Then I looked
at the MIME recommendataions about characters that make it through
mailers without harm. But in the end, I settled on the set from the
isAcceptable table from HTParse.c in the libwww distribution out of
Mosaic.

So the data characters are letters, numbers, period, hyphen,
underscore, at-sign and asterisk. In regexp-speak, that's
	[0-9A-Za-z*\.@_-]

FTP CHANGES

The first result of this is that the user@host in
	ftp://user@host/dir/file.ext
is one word to the parser. So it's no longer part of the URI syntax --
it's specific to the FTP scheme. This is handy in that it makes the
grammar LR(1) again! There is a conflict when using user:passwd@host,
though. The ':' is special and can't be part of a word unless it's
escaped. So the full ftp syntax will have to change to:
	ftp://user*passwd@host/dir/file.ext
or
	ftp://user%3Apasswd@host/dir/file.ext
or something else where the whole user/passwd/host triple is one word.


WAIS STUFF

The other result of picking that char set is that all the other
characters ("!@#$%^&*()=+~`':...") are either markup or reserved.

This caused a conflict with WAIS URLs. So I extended the grammar to
include ';' and '=' as tokens, and added keyword=value syntax. So
the syntax for WAIS files is:
	wais://host/database/type/size/keyword=value;keyword=value;...
and the parser extracts the keyword/value pairs.

The keyword=value syntax is allowed in the path and in the search
string. So the syntax includes things like:
	x-smart-database://host/database-name?author=fred;year=1994
	x400:/G=Jack;S=Jansen;O=cwi;PRMD=surf;ADMD=400net;C=nl
	x500:/c=GB@o=NEXOR%20Ltd@cn=Martijn%20Koster

URNs vs URLs vs RELATIVE URIs

I have been thinking about near-term ways to deploy URNs. Even if
there is no generalized way to resolve a URN to a URL, they are
useful. For example, I have a whole bunch of cached documents from the
web in my local filesystem. But the connection between them and the
place they came from is lost. So when I'm browsing some document that
references the MIME RFC, for example, my browser has no way of taking
advantage of the fact that I've already got a copy of it locally. And
the problem scales as documents are copied, mirrored, cached, etc.

On the other hand, if we had an rfc: URN scheme registered, I could
perhaps configure my browser (or my proxy server) to map
	rfc:*	=> local-file:/u/connolly/web/rfc/*	(try this first)
		=> ftp://ds.internic.net:/rfc/*		(try this next...)

The same is true of mailing lists. When I'm browsing the www-talk
archive, I actually have local copies of many of the messages. We
could register a message-id:<id> scheme or even mailing-list:mbox/<id>
scheme. The I could map
	mailing-list:www-talk@info.cern.ch/*
		=> local-file:/u/connolly/Mail/by-id/www-talk/*
	newsgroup:comp.text.sgml/*
		=> local-file:/u/connolly/News/by-id/comp.text.sgml/*
		=> wais://ifi.no/comp.text.sgml/TEXT/99999/*

I extended the grammar to include relative URIs, and I invented a way
to merge URNs into the URL namespace while still begin able to tell
them apart. A URL always looks like:
	scheme://WORD...
or
	scheme:/WORD...
whereas a URN always looks like:
	scheme:WORD...
(i.e. no slash)

So we can begin to deploy things like:
	message-id:9403161725.AA11467@dragget.hpl.hp.com
	isbn:IBM/832u9283
	issn:29o3u7982
by, for example, using the www_proxy mechanism in Mosaic.

Why is it necessary to distinguish URNs from URLs? To me, the
distinction between URNs and URLs is that URNs identify immutable
objects, and URLs identify mutable objects. Once you've resolved a
URN, you can keep that copy forever and use it to satisfy other
queries for that URN. As to the issues of versioning, translation,
etc., I'd say that a URNs may identify a set of documents, and the
versions, translations, etc. are elements of the set.

For example, the URNs
	rfc:rfc822.ps
and
	rfc:rfc822.txt
are elements of, say
	rfc:rfc822.*

The last URN above can't be directly resolved.

In many ways, the URN <rfc:rfc822.txt> is the same as the URL
<ftp://ds.internic.net/rfc/rfc822.txt>. But a WWW client has no
was of knowing that the ftp file is guaranteed not to change.

Hmmm... this isn't all coming together like I had hoped. The goal is
to deploy the more sophisticated "URCs" or IAFA-templates or whatever
is a scalable, distributed fashion. In the short term, I'd like to be
able to compose documents with references like:

  <REFERENCE linkend="x1">RFC 822: Format for Internet Mail Messages
	</REFERENCE>
  <urnloc ID="x1" locsrc="loc1"
      DATE="19910434094433" EXPIRATION="19990101000000">rfc:rfc822.txt</urnloc>
  <url ID="loc1" backup="loc2">local-file://ulua.hal.com/u/connolly/rfc/822.txt
	</url>
  <url ID="loc2" backup="loc3">wais://host/rfcs/.../822.txt
	</url>
  <url ID="loc3" backup="loc4">ftp://ds.internic.net/rfc/rfc822.txt</url>
  <bibloc ID="loc4">
0822 S     D. Crocker, "Standard for the format of ARPA Internet text  
           messages", 08/13/1982. (Pages=47) (Format=.txt) (Obsoletes 
           RFC0733) (STD 11) (Updated by RFC1327, RFC0987) 
  </bibloc>


Anyway... this citation stuff is still muddling around at this point.
But I think I've got most of the URL issues hammered out, while
leaving room for URNs and allowing this stuff to be used in URCs.

Dan



From FisherM@is3.indy.tce.com  Wed Mar 16 22:06:48 1994 --100
Message-Id: <2D876CE8@MSMAIL.INDY.TCE.COM>
Date: Wed, 16 Mar 1994 22:06:48 --100
From: FisherM@is3.indy.tce.com (Fisher Mark)
Subject: RE: Testing URIs for equality


Two things:
1. I really hate to break existing code and/or documents; but
2. Special cases in code or data are the software engineer's nightmare.

It seems much simpler to allow everything to be escaped.  In that case, 
multiple escaping would be prohibited, so that:
     %25%32%30
is turned into:
     %20
but not then turned into " " (single blank).  The problem with this, of 
course, is -- how many servers and clients would this break?
======================================================================
Mark Fisher                            Thomson Consumer Electronics
fisherm@tcemail.indy.tce.com           Indianapolis, IN

"Just as you should not underestimate the bandwidth of a station wagon
traveling 65 mph filled with 8mm tapes, you should not overestimate
the bandwidth of FTP by mail."



From connolly@hal.com  Wed Mar 16 23:09:40 1994 --100
Message-Id: <9403162156.AA10729@ulua.hal.com>
Date: Wed, 16 Mar 1994 23:09:40 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Testing URIs for equality 

In message <2D876CE8@MSMAIL.INDY.TCE.COM>, Fisher Mark writes:
>
>Two things:
>1. I really hate to break existing code and/or documents; but
>2. Special cases in code or data are the software engineer's nightmare.
>
>It seems much simpler to allow everything to be escaped.  In that case, 
>multiple escaping would be prohibited, so that:
>     %25%32%30
>is turned into:
>     %20
>but not then turned into " " (single blank).  The problem with this, of 
>course, is -- how many servers and clients would this break?

I think I need more context to completely understand your example.
But if what you're saying is that
	URI_compare("%25%32%30", "%2520") should return TRUE,
then I'm with you -- the first argument just has to be reduced
to canonical form before parsing or whatever...

If, on the other hand, you're saying that
	URI_compare("%25%32%30", "%2520")
should accomplish this by undoing _all_ the escapes and doing:
	strcmp("%20", "%20")
then we've got a problem. It doesn't show up in this case, but
what about

	URI_compare("foo%23xxx", "foo#xxx");

by this suggestion, we end up with
	strcmp("foo#xxx", "foo#xxx")
which returns TRUE, even though the first URI means
	the file "foo#xxx"
and the second means
	the "xxx" fragment of file "foo"
which are different things altogether.
	
Dan



From gavin@jupiter.qub.ac.uk  Thu Mar 17 00:17:52 1994 --100
Message-Id: <9403170716.AA03168@jupiter.qub.ac.uk>
Date: Thu, 17 Mar 1994 00:17:52 --100
From: gavin@jupiter.qub.ac.uk (Gavin Bell)
Subject: Re: The future... etc

Hello,
        I'm newish to the list, but this area is exactly what I'm starting
to research.  I agree with what Rob Raisch's point that unless you know
that the information you seek is out there, you will have difficuly finding
it at best.  My supervisor and I were discussing this yesterday.  The later
point that a skilled researcher does not just do a simple keyword search
for information, but does a refined search which can be iterative is also
important.
A study was done in the Computer Science department here a few years ago
showing that an initial keyword search found approximately 25% of the
actual books present, a later refined search found approx 40%, finally a
librarian was asked to do a comprehensive search of the subject area and
found I think 60%.  The number of books had been hand counted previously so
even a highly trained researcher using a familiar system was unable to get
all resources.  There was a study in the 70's by a guy called Miller which
was similar to this, sorry I can't remember the ref.  I included this as an
example of the problems we face even if we use a multiple keyword indexing
approach.  Of course this relies on the ability of the author to describe
his material in a suitable manner.  At the end of this message I'll put in
my keywords for this text I'm sure that there will be disagreement on them.

        It has already been pointed out that multiple subject based
catalogues is the best way to move forward on this, I agree with this, but
would like to make a few comments.  I work at an interface of knowledge,
having done a degree in Psychology & Computer Science so finding
information relevant to me involves me looking in a multitude of sources,
often returning the same information.  Many other people work at similar
interfaces, infact nobody is really interested in just one subject domain
so wil need to be aware that people will be searching from any domain for
information.
An example
        A philosophy arts student wants information on Medical genetics,
specifically sex selection.  He/she knows little about the specifics of the
subject domain so will not know the correct keywords to search under.  What
is the best way for him/her to start.  A search for (ethics & sex) or
(genetics and ethics).  I actually have had to do this search for a friend
so this is a real world example.  I tried on the BIDS system which is an
ISI citation index.  Eventually I got back articles via a search for ethics
+ gene as a stem.  This example is used to show that people from different
back grounds need to search in different domains.  What is needed in this
case is a means of guiding the search, a system like NetNews where you can
educate the search engine maybe by a like that / not like that system.
Maybe that is a bad example, but you get what I mean I hope.

        To visualise the information held at each site, in addition to
subject based indices, why not use an ISMAP construct as an alternate home
page.  You could have a normal plain html home page with a link to a graph
of the information content of the server.  This graph could be created
automatically from the information index held at each site.  I envisage
each server with its own index, Martijn's .idx file seems a good idea, all
we need is a common format for the data.  There is information on directed
graph creation in Trinity College Dublin under
http://www.dsg.cs.tcd.ie:1969/afc_draft.html

        What I am thinking of is a system whereby you can graphically
browse the net on a subject based method.  This would allow you to
visualise the information on a non geographical level.  The client software
would allow you  to choose what you wanted presented instead of being
presented with a thousand different subjects.  You could eventually create
your own personal view of the net instead of having what is there presented
to you.  Graphical means on their own I realise are not sufficient, but
graphical means to visualise and textual to specify seems a good
compromise.

        A final point two way linking, locally at least would be a good
idea, it took me 10 minutes to find that href above, as it is at the end of
a link.  A link back to the previous page on the server would be good say
<A HREF="http://www.univ.ac.uk/hyperworld/docs/install.html">link</A>
<RETURN="http://www.univ.ac.uk/hyperworld/docs/about.html>
I'm not sure how you would add this to the existing specification, keys and
double clicking on links will not work, but tis a suggestion anyway.

Gavin

Keywords: WWW navigation searching information retrieval visualisation
hypermedia






From M.T.Hamilton@lut.ac.uk  Thu Mar 17 00:24:02 1994 --100
Message-Id: <199403162318.XAA08797@lust.mrrl.lut.ac.uk>
Date: Thu, 17 Mar 1994 00:24:02 --100
From: M.T.Hamilton@lut.ac.uk (Martin Hamilton)
Subject: Stab in the dark

--%#%record%#%
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
Content-Length: 2111      

Hi folks,

The attached code is a very rough sketch of a possible solution to
the URN->UR*->URL scenario which relies on the DNS, HTTP and HTML.
I'm not suggesting this is The Answer as far as URNs are concerned.
An Answer, perhaps? :-)

There is a demo running with the NCSA httpd on

  http://www.mrrl.lut.ac.uk/pickup.html

The theory is that the first script is run under the nph- "rule"
by the Web server, which enables it to send re-directs to the
client (your client must support re-directs :-).

In attempting to resolve a URN of the form

  urn://urn.mrrl.lut.ac.uk/martin/top

it does a DNS TXT lookup on the domain component, e.g.

  urn.mrrl.lut.ac.uk.	86400	TXT	http://www.mrrl.lut.ac.uk/urn

Because I'm lazy, the first http URL returned is taken as the URL
of the server which is willing to perform the URN resolution.  Then
the rest of the URN is appended - in the example this gives us

  http://www.mrrl.lut.ac.uk/urn/martin/top

In the demo /urn is remapped to the second script via a ScriptAlias

  ScriptAlias /urn/ /usr/local/etc/httpd/cgi-bin/stuff/

which does an on-the-fly HTML rendering of an URC template-style
thing, e.g.

  Title: Top level "URN"
  Description: This is my first attempt to play around with the URN+URC
   approach to organizing (meta? ;-)information.  It would probably be
   better if the URC were interpreted by the client!
  URL-v0: http://www.mrrl.lut.ac.uk/
  Title-v0: MRRL Web server
  URL-v1: http://hill.lut.ac.uk/
  Title-v1: Hill's Web server
  URL-v2: gopher://info.lut.ac.uk/
  Title-v2: Computing Services gopher server

becomes what you see in

  http://www.mrrl.lut.ac.uk/urn/martin/top

Should this idea catch on, I presume most of this code would end up in
the WWW client.  Thus, deciding which TXT RR to use, and how the URC
is processed (i.e. more likely as a template/urc than a text/html)
would probably be left up to the client (with hints from the user!).
Adding the client side code (in say Mosaic or Lynx) doesn't look like
it should be a big deal, but I'm sure the client authors will
immediately tell me why it is...

Cheerio,

Martin


--%#%record%#%
Content-Type: text/plain
Content-Name: nph-urn2urc
Content-Length: 783

#!/usr/local/bin/perl -- #-*-Perl-*-

$dig = "/usr/local/bin/dig";
$default = "http://mrrl.lut.ac.uk/quackquackoops.html";

$urn = $ENV{"QUERY_STRING"};
$urn =~ tr/;\`//;
$urn =~ tr/+/ /;
$urn =~ s/%(..)/pack("c",hex($1))/ge;
$urn =~ s/^urn=//;

($host,$resource) = $urn =~ m!^urn://([^/]+)/(.*)$!;

chop(@DIG = `$dig txt $host +pfset=0x2020`);
foreach(@DIG){
    next if /^\s+$/;

    if (/^$host\.\s+TXT\s+(.*)$/) {
	$url = $1;
	$url =~ s/;//g;
	$url =~ s/\`//g;

	if ($url =~ /^http/) {  # simple-minded - pick first http !!
	    print "HTTP/1.0 302 Found\n";
	    print "Location: $url/$resource\n";
	    print "Server: $ENV{\"SERVER_SOFTWARE\"}\n";
	    exit;
	}
    }
}

print "HTTP/1.0 302 Found\n";
print "Location: $default\n";
print "Server: $ENV{\"SERVER_SOFTWARE\"}\n";

--%#%record%#%
Content-Type: text/plain
Content-Name: stuff
Content-Length: 741

#!/usr/local/bin/perl

($user,$path) = $ENV{"PATH_INFO"} =~ m!^/([^/]+)/(.*)$!;
$top = (getpwnam($user))[7];

print "Content-Type: text/html\n\n<html><head>\n";

open(TOP,"$top/.world/$path")||die "Couldn't open $top/.world/$path: $!";
while(<TOP>) {

  if (/^Title:\s*(.*)$/) { 
    print "<title>$1</title></head><body><h2>Title</h2>$1<p>\n";
  }

  if (/^Description:\s*(.*)$/) { 
    print "<h2>Description</h2>$1\n";
  
    while(<TOP>) {
      last unless /^\s/;
      print;
    }

    print "<p><h2>Links</h2><ul>\n";
  }

  if (/^URL-v([^:]+):\s*(.*)$/) {
    $urlv = $2; 
    next;
  }

  if (/^Title-v([^:]+):\s*(.*)$/) {
    print "<li><a href=\"$urlv\">$2</a>\n";
    next;
  }
}
close(TOP);

print "</ul><p></body></html>\n";

--%#%record%#%
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
Content-Length: 18        


--%#%record%#%--



From gavin@jupiter.qub.ac.uk  Thu Mar 17 01:01:42 1994 --100
Message-Id: <9403170758.AA03214@jupiter.qub.ac.uk>
Date: Thu, 17 Mar 1994 01:01:42 --100
From: gavin@jupiter.qub.ac.uk (Gavin Bell)
Subject: academic www research in uk

Hello
I'm just curious who in the UK is doing academic research into www, be it
usage or user modelling, specific httpd or html issues, or the subject
space modelling we just talked about.

I'm posting here as I'll get the developers and researchers rather than the
users who I want to survey later, so if anybody feels that this is junk,
ignore it.

I mentioned in my previous post that I'm doing subject space research,
initially surveying user needs and problems to see if they have anything to
say. I have seen evidence for at least half a dozen other places.  What I
want to know is, is it just isolated people or are there active research
groups in the UK.

Thanks
Gavin


--
Gavin Bell, Graduate Demonstrator & Ph.D Student
School of Psychology, Queen's University, Belfast BT7 1NN.
Phone + 44 232 245133 x4327     www.qub.ac.uk coming RSN.....
gavin@jupiter.qub.ac.uk (mime) gavin.bell@v2.qub.ac.uk (JANET)
-<My opinions are vaguely associated with me and not with QUB>-





From moore@cs.utk.edu  Thu Mar 17 05:18:39 1994 --100
Message-Id: <199403170412.XAA01188@wilma.cs.utk.edu>
Date: Thu, 17 Mar 1994 05:18:39 --100
From: moore@cs.utk.edu (Keith Moore)
Subject: Re: Stab in the dark 


> The attached code is a very rough sketch of a possible solution to
> the URN->UR*->URL scenario which relies on the DNS, HTTP and HTML.
> I'm not suggesting this is The Answer as far as URNs are concerned.
> An Answer, perhaps? :-)

it's okay as a proof-of-concept.  I don't think it's quite ready
for production, yet, though.

> In attempting to resolve a URN of the form
> 
>   urn://urn.mrrl.lut.ac.uk/martin/top
> 
> it does a DNS TXT lookup on the domain component, e.g.
> 
>   urn.mrrl.lut.ac.uk.	86400	TXT	http://www.mrrl.lut.ac.uk/urn
> 
> Because I'm lazy, the first http URL returned is taken as the URL
> of the server which is willing to perform the URN resolution.  

certainly you'd want the client to be able to pick more intelligently
(trying other servers if attempts to use the first one failed, and
perhaps attempting to locate a nearby server first)

> Then
> the rest of the URN is appended - in the example this gives us
> 
>   http://www.mrrl.lut.ac.uk/urn/martin/top

Yikes!! I see no reason that a URN should be constrained to contain
any part of an eventual URL (or likewise, why a URL should have to
contain any part of any of the URNs that might point to it.)
And I can see some very good reasons NOT to impose this constraint.

Keith



From J.P.Knight@lut.ac.uk  Thu Mar 17 12:25:01 1994 --100
Message-Id: <Pine.3.05.9403171136.E28520-b100000@suna>
Date: Thu, 17 Mar 1994 12:25:01 --100
From: J.P.Knight@lut.ac.uk (Jon P. Knight)
Subject: Re: Stab in the dark

On Thu, 17 Mar 1994, Keith Moore wrote:
> > Then
> > the rest of the URN is appended - in the example this gives us
> > 
> >   http://www.mrrl.lut.ac.uk/urn/martin/top
> 
> Yikes!! I see no reason that a URN should be constrained to contain
> any part of an eventual URL (or likewise, why a URL should have to
> contain any part of any of the URNs that might point to it.)
> And I can see some very good reasons NOT to impose this constraint.
> 

Martin showed me this last night and I was under the impression that the
URL <http://www.mrrl.lut.ac.uk/urn/martin/top> wasn't an eventual URL that
the URN was pointing at but was the URL of the URC which contained
multiple URLs which you then followed to get to the resource.  I can't
really see why the system couldn't tack more or less anything from the end
of the URN onto the URL stem returned from the DNS to point to the URC; surely
this would be up to the administrators of that URC to decide?  What's the
good reasons that this won't work?

Jon

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
Jon Knight, Research Student in High Performance Networking and Distributed
Systems in the Department of _Computer_Studies_ at Loughborough University.
* Its not how big your share is, its how much you share that's important. *









From tom@fatty.law.cornell.edu  Thu Mar 17 13:49:29 1994 --100
Message-Id: <9403171245.AA13037@fatty.law.cornell.edu>
Date: Thu, 17 Mar 1994 13:49:29 --100
From: tom@fatty.law.cornell.edu (Thomas R. Bruce)
Subject: Cello v1.01a released


Folks:

The Legal Information Institute is pleased (well, _mildly_ 
pleased) to announce the release of version 1.01a of Cello, an 
Internet browser for MS-Windows. 

The new version is available from ftp.law.cornell.edu in the 
/pub/LII/Cello directory.

1.01a is essentially a maintenance release which repairs a 
number of reported problems, including misbehavior with Gopher+ 
servers and improper handling of text/plain documents in 
certain situations.

For those of you not familiar with Cello:

--------------------------------------------------------------
What is Cello?
--------------------------------------------------------------
Cello is a multipurpose Internet browser which permits you to 
access information from many sources in many formats.  
Technically, it's a WorldWideWeb client application.  This 
means that you can use Cello to access data from WorldWideWeb, 
Gopher, FTP, and CSO/ph/qi servers, as well as X.500 directory 
servers, WAIS servers, HYTELNET, TechInfo, and others through external 
gateways.  You can also use Cello and the WWW-HTML hypertext 
markup standard to build local hypertext systems on LANS, on 
single machines, and so on.  Cello also permits the 
postprocessing of any file for which you've set up an 
association in the Windows File Manager -- for example, if you 
download an uncompressed Microsoft Word file from an FTP site, 
and the appropriate association exists in File Manager, Cello 
will run MS-Word on it for you.  This same capability is used 
to view graphics and listen to sound files you get from the Net.

--------------------------------------------------------------
What you need to run it
--------------------------------------------------------------
To run Cello, you need the files in the CELLO.ZIP archive, plus some 
flavor of Winsock TCP/IP stack -- a piece of "middleware" which 
communicates with the Net.  Cello works with all of the popular 
Winsock packages, although some trouble has been reported with 
certain implementations (see the README in the ZIP archive).  You will, of course, need 
to be directly connected to the network or have access to a 
SLIP or PPP server, depending on which of these your Winsock 
package supports.

Cello runs on any hardware with a 386SX chip or better.  We 
have seen it run with 2Mb RAM (with swapping on) on a 
386/SX-16, but it's not a pretty sight.  We recommend a minimum 
of 4 MB RAM.  You will want to install -- if you haven't already --
a video driver which supports 256-color operation.

Have fun with it,
Tb.

-- 
+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
|  Thomas R. Bruce                           trb2@cornell.edu |
|  Research Associate                                         |
|  Cornell Law School                     Voice: 607-255-1221 |
|  Myron Taylor Hall                        FAX: 607-255-7193 |
|  Ithaca, NY 14853                                           |
+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+



From dsr@hplb.hpl.hp.com  Thu Mar 17 14:15:55 1994 --100
Message-Id: <9403171311.AA13912@dragget.hpl.hp.com>
Date: Thu, 17 Mar 1994 14:15:55 --100
From: dsr@hplb.hpl.hp.com (Dave_Raggett)
Subject: Re: Updated URI test suite; resolving some issues...

> I have been thinking about near-term ways to deploy URNs.

Me too!

If we can agree on a syntax, then the next step is to define
a protocol for client-server name resolution. I like SOLO's
approach, but unfortunately they don't see the need for
hierarchical name spaces, which to me and to the X.500 community
are critical to the ability to scale the service up and up and up ...

Given a simple syntax and protocol we could rapidly deploy a URN->URL
service. I have given some thought on how to manage server to server
queries to permit distributed name resolution, and think a scalable
solution could be developed in a very short space of time.

> I extended the grammar to include relative URIs, and I invented a way
> to merge URNs into the URL namespace while still begin able to tell
> them apart. A URL always looks like:
>        scheme://WORD...
> or
        scheme:/WORD...
> whereas a URN always looks like:
>         scheme:WORD...
> (i.e. no slash)

> So we can begin to deploy things like:
>         message-id:9403161725.AA11467@dragget.hpl.hp.com
>         isbn:IBM/832u9283
>         issn:29o3u7982

Looks good to me.

> Once you've resolved a URN, you can keep that copy forever and use
> it to satisfy other queries for that URN. As to the issues of versioning,
> translation, etc., I'd say that a URNs may identify a set of documents,
> and the versions, translations, etc. are elements of the set.

Yes, but your assumption that you can keep "that copy forever" is wrong.
This depends on other factors, e.g. the expiry date of the document.
You might specify a URN for a journal and ask for the "latest" copy.
This will expire when the next edition is due.

I would like the URN syntax to support an optional set of attribute/values
as a suffix. These act to subset the set of documents identified by the
base URN. One approach for this is to use the existing "?" suffix for
URLs, another is to include the selectors in [ brackets ]. What do
you suggest for this?

> The goal is to deploy the more sophisticated "URCs" or IAFA-templates or
> whatever is a scalable, distributed fashion. In the short term, I'd like to
> be able to compose documents with references like: ...

This seems to be related to the goals behind some of the HyTime addressing
concepts. I think we need to work at this to get a deeper understanding.

Dave Raggett



From robm@ncsa.uiuc.edu  Thu Mar 17 15:28:30 1994 --100
Message-Id: <9403171422.AA09321@void.ncsa.uiuc.edu>
Date: Thu, 17 Mar 1994 15:28:30 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI/1.1 draft

/*
 * Re: CGI/1.1 draft  by George Phillips  (Tel (604)-822-4230)
 *    written on Mar 16,  1:23pm.
 *
 * >I don't agree. I think that with dummy inputs available in forms, we can
 * >finally move away from using PATH_INFO to convey state information to
 * >scripts and go back to using them for their intended purpose: To allow
 * >scripts to access the server's virtual->physical translation and access
 * >authorization for auxillary files. If you're using filenames in PATH_INFO
 * >then you don't have to escape the information, and if you have it as dummy
 * >inputs in a form then your data is already escaped anyway.
 * 
 * I agree that PATH_INFO is not the right place for user input, but
 * PATH_INFO is something generated by the script for use by the script.
 * The server shouldn't be touching it.  It shouldn't even have any
 * idea if % or some other escaping is done on the information there.
 * As long as there are no bad characters in it, it just doesn't matter.

But it does touch it... it has to make PATH_TRANSLATED.

 * I certainly don't agree with your idea of the intended purpose of
 * CGI scripts.  I use them all the time for dynamically translating
 * data into browser-understandable formats (like HTML).  Input
 * forms and searches are just one possible use.
 */

I didn't say anything about the purpose of CGI scripts. I said something
about the intended purpose of PATH_INFO. I use CGI for much more than forms
too, and in the future these other uses will become very important. I just
don't think that having binary data in PATH_INFO is either a good idea or
a necessary action.

--Rob



From browne@cs.utk.edu  Thu Mar 17 16:08:58 1994 --100
Message-Id: <199403171503.KAA25713@pebbles.cs.utk.edu>
Date: Thu, 17 Mar 1994 16:08:58 --100
From: browne@cs.utk.edu (browne@cs.utk.edu)
Subject: Re: Stab in the dark

>On Thu, 17 Mar 1994, Jon Knight wrote:
>> > Then
>> > the rest of the URN is appended - in the example this gives us
>> > 
>> >   http://www.mrrl.lut.ac.uk/urn/martin/top
>> 
>> Yikes!! I see no reason that a URN should be constrained to contain
>> any part of an eventual URL (or likewise, why a URL should have to
>> contain any part of any of the URNs that might point to it.)
>> And I can see some very good reasons NOT to impose this constraint.
>> 

>Martin showed me this last night and I was under the impression that the
>URL <http://www.mrrl.lut.ac.uk/urn/martin/top> wasn't an eventual URL that
>the URN was pointing at but was the URL of the URC which contained
>multiple URLs which you then followed to get to the resource.  I can't
>really see why the system couldn't tack more or less anything from the end
>of the URN onto the URL stem returned from the DNS to point to the URC; surely
>this would be up to the administrators of that URC to decide?  What's the
>good reasons that this won't work?

This was my impression also, or rather that the prefix returned by
DNS would be the address of a URN to URT lookup service (URT as in
<a href="http://www.gatech.edu/urm.paper"> Michael Mealling's URI
paper</a>, and that the opaque string part of the URN would
be tacked onto this address to form the URL for doing the lookup.
Ideally, the URT returned by the lookup would be interpreted by
the client and used to decide (possibly in cooperation with
the user) exactly what files to retrieve.  URTs could also be
cached by the client.  

In the discussion that follows below, I am assuming the URN
syntax described in Weider and Deutsch's 
<a href="ftp://ds.internic.net/internet-drafts/draft-ietf-uri-resource-names-01.txt"> 
Uniform Resource Names</a> Internet draft, i.e.,

  <URN:Encoding_Scheme:Naming_Authority_Scheme:Naming_Authority_ID:opaque_string>

for example,

  <URN:ASCII:IANA:merit.edu:1929642>
or
  <URN:ASCII:IANA:www.mrrl.lut.ac.uk:/martin/top>

I like the idea of using DNS to do the URN to URN-to-URT-lookup-server
lookup.  I like it because it uses an existing namespace to
resolve URN naming authority identifiers, which I think would
be less confusing and faster than developing a new namespace
and implementing another DNS-like distributed database
(although perhaps some would argue it would be more confusing to
use the same namespace for two things).
Is using DNS in this way workable, do you think?
I would be in favor of having the client do the DNS lookup
rather than a server.

************************************************************************
Shirley Browne         Research Associate      107 Ayres Hall
browne@cs.utk.edu      Computer Science Dept.  University of Tennessee
(615) 974-5886         Fax (615) 974-8296      Knoxville, TN 37996-1301
*************************************************************************



From Paul.Wain@brunel.ac.uk  Thu Mar 17 16:57:05 1994 --100
Message-Id: <10262.9403171544@thor.brunel.ac.uk>
Date: Thu, 17 Mar 1994 16:57:05 --100
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: URL lengths

Hi,

I probably shouldnt post this one here but until I remember Lou's
address (oops!) I will since it seems to apply to more than one package
anyway :)

Are there any (known) limits on the lenghts of URLs? Im working on
something right now that needs to encrypt a LOT of information into a
302 redirect and it appears to cause problems if the line gets too
long.

The redirect is somewhat nasty since it needs to contain a LOT of path
information. To see what I mean I guess the best thing to do is look at:

		http://http2.brunel.ac.uk:8080/paul/X.500/

Submit this form without entering a search pattern (a valid request in
the case of the way that the program was designed) and you submit some
data to be resolved into a redirect to:

		http://echo.brunel.ac.uk:4040/

This then returns another form (eventually the 1st form wont be there
but for now this is the way it works) configured from the 1st one.

However:

a) Mosaic wont redirect, and returns the page that is "Forward" in its
history, (only if you have a forward history mind otherwise it does
nothing) and

b) Lynx tries to render the page that has the redirect in it.

c) PC Mosaic barfs at the redirect despite claiming to be HTTP/1.0
compliant :)

I think then that I am running out of space in a buffer somewhere. The
URL is about 270 chars long at this point so Im thinking that somewhere
down the line the clients have a char buffer[256] or something :)

Anyway, ideas would be appreciated. Reasonably sneaky programming has
enabled a work around locally but I dont think it would work if the work
I am doing was done at another site.

For a look at what it is I am doing, (cos I am sure you are all curious
now that I mention it) an alpha1 is located on:

		http://echo.brunel.ac.uk:4040/

and the alpha2 (which is the one causing problems) is through:

		http://http2.brunel.ac.uk:8080/paul/X.500

The 2nd one may not always be avaliable though since it is running under
dbx and has at least one memory leak that is causing problems.

Anyway clues as to if I am breaking a wwwlib or something that the
clients are doing internally for redirects would be nice.

(BTW I did scan the HTTP/1.0 and HTML/1.0 specs and couldnt find any
limits on URL lengths... is that right?)

Many thanks,

Paul
.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From hoymand@joe.uwex.edu  Thu Mar 17 17:35:51 1994 --100
Message-Id: <9403171627.AA16997@joe.uwex.edu>
Date: Thu, 17 Mar 1994 17:35:51 --100
From: hoymand@joe.uwex.edu (Dirk Herr-Hoyman)
Subject: Re: Stab in the dark

At 11:06 AM 3/17/94 +0000, Jon P. Knight wrote:
>On Thu, 17 Mar 1994, Keith Moore wrote:
>> > Then
>> > the rest of the URN is appended - in the example this gives us
>> > 
>> >   http://www.mrrl.lut.ac.uk/urn/martin/top
>> 
>> Yikes!! I see no reason that a URN should be constrained to contain
>> any part of an eventual URL (or likewise, why a URL should have to
>> contain any part of any of the URNs that might point to it.)
>> And I can see some very good reasons NOT to impose this constraint.
>> 
>
>Martin showed me this last night and I was under the impression that the
>URL <http://www.mrrl.lut.ac.uk/urn/martin/top> wasn't an eventual URL that
>the URN was pointing at but was the URL of the URC which contained
>multiple URLs which you then followed to get to the resource.  I can't
>really see why the system couldn't tack more or less anything from the end
>of the URN onto the URL stem returned from the DNS to point to the URC; surely
>this would be up to the administrators of that URC to decide?  What's the
>good reasons that this won't work?
>
This is an interesting way to go.  You could probably use a similar
approach with gopher.  HTTP might provide more features, though, in the use
of MIME headers for carrying some of the details.  

My question would be, is this going to scale?  And, is this better than
using whois++, which is designed to scale?  Martin, didn't I recall you had
a prototype whois++ server?  Any chance of running a whois++ server
side-by-side for comparisons?





From letovsky-stan@CS.YALE.EDU  Thu Mar 17 19:24:40 1994 --100
Message-Id: <199403171820.AA11593@RA.DEPT.CS.YALE.EDU>
Date: Thu, 17 Mar 1994 19:24:40 --100
From: letovsky-stan@CS.YALE.EDU (Stan Letovsky)
Subject: Re: The future of meta-indices/libraries 

Webmeisters:

I have been thinking about the issue of indexing of distributed
information services for some time, and I must say I find the recent
discussion of the issue this past week lacking in ambition, although
no doubt pragmatic, in a low-level, head-in-the-bits sort of way. As
an antidote to this pragmatism I would like to offer a grand,
impractical vision, and some perhaps less unrealistic steps that could
be taken in the direction of that vision.

VISION:

	"A place for everything and everything in its place."

Knowing my question, I should know where to look for the answer,
or in practice, I should be able to follow an algorithm that
will lead me to the answer in time proportional to the log of the
total size of the Web, or to a certain conclusion that the answer
is not in the Web. This type of capability could be called
fact-hashing.

This proposal emphasizes access to answers (facts), not documents.
Documents are what are currently served, and will probably persist for
the forseeable future, but because they package facts in arbitrary
ways they will always be part of the problem, an obstacle to random
fact-access, not the solution. In particular, documents need not have
any internal indexing structure, and there is no limit to redundancy
between documents, which means there are no canonical addresses for
facts (UFLs? :-). A large web may have a lot of information, or it may
have a small amount of information redundantly expressed.  Redunduncy
leads to index inflation -- each query gets many hits -- which makes
information harder to access, not easier. Better one totally relevant
hit than a thousand somewhat relevant hits which then have to be
searched again by hand.

Where can we look for models for a fact-hashing technology, let alone
a distributed one? The discussion of this past week seemed concerned
primarily with reproducing the (failed :-) library technologies of the
past in the Web: subject indexes, title-indexing, keyword-indexing of
documents, indexing of hand-assigned document keywords, etc. This
would make of the Web a distributed on-line library: as good, perhaps,
but no better than, conventional electrically indexed libraries. My
challenge to the Webmasters of the world are these: THAT IS NOT GOOD
ENOUGH! and WE CAN DO BETTER!

The model I view as the starting point for a discussion of
super-indexing is the semantic network, popular in AI research for
representing conceptual information. In these representations the
distinction between indexing and content largely vanishes; the
fraction of the total knowledge devoted to indexing increases to the
point where most of the content of a fact is represented in its
address. The fact that Clyde is an elephant, to use an ancient and
trivial example, is represented simply by locating the Clyde node
under the elephant category in the network. Extensions of this idea
allow complex assertions to made simply by adding small amounts of new
graph-structure to an existing (Web-like?) maze of pointers.  Crucial
to this idea is that information is not added to the system like water
into a bucket; it is placed carefully in finely-discrminated
pigeonholes.

Imagine if the card catalog of the Library of Congress extended
without any discontinuity into the indexes and tables of contents of
all the books, and imagine further that the books were fragmented into
their component individual ideas, and that a great hand came along and
squeezed all the redundancy out of these ideas so that each existed in
a single canonical copy, and you have a picture of the kind of Web
index I would like to see. Another model is the CYC project at MCC,
which is attempting to encode large bodies of commonsense knowledge
in machine-usable semantic networks. Could we make a Web-CYC conceptual
index to all Web-accessible knowledge?

REALITY

OK, so after decades of AI research we still really don't know how to
build semantic networks very well, and we know less about how to use
them as interactive indexes to encyclopedic knowledge bases (although
there is a fair amount of literature on this), and even less
about how to construct distributed networked versions of them
on the Web. Can we take a small step in this direction? Perhaps.
A small step, for me, would be a distributed topic-index that
allowed, at least in principle, for index nodes at arbitrarily
fine granularity. 

A subject index is a coarse-grained partitioning of knowledge, along
the lines of <A
HREF="http://info.cern.ch/hypertext/DataSources/bySubject/Overview.html">WWW
Virtual Library</A>. One problem with this is that it bails out too
early into a list of servers, E.g. I go down to the biology level, and
then I get a list of biology servers, perhaps with some
subcategorization. Some of these servers have internal WAISindexing,
which helps, but I don't know of any subject-domains which have set up
domain-wide WAISindexing, so there is a gap in between index-browsing
and index-searching which I must fill by hand, by iteratively
searching each potential source.  If the sources are not WAISindexed
the situation is worse.

A topic-index, as I use the term (I am not sure if the term has an
official meaning, or if it is the same) is a recursive subject index
that goes down to very fine topic granularities. An upper node might
be "biology", but 10 levels down we might have "regulation of cell
division in eukaryotes", and under that "known mutants" (by species),
"regulatory proteins", etc. At this level a topic index starts to look
like a micro-review article.  Often the best subindex for a topic
takes the form of a diagram: a map of the US with states provides a
geographic subindex; an anatomical diagram, a flow-chart, a metabolic
diagram, etc.  all provide topic breakdowns. Such diagramatic subtopic
indexes could easily be embedded in today's Web, as could simpler
textual ones.

How could topic indexes be maintained?  There are sociological and
technological components to the answer. The sociological answer is
that every topic has a curator (group), and an associated network
host. The amount of responsibility associated with curation could vary
at the curator's discretion, depending on how much effort they wanted
to put into annotating the topic description with diagrams, review
text, etc.  The resource commitment involves serving up a single,
probably not large, html document, and the associated CPU load, which
might be balanced by having duplicate servers for popular topics if
neeed. Other curator responsibilities would include finding a
successor when you quit, if needed, and identifying opportunities to
split the topic into subtopics.

The other main task associated with a topic server is finding links to
documents and subtopics that should be included in the topic document.
This could be done in automatic or semiautomatic ways, such as by
hitchhiking on the indexing mechanisms discussed by earlier
respondents. For example a document could include or have associated
with it a topic-list, which a Web-robot would encounter and notify the
appropriate topic server to include a pointer. The curators may want
to intervene to impose some discretionary judgement at this point, or,
if they don't want to expend the effort, they could let the process
run automatically. Certain rules might givern the behavior of the
indexing robots, e.g. documents are only indexed with the most
specific relevant subtopics; documents cannot be indexed in too many
topics (some idiot wants his picture accessible from all
subtopics...), some topics allow subtopics but not documents, etc. The
total effect would be something like internet bboards, but without the
temporal/sequential bias, and with dynamic reorganization, splitting
of subthreads, erasure of obsolete or irrelevant documents, etc.,
as well as ongoing local maintenance of documents by the owners. A
persistent nested bboard, if you like.

The astute reader should now ask, How is this different from simply
having keywords associated with documents, and having a single
Veronica/ALIWEB-style server gather all the keywords and document
addresses into a single database, which is then searched? Isn't this
actually better, since the user can define particular combinations of
keywords on the fly that may not have been anticipated as a single
topic? The answer is that the crucial flaw with that scheme is that
there is no mechanism for coordination of keyword assignments. You say
tomato and I say tomatoe and our documents are not properly
correlated. Keyword systems, even those based on hand-assigned
keywords, are excessively optimistic about the correlation between
keywords and facts, as an earlier post on reliability statistics
of keyword search pointed out.  To leave out of our indexing scheme
any system for coordination of keywords is to leave a hole big enough
for a bandersnatch to pass through. A topic-curator system would allow
this problem to be partitioned among a responsible community in a
nonburdensome manner.

	The above objection does suggest an alternative approach to
topic indexing, however: that the role of a topic curator is to
maintain an annotated list of keywords to be used to index documents
in that topic area. That would impose some discipline on the indexing
that would improve the usefulness of the index database. If there is a
script that turns the keywords in the curators' (html) keyword into
lists live links that search the index database, you get back to
something very much like the first proposal, with a slightly different
implementation -- central storage of index info rather than
distributed.

I could go on (inverse citation index databases, topic-wide WAIS-indexes...)
but I suspect no one will make it this far...

Let's aim high, gang.

-Stan

http://cgsc.biology.yale.edu/stan.html



From atotic@ncsa.uiuc.edu  Thu Mar 17 19:39:45 1994 --100
Message-Id: <9403171834.AA13838@void.ncsa.uiuc.edu>
Date: Thu, 17 Mar 1994 19:39:45 --100
From: atotic@ncsa.uiuc.edu (Alexsander Totic)
Subject: Re: URL lengths

> Are there any (known) limits on the lenghts of URLs? Im working on
> something right now that needs to encrypt a LOT of information into a
> 302 redirect and it appears to cause problems if the line gets too
> long.
There are no limits on the length of URL in HTML. But in libwww, the MIME
header parsing code seems to limit the length of any field value to 256.
This means that your redirect_url gets truncated. I could not confirm
this, since MacMosaic does not support forms yet. If anyone patches it,
can you mail the patch to me?

Aleks
-- 
Aleksandar Totic        -- MacMosaic developer --          atotic@ncsa.uiuc.edu
Software Development Group      National Center for Supercomputing Applications
           http://www.ncsa.uiuc.edu/SDG/People/atotic/alex.html



From M.T.Hamilton@lut.ac.uk  Thu Mar 17 19:58:57 1994 --100
Message-Id: <199403171854.SAA12448@lust.mrrl.lut.ac.uk>
Date: Thu, 17 Mar 1994 19:58:57 --100
From: M.T.Hamilton@lut.ac.uk (Martin Hamilton)
Subject: Re: The future of meta-indices/libraries

One thing Stan's posting reminded me of - let's say we adopt a
particular library classification scheme to put in resources'
meta-information (pick your favourite :-).  Books can (usually!)
only be on one shelf at a time, but there's no reason why an
on-line resource can't belong to any number of classification
systems.

I think a simple "standardised" classification system using the
meta-info for Internet resources would make it easy to bootstrap
topic curators (or their moral equivalent).  It also conjures up
amusing pictures of intelligent clients which used the
classification info to weed out (say) unwanted hits in search
results - a la Usenet killfiles!

Martin

PS Incidentally, I was thinking about topic == URN :-)




From dkulp@gdb.org  Thu Mar 17 20:23:01 1994 --100
Message-Id: <9403171918.AA06528@dev.gdb.org>
Date: Thu, 17 Mar 1994 20:23:01 --100
From: dkulp@gdb.org (David Kulp)
Subject: Re: The future of meta-indices/libraries

> One thing Stan's posting reminded me of - let's say we adopt a
> particular library classification scheme to put in resources'
> meta-information (pick your favourite :-).  Books can (usually!)
> only be on one shelf at a time, but there's no reason why an
> on-line resource can't belong to any number of classification
> systems.
> 
> I think a simple "standardised" classification system using the
> meta-info for Internet resources would make it easy to bootstrap
> topic curators (or their moral equivalent).  It also conjures up
> amusing pictures of intelligent clients which used the
> classification info to weed out (say) unwanted hits in search
> results - a la Usenet killfiles!
> 
> Martin

I agree with Martin.  Instead of relying on information curators to
organize information in an ad-hoc manner, why not set-up an initial
very detailed full tree and ask information providers to climb
this standard tree, hanging their ornaments on the proper,
accepted branches.  In this sense, that's how "Joel's" index works,
except Joel does all the work decorating the tree.

-david.



From waterbug@epims1.gsfc.nasa.gov  Thu Mar 17 20:33:36 1994 --100
Message-Id: <9403171928.AA13819@epims1>
Date: Thu, 17 Mar 1994 20:33:36 --100
From: waterbug@epims1.gsfc.nasa.gov (Steve Waterbury)
Subject: Re: The future of meta-indices/libraries



Stan Letovsky writes:

> How could topic indexes be maintained?  There are sociological and
> technological components to the answer. The sociological answer is
> that every topic has a curator (group), and an associated network
> host....

This sounds like a good concept, although the actual 
implementation will no doubt be determined by the interplay of 
the sociological and technological components!

> The astute reader should now ask, How is this different from simply
> having keywords associated with documents ...?
> The answer is that the crucial flaw with that scheme is that
> there is no mechanism for coordination of keyword assignments. You say
> tomato and I say tomatoe ....  A topic-curator system would allow
> this problem to be partitioned among a responsible community in a
> nonburdensome manner.

"Curators" of sorts already exist in some areas:  standards groups.  
IEC TC3 is creating an international standard dictionary of "data 
elements" used in the description of electronics, for example.  

The important point is the terms need to be "standardized".  The 
next important point is that context is often critical to terms' 
meanings -- which means there will be needed at _least_ an 
elementary form of "semantic model" -- something like an 
"entity-relationship" model, in which the terms will have their 
proper context, and on which the relationships between topic-servers 
in different domains can be properly understood to enable cross-
domain queries (okay, kind of wild, but you know it will happen ...).  

Anyway, thanks for sharing that vision, Stan.  Great minds rant 
alike!  

Incidentally, I'm still busily implementing my own pet version:  
using non-HTML SGML tags to identify data that needs to be 
indexed in a document, and having special "agents" that would be 
told what sites to go to and pull the info out of documents with 
the tags they are looking for, to be brought back to a local 
database, where the URL's/URN's would be stored along with the 
indexed attribute data, so that local queries could be done and 
the relevant docs summoned from wherever they live.  

If anyone is curious, I have put a real Failure Analysis Report 
into the format I have in mind:

http://epims1.gsfc.nasa.gov/fa/fa_82713.html

Check the HTML source for the SGML meta-data tags that would be 
pulled out (with their instance data) by such an indexing agent.  

This scheme is probably best adapted to engineering/scientific data, 
but might be useful for other forms also. 

BTW, if anyone from Stanford or Lockheed is listening, I would be 
very interested in your thoughts, and whether you have any agent 
software availble or adaptable to this.   


Steve Waterbury
WWW Virtual Library:  Engineering.                                               
                                           oo _\o
                                            \/\ \
                                              /
____________________________________________ oo ____________      
"Sometimes you're the windshield; sometimes you're the bug."



From connolly@hal.com  Thu Mar 17 20:43:31 1994 --100
Message-Id: <9403171931.AA11153@ulua.hal.com>
Date: Thu, 17 Mar 1994 20:43:31 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: The future of meta-indices/libraries 

In message <9403171918.AA06528@dev.gdb.org>, David Kulp writes:
>> I think a simple "standardised" classification system using the
>> meta-info for Internet resources would make it easy to bootstrap
>> topic curators (or their moral equivalent).  It also conjures up
>> amusing pictures of intelligent clients which used the
>> classification info to weed out (say) unwanted hits in search
>> results - a la Usenet killfiles!
>> 
>> Martin
>
>I agree with Martin.  Instead of relying on information curators to
>organize information in an ad-hoc manner, why not set-up an initial
>very detailed full tree and ask information providers to climb
>this standard tree, hanging their ornaments on the proper,
>accepted branches.

The USENET newsgroup hierarchy is handy. It's widely deployed, with
all the right mechanisms for expanding it, etc. The granularity is
perhaps too coarse, but it's there...

Dan



From M.T.Hamilton@lut.ac.uk  Thu Mar 17 20:59:28 1994 --100
Message-Id: <199403171954.TAA12546@lust.mrrl.lut.ac.uk>
Date: Thu, 17 Mar 1994 20:59:28 --100
From: M.T.Hamilton@lut.ac.uk (Martin Hamilton)
Subject: Re: Stab in the dark

Dirk said:

$ This is an interesting way to go.  You could probably use a similar
$ approach with gopher.  HTTP might provide more features, though, in the use
$ of MIME headers for carrying some of the details.  

Indeed.  My demo was, as I'm sure you can tell, hacked up in a couple
of hours.  FWIW, the intention was to demonstrate that a form of URN
lookup is perfectly feasible within the confines of the WWW software,
not to provide a complete implementation!

If we're talking about building in client support, I think HTTP would
be the best place for this info - servers could have built-in support
for converting templates into suitable multipart MIME messages, and
clients would say

  1) Do DNS lookup on the location component of the URN
  2) Pick one of the results according to a sensible algorithm
      e.g. xarchie style domain weightings and DNS preference values
  3) Send the resource identifier portion of the URN to
      the chosen server
  4) Display a suitably rendered version of the response 

Don't forget that URNs could be deployed in the meantime, using some
variation on the scheme I outlined! :-)

$ My question would be, is this going to scale?  And, is this better than
$ using whois++, which is designed to scale?  Martin, didn't I recall you had
$ a prototype whois++ server?  Any chance of running a whois++ server
$ side-by-side for comparisons?

I think it depends what context you have in mind when you ask about
scaling - DNS? Template contents?  There would only be a small
number of extra DNS entries (of the order of one per publisher? :-)
and templates would be cached using the mechanisms currently being
developed for WWW in general.  How about that?

Martin




From phillips@cs.ubc.ca  Thu Mar 17 21:04:45 1994 --100
Message-Id: <7804*phillips@cs.ubc.ca>
Date: Thu, 17 Mar 1994 21:04:45 --100
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: CGI/1.1 draft

Rob said:
>But it does touch it... it has to make PATH_TRANSLATED.

.. and the penny drops

>I didn't say anything about the purpose of CGI scripts. I said something
>about the intended purpose of PATH_INFO. I use CGI for much more than forms
>too, and in the future these other uses will become very important. I just
>don't think that having binary data in PATH_INFO is either a good idea or
>a necessary action.

Sorry, when you said "we can finally move away from using PATH_INFO
to convey state information to scripts and go back to using them for their
intended purpose" I assumed you meant CGI script because "their" didn't
seem to refer to PATH_INFO.

Taking that into account, I see that we have very different ideas
about PATH_INFO.  I gather that you expect it to be a file relative
to the root of your document tree.  This certainly clears up a few
things for me; I never did understand what PATH_TRANSLATED was until
I looked at the httpd source just now.  To me, PATH_INFO is just state
for the script and has nothing to do with the server document tree
(a httpd server specific concept).  For an example of how I typically
use PATH_INFO, see http://www.cs.ubc.ca/nhl  Nothing there maps
to a real file.

>From there, I'm sure you could imagine some kind of CGI script
storing binary data in PATH_INFO.  Certainly the author might
use a more compact encoding like base-64, but if they decide to
use the "standard" %-encoding, they'll be screwed.

I'm not sure we can reconcile our two views, but how about this.
PATH_INFO is just as it appeared from the "GET /url" command
(i.e., no un-escaping) and PATH_TRANSLATED is the unescaped version.
I think that would cover the different uses of PATH_INFO well.



From joe@MIT.EDU  Thu Mar 17 21:19:18 1994 --100
Message-Id: <9403172014.AA20602@theodore-sturgeon.MIT.EDU>
Date: Thu, 17 Mar 1994 21:19:18 --100
From: joe@MIT.EDU (Joseph Wang)
Subject: Re: The future of meta-indices/libraries (gna-p gna-meta-library)


(This message was sent to www-talk mailing list)

The GNA meta-library already has a set of topic codes which anyone who
there is free to use.  The codes are of the form

topic;subtopic;subsubtopic

and are therefore easily extendable.

The meta-library is located at

http://uu-gna.mit.edu:8001/uu-gna/meta-library/index.html

The topic list is located at

http://uu-gna.mit.edu:8001/uu-gna/tech/meta-library/users/topics.html

An explanation of coverage codes and why they are necessary is located at

http://uu-gna.mit.edu:8001/uu-gna/tech/meta-library/users/coverage.html





From dduchier@csi.uottawa.ca  Thu Mar 17 21:40:15 1994 --100
Message-Id: <9403172035.AA20707@csi0.csi.uottawa.ca>
Date: Thu, 17 Mar 1994 21:40:15 --100
From: dduchier@csi.uottawa.ca (Denys Duchier)
Subject: Re: A place for everything and everything in its place

Hi Stan, long time no see.

[This is a reply to Stan Letovsky's message to www-talk.
 I am CC'ing to some people who might be interested]

I am a bit pressed by time, so I'll have to keep it brief.  Part of
the redundancy problem is already addressed by URNs.  Now, I'll
restrict your proposed usage of the term "fact" to only denote a
statement associating a URN with a classification category, or one
associating one category with another.  Possible applications are for
stating that a document is an element of a category, or that category
A is a sub-category of category B, or other forms of such
relationships.  I have used the generic term categories to avoid
stronger connotations, but they could stand for topics, or taxonomic,
meronomic, or in fact arbitrary hierarchies.  Which brings us to the
subject of semantic networks.  I think that you are on the right
track, but that you should also look to the knowledge acquisition
community.  Much work has been done on the representation and sharing
of ontologies (i.e.  Genesereth etal, Gruber, Lenat^H^H^H^H^H :-),
Porter, Skuce, etc...).  I am most familiar with the work of Doug
Skuce and Tim Lethbridge here at the University of Ottawa, and it is
no accident that I used the term "statement" earlier as this is the
basis of their approach.  One of its nice features is that a statement
is a concept just like the property it is about and the subject and
object to which it applies.  All concept items can have meta-concepts
to express knowledge about them: possible applications might be
expiration date, identity of maintainer, editorial judgments and
evaluation, etc.  The problem you raise about the coordination of
"keyword assignment" would also be present, only more so, in a
distributed knowledge representation.  The solution you propose,
namely that of designating curators for parts of the topic hierarchy
does not address the need to maintain concurrently a plurality of
views of the shared knowledge space.  The interpedia project has
proposed the notion of SOAP - Seals Of APproval - that would allow
multiple "Editors" to express their endorsement of articles
contributed to the interpedia.  A similar scheme can be extended to
apply to statements as described above.  By subscribing to a subset of
these seals you would perceive the shared knowledge space as being
structured by a combination of their views.  I cannot elaborate on
these ideas at this time...But I'll be back!

--Denys

PS: connecting knowledge bases to the net is not just a pipe dream; I
have put together a preliminary prototype WWW interface to a Code4
knowledge server which students have been using to explore existing
ontologies through Mosaic.



From montulli@stat1.cc.ukans.edu  Thu Mar 17 22:59:18 1994 --100
Message-Id: <9403172149.AA47568@stat1.cc.ukans.edu>
Date: Thu, 17 Mar 1994 22:59:18 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: URL lengths

> 
> > Are there any (known) limits on the lenghts of URLs? Im working on
> > something right now that needs to encrypt a LOT of information into a
> > 302 redirect and it appears to cause problems if the line gets too
> > long.
> There are no limits on the length of URL in HTML. But in libwww, the MIME
> header parsing code seems to limit the length of any field value to 256.
> This means that your redirect_url gets truncated. I could not confirm
> this, since MacMosaic does not support forms yet. If anyone patches it,
> can you mail the patch to me?

The value field is currently a staticly defined array of characters
with a set limit of 128 characters.  It is defined that way due to
the fact that characters are added one at a time to the value field.
A real fix will require going to a chunck based structure or 
doing multiple StrAllocCats which would seem horribly inefficient.

Since I'm lazy I just increased the VALUE_SIZE to 1024.  That should
handle most of them until we can get a real solution.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *      Let's face it, I'm a nerd, why else would I have a sig file?      *
  **************************************************************************



From connolly@hal.com  Thu Mar 17 22:53:56 1994 --100
Message-Id: <9403172141.AA11217@ulua.hal.com>
Date: Thu, 17 Mar 1994 22:53:56 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Updated URI test suite; resolving some issues... 

In message <9403171311.AA13912@dragget.hpl.hp.com>, Dave_Raggett writes:
>> I have been thinking about near-term ways to deploy URNs.
>
>Me too!
>
>Given a simple syntax and protocol we could rapidly deploy a URN->URL
>service.

Truth be told, I'm not all that interested in the URN->URL mapping
problem right now. I'm more interested in the properties of the
namespace itself.

The example I keep harping on is that I'd like to compose an article
with, for example, a reference to RFC1521, the MIME spec. I should be
able to point at:
	rfc:rfc1521.txt
with perhaps auxiliary pointers to:
	ftp://ds.internic.net/rfc/rfc1521.txt
with the understanding that if the reader has a local copy of this document,
s/he can use it in stead of ftping the file.

>Yes, but your assumption that you can keep "that copy forever" is wrong.

Not in this case: the relation
	(rfc number, octet-stream-of-text)
is in fact a function: once an RFC is published, it never changes.
That rfc number is bound for all time.

The same is true of email message ID's and USENET news article ID's --
once that message goes out on the net, that ID is forever associated
with that stream of octets. (with some exceptions: (1) Received:
headers and such, and (2) message IDs are specified to last about 2
years).

Not even messages with Expires: headers really change -- they just get
superceded.

Now filesystem namespaces don't work this way: the name
	/dir/file
may be bound to one octet string at one moment, and at another the
next. But we need only add a time element to this namespace to make it
work like the above:

	pathname x time -> octet-string

is a function.

And in practice, nobody really means to link to an inode -- they intend
to link to a piece of information that was once stored in that inode.
So it makes more sense to link to:

	http://xhost/yfile;date=19930317092345Z;md5=2l3k4j2lkj423l

i.e. it's a link to "the sequence of octets whose md5 signature
is 2l3k4j2lkj423l and which was retrieved from the yfile on the
xhost http server at 19930317092345Z."

To make sense of all this, we need to think of links not between
amorphous object, but between pieces of information. The unit of
information is the bit; put a bunch of them together, and you've got
an octet string. (of course the octets only make sense when
interpreted in the intended manner, in the assumed context...)

What we do today, with links to
	ftp://host/dir/file
is a heuristic approximation of the above: it says "I found some
info in /dir/file on the ftp server at host. Go look there. If you can
find something there and make sense of it, it's probably still relavent."

To me, the string
	ftp::/host/dir/file
identifies a _set_ of resources -- different elements at different
times. The Date:, Expires:, and If-Modified-Since: headers are steps
in the right direction in the HTTP protocol.

But nobody seems interested in giving authors and/or users control in
this area. Everybody seems to assume all documents are changing all
the time, and that everybody wants the latest version all the time.

I think it would be great if we could write:

	Comments on <A HREF="ftp://host/dir/file;date=19943002323Z">the
	magic cookie draft</A>

so that when the consumer followed the reference, s/he would get
notification if there was a newer version, or if the old one wasn't
available, etc.

Now then... we can't expect to be exact with every query -- we need
some slop for caching and stuff. So it seems that the
user/consumer/client should be able to say:

	"Get any copy of RFC822" (I know they're all the same)
	"Get any copy of mime.faq as of March 15, up to a month later"
		(I know it changes monthly)
	"Get any copy of mime.faq as of today, up to a month earlier"
		(I want the latest copy, and I know it changes
		monthly, so the odds are around 50% that any copy
		less than a month old is the same as the current one)
	"Get the March 15 version of foo.txt"
		(where the server is somehow able to zen the
		expiration period and guess that it's got the current
		one)
	"Get the current version of foo.txt"
		(as above)

this also begs the question of

	"Get any postscript version of RFC822"
	"Get any French version of RFC822"

and such... we need a system that encompases all these axes of the
namespace.

>I would like the URN syntax to support an optional set of attribute/values
>as a suffix. These act to subset the set of documents identified by the
>base URN. One approach for this is to use the existing "?" suffix for
>URLs, another is to include the selectors in [ brackets ]. What do
>you suggest for this?
>

This is included in the grammar and test suite I released:

path : /* void */ { printf("its a directory.\n"); }
        | pathname '/' path
        | pathname
        | params
        ;

params : keyword '=' value
        | keyword '=' value ';' params
        ;

searchStuff: /* void */ | words | params

>> The goal is to deploy the more sophisticated "URCs" or IAFA-templates or
>> whatever is a scalable, distributed fashion. In the short term, I'd like to
>> be able to compose documents with references like: ...
>
>This seems to be related to the goals behind some of the HyTime addressing
>concepts. I think we need to work at this to get a deeper understanding.

Exactly...

Dan



From masinter@parc.xerox.com  Fri Mar 18 00:14:24 1994 --100
Message-Id: <94Mar17.150931pst.2732@golden.parc.xerox.com>
Date: Fri, 18 Mar 1994 00:14:24 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Stab in the dark

Any scheme which embeds a 'location' in a URN is really begging the
question; URNs need to be location independent, and in general, the
lifetime of a URN needs to be longer than the lifetime of the DNS
entry of the original author.

It is possible that using DNS to implement some part of URN -> URL
resolution can be made to work, but not if you tie the ownership of
URN name entry to the ownership of the locations that go with it.

I'm worried a bit about authority in these URN resolution schemes;
you'd like the URN to be secure (in that someone couldn't spoof
urn:/us/government/whitehouse/speeches/clinton/9-10-93/a and supplant
it with some other document).

I don't think we've yet addressed the security aspects of URNs,
although I believe there ARE security requirements.





From omy@San-Jose.ate.slb.com  Fri Mar 18 01:31:53 1994 --100
Message-Id: <9403180026.AA08149@San-Jose.ate.slb.com>
Date: Fri, 18 Mar 1994 01:31:53 --100
From: omy@San-Jose.ate.slb.com (Omy Ronquillo)
Subject: Group Annotations


Hi Webmeisters,

Does anybody know when the Group annotation is planned to be working again?

Or can I set up my own annotation server and have this feature working?

Thanks.
Omy



From robm@ncsa.uiuc.edu  Fri Mar 18 09:32:21 1994 --100
Message-Id: <9403180827.AA01868@void.ncsa.uiuc.edu>
Date: Fri, 18 Mar 1994 09:32:21 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: Re: CGI/1.1 draft

/*
 * Re: CGI/1.1 draft  by George Phillips  (Tel (604)-822-4230)
 *    written on Mar 17, 11:56am.
 *
 * Taking that into account, I see that we have very different ideas
 * about PATH_INFO.  I gather that you expect it to be a file relative
 * to the root of your document tree.  This certainly clears up a few
 * things for me; I never did understand what PATH_TRANSLATED was until
 * I looked at the httpd source just now.  To me, PATH_INFO is just state
 * for the script and has nothing to do with the server document tree
 * (a httpd server specific concept).  For an example of how I typically
 * use PATH_INFO, see http://www.cs.ubc.ca/nhl  Nothing there maps
 * to a real file.

Actually, that's the another intended purpose. Right now, you have
/nhl/93-94/sched/foo mapping to the schedule for foo, which I assume goes
through a master schedule file and pulls out the relevant portions. Now,
what if, six months from now, that master schedule gets way too large and
you can no longer afford to search through it for each query? Now, you can
split it into files and make sched a directory with foo, bar, or the
schedules for whatever team you're using, and remove the script all
transparently.

 * >From there, I'm sure you could imagine some kind of CGI script
 * storing binary data in PATH_INFO.  Certainly the author might
 * use a more compact encoding like base-64, but if they decide to
 * use the "standard" %-encoding, they'll be screwed.

I still draw the line at binary data, sorry. If someone really wants binary
data in there they should encode it a different way.

 * I'm not sure we can reconcile our two views, but how about this.
 * PATH_INFO is just as it appeared from the "GET /url" command
 * (i.e., no un-escaping) and PATH_TRANSLATED is the unescaped version.
 * I think that would cover the different uses of PATH_INFO well.
 */

But we can't do that, it will break existing scripts which may depend on
this behavior. I simply do not want to do that and field the questions of
WHY IS MY SCRIPT BREAKING IT WAS WORKING IN CGI/1.0??? for something which I
do not view as an important issue. 

In addition, future versions of CGI must be backward compatible with old
versions of CGI. This is not just because we felt it was a good idea, it's
because CGI scripts have no way of telling the server what version of CGI
they run under, they can only see after their execution what the CGI
revision the server is compliant with.

--Rob



From Paul.Wain@brunel.ac.uk  Fri Mar 18 15:59:17 1994 --100
Message-Id: <27564.9403181453@thor.brunel.ac.uk>
Date: Fri, 18 Mar 1994 15:59:17 --100
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: HTML+ Spec

Quick question,

Where can I get the HTML+ spec draft from? Postscript preferable, but
text would do :)

Paul
.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From terry@ora.com  Fri Mar 18 18:17:22 1994 --100
Message-Id: <199403181712.AA22830@rock.west.ora.com>
Date: Fri, 18 Mar 1994 18:17:22 --100
From: terry@ora.com (Terry Allen)
Subject: Choosing Subject Keywords for WWW

It would be best to try to constrain the keywords used as a basis
for indexing Web documents.  A well known and tested scheme is the
Library of Congress's set of Subject Headings.  You can browse
these on-line through LOCIS (also available through Marvel, but
still only as a telnet session):
        telnet://locis.loc.gov:23/
then choose 1, then 6.

It would be useful to have a tool that guided people through this
process, prompted for additional desired info (last-modified, etc)
and spit out an HTML HEAD section (or whatever other format turns
out to be appropriate).

For technical subjects not well covered by the LC Subj Heads,
there must be analogous sets of terms developed by librarians
in those subject areas.

Regards,

-- 
Terry Allen  (terry@ora.com)
Editor, Digital Media Group
O'Reilly & Associates, Inc.
Sebastopol, Calif., 95472



From dsr@hplb.hpl.hp.com  Fri Mar 18 19:33:07 1994 --100
Message-Id: <9403181829.AA15939@dragget.hpl.hp.com>
Date: Fri, 18 Mar 1994 19:33:07 --100
From: dsr@hplb.hpl.hp.com (Dave Raggett)
Subject: Re: Stab in the dark

> I'm worried a bit about authority in these URN resolution schemes;
> you'd like the URN to be secure (in that someone couldn't spoof
> urn:/us/government/whitehouse/speeches/clinton/9-10-93/a and supplant
> it with some other document).

Surely, you can include the md5 signature of the document as a parameter
tacked onto the URN, using ";" as a separator as Dan suggested. This way
is safe if the document including the URN is trusted. The actual ownership
of the document can be checked provided it includes a public key signature
and related issuer certificates.

Best wishes,

Dave Raggett,

-----------------------------------------------------------------------------
Hewlett Packard Laboratories,           +44 272 228046
Bristol, England                        dsr@hplb.hpl.hp.com



From masinter@parc.xerox.com  Fri Mar 18 19:45:13 1994 --100
Message-Id: <94Mar18.103646pst.2732@golden.parc.xerox.com>
Date: Fri, 18 Mar 1994 19:45:13 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Stab in the dark

> Surely, you can include the md5 signature of the document as a parameter
> tacked onto the URN, using ";" as a separator as Dan suggested. This way
> is safe if the document including the URN is trusted. The actual ownership
> of the document can be checked provided it includes a public key signature
> and related issuer certificates.

If URNs are allowed to refer to multiple formats of documents, or
multiple versions of updating documents, or online streams of
information that you might telnet to, then no, you can't easily
include the MD5 signature of the document.

What you're saying is that you don't have to trust the URN -> URL
resolution process, because you will verify the entire URN ->
<resource> resolution?

I'm not sure I can live with that, although I'd like to think about it
a bit more.



From connolly@hal.com  Fri Mar 18 20:50:04 1994 --100
Message-Id: <9403181938.AA12004@ulua.hal.com>
Date: Fri, 18 Mar 1994 20:50:04 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Reliable links [Was: Stab in the dark ]

In message <94Mar18.103646pst.2732@golden.parc.xerox.com>, Larry Masinter write
s:
>
>If URNs are allowed to refer to multiple formats of documents, or
>multiple versions of updating documents, or online streams of
>information that you might telnet to, then ...

.. then you can't do anything reliably!!!! Ha Ha Ha!!!  I've been
trying to make this point for TWO YEARS!

There is some value in having names for things that are not defined
as octet streams (fulltext indexes, newsgroups, FTP directories, etc.)
but as it is, we are missing out on the tremendous value of taking
advantage of the multitude of things that _are_ defined as octet
streams: software distributions, documents (once represented in some
format), news articles, email messages, ... ... ...

I suggest that the basic http query:

	GET url

is not reliable. In version HTTP 0.9 (and gopher, incidentally), there
isn't necessarily ANY relationship between the url and the returned
data. At least in HTTP 1.0, there's a status code so the server can
tell you whether it _thinks_ it has answered your query in a sensible
way.

But in either case, you can give the same url twice and there's no
mechanism to guarantee that you'll get the same thing back, and no way
to test to see if you did! Isn't this the basic feature of a
reference, link, or citation?  I write "See page 123 for info on
Widget Co."  with the understanding that when my reader turns to page
123, he'll see the same information I'm talking about.

With paper book publishing, the reference is bound with the target
information at publishing time. But in a distributed system, different
parts of the information base are changing at different times.

I perceive that there is a REQUIREMENT to be able to write reliable
links. The first step is to acknowledge this as a requirement and
define what reliable means. Then we can look into various methods for
various levels of Quality Of Service.

My working definition is that we define a namespace of keys and a
mapping:

	resolve: key -> octet-string

such that it is a function; i.e. if resolve(x) = y and resolve(x) = z,
then y = z. (we can define a superset of this mapping to include the
things that aren't octet-strings...)

If we look at the original definition of the set of keys, i.e.

	key = scheme x string

then we see that it wasn't designed to satisfy the definition of
reliability. For example,

	resolve((http, "//info.cern.ch/default.html"))

has different values at different times. One solution is to extend the
http scheme to include a time. We can be sure, for example, that
	resolve((http, "//info.cern.ch/default.html", March 18 1pm CST))
has only one value.

As for variations on format, language, etc, we can defevelop a syntax
for "the set of format variations of default.html", for example:

	http://info.cern.ch/default.*

but resolve((http, "//info.cern.ch/default.*")) is not well-defined.

The HTTP Accept method does provide reliability to this situation.
The query "get the format-variant of /foo/bar that minimizes the
penalty function with these parameters..." is well-defined.

But I think all URI resolution schemes should address reliability. And
I think perhaps it should be evident from the syntax of a URI --
independent of scheme -- whether or not it identifies a unique. And on
a per-scheme basis, it should be specified how a URI can be resolved
reliably.

Dan




From sanders@BSDI.COM  Fri Mar 18 21:23:02 1994 --100
Message-Id: <199403182020.OAA15469@austin.BSDI.COM>
Date: Fri, 18 Mar 1994 21:23:02 --100
From: sanders@BSDI.COM (Tony Sanders)
Subject: Re: Reliable links [Was: Stab in the dark ] 

"Daniel W. Connolly" writes:
> But in either case, you can give the same url twice and there's no
> mechanism to guarantee that you'll get the same thing back,
This is true with a given URL but note the folowing from the HTTP spec
where it talks about the URI: header:

    However, it is guaranteed that if an object is successfully retrieved
    using that URI it will be to a certain given degree the same object as
    this one.  If the URI is used to refer to a set of variants, then the
    dimensiosn in which the variants may differ must be given with the "vary"
    parameter:

    Syntax          URI: <uri>  [ ; vary = dimension [ , dimension ]* ]
    dimension       content-type[12] | language[13] | version[14]

    If no "vary" parameters are given, then the URI may not return anything
    other than the same bit stream as this object.

    Multiple occurencies of this field give alternative access names or

I think this addresses a lot of the points you made but even more important
it makes it clear that reliable references to bitstreams have been thought
about.  However, *MOST* references should not be reliable in this fashion.
For example, you almost always want a vary=language, vary=content-type.

--sanders



From connolly@hal.com  Fri Mar 18 22:09:34 1994 --100
Message-Id: <9403182054.AA12042@ulua.hal.com>
Date: Fri, 18 Mar 1994 22:09:34 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Reliable links [Was: Stab in the dark ] 

In message <199403182020.OAA15469@austin.BSDI.COM>, Tony Sanders writes:
>"Daniel W. Connolly" writes:
>> But in either case, you can give the same url twice and there's no
>> mechanism to guarantee that you'll get the same thing back,
>This is true with a given URL but note the folowing from the HTTP spec
>where it talks about the URI: header:
>
>    However, it is guaranteed that if an object is successfully retrieved
>    using that URI it will be to a certain given degree the same object as
>    this one.  If the URI is used to refer to a set of variants, then the
>    dimensiosn in which the variants may differ must be given with the "vary"
>    parameter:
>
>    Syntax          URI: <uri>  [ ; vary = dimension [ , dimension ]* ]
>    dimension       content-type[12] | language[13] | version[14]
>
>    If no "vary" parameters are given, then the URI may not return anything
>    other than the same bit stream as this object.
>
>    Multiple occurencies of this field give alternative access names or
>
>I think this addresses a lot of the points you made but even more important
>it makes it clear that reliable references to bitstreams have been thought
>about.  However, *MOST* references should not be reliable in this fashion.
>For example, you almost always want a vary=language, vary=content-type.

Ah! So the issue has been addressed somewhere... but (1) the scope of
this mechanism is only HTTP -- I can't make reliable links to FTP
files, and (2) shouldn't the URI: header tell where this document is
on the various dimenstions so that I can retrieve it again?

For example, suppose I ask:

	GET: /foo/bar

and the server says:

	HTTP/1.0 200 Message follows
	URI: http://host/foo/bar ; vary=version

How do I make a reference to this document? (or what do I scribble in
my cache to uniquely identify this doc?) It needs to say something like:

	URI: http://host/foo/bar ; vary=version=1.0

so that I can write

	<A HREF="http://host/foo/bar" VERSION="1.0">

I hashed this over with a friend last night, and we talked a lot about
what it would take to migrate documents around the net something like
NNTP broadcasting or IP routing tables and such. We decided there
wasn't a clear scalable strategy, but for the case, we came up with
a workable solution. The GET request should say something like:

	"Give me any copy of /foo/bar dated March 1 thru March30"

and proxy servers keep an "lifetime" for each document in the cache.
Some documents, FAQ postings for example, explicitly contain  the
lifetime. The NCSA folks worked out a set of heuristics for other
types of documents.

Then, when the proxy server gets a "GET(doc, t0, t1)" request, it
looks up doc in its cache, and if the lifetime intersects [t0, t1],
the query is resolved. Else, it turns around and makes the request to
the original server (or some neighbors or some such...).

This generalizes fairly well... things like CGI script results should
have very short lifetimes. RFC documents should have very long
lifetimes.

For the content-type dimension, the format negociation algorithm in
HTTP works pretty well...

But in all these cases, I'd like to be able to put version, format,
language, etc. info in the reference itself, if I choose. For example,
I may know that
	ftp://foo.com/lksjfli4jlij43
is a postscript file. But there's currently no way to express this.
And Mosaic, for example, will assume it's a plain text file.

Dan



From terry@ora.com  Fri Mar 18 23:09:00 1994 --100
Message-Id: <199403182205.AA04691@rock.west.ora.com>
Date: Fri, 18 Mar 1994 23:09:00 --100
From: terry@ora.com (Terry Allen)
Subject: Stab at URNs in the dark

| Return-Path: <www-talk@www0.cern.ch>
| Date: Fri, 18 Mar 1994 20:49:16 --100
| Message-Id: <9403181938.AA12004@ulua.hal.com>
| From: "Daniel W. Connolly" <connolly@hal.com>
| Subject: Reliable links [Was: Stab in the dark ]
| 
| . . . Larry Masinter writes:
| >
| >If URNs are allowed to refer to multiple formats of documents, or
| >multiple versions of updating documents, or online streams of
| >information that you might telnet to, then ...
| 
| .. then you can't do anything reliably!!!! Ha Ha Ha!!!  I've been
| trying to make this point for TWO YEARS!

We need something to do exactly these things, though.  If not
URNs then what, Dan?  (you write of URIs later on)

| There is some value in having names for things that are not defined
| as octet streams (fulltext indexes, newsgroups, FTP directories, etc.)
| but as it is, we are missing out on the tremendous value of taking
| advantage of the multitude of things that _are_ defined as octet
| streams: software distributions, documents (once represented in some
| format), news articles, email messages, ... ... ...

These are the trivial cases:  no versions, no variety of languages,
no variety of formats.  Solve the harder cases and the solution to
these falls out.

[...]
| But in either case, you can give the same url twice and there's no
| mechanism to guarantee that you'll get the same thing back, and no way
| to test to see if you did! 

Sure there is.  Save it the first time, or save its digital signature.
If digital signatures of documents were disseminated publicly 
at the time of publication, one could even authenticate a doc 
from an untrusted, even untrustworthy source.  Only some such
mechanism will replace the binding of reference with target you
mention below:

| Isn't this the basic feature of a
| reference, link, or citation?  I write "See page 123 for info on
| Widget Co."  with the understanding that when my reader turns to page
| 123, he'll see the same information I'm talking about.
| With paper book publishing, the reference is bound with the target
| information at publishing time. But in a distributed system, different
| parts of the information base are changing at different times.

That is the case when documents are being revised continuously *and*
previous recensions are not being archived.  But I can also have
the same info available in a variety of renderings, all using
the same reference scheme (the page numbers of a reference 
format, if desired).  Now what am I going to use to call that collection,
or the archive of all versions of a doc throughout time, if I bother
to save them all?  and what information does the URN > URL resolver 
need to distinguish among versions, formats, and other possible 
variations *of the same literary work*?  (Sameness being determined initially 
by the publisher and maybe revised a bit by librarians and archivists.)

If you supply a URN that doesn't point to a unique thing, then
you'll either get a list of URLs back or you'll have to supply
extra info alongside the URN (for the URN designating the Los
Angeles Times, give me the latest city edition, sports section).
Right?




-- 
Terry Allen  (terry@ora.com)
Editor, Digital Media Group
O'Reilly & Associates, Inc.
Sebastopol, Calif., 95472



From joe@MIT.EDU  Fri Mar 18 23:58:07 1994 --100
Message-Id: <9403182255.AA25916@theodore-sturgeon.MIT.EDU>
Date: Fri, 18 Mar 1994 23:58:07 --100
From: joe@MIT.EDU (joe@MIT.EDU)
Subject: Announcement of experimental online internet class (gna-p gna-internet-course)


The Globewide Network Academy wishes to announce that it is currently
constructing an online class about how to use the internet.  The class
consists of a hypertextbook which is being written, and a consultants
room where users can ask questions about the internet in real time.

The draft bootstrap document is enclosed.  This document is intended to 
provide the minimum amount of information needed to allow a new user to
access the class.

While parts of the class are currently operational, please realize
this all of this is experimental and that there will be "bugs" and
difficulties in conducting the course.  This class is a prototype and
the lessons learned from conducting this class will be applied to
future online class offerings by the Globewide Network Academy.
Comments and criticisms about the class are welcome.  (In particular,
I'm wondering if the bootstrap document is really all the information
needed to access the course).

Bootstrap document follows

                                Introduction to the Internet Bootstrap Document
     INTRODUCTION TO THE INTERNET BOOTSTRAP DOCUMENT (UNDER CONSTRUCTION)
                                       
Purpose

   The purpose of this document is to provide the minimum information needed to
   access the GNA Introduction to the Internet Class.
   
What is the World Wide Web?

   The World Wide Web [WWW] is a distributed HyperText system (a network of
   documents connected by links which can be activated electronically).  The
   Introduction to the Internet class notes are located on WWW.
   
How do I get to the class notes?

   Documents on WWW each have an address which is known as a universal resource
   locator (or URL) and can be read on local machines using a program known as
   a client.  In order to get to the class notes ask your local  system manager
   about the commands needed to use WWW and to the address
   

http://uu-gna.mit.edu:8001/uu-gna/text/internet/index.html

If that fails.......

   If this fails, try to execute the following command to enter WWW:
   

telnet info.cern.ch

   Once you are connected to the Web, enter the following command
   

go http://uu-gna.mit.edu:8001/uu-gna/text/internet/index.html

I'm still lost.......

   If all else fails, please e-mail the class coordinator (joe@mit.edu) with
   your problem.




From terry@ora.com  Sat Mar 19 00:07:01 1994 --100
Message-Id: <199403182303.AA06332@rock.west.ora.com>
Date: Sat, 19 Mar 1994 00:07:01 --100
From: terry@ora.com (Terry Allen)
Subject: Problem FTP site

I've found that of www, lynx, and Mosaic, only lynx works for
me in retrieving files from the comp.text.sgml archive at Oslo.
        ftp://ftp.ifi.uio.no/pub/SGML/comp.text.sgml/by.date/
gives a directory of subdirectories, one per day, under which
are the actual archived messages.  Both www and Mosaic refuse
to retrieve (or are refused) the message files; lynx does fine,
as does plain old ftp.

Can anyone say why?  does it matter that the message filenames
include colons?

Regards,

-- 
Terry Allen  (terry@ora.com)
Editor, Digital Media Group
O'Reilly & Associates, Inc.
Sebastopol, Calif., 95472



From stripes@uunet.uu.net  Sat Mar 19 07:33:16 1994 --100
Message-Id: <9403190624.AA18301@rodan.UU.NET>
Date: Sat, 19 Mar 1994 07:33:16 --100
From: stripes@uunet.uu.net (Josh Osborne)
Subject: Re: Reliable links [Was: Stab in the dark ]

[...disscussion of how to deal with links to data that change over time...]
>For the content-type dimension, the format negociation algorithm in
>HTTP works pretty well...
>
>But in all these cases, I'd like to be able to put version, format,
>language, etc. info in the reference itself, if I choose. For example,
>I may know that
>	ftp://foo.com/lksjfli4jlij43
>is a postscript file. But there's currently no way to express this.
>And Mosaic, for example, will assume it's a plain text file.


I don't think we want to tie a URI to a format, version or language.

If a document has a link to http://foo.com/lksjfli4jlij43, which is
some sort of picture and I am at a color workstation with a fast network 
link I want to retrive whatever image looks best in color, regardless of
(data) size.  If I am at a workstation with a slow link, and a monochrome
display, I want the smallest image that looks good in monochrome.

Both pictures should show me the same object.  Both may be diffrent
bits.

Also http://foo.com/dr.fun/most_recent.jpg, should not be constained to
pointing at one image, it should be free to change from day to day.


If you want to cache at the application level, you can assume whatever
object you fetched last is still valid, unless the user changed some
defualt or other that might effect things ('color/monochrome').  You
should also timeout data when it expires (if the protocall has a TTL or
expire date like HTTP), or after some fixed length of time (a few hours).
If you are doing caching _outside_ the application (like a gateway) you 
need to understand enough of the protocall being used to figure out what
version of the bits is being fetched, and either handle it from the cache,
or fetch it from the real source.
-- 
Not speaking for UUNET Technolgies



From garylang@netcom.com  Sat Mar 19 21:32:24 1994 --100
Message-Id: <Pine.3.85.9403191215.A28797-0100000@netcom2>
Date: Sat, 19 Mar 1994 21:32:24 --100
From: garylang@netcom.com (Gary Lang)
Subject: Re: HTML+ Spec

Please let me know when and if you find this. 

Thanks,

-g


Gary Lang 
Email:garylang@netcom.COM





From joe@MIT.EDU  Sun Mar 20 11:17:44 1994 --100
Message-Id: <9403201010.AA25602@theodore-sturgeon.MIT.EDU>
Date: Sun, 20 Mar 1994 11:17:44 --100
From: joe@MIT.EDU (Joseph Wang)
Subject: TkWWW 0.11 prerelease ok or not?


Has anyone out there gotten the prerelease of tkWWW 0.11 to work?
Should I assume from the lack of bug reports that 1) it works o.k. or
2) no one has gotten it to work and everyone has given up?





From peter@wonderland.org  Sun Mar 20 14:10:12 1994 --100
Message-Id: <199403201226.MAA05149@wonderland.org>
Date: Sun, 20 Mar 1994 14:10:12 --100
From: peter@wonderland.org (Peter Galbavy)
Subject: Re: TkWWW 0.11 prerelease ok or not?

> Has anyone out there gotten the prerelease of tkWWW 0.11 to work?
> Should I assume from the lack of bug reports that 1) it works o.k. or
> 2) no one has gotten it to work and everyone has given up?

works fine for me under NetBSD-current. Except for a minor buglet:

In the save window, clicking on a symlink pointing to a directory
saves a file in the destination directory with the name "link@",
where "link" is the original name of the link, instead of chdir'ing
into that directory.

-- 
Peter Galbavy				e-mail: P.Galbavy@wonderland.org
Wonderland				  work: peterg@demon.co.uk (soon)

	I like my food, I am what I eat, therefore I like myself



From luotonen@ptsun00.cern.ch  Sun Mar 20 22:03:19 1994 --100
Message-Id: <9403202101.AA29931@ptsun03.cern.ch>
Date: Sun, 20 Mar 1994 22:03:19 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: If-Modified-Since enhancement


We have currently spec'd the conditional GET to work as follows:

	GET /what/ever HTTP/1.0
	If-Modified-Since: Wednesday, 16-Mar-94 10:57:20 GMT

If /what/ever has been modified (or server doesn't support
If-Modified-Since: header) the doc is sent back normally.
However, if the doc is unchanged, server replies:

	HTTP/1.0 304 Not modified

I suggest that the rest of this response would be as if the client
(proxy) had sent a HEAD request -- this way the server can upload
a new expiry date (and other meta info) to proxy:

	HTTP/1.0 304 Not modified
	Expires: Sunday, 27-Mar-94 12:00:00 GMT
	...

-- Cheers, Ari --




From stumpf@informatik.tu-muenchen.de  Sun Mar 20 23:53:46 1994 --100
Message-Id: <2mik1c$12q@hpsystem1.informatik.tu-muenchen.de>
Date: Sun, 20 Mar 1994 23:53:46 --100
From: stumpf@informatik.tu-muenchen.de (Markus Stumpf)
Subject: Re: If-Modified-Since enhancement

luotonen@ptsun00.cern.ch (Ari Luotonen) writes:
>I suggest that the rest of this response would be as if the client
>(proxy) had sent a HEAD request -- this way the server can upload
>a new expiry date (and other meta info) to proxy:
>	HTTP/1.0 304 Not modified
>	Expires: Sunday, 27-Mar-94 12:00:00 GMT
>	...

I'd like to propose some more infos:

I am currently working on a caching/proxy only server.
This would allow configuring of "forwarders", i.e. intermediate
servers for a hierarchy of caching servers, read only access to caches
and so on ...
Therefor it'll be really nice to have some information about the update
rate of the caching servers like:
"File is up to date for me but I haven't checked for n minutes".

Another problem for this topic:
How do I specify Expires: for e.g. GIF files? Would I have to do this
via server config files?

We currently use a "Expires:" of about 14 days for GIFs, assuming
most of them are icons, which change seldon, if not never.
But with all the weather GIFs and coffee machines :) out there ...

	\Maex

P.S. If you have any proposals what caching servers should be able to do,
     drop me a note ...



From dsr@hplb.hpl.hp.com  Mon Mar 21 11:10:29 1994 --100
Message-Id: <9403211007.AA20219@dragget.hpl.hp.com>
Date: Mon, 21 Mar 1994 11:10:29 --100
From: dsr@hplb.hpl.hp.com (Dave Raggett)
Subject: Re: HTML+ Spec

Gary Lang writes:

> Please let me know when and if you find this. 

The internet draft for HTML+ can be found at:

        ftp://15.254.100.100/pub/draft-raggett-www-html-00.ps

The spec has continued to evolve in response to a review last
November and discussions on the www-talk. The upto date document
type definition can be found at:

        ftp://15.254.100.100/pub/htmlplus.dtd.txt

HTML+ will be reviewed at the WWW Conference this May and I will
be illustrating the talk with an X11 HTML+ browser. A revised version
of the internet draft will be published after the conference.

--
Best wishes,

Dave Raggett,

-----------------------------------------------------------------------------
Hewlett Packard Laboratories,           +44 272 228046
Bristol, England                        dsr@hplb.hpl.hp.com



From G.Joly@cs.ucl.ac.uk  Mon Mar 21 11:49:26 1994 --100
Message-Id: <9403211037.AA19467@dxmint.cern.ch>
Date: Mon, 21 Mar 1994 11:49:26 --100
From: G.Joly@cs.ucl.ac.uk (Gordon Joly)
Subject: X500 gateway sought for Solaris.


I am aware of http://x500.tu-chemnitz.de:8888/

Could somebody tell which X500 to WWW gateway would be suitable to
install? I am running hhtpd 1.1 under Solaris 2.3

Gordon Joly         Phone +44 71 380 7934       FAX +44 71 387 1397
Email: G.Joly@cs.ucl.ac.uk    UUCP: ...!{uunet,uknet}!ucl-cs!G.Joly
Comp Sci, University College, London, Gower Street, LONDON WC1E 6BT
XXX YYY WWW & http://www.cs.ucl.ac.uk/mice/gjoly.html & WWW YYY XXX




From reinpost@info.win.tue.nl  Mon Mar 21 11:55:02 1994 --100
Message-Id: <199403211052.LAA11804@wsinis10.info.win.tue.nl>
Date: Mon, 21 Mar 1994 11:55:02 --100
From: reinpost@info.win.tue.nl (Reinier Post)
Subject: forwarding cache requests

You (Markus Stumpf) write:

>I'd like to propose some more infos:
>
>I am currently working on a caching/proxy only server.
>This would allow configuring of "forwarders", i.e. intermediate
>servers for a hierarchy of caching servers, read only access to caches
>and so on ...
>Therefor it'll be really nice to have some information about the update
>rate of the caching servers like:
>"File is up to date for me but I haven't checked for n minutes".
>
>Another problem for this topic:
>How do I specify Expires: for e.g. GIF files? Would I have to do this
>via server config files?
>
>We currently use a "Expires:" of about 14 days for GIFs, assuming
>most of them are icons, which change seldon, if not never.
>But with all the weather GIFs and coffee machines :) out there ...
>
>	\Maex
>
>P.S. If you have any proposals what caching servers should be able to do,
>     drop me a note ...

In my cache server ('Lagoon'), few of the provisions in the HTTP MIME header
specifications are currently implemented, but I have already noticed the
need for more headers.  To send information on the cache status of a document
to the client, I leave the headers obtained from the remote source intact,
and consider adding the following ones:

Cache-date: <date>
Cache-last-refreshed: <date>
Cache-last-modified: <date>
Cache-via: <url> [, <url>]*

This provides: the time the document was served from the cache in answer
to the present request, the time the document was last fetched into the
cache, the time it was last fetched and found to be different from the
previous version, and the sequence of URLs by which the document was
subsequently fetched.  This last header, which provides a comma separated
list of URLs, is required in order for cache servers to break loops in chains
of forwarded requests.  (Lagoon 0.11a now supports such forwarding, but it
doesn't check for loops yet.)  The last URL in this sequence is always
the URL of the present request (with the '#name' relative anchor suffix
removed).  All headers are completely optional, of course.

This only addresses the cache specific information added to the HTTP response,
but there is a similar design problem with adding cache information to the
request.  For example, the User-agent: and From: headers, are they supposed to
be as sent by the client, or are they supposed to identify the cache mechanism
sending the actual request?

Before I implement anything that's out of line, dear incrowd, please give
me some comments or suggestions for improvement.

-- 
Reinier Post						 reinpost@win.tue.nl
a.k.a. <A HREF="http://www.win.tue.nl/win/cs/is/reinpost/reinier.E.html">me</A>



From koblas@netcom.com  Mon Mar 21 12:03:17 1994 --100
Message-Id: <199403211102.DAA11776@mail.netcom.com>
Date: Mon, 21 Mar 1994 12:03:17 --100
From: koblas@netcom.com (David Koblas)
Subject: How do you maintain image maps?


I recently got around to reading the NCSA "document" on how to setup
clickable image maps.  Now the big question is how many people use
this method (XPaint & XV) for creation maintinace of your image maps, or
what do you use?

What features in a tool would you like to see to make this task easier?

--koblas@netcom.com



From P.Barker@cs.ucl.ac.uk  Mon Mar 21 12:26:41 1994 --100
Message-Id: <9403211123.AA00372@dxmint.cern.ch>
Date: Mon, 21 Mar 1994 12:26:41 --100
From: P.Barker@cs.ucl.ac.uk (Paul Barker)
Subject: Re: X500 gateway sought for Solaris.

What do you want - a pointer to some code?

I can also offer a pointer to another experimental gateway.

http://echo.brunel.ac.uk:4040/

Paul



From luotonen@ptsun00.cern.ch  Mon Mar 21 12:37:53 1994 --100
Message-Id: <9403211136.AA00716@ptsun03.cern.ch>
Date: Mon, 21 Mar 1994 12:37:53 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: forwarding cache requests


> For example, the User-agent: and From: headers, are they supposed to
> be as sent by the client, or are they supposed to identify the cache mechanism
> sending the actual request?

cern_httpd appends "via gateway CERN-HTTPD ..." at the end of User-Agent:.

-- Cheers, Ari --




From vamp@csulb.edu  Mon Mar 21 13:39:14 1994 --100
Message-Id: <Pine.3.88.9403210428.A13534-0100000@wren.acs.csulb.edu>
Date: Mon, 21 Mar 1994 13:39:14 --100
From: vamp@csulb.edu (VampLestat)
Subject: Re: How do you maintain image maps?

> I recently got around to reading the NCSA "document" on how to setup
> clickable image maps.  Now the big question is how many people use
> this method (XPaint & XV) for creation maintinace of your image maps, or
> what do you use?

Since most of the clients here are MacMosaic, most of the work is done on 
a Macintosh using Adobe Photoshop for both creation and for getting the 
coordinates.  We have very few X users here.

> What features in a tool would you like to see to make this task easier?

A simple tool to create and save the polygon data might be nice.  You 
could simply click on points and it would leave a trail and then we 
finished output the data.  Of an X verion would be the obvious choice 
now, but probably wouldnt do *me* much good.

_O_  Ryan L. Watkins                                     vamp@csulb.edu
 |   Academic Computing Services Cal State Long Beach - Network Support
 |   pgp key available via 'finger vamp@beach.csulb.edu' or key server





From M.T.Hamilton@lut.ac.uk  Mon Mar 21 14:44:57 1994 --100
Message-Id: <199403211342.NAA18459@lust.mrrl.lut.ac.uk>
Date: Mon, 21 Mar 1994 14:44:57 --100
From: M.T.Hamilton@lut.ac.uk (Martin Hamilton)
Subject: Re: X500 gateway sought for Solaris.

Hi Gordon, you said:

$ I am aware of http://x500.tu-chemnitz.de:8888/
$ 
$ Could somebody tell which X500 to WWW gateway would be suitable to
$ install? I am running hhtpd 1.1 under Solaris 2.3

FWIW, source for the above can be found at

  ftp://isode.tu-chemnitz.de/pub/other-src/

No experience running it under the dreaded Solaris though! :-)

Cheers,

Martin




From Paul.Wain@brunel.ac.uk  Mon Mar 21 15:21:44 1994 --100
Message-Id: <6297.9403211418@thor.brunel.ac.uk>
Date: Mon, 21 Mar 1994 15:21:44 --100
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Re: X500 gateway sought for Solaris.

@ Paul Barker says:
@ 
@ PaulB> What do you want - a pointer to some code?
@ PaulB> I can also offer a pointer to another experimental gateway.
@ PaulB> http://echo.brunel.ac.uk:4040/
@ 
@ Could you supply the X500 -> WWW interface?

At the moment the code is still very much in the development stage
and new/important features are still being added to it all the time.
Currently it is able to do the following:

	o UFN lookup
	o DN lookup (not really a feature but needed)
	o Uses 302 redirects to point a UFN search to the correct entry
	o Uses the Brunel DNTree code to list multiple hits
	o Browse below entry is almost finish
	o Browse on a level with viewed entry pending
	o Path configuration is now working. To see this enter via
		http://http2.brunel.ac.uk:8080/paul/X.500/
	  The configuration there sets it up as if you were using a
	  client with its Local DIT set as Brunel.
	o ``Hyperlinked'' "See Also"s
	o ``Hyperlinked'' RFC822 mailaddresses.
	o Uses Brunel's X.500 Query Engine. As far as we know this is
	  the only X.500/WWW gateway with this capability :)

The interface is working reasonably smoothly now, although it still
doesnt support images or sound at the moment. Hopefully it will do soon
one I get a URL naming scheme sorted out for them. In addition to this
the presentation needs sorting out. Any comments? Is the current output
okay?

I cant comment on the avaliability of the code since my boss is away at
the moment. He will be back next Monday so I dont feel as if I can make a
coment until he returns. In the mean time I hope that the above
description is a help. 

@ NB: Solaris 2.3

I  did try it under Solaris 2.3 in BC mode last week, and it seemed to
work quite well. The problem is however that I dont have ELF based ISODE
and DSAP libraries, (or for that matter a Solaris version of the Brunel
Query Engine), so I cant comment on how well it would work compiling
under Solaris.

Paul

.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From rst@ai.mit.edu  Mon Mar 21 16:11:00 1994 --100
Message-Id: <9403211507.AA01703@volterra>
Date: Mon, 21 Mar 1994 16:11:00 --100
From: rst@ai.mit.edu (Robert S. Thau)
Subject: Future of meta-indices: site indexing proposal and Perl script

In the latest spin of the Archie-for-the-Web discussion, there seems to be
a consensus (at least among the discussants) that having individual sites
provide an index for their contents, along the lines of the IAFA templates
used by Martijn Koster's ALIWeb, is a good way for such a service to work.

Now, harvesting the titles and URIs from an existing site is just a SMOP.
(It took me about an hour to write the Perl code to do it, starting from my
(NCSA) server config files --- pointer to the code below).  However,
Martijn's recommendations for ALIWeb strongly recommend "Keywords" and
"Description" fields in the templates as well.  No automated script is
likely to do a really good job writing Description fields in the reasonably
near term, and even automatically choosing appropriate keywords from among
a preselected base set is a non-trivial problem.

What I'm doing at least provisionally to drive the final version of my Perl
script (site-index.pl v0.1, pointer to code below), is to put the source
for the IAFA descriptions and keywords fields inside the document itself,
by (ab?)use of the <META ...> tag which was discussed on this list some
time ago to solve a different problem.  For example, the document at
http://www.ai.mit.edu/events/events-list.html contains, near the top, the
following:

  <meta name="iafa-description"
  value="MIT AI lab events, including seminars, conferences, and tours">
  <meta name="iafa-keywords"
  value="MIT, Artificial Intelligence, seminar, conference">

(There's one other kind of meta-information my indexer uses --- if it sees
<meta name="iafa-type" value="service">, it indexes the page in question
with a SERVICE template, as opposed to a DOCUMENT template.  This is useful
for cover pages of search engines and the like).

This use of <META ...> solves another problem as well, that of determining
which documents make the index.  Files with the <meta name="iafa-...">
fields get indexed; the rest don't.  So, once these tags are in the
documents, the rest of IAFA template preparation (finding the files,
getting the titles out and the URIs right) can be completely automated
(which is effectively what my site-index.pl script does).

However, the <META ...> tags do raise another problem, that of whether this
use of <META ...> is appropriate, and if it is, making sure that the uses
which different tools may eventually make of meta-information don't
conflict.  A central registry of meta-information names would be a good
idea, if people are going to start using it.

BTW, the Perl code I'm using to build my site index from the <meta ...>
tags (site-index.pl v0.1) is available at

  http://www.ai.mit.edu/xperimental/site-index.pl

N.B. the script knows the structure of NCSA httpd site configuration files,
which it reads to find out which directories (and their subdirectories) to
index; it would have to be modified to work with the configuration files of
another server, but that shouldn't be hard.  A sample of the output is,
naturally, at

  http://www.ai.mit.edu/site.idx

For the moment, the script has to be configured by changing some variables
at the top of it, as described in the comments; cleaner configuration and
documentation will be there eventually.

Incidentally, even if you don't want to deal with the <meta ...> tags,
site-index.pl can still be some use; setting the $require_meta variable to
0 in the prolog will get you a draft site.idx with entries for every HTML
document you have with so much as a title.  The result is probably not
suitable for direct submission to an indexing service, but culling the
inappropriate files and filling in the blanks is probably a better way to
construct a useful site.idx than typing the whole thing in from scratch.

rst



From chiles@cherokee.nsuok.edu  Mon Mar 21 17:27:32 1994 --100
Message-Id: <9403211625.AA12875@dxmint.cern.ch>
Date: Mon, 21 Mar 1994 17:27:32 --100
From: chiles@cherokee.nsuok.edu (Bill Chiles)
Subject: Token rings?

I am new to the world of WWW and I need some information, quick.

Can anyone tell me how, if possible, I can access the Internet
from a PC using NetWare 3.2 and an IBM token ring card?  Any info
on this would be much, much appreciated.
--
Bill A. Chiles
Computing Center
Northeastern State University
Tahlequah, OK 74464
e-mail:  chiles@cherokee.nsuok.edu




From G.Joly@cs.ucl.ac.uk  Mon Mar 21 18:01:45 1994 --100
Message-Id: <9403211658.AA20910@dxmint.cern.ch>
Date: Mon, 21 Mar 1994 18:01:45 --100
From: G.Joly@cs.ucl.ac.uk (Gordon Joly)
Subject: Re: X500 gateway sought for Solaris.




>> @ Paul Barker says:
>> @ 
>> @ PaulB> What do you want - a pointer to some code?
>> @ PaulB> I can also offer a pointer to another experimental gateway.
>> @ 

and gordon asked:
>> @ Could you supply the X500 -> WWW interface?



Just for the record:-)

Gordon Joly         Phone +44 71 380 7934       FAX +44 71 387 1397
Email: G.Joly@cs.ucl.ac.uk    UUCP: ...!{uunet,uknet}!ucl-cs!G.Joly
Comp Sci, University College, London, Gower Street, LONDON WC1E 6BT
XXX YYY WWW & http://www.cs.ucl.ac.uk/mice/gjoly.html & WWW YYY XXX




From masinter@parc.xerox.com  Mon Mar 21 19:14:36 1994 --100
Message-Id: <94Mar21.101133pst.2732@golden.parc.xerox.com>
Date: Mon, 21 Mar 1994 19:14:36 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: file://localhost  =>  local: ?

I noticed a couple of problems with using the proxy caching gateway
from CERN. One was that I was getting core dumps. I tracked this down
some uninitialized pointers; I sent a patch to mosaic-x, but in case
others of you were trying to use Mosaic-2.2 with proxy, you might want
the patches I enclose at the end of this message.

Another more serious problem is that many HTML documents on the net
still use 'file:' instead of 'ftp:'. To get this to work with the
proxy gateway, I set things up so that 

setenv file_proxy = http://myserver:/

and then set up the httpd.conf on the server so that:
	Pass	file:*	ftp:*

This seems to work fine for dealing with these 'obsolete' URLs, except
for those folks who want to use file: to refer to local files on their
own machine.

Personally, I'd like to see a totally different URL constructed to
mean 'file on the client's workstation' for use with lynx, Mosaic,
etc. to get rid of the ambiguity. I propose using:

	local:<local-host-pathname>

What do you think?

================================================================
Date: Sun, 20 Mar 94 17:23:38 EST
To: mosaic-x@ncsa.uiuc.edu
Subject: memory errors
From: Larry Masinter <masinter@parc.xerox.com>

After fidding with purify for a while, I found something that might be
causing the core dumps when using proxy gateways: most of the memory
management routines like StrAllocCopy etc. seem to not want
uninitialized pointers (they free them.)

These seem to fix the problem:

================================================================
in libwww2/HTAccess.c
================================================================
*** HTAccess.c.~1~	Tue Feb  8 13:20:55 1994
--- HTAccess.c	Sun Mar 20 16:48:47 1994
***************
*** 138,144 ****
  #define USE_GATEWAYS
  #ifdef USE_GATEWAYS
      {
! 	char *gateway_parameter, *gateway, *proxy;
  
  	/* search for gateways */
  	gateway_parameter = (char *)malloc(strlen(access)+20);
--- 138,144 ----
  #define USE_GATEWAYS
  #ifdef USE_GATEWAYS
      {
! 	char *gateway_parameter = 0, *gateway = 0, *proxy = 0;
  
  	/* search for gateways */
  	gateway_parameter = (char *)malloc(strlen(access)+20);
***************
*** 174,180 ****
  
  	/* proxy servers have precedence over gateway servers */
  	if (proxy) {
! 		char * gatewayed;
  		StrAllocCopy(gatewayed,proxy);
  		StrAllocCat(gatewayed,addr);
  		using_proxy = YES;
--- 174,180 ----
  
  	/* proxy servers have precedence over gateway servers */
  	if (proxy) {
! 		char * gatewayed = 0;
  		StrAllocCopy(gatewayed,proxy);
  		StrAllocCat(gatewayed,addr);
  		using_proxy = YES;
***************
*** 185,191 ****
  		access =  HTParse(HTAnchor_physical(anchor),
  			"http:", PARSE_ACCESS);
  	} else if (gateway) {
! 		char * gatewayed;
  		StrAllocCopy(gatewayed,gateway);
  		StrAllocCat(gatewayed,addr);
  		using_gateway = YES;
--- 185,191 ----
  		access =  HTParse(HTAnchor_physical(anchor),
  			"http:", PARSE_ACCESS);
  	} else if (gateway) {
! 		char * gatewayed = 0;
  		StrAllocCopy(gatewayed,gateway);
  		StrAllocCat(gatewayed,addr);
  		using_gateway = YES;





From m.koster@nexor.co.uk  Mon Mar 21 19:20:18 1994 --100
Message-Id: <9403211818.AA06202@dxmint.cern.ch>
Date: Mon, 21 Mar 1994 19:20:18 --100
From: m.koster@nexor.co.uk (Martijn Koster)
Subject: Re: X500 gateway sought for Solaris. 


While we're on the subject of experimental X.500 gateways, you may
want to have a look at <http://web.nexor.co.uk/x500gw/x500gw.html>.

Currently it only allows you to search for people below NEXOR,
but it illustrates some further Web integration:

        o Grouped presentation
        o ``Hyperlinked'' Manager/Secretary's/See Also etc.
        o Inline JPEG and Fax photo's
        o Audio
        o WWW specific attributes (nonstandard)
        o X.509 Security certificates
        o Uses an LDAP Query Engine
        o It's Perl, integrated in the Plexus server.

Comments welcome.

-- Martijn
__________
Internet: m.koster@nexor.co.uk
X-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M
X-500: c=GB@o=NEXOR Ltd@cn=Martijn Koster
WWW: http://web.nexor.co.uk/mak/mak.html



From connolly@hal.com  Mon Mar 21 19:31:25 1994 --100
Message-Id: <9403211820.AA12816@ulua.hal.com>
Date: Mon, 21 Mar 1994 19:31:25 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: file://localhost => local: ? 

In message <94Mar21.101133pst.2732@golden.parc.xerox.com>, Larry Masinter write
s:
..
>Another more serious problem is that many HTML documents on the net
>still use 'file:' instead of 'ftp:'. To get this to work with the
>proxy gateway, I set things up so that 
..
>Personally, I'd like to see a totally different URL constructed to
>mean 'file on the client's workstation' for use with lynx, Mosaic,
>etc. to get rid of the ambiguity. I propose using:
>
>	local:<local-host-pathname>
>
>What do you think?

I suggest the name "local-file" with semantics as defined by MIME.

I suggest the name "anon-ftp" be supported as well.

I think the semantics of URLs and MIME external bodies should be
the same, and in this vein, it is important that they share
namespaces. i.e. it would be a Bad Thing if a URL scheme and a MIME
access type had the same spelling but different meanings. For the same
reasons, I think it's a Bad Thing for the same mechanism to have
different names under MIME and WWW.

Dan



From connolly@hal.com  Mon Mar 21 19:56:53 1994 --100
Message-Id: <9403211836.AA12832@ulua.hal.com>
Date: Mon, 21 Mar 1994 19:56:53 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: Reliable links [Was: Stab in the dark ] 

In message <9403190624.AA18301@rodan.UU.NET>, Josh Osborne writes:
>[...disscussion of how to deal with links to data that change over time...]
>>For the content-type dimension, the format negociation algorithm in
>>HTTP works pretty well...
>>
>>But in all these cases, I'd like to be able to put version, format,
>>language, etc. info in the reference itself, if I choose. For example,
>>I may know that
>>	ftp://foo.com/lksjfli4jlij43
>>is a postscript file. But there's currently no way to express this.
>>And Mosaic, for example, will assume it's a plain text file.
>
>
>I don't think we want to tie a URI to a format, version or language.

I did not suggest that. Please read carefully. I took the trouble to
construct a concrete example motivating the OPTION to specify a
specific sequence of octets as the referent of my citation.

>If a document has a link to http://foo.com/lksjfli4jlij43, which is
>some sort of picture and I am at a color workstation with a fast network 
>link I want to retrive whatever image looks best in color, regardless of
>(data) size.  If I am at a workstation with a slow link, and a monochrome
>display, I want the smallest image that looks good in monochrome.

I agree. I wrote:
>>For the content-type dimension, the format negociation algorithm in
>>HTTP works pretty well...

Now, back to my scenario: what about FTP?

>Also http://foo.com/dr.fun/most_recent.jpg, should not be constained to
>pointing at one image, it should be free to change from day to day.

What if I have a complaint about tuesday's version which is fixed in
wednesday's version, and then somebody reads my complaint on thursday?
Then I look silly, cuz it's not apparent to the thursaday reader that
my complaint was once valid. Consider online document reviews,
contract negociations, most other CSCW applications...

>If you want to cache at the application level, you can assume whatever
>object you fetched last is still valid, unless the user changed some
>defualt or other that might effect things ('color/monochrome').  You
>should also timeout data when it expires (if the protocall has a TTL or
>expire date like HTTP), or after some fixed length of time (a few hours).
>If you are doing caching _outside_ the application (like a gateway) you 
>need to understand enough of the protocall being used to figure out what
>version of the bits is being fetched, and either handle it from the cache,
>or fetch it from the real source.

So much for scalable distributed systems... if I've got my own caching
strategy, and you've got yours, and we don't agree on how to encode
meta-information, then we can't share cache namespaces.

fI really disagree fundamentally with the notion that every resource
has a "home" and you must make a round-trip to that home to access the
resource in any well-defined way.  This means that caches are all
heuristic optimizations, doomed to failure in unanticipated cases
(since all the cases aren't specified.)

Some resources are opaque and need to work that way, but many are not.
RFC822 is just a sequence of bytes. It doesn't matter where you get
them from, as long as you get the right bytes.

Dan




From kevinh@eit.COM  Mon Mar 21 20:01:37 1994 --100
Message-Id: <9403211856.AA09172@eit.COM>
Date: Mon, 21 Mar 1994 20:01:37 --100
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Re:  How do you maintain image maps?


> From: koblas@netcom.com (David Koblas)
> 
> I recently got around to reading the NCSA "document" on how to setup
> clickable image maps.  Now the big question is how many people use
> this method (XPaint & XV) for creation maintinace of your image maps, or
> what do you use?
> 
> What features in a tool would you like to see to make this task easier?

	I used to use XV to get the coordinates, but now I'm finding
that HyperMapEdit, a Macintosh HyperCard stack, is much easier, since
it automatically generates the imagemap config file for you!
	I don't have the URL, but email gasser@eniac.seas.upenn.edu
for details.
	I would certainly like to see an X program that allows you to
create the shapes over an image, showing a transparent overlay of each
shape to easily show any overlapping areas. I'd like it to be able to
reload the shapes from a config file and have the points editable, too!

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://www.eit.com/)
Hypermedia Industrial Designer * Duty now for the future!



From duns@vsdeol.cern.ch  Mon Mar 21 20:10:50 1994 --100
Message-Id: <9403211855.AA12427@dxmint.cern.ch>
Date: Mon, 21 Mar 1994 20:10:50 --100
From: duns@vsdeol.cern.ch (MARK DONSZELMANN)
Subject: Version 2.16beta of WWW for VMS

Version 2.16beta is partially ready for VMS. It runs on both ALPHA and VAX
for both MULTINET and UCX. (If people can provide me with a good connection,
20000 blocks space and another communication package, I will be happy to try it)

You can get it (tomorrow) from info.cern.ch, pub/www/src/www216betavms.tar-gz

or (now) from http://delonline.cern.ch/disk$user/duns/doc/vms/distribution.html

the last file also contains documentation on installation, running and changes.

For comments, bug reports or installation discrepancies, mail

			duns@vxdeop.cern.ch

Mark Donszelmann (CERN)




From altis@ibeam.jf.intel.com  Mon Mar 21 20:06:43 1994 --100
Message-Id: <m0pipCr-00043NC@ibeam.intel.com>
Date: Mon, 21 Mar 1994 20:06:43 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: file://localhost  =>  local: ?

At  7:14 PM 3/21/94 +0000, Larry Masinter wrote:
>Another more serious problem is that many HTML documents on the net
>still use 'file:' instead of 'ftp:'. To get this to work with the
>proxy gateway, I set things up so that
>
>setenv file_proxy = http://myserver:/
>
>and then set up the httpd.conf on the server so that:
>        Pass    file:*  ftp:*

Regardless of the URL confusion, you shouldn't have had to change X Mosaic
or the cern_httpd. URLs of the form file://host.domain/ are interpreted by
Cello, Lynx, Mosaic, etc. as ftp URLs, so the proxying still works
correctly. URLs of the form file://localhost/ are considered local
filesystem references.

Yes, use of file: rather than ftp: is rather confusing. It is the job of
the URI working group to document correct URL usage, but it will be
sometime before people can be persuaded not to use file: instead of ftp:.

ka





From luotonen@ptsun00.cern.ch  Mon Mar 21 21:21:04 1994 --100
Message-Id: <9403212019.AA00845@ptsun03.cern.ch>
Date: Mon, 21 Mar 1994 21:21:04 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: If-Modified-Since enhancement


Since nobody objected, and some gave me feedback saying it was
a good idea, the HTTP spec now says:


  NOT MODIFIED 304
 
       If the client has done a conditional GET and access is allowed, but
       the document has not been modified since the date and time specified
       in If-Modified-Since field, server responds with 304 status code, and
       does not send the document back to the client.
 
NEW:   Response headers are as if the client had originally sent a HEAD request.
NEW:   This way the server can upload a new expiry date and other necessary
NEW:   metainformation to the requesting client (often a caching proxy).
 
       This is to allow fast update of caches.
 
 
-- Cheers, Ari --




From raisch@internet.com  Mon Mar 21 21:25:18 1994 --100
Message-Id: <Pine.3.85.9403211153.A24484-0100000@hmmm>
Date: Mon, 21 Mar 1994 21:25:18 --100
From: raisch@internet.com (Rob Raisch, The Internet Company)
Subject: Re: Indexing the List of Lists

Alan Emtage writes regarding Lists of Lists:
>... I don't believe that manual maintenance of this kind of data
>is feasible any longer.... the Internet is now too big for this kind of
>thing. 

Alan, in the general case I believe you are right.  But in the specific 
case of the information required to identify a "resource" on the net, I 
don't think so.  Let me 'splain...

The issues of indexing Internet content are vast, but we seem to have a 
number of pilot projects which attempt to address the issues.  But is 
this all the user REALLY needs?  I suggest not.

In my experience, when I am looking for information on agriculture, I am
not looking for 'grain.tar.Z' or for 'Name=Thoughts on Triticalae and the
Wheat Borer Beetle.' Rather, I am looking for 'things having to do with
agriculture.'

Indexing content is a very large problem and one I'll freely admit most 
likely needs to be completely automated.  But, identifying collections of 
value -- what I refer to as a 'resource' -- is something which can only 
be managed and maintained by the agency in authority over that resource.

On The Electronic Newsstand, Out Magazine represents a 'resource' -- a 
collection of value on a given topic, but the articles in the Out archive 
are not easily identifable by their names and pose large problems of 
cataloging.  The Electronic Newsstand is a resource, as well, as it 
represents a collection of magazines and their content, but the FAQ about 
the Enews is not a resource.

While individual files and documents number (perhaps) in the millions, 
resources of this kind are still in the very low thousands.  Now is the 
time to put infrastructure in place to handle the load.

Of course, no project works unless there is a reason for the resource 
administrator to provide the meta-information necessary.  I believe that 
there is sufficient motivation to do so, since an effort of this nature 
answers the very question we all attempt to answer by putting our 
information up for view:  How can I get people to use what I provide?

I strongly suggest that a first step in any effort of this kind must be 
the definition of exactly what we are trying to collect information on 
because the issues of indexing vs. the generation of a 'table of 
contents' are very different indeed.  

--  </rr>  Rob Raisch, The Internet Company









From kevinh@eit.COM  Mon Mar 21 22:12:30 1994 --100
Message-Id: <9403212109.AA10976@eit.COM>
Date: Mon, 21 Mar 1994 22:12:30 --100
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: Getting HyperMapEdit



	For those who want to get HyperMapEdit, the HyperCard
imagemap configuration file creator, FTP to one of the info-mac mirror
sites - try the main site (sumex-aim.stanford.edu), or ftp.hawaii.edu in
/mirrors/info-mac, which is usually the fastest mirror site.
	From the /info-mac directory, you can find the binhex'd (hqx)
file under /comm/net as "hyper-map-edit-hc.hqx".

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://www.eit.com/)
Hypermedia Industrial Designer * Duty now for the future!



From stumpf@informatik.tu-muenchen.de  Tue Mar 22 00:33:46 1994 --100
Message-Id: <2mlan4$2bc@hpsystem1.informatik.tu-muenchen.de>
Date: Tue, 22 Mar 1994 00:33:46 --100
From: stumpf@informatik.tu-muenchen.de (Markus Stumpf)
Subject: Re: forwarding cache requests

reinpost@info.win.tue.nl (Reinier Post) writes:
[ sorry, I've restructured the text a little ]

>Cache-date: <date>
>    the time the document was served from the cache in answer
>    to the present request

So this is the same as "Date:" ???

>Cache-last-refreshed: <date>
>    the time the document was last fetched into the cache,

Do you mean with "fetched" "checked to be valid" ???

>Cache-last-modified: <date>
>    the time it was last fetched and found to be different from the
>    previous version,

This is the only one I am currently sure I know what you mean with it :/

>Cache-via: <url> [, <url>]*

Would here be a approach like the SMTP mailers do with adding Received:
lines sufficient? i.e. allow for a unlimited number of those tags
and each cache/proxy that gates the document adds a "identifier"
that it could recognize again and thus detect loops.

Okay, let me explain my thoughts on this topic.
I have a working proxy/cache server running based on ncsa httpd-1.1.
I did the proxy module, Guenther Fischer from Chemnitz made the cache
module. The approach Guenther uses is as follows:

If you don't have the document in the cache, fetch it and put it in the cache.
If you have the document in the cache,
   check with a stat() the last modification time of the file.
   if this is longer than a certain timeout
      send a HEAD request and check if file has changed.
      if it has changed, update the cache
         else update the last modification time of the file (utime()).

I currently don't know the strategy of the CERN server, but I think it's
rather similar.

What do we need for "good" caches.

1) "forwarders": if you want to reduce e.g. national and international
   traffic, one could imagine a big national cache, which acts as proxy
   to international sites that could be used by local proxy or cache
   servers.
2) we should be able to have read/write and read/only caches.
   we could have a "master" that writes the cache and "slaves" that
   only have read/only access to the cache and ask the master to
   update the cache if necessary. This would allow distributing
   the load of fetching documents to some machines accessing the same
   cache over e.g. NFS and having the burden of updating the cache,
   which is IMHO rare, as most of the documents are rather static,
   to one master.

What is needed for inter-cache communication:

o   what I think is REALLY URGENTLY NEEDED is another way to handle
    GET requests. I'd asked that before but got no answer. I don't
    see any problems in requesting more than one document within
    one server connection. BUT currently all servers close the connection
    after the last byte sent, EVEN if there is a Content-length: field.
      I'd like to propose that if there is such a field the client
    has to close the connection if it doesn't want more documents from
    that server or be able to send another GET or whatever!
    (but maybe this should be discussed under another subject).

o   one great idea is the conditionally GET via If-Modified-Since:
    The only problem I see currently is: how do I determine as
    forwarder or client if the other side supports it? If I send
    a conditionally GET and the server on the other side does not
    support it, it will send the document and this is currently worse
    than the possible overhead of sending a HEAD followed by a GET.

o   As with the approach Guenther Fischer uses a tag like
    Cache-last-modified: would be sufficient, as the client or forwarder
    could see from this date, when the cache server has last checked for
    accuracy of the document in the cache. What would be informative
    would probably be a Cache-Update-Interval: tag (in minutes) for this
    specific document, to tell the forwarder when it is useful to
    check for this document at this cache again, or, that it would
    not get a newer version from that cache within the next n minutes
    anyway. (of course the Cache-via: useful and needed!)

Is this sufficient?
Comments? Ideas?

	\Maex



From peterd@bunyip.com  Tue Mar 22 05:04:46 1994 --100
Message-Id: <9403220343.AA19064@expresso.bunyip.com>
Date: Tue, 22 Mar 1994 05:04:46 --100
From: peterd@bunyip.com (Peter Deutsch)
Subject: Re: Stab in the dark

[ Larry wrote: ]
.  .  . 
> If URNs are allowed to refer to multiple formats of documents, or
> multiple versions of updating documents, or online streams of
> information that you might telnet to, then no, you can't easily
> include the MD5 signature of the document.

Getting metainfo is a problem, and we have to solve it,
but do we have to solve it here with URNs and URLs?
It seems to me like we're wandering into the relm of the
underlying transport protocols. IF the protocol supports a
"meta-info" operation you could query for the associated
checksum, etc. If it doesn't you can't.  This seems like
the kind of functionality that would drive "buyer choices"
in selecting the next generation of services.

As a sign of things to come, both Gopher and WWW are able
to provide you with basic meta info now and work continues
to extend these, for example by finding new ways to access
interactive services. I'm not sure that it's really needed
to incorporate metainfo directly into URN or URL format
itself (I may be misunderstanding your comments here).


> What you're saying is that you don't have to trust the URN -> URL
> resolution process, because you will verify the entire URN ->
> <resource> resolution?
> 
> I'm not sure I can live with that, although I'd like to think about it
> a bit more.


The way I see it, we can build a system with a single URN
encoding strategy (eg. mandate MD5 and be done), or
provide a more general architecture which is used to
support multiple URN strategies (implicit in our
assumptions about leaving decisions about versioning to
URN assignors, etc). My money is on the bet that the later
is what we'll end up with, since it is the only way to
provide for grand-fathering and the rough consensus thing
we have to have.

Now, if we allow multiple URN schemes, you will simply
have no choice but to accept that there will be some
schemes "you can't live with". They will have
characteristics you don't find acceptable but they're out
there anyways. The best you can do is recognize the
general scheme and alert your users with appropriate
messages.

I don't actually see this as a serious problem. Darwinian
selection will determine the most appropriate URN schemes
in practice. 


					- peterd

-- 
------------------------------------------------------------------------------
  My proposal for funding the Internet is pretty simple. I vote we institute
  an "Information Superhighway" tax, the proceeds of which will be used to
  fund network infrastructure. The way this would work is simple - every time
  someone uses the words "Information Superhighway" or any of its derivatives
  we strike them with a sharp object and make them pay a $10 fee (of course,
  the sharp object is not actually needed to make this scheme work, it's just
  in there because it seems an appropriate thing to do...)
------------------------------------------------------------------------------



From fielding@simplon.ICS.UCI.EDU  Tue Mar 22 07:43:18 1994 --100
Message-Id: <9403212241.aa14001@paris.ics.uci.edu>
Date: Tue, 22 Mar 1994 07:43:18 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: If-Modified-Since enhancement 

Ari said:

> Since nobody objected, and some gave me feedback saying it was
> a good idea,

That's what I get for sleeping-in this morning...

> the HTTP spec now says:
> 
> 
>  NOT MODIFIED 304
>  
>      If the client has done a conditional GET and access is allowed, but
>      the document has not been modified since the date and time specified
>      in If-Modified-Since field, server responds with 304 status code, and
>      does not send the document back to the client.
>  
>NEW:  Response headers are as if the client had originally sent a HEAD request.
>NEW:  This way the server can upload a new expiry date and other necessary
>NEW:  metainformation to the requesting client (often a caching proxy).
>  
>      This is to allow fast update of caches.


Sorry for the late response (it's Monday here in California), but I don't
support this change at all.  The whole purpose of the If-Modified-Since
proposal is to eventually save network bandwidth.  That purpose is partially
defeated if we send ALL the document headers even though both the server
and the cache manager know that the document has not been modified.
At least 99.999999999% of the time they will be identical to those already
stored by the cache manager.

The only headers which should be sent are

    Date:
    Server:
    Expires:

all of the other headers (especially the MIME ones) serve no purpose
in a NOT MODIFIED 304 response and thus IMHO should never be sent.

Have I overlooked something?


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From luotonen@ptsun00.cern.ch  Tue Mar 22 09:42:01 1994 --100
Message-Id: <9403220840.AA00998@ptsun03.cern.ch>
Date: Tue, 22 Mar 1994 09:42:01 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: forwarding cache requests


> If you don't have the document in the cache, fetch it and put it in the cache.
> If you have the document in the cache,
>    check with a stat() the last modification time of the file.
>    if this is longer than a certain timeout
>       send a HEAD request and check if file has changed.
>       if it has changed, update the cache
>          else update the last modification time of the file (utime()).
> 
> I currently don't know the strategy of the CERN server, but I think it's
> rather similar.

Yep, something like that, just not HEAD, rather GET with
If-Modified-Since header.


> o   what I think is REALLY URGENTLY NEEDED is another way to handle
>     GET requests. I'd asked that before but got no answer. I don't
>     see any problems in requesting more than one document within
>     one server connection. BUT currently all servers close the connection
>     after the last byte sent, EVEN if there is a Content-length: field.
>       I'd like to propose that if there is such a field the client
>     has to close the connection if it doesn't want more documents from
>     that server or be able to send another GET or whatever!
>     (but maybe this should be discussed under another subject).

Currently all the clients I know of do not use Content-Length -- Mosaic
reads it to give info of how many bytes of how many have been received
but still, the closing of connection is used to indicate end-of-document
(Content-Length could even be wrong and it still works).

So in other words server relying on this will not work.  Therefore, clients
somehow need to indicate that they are willing to respect Content-Length.
I'm not too much against this idea, let's just see what the others think.


> o   one great idea is the conditionally GET via If-Modified-Since:
>     The only problem I see currently is: how do I determine as
>     forwarder or client if the other side supports it?

Rob said he supports it in ncsa_httpd 1.2 out soon, I will support
it in cern_httpd 2.17beta out soon, and Tony Sanders certainly "adds
support for this into Plexus" before we know it... :-)  Big sites
usually upgrade their servers pretty fast, so by the time we have a
good caching system all the major sites support conditional GET.


-- Cheers, Ari --





From fielding@simplon.ICS.UCI.EDU  Tue Mar 22 10:08:03 1994 --100
Message-Id: <9403220103.aa23418@paris.ics.uci.edu>
Date: Tue, 22 Mar 1994 10:08:03 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: forwarding cache requests 

Reinier Post writes:

> In my cache server ('Lagoon'), few of the provisions in the HTTP MIME header
> specifications are currently implemented, but I have already noticed the
> need for more headers.  To send information on the cache status of a document
> to the client, I leave the headers obtained from the remote source intact,
> and consider adding the following ones:
> 
> Cache-date: <date>
> Cache-last-refreshed: <date>
> Cache-last-modified: <date>
> Cache-via: <url> [, <url>]*
> 
> This provides: the time the document was served from the cache in answer
> to the present request, the time the document was last fetched into the
> cache, the time it was last fetched and found to be different from the
> previous version, and the sequence of URLs by which the document was
> subsequently fetched. ...

My first question is: why does the client need to see these headers?
In other words, what task do you want the client to do that cannot be
done without these headers?  I will assume for now that the only reason
is to support hierarchies of cache managers.

Assuming that, here is my opinion about the headers listed above,

Cache-date:
     is inappropriate -- the Date: header of the message should
     list the date/time in which the cache manager generated the
     HTTP message (as a whole) for delivery to the client (i.e. it
     should always be the current date/timestamp.

Cache-last-modified:
     should be Cache-last-updated: and should reflect the date/timestamp
     of last comparison with the original (i.e. the Date: header returned
     by the source document's HTTP server in that comparison).  Note that
     this should not be changed by cache managers more than once-removed
     from the origin.  This is because the act of checking the cache
     consistency with the original document is equivalent to getting a new
     copy of that document, but the act of checking cache consistency with
     a higher-level cache is only equivalent to copying that cache.

Cache-last-refreshed:
     is unnecessary given Cache-last-updated.

Cache-via:

> ...  This last header, which provides a comma separated
> list of URLs, is required in order for cache servers to break loops in chains
> of forwarded requests.  (Lagoon 0.11a now supports such forwarding, but it
> doesn't check for loops yet.)  The last URL in this sequence is always
> the URL of the present request (with the '#name' relative anchor suffix
> removed).  All headers are completely optional, of course.

I am not convinced that loops are possible. Could you give us an example
where a normal (non-psychotic) cache hierarchy could result in a loop?

> This only addresses the cache specific information added to the HTTP response,
> but there is a similar design problem with adding cache information to the
> request.  For example, the User-agent: and From: headers, are they supposed to
> be as sent by the client, or are they supposed to identify the cache mechanism
> sending the actual request?

>From the HTTP2 spec, I'd say that they should reflect the originator of the
request. I.e. if the request was caused in response to a client, then that
client User-agent:, From:, and Referer: headers should be passed on.  However,
if the request was caused by a periodic function of the cache manager, then
it should only include the User-agent: and From: identifiers corresponding to
the cache manager.

I would recommend that cache managers append their product/version tag
to the User-Agent: header, but that topic deserves its own message.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From luotonen@ptsun00.cern.ch  Tue Mar 22 10:33:56 1994 --100
Message-Id: <9403220932.AA01014@ptsun03.cern.ch>
Date: Tue, 22 Mar 1994 10:33:56 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: If-Modified-Since enhancement


> Sorry for the late response (it's Monday here in California)

Sorry, I lost my sense of time 'cause I was working the whole weekend
and didn't realize it was only the first working day after my original
message.

> The only headers which should be sent are
> 
>     Date:
>     Server:
>     Expires:

You're right.  How about saying:

  Response headers are as if the client had originally sent a HEAD
  request, but not all the headers need to be present, only the ones
  that make sense in this context.  That means all the headers that
  are relevant to cache managers, like Expires: and Date:.

  The purpose of this feature is that the server can upload a new expiry
  date and other necessary metainformation to the requesting client
  (often a caching proxy). This is to allow fast update of caches.

-- Cheers, Ari --




From fielding@simplon.ICS.UCI.EDU  Tue Mar 22 11:27:58 1994 --100
Message-Id: <9403220224.aa28075@paris.ics.uci.edu>
Date: Tue, 22 Mar 1994 11:27:58 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script 

Robert S. Thau writes:

> What I'm doing at least provisionally to drive the final version of my Perl
> script (site-index.pl v0.1, pointer to code below), is to put the source
> for the IAFA descriptions and keywords fields inside the document itself,
> by (ab?)use of the <META ...> tag which was discussed on this list some
> time ago to solve a different problem.  For example, the document at
> http://www.ai.mit.edu/events/events-list.html contains, near the top, the
> following:
> 
>   <meta name="iafa-description"
>   value="MIT AI lab events, including seminars, conferences, and tours">
>   <meta name="iafa-keywords"
>   value="MIT, Artificial Intelligence, seminar, conference">

I would have to label this as partial abuse. Yes, this is metainformation
and is thus reasonable to be in a META element.  However, you are using
name attributes which only make sense to your own tool, thereby defeating
the general usefulness of that metainformation.

How about:

    <meta name="Summary"
    value="MIT AI lab events, including seminars, conferences, and tours">
    <meta name="Keywords"
    value="MIT, Artificial Intelligence, seminar, conference">

Also, don't forget that the purpose of META is so that a server capable
(and willing) to parse metainfo can then send the headers

    Summary: MIT AI lab events, including seminars, conferences, and tours
    Keywords: MIT, Artificial Intelligence, seminar, conference

as part of the HTTP response object headers.  Thus, use of the META
element should be limited to things for which headers are desirable.

> (There's one other kind of meta-information my indexer uses --- if it sees
> <meta name="iafa-type" value="service">, it indexes the page in question
> with a SERVICE template, as opposed to a DOCUMENT template.  This is useful
> for cover pages of search engines and the like).

Now that is something which is not of general usefulness.  IMHO, it should
be implemented as just an SGML comment and not a META element.  E.g.

<!-- IAFA-TYPE service -->

(or, at the very least, define a different metainfo name which serves the
same purpose but corresponds to something generally useful).

> This use of <META ...> solves another problem as well, that of determining
> which documents make the index.  Files with the <meta name="iafa-...">
> fields get indexed; the rest don't. ...

That also reflects a tool-specific comment rather than metainfo.

> ... So, once these tags are in the
> documents, the rest of IAFA template preparation (finding the files,
> getting the titles out and the URIs right) can be completely automated
> (which is effectively what my site-index.pl script does).

Sounds like a great script -- thanks for making it available.

> However, the <META ...> tags do raise another problem, that of whether this
> use of <META ...> is appropriate, and if it is, making sure that the uses
> which different tools may eventually make of meta-information don't
> conflict.  A central registry of meta-information names would be a good
> idea, if people are going to start using it.

Most examples of appropriate metainfo names can already be found in
NNTP (rfc1036) and rfc822.  However, you are probably right in that we
should have some sort of specification for what the names mean.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From guenther.fischer@hrz.tu-chemnitz.de  Tue Mar 22 11:36:53 1994 --100
Message-Id: <9403221034.AA22018@flash1.hrz.tu-chemnitz.de>
Date: Tue, 22 Mar 1994 11:36:53 --100
From: guenther.fischer@hrz.tu-chemnitz.de (Guenther Fischer)
Subject: http <--> ftp

Hi folks,

I would like to have more traditional ftp-archives giving their
informations via http too. The actual httpd-servers (I use NCSA-1.1 with
extensions) have many nice features to give a better view to archives
then with ftp. Try
http://www.tu-chemnitz.de/ftp-home or
ftp://ftp.tu-chemnitz.de/

and also
http://www.tu-chemnitz.de/ftp-home/pub/gnu
ftp://ftp.tu-chemnitz.de/pub/gnu

Try also the hyperlink at the icons to look in archives and so on.

Please send me your URLs (http/ftp) if you give your ftp-archives via
http too. I'll collect them and create a list of them on my server.

The list http://www.tu-chemnitz.de/~fischer/ftp-http-list.html

	~Guenther


-- 
Name:      Guenther Fischer / Institute: TU Chemnitz, Universitaetsrechenzentrum
Phone:     0371 668 361     / mail:      fischer@hrz.tu-chemnitz.de
URL: <A HREF="http://www.tu-chemnitz.de/adressen/fischer.html"> me </A>



From fielding@simplon.ICS.UCI.EDU  Tue Mar 22 12:03:43 1994 --100
Message-Id: <9403220300.aa29856@paris.ics.uci.edu>
Date: Tue, 22 Mar 1994 12:03:43 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: If-Modified-Since enhancement 

Ari said:

> How about saying:
> 
>   Response headers are as if the client had originally sent a HEAD
>   request, but not all the headers need to be present, only the ones
>   that make sense in this context.  That means all the headers that
>   are relevant to cache managers, like Expires: and Date:.
> 
>   The purpose of this feature is that the server can upload a new expiry
>   date and other necessary metainformation to the requesting client
>   (often a caching proxy). This is to allow fast update of caches.

That's much better, but I think I would phrase it slightly different. E.g.
(as extracted from http://info.cern.ch/hypertext/WWW/Protocols/HTTP/HTRESP.html)
-------------------------------------------
Not Modified 304

If the client has done a conditional GET and access is allowed,
but the document has not been modified since the date and time
specified in the If-Modified-Since field, the server responds with
a 304 status code and does not send the document body to the client. 

Response headers are as if the client had sent a HEAD request, but 
limited to only those headers which make sense in this context. This
means only headers that are relevant to cache managers and which may
have changed independently of the document's Last-modified date. 
Examples include Date, Server, and Expires.

The purpose of this feature is to allow efficient updates of local
cache information (including relevant metainformation) without requiring
the overhead of multiple HTTP requests (e.g. a HEAD followed by a GET)
and minimizing the transmittal of information already known by the
requesting client (usually a caching proxy).
-------------------------------------------

How's that?


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From luotonen@ptsun00.cern.ch  Tue Mar 22 13:03:13 1994 --100
Message-Id: <9403221201.AA01192@ptsun03.cern.ch>
Date: Tue, 22 Mar 1994 13:03:13 --100
From: luotonen@ptsun00.cern.ch (Ari Luotonen)
Subject: Re: If-Modified-Since enhancement


So I changed If-Modified-Since spec phrasing.  Essential content
is the same.  Thanks, Roy!

> Not Modified 304
> 
> If the client has done a conditional GET and access is allowed,
> but the document has not been modified since the date and time
> specified in the If-Modified-Since field, the server responds with
> a 304 status code and does not send the document body to the client. 
> 
> Response headers are as if the client had sent a HEAD request, but 
> limited to only those headers which make sense in this context. This
> means only headers that are relevant to cache managers and which may
> have changed independently of the document's Last-modified date. 
> Examples include Date, Server, and Expires.
> 
> The purpose of this feature is to allow efficient updates of local
> cache information (including relevant metainformation) without requiring
> the overhead of multiple HTTP requests (e.g. a HEAD followed by a GET)
> and minimizing the transmittal of information already known by the
> requesting client (usually a caching proxy).

-- Cheers, Ari --




From jared@osiris.ac.hmc.edu  Tue Mar 22 15:43:46 1994 --100
Message-Id: <199403221440.GAA19118@osiris.ac.hmc.edu>
Date: Tue, 22 Mar 1994 15:43:46 --100
From: jared@osiris.ac.hmc.edu (Jared Rhine)
Subject: SOAP Requirements

Although this message in Interpedia-specific, there are numerous HTTP
questions contained herein that I would like some feedback on.

I apologize if the length of this message is a burden on your mail system,
but one message is probably computationally cheaper than dozens of little
messages.

What information do we need to encode into a SOAP? (The Interpedia FAQ
answer on "What are SOAPs?" is attached at the bottom) I'm looking for
suggestions to nail down a design specification and possible encoding.

Suggestions so far:

  1. Digital signature

     This is necessary to ensure that a given SOAP has not been forged.  In
     some sense, given the model I propose below, it seems that this boils
     down to trusting the transaction in general.  If the server is trusted,
     then one can assume that it is providing the correct list of SOAPs
     associated with the document.

     Recent discussions on www-talk have allowed some people to get a
     slightly better idea of the issues involved with client/server
     authentication and how they relate to anonymous information systems.
     Those ideas should be able to make it possible to (fairly) reliably
     authenticate the server.  This will allow the organization to submit
     their SOAP to the Resource-Location Service (RLS) in an authenticated
     manner and also to allow servers to exchange that information with some
     degree of trust.

     Given these two things, one should be able to be assured that any SOAPs
     the server claims are attached to a given document are indeed
     authentic.

     Is such a mechanism sufficient?

  2. Organization identification

     We need to uniquely identify the organization issuing a given SOAP.
     Thus, we need to provide a flat namespace for organizations.  I assume
     similiar issues have been addressed by the URI/URN working groups; can
     anyone provide specifications on what they have come up with?

     I would recommend that this namespace be heirarchical in nature.
     Potentially, every person on the planet will be able to issue multiple
     SOAPs, so the namespace scheme _must_ be designed with that in mind.
     Granted, such flexibility may not be necessary in the near future (nor
     likely ever), but recent suggestions that we limit the SOAP approval
     domain value to "something simple" like 1 to 10 require me to insist
     that we think in terms of future expansion.

  3. SOAP name

     Given a heirarchical flat namespace for identifying an organization, it
     makes sense to me to simply expand the domain to include the name of
     the SOAP as well.  In other words, a single heirarchical namespace
     would identify both the organization and the specific SOAP.  Is there
     any reason not to do it this way?

  4. Definition of range value types

     Is it sufficient that we define the range value to be limited to (for
     example) 16-bit signed integers?  Or would some SOAPs work better if
     the range value could include an enumerated type?

     This is a tough issue since restricting the range to numeric values (of
     any type; reals could work, too) would simplify implementation issues.
     Any introduction of a string-based enumerated type would require that
     the clients and servers agree on the type values somehow.  This can be
     done either with some sort of negotiation procedures and some way to
     convey the semantics associated with the type (yuck), or by defining
     the types via an ICC standard.

     I propose that we categorize SOAPs into two types: numeric types and
     enumerated types (better word, anyone?).

     Numeric types should be based on 16-bit signed integers and be used to
     specify should degree of approval.  Negative values would signify
     disapproval, as expected.

     Enumerated types are SOAPs for which the range has some predefined
     value, based upon Interpedia standards.  This could be extended to
     encompass SOAPs that have meaning based solely on their existance (or
     in other words, the range has only one value).

     Although enumerated types are more difficult to manage, I think they
     are useful and should be included in the specification.

     Of course, since signed integers is really an enumerated type, I'm
     promptly dropping the distinction :)

  5. Encoding scheme

     I'd highly recommend an arbitrary length alphanumeric string for the
     SOAP domain.  I really don't feel like dealing with arbitrary data in
     my browser and client implementations.  It doesn't make a whole lot of
     sense, anyway, to say, "Give me SOAP ^&#*$&".

     To facilitate heirarchical spaces, one scheme that I've used in a
     number of databases I've implemented is to use the exclamation point as
     a path separator.  Is there a better way to do this?  I've also use
     '!:!' to make the separator more unique, but this is the same general
     idea.  As far as I know, at least one character must be reserved, since
     the path separator is basically "out-of-band" data.  Are there any
     other suggestions?  I also note that if there are multiple "parts" to
     the SOAP (if we encode more than just the heirarchical organization
     name), we need another separator.  I usually use the double-colon,
     '::', but since the entire key should be RFC822 compliant, that is out.
     For now, I'll use the hash mark, although I am loath to remove more
     characters from the domain than necessary.

     Hmmm, it occurs to me that similiar work has been done for encoding
     URNs.  The March 26th version of the Sollins draft, "Specification of
     Uniform Resource Names", includes no physical representation of URNs.
     I don't have time right now to go bouncing all over the Internet
     looking for the most recent one (I think the encoding type has been
     dropped since the last copy I have).  Pointer, anyone?

     Hmmm, I seem to remember there being colons in the URN spec; does that
     mean that URNs can't be transmitted as HTTP object headers, but
     only as request URIs?

     Note that the concept of "alphanumeric" could potentially be extended
     as extended character sets become more popular (has anyone thought of
     this for HTTP request/object headers?).  For now (at the prototype
     stage), I recommend specifying a restricted ASCII (see RFC822, section
     3.1.2 for more details).

  6. Mechanism for SOAP inheritance

     Beats me.  I'd rather put this off until I have at least bare-minimum
     SOAPs implemented.  Whatever inheritance heirarchy is set-up, it should
     probably be maintained separate from the SOAPs themselves.  As such,
     the above scheme for naming SOAPs doesn't appear to impend later
     implementation of SOAP inheritance.

A note on numeric SOAPs: different organizations will use the numeric SOAP
range in different ways.  IMHO, this can not be prevented.  Different people
rate things in different ways; how are you ever going to get around that?
People who utilize a given SOAP will quickly find out what number
constitutes a good rating for a given SOAP.  To facilitate mutli-SOAP
searches ("prune this search by only selecting articles for which these
numeric SOAPs are over 30000"), the client could take care of scaling the
SOAPs.  You enter the SOAP value as 30000, but the client knows that a
certain SOAP is known to use values that are a little too low, so it should
actually request a value of 20000 for that specific SOAP.  The client has to
have some user-level mechanism for weighting SOAPs, anyway; this action
would just be a part of that.

It seems likely that the metainformation for the SOAP should be kept
external to the document itself.  Assuming use of the HTTP protocol as the
transfer method (as my prototypes do), I think the HEAD information should
probably be generated from information external to the document.  Although
this introduces some problems with respect to keeping all the documention
information "in one place", I think the difficulties associated with
hard-coding the information in the document are far greater.

Given that, it seems that a good location for the SOAPs would be by using
the META tag of HTML+.  As such, the entire line must conform to RFC822 so
that the HTTP protcol can transfer it as header field (key: value).

Thus, roughly, for an enumerated SOAP, the HEAD of a document would contain

  <meta name="x-SOAP#American_Children's_Association!suitable-for-children"#
	value="boolean#true">

or, other example of an enumerated type,

  <meta name="x-SOAP#American_Children's_Association!reading-level"
	value="reading-level#8">

  (enumerated type signifying reading grade-level)

or, a numeric type

  <meta name="x-SOAP#American_Children's_Association" value="numeric#35000">

So I seem to have answered my original question; the minimum information
that needs to be encoded into a SOAP is the type of the range, and the
Organization/SOAP name, specified in some heirarchical manner.  Assuming
some key/content model (motivated by its natural application to database
implementation), all the information must be encoded into either the key or
the content.  It doesn't matter a great deal where the division comes (one
could indicate the range type in the key portion above.

As you can see, there are numerous ways to take care of these details; what
I'd like to do is nail down a preliminary specification, subject to change,
so I can get on with finishing my prototypes.  To save myself grief, I'd
like to refrain from implementing anything until I get at least a quorum
vote on the matter (quorum meaning people who care).  Hopefully, that quorum
won't be one person (me!).

On other track, what should be the mechanism by which a SOAP enters into the
SOAP-space?  To a great extent, this cannot even begin to be answered until
we have some sort of idea of what the protocols and mechanisms of the
underlying distributed service are.  The best system I currently know of for
distributed services is DNS, but that is inadequate to our needs (despite
recent discussions prompted by Martin Hamilton about how to implement a
URN->URL resolution scheme using DNS).  I imagine that the Xanadu projects
have put a great deal of thought into this area, but I frankly admit that I
am loath to pursue any technology that we would be required to license.  At
this point, it seems likely that my senior research project will focus on
exactly this issue of distributed databases, and specifically their uses to
distributed information systems.  (Speaking of which, can anyone offer
references to literature on the subject of distributed databases?  There
seems to be a dearth of literature which doesn't focus on the issue of
keeping the database consistent; something important to business community,
but distinctly less important to information systems).  So, in a little over
a year, we'll have at least one backend for all of this, but I'd like to get
work done far before that.

======================================================================
Subject: 4.2 What are Seals of Approval (SOAP)?

The concept of a seal-of-approval (SOAP), introduced by Erik Seielstad,
is currently being actively discussed.  SOAPs have achieved some
prominence, and have subsequently been referred to in several
comp.info.xxx newsgroups.  A new notion is that SOAPS could be
hierarchical, in that a SOAP could indicate approval or disapproval of
a group of other SOAPs.  Another is that a SOAP could point to two
types of articles, ones that agree with and ones that disagree with
the article to which the SOAP applies.  The subject of links is also
being discussed and is running a separate but parallel course.
A principle difficulty seems to be in deciding how to implement SOAPs.
Below, are three articles that describe SOAPS.

Doug Wilson <dwilson@crc.sd68.nanaimo.bc.ca> wrote:
A seal-of-approval is data provided by a person or persons which
indicates that some article is good.  (Seals of disapproval have
also been proposed.)  Seals-of-approval will be used by people in
deciding what articles to read, but will also be used by the
Interpedia software to decide which articles to make most easily
available to people, according to their stated preferences.
If you set a user-parameter indicating you only want articles which
have the Jeff-Foust-Quality-Assurance-Board-Seal-Of-Approval,
then only those articles will be set up for convenient (default)
access -- although all other articles will still be accessible,
with a bit more effort.
(Doug Wilson)

Jeff Foust <jfoust@mit.edu> wrote:
Seals-of-approval have been suggested as a way to provide editorial
input on articles submitted to the Interpedia without subjecting all
Interpedia users to the editorial opinions of a few.  In short, any
user would be able to create a "seal" that could be affixed to
articles that the user found to be factually correct, well written, or
ideologically agreeable to him/her.  There would be no limit on the
number of seals that could exist.  There would likely be a directory
of seals kept, so that users could refer to the directory to determine
who the authors of a particular seal are and also obtain some basic
information on it (e.g., what classes of articles are typically given
this seal, what criteria the authors use to assign seals, etc.)
(Jeff Foust)

Jared Rhine wrote <jared_rhine@hmc.edu> wrote:
The concept of SOAPs was invented to help solve the problem of balancing
editorial issues, academic freedom, and database viewpoints.  The
problem is, how can one construct the Interpedia in such a way that
anyone can contribute, and yet allow the user to retain a focused view
of the articles available?  If the Interpedia is to scale well as it
grows, it would (ironically) not be acceptable to have the number of
articles returned in response to a particular query to grow at the
same rate.
SOAPs are issued by any organization (or individual) that wishes to
produce one.  Any given document can (and will) have multiple SOAPs.
Each SOAP represents a rating of that particular document by a
particular organization.  In general, there would be a number of
institutions whose opinion you respect.  If that organization rated a
particular document highly, it is likely that you would consider it a
valuable document, too.  Articles containing a SOAP from those
organizations would be included in your view of the Interpedia.
Note that SOAPs can provide arbitrary slices of the dataspace of the
Interpedia.  Any given search on the entire space of the Interpedia
would return a large number of documents.  From that set of documents,
you _apply_ a SOAP, which based on some criteria, selects a subset of
those documents.  As you apply more and more SOAPs, the set of documents
becomes smaller and smaller.  You could conceivably also perform set-
theoretic operations with SOAPs, ie unions, intersections, and so forth.
Some examples of SOAPS I have envisioned based on discussions from
the list:
   * A SOAP from the American's Children Association.  A particular
     browser could be configured to return only those documents which
     are suitable for children.
   * Peer review journals based solely on the Interpedia.  a technical
     article could be published by anyone;  how do you know it isn't
     completely invalid?  Because the IEE has certified this document
     as having technical merit.
   * A SOAP for poets who insist that every document be written in
     iambic pentameter.
   * SOAPs could also have numeric ratings associated with them; a
     particular article might have a readability index of 78.



From rst@ai.mit.edu  Tue Mar 22 17:41:37 1994 --100
Message-Id: <9403221638.AA02557@volterra>
Date: Tue, 22 Mar 1994 17:41:37 --100
From: rst@ai.mit.edu (Robert S. Thau)
Subject: Future of meta-indices: site indexing proposal and Perl script 

   Date: Tue, 22 Mar 1994 11:29:11 --100
   From: "Roy T. Fielding" <fielding@simplon.ics.uci.edu>

One open question here (on which there is perhaps some disagreement between
your note and Dave Raggett's earlier one) is the intended purpose of the
<meta ...> tag.  If it's a general-purpose hook for all sorts of
metainformation (some of which may not be appropriate to be sent out as
headers for every GET request), one set of tradeoffs are appropriate; if
it's simply a hook to get things into the headers, then another.  (As I
said, I'm aware that the whole <META ...> business was originally proposed
for another purpose, and to that extent, at least, I'm poaching).  However,
I do have a few comments either way:

   How about:

       <meta name="Summary"
       value="MIT AI lab events, including seminars, conferences, and tours">
       <meta name="Keywords"
       value="MIT, Artificial Intelligence, seminar, conference">

   Also, don't forget that the purpose of META is so that a server capable
   (and willing) to parse metainfo can then send the headers

       Summary: MIT AI lab events, including seminars, conferences, and tours
       Keywords: MIT, Artificial Intelligence, seminar, conference

   as part of the HTTP response object headers.  Thus, use of the META
   element should be limited to things for which headers are desirable.

I'm not completely sure this is a good idea, for this information.  The
"IAFA-description"s may run on for some length --- for instance, the entire
abstract of a technical paper.  (FWIW, the IAFA-Publishing Internet Draft
says that the Description entry on templates should be 'the "abstract" in
the case of documents'; also, technical papers are starting to appear on
the Web as hypertext --- see, for instance, AI Technical Report 1315 at
http://www.ai.mit.edu/people/ellens/why.html, or the Transit project docs
at http://www.ai.mit.edu/projects/transit/transit_home_page.html, to cite
two examples close to home).

Abstracts are clearly metainformation in the general sense, but they seem,
to my taste at least, a bit heavy-duty for a HEAD request.  (Do we really
want large HEADs to be routinely larger than small documents?)

As to the more general point of how these META-things are named, I could
certainly use general "Summary" and "Keywords" fields, if their definition
fits my (indexing) application, but I feel it's important to be sure how
they're defined before taking such a high-visibility chunk out of the
global "meta-thing" namespace, and I wasn't sure of that this weekend.  ;-)

   > (There's one other kind of meta-information my indexer uses --- if it sees
   > <meta name="iafa-type" value="service">, it indexes the page in question
   > with a SERVICE template, as opposed to a DOCUMENT template.  This is useful
   > for cover pages of search engines and the like).

   Now that is something which is not of general usefulness.

Well, the SERVICE/DOCUMENT distinction does come from the IAFA templates,
which were intended to be useful off-site.  Perhaps there's room for
improvement there as well, but it does seem to me to be a "useful"
distinction.  (Searching for documents about Archie or WAIS is different
from searching for a gateway, for example).

Whether it's appropriate to be shipped out as an extra HTTP header with
every GET or HEAD request, that's another issue altogether.

   Most examples of appropriate metainfo names can already be found in
   NNTP (rfc1036) and rfc822.  However, you are probably right in that we
   should have some sort of specification for what the names mean.

That is certainly so *if* the universe of meta-tagged metainformation is
limited to items appropriate for HTTP/MIME headers, which wasn't entirely
clear from prior discussion.  I guess what I'm trying to do is canvas the
community on the issue.

   ...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
		      (fielding@ics.uci.edu)
       <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>

rst



From dsr@hplb.hpl.hp.com  Tue Mar 22 19:18:12 1994 --100
Message-Id: <9403221815.AA22873@dragget.hpl.hp.com>
Date: Tue, 22 Mar 1994 19:18:12 --100
From: dsr@hplb.hpl.hp.com (Dave Raggett)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script

> I'm not completely sure this is a good idea, for this information.  The
> "IAFA-description"s may run on for some length --- for instance, the entire
> abstract of a technical paper.  (FWIW, the IAFA-Publishing Internet Draft
> says that the Description entry on templates should be 'the "abstract" in
> the case of documents'; also, technical papers are starting to appear on
> the Web as hypertext --- see, for instance, AI Technical Report 1315 at
> http://www.ai.mit.edu/people/ellens/why.html, or the Transit project docs
> at http://www.ai.mit.edu/projects/transit/transit_home_page.html, to cite
> two examples close to home).

> Abstracts are clearly metainformation in the general sense, but they seem,
> to my taste at least, a bit heavy-duty for a HEAD request.  (Do we really
> want large HEADs to be routinely larger than small documents?)

HTML+ includes the ABSTRACT element for this. The "Summary" HTTP
header should therefore be reserved for a brief description.

At some point in the future we may wish to extend HTTP
to allow clients to request named portions of the document.
Right now the #ID suffix is interpreted by the client, but there
is now reason why with a suitable HTTP method (e.g. "EXTRACT")
the server couldn't extract the contents of the named container
and send that rather than the entire document.

Dave Raggett



From altis@ibeam.jf.intel.com  Tue Mar 22 19:35:56 1994 --100
Message-Id: <m0pjBG4-00043CC@ibeam.intel.com>
Date: Tue, 22 Mar 1994 19:35:56 --100
From: altis@ibeam.jf.intel.com (Kevin Altis)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script

At  7:18 PM 3/22/94 +0000, Dave Raggett wrote:
>At some point in the future we may wish to extend HTTP
>to allow clients to request named portions of the document.
>Right now the #ID suffix is interpreted by the client, but there
>is now reason why with a suitable HTTP method (e.g. "EXTRACT")
>the server couldn't extract the contents of the named container
>and send that rather than the entire document.

Yes, in fact, if you have suitable tools on the server side, you should be
able to extract particular sections or pages of any document. So, you could
retrieve page 3-4 of a PostScript document, paragraph three on page 4 of a
Word document, a rectangle from 10,15 to 108,112 of a GIF image, etc. This
goes back to discussions on this list a year ago (maybe more?) when we were
talking about non-static HyperText references.

ka





From jcma@reagan.ai.mit.edu  Tue Mar 22 20:29:56 1994 --100
Message-Id: <19940322192733.5.JCMA@JEFFERSON.AI.MIT.EDU>
Date: Tue, 22 Mar 1994 20:29:56 --100
From: jcma@reagan.ai.mit.edu (John C. Mallery)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script

    From: Dave Raggett <dsr@hplb.hpl.hp.com>

    > Abstracts are clearly metainformation in the general sense, but they seem,
    > to my taste at least, a bit heavy-duty for a HEAD request.  (Do we really
    > want large HEADs to be routinely larger than small documents?)

    HTML+ includes the ABSTRACT element for this. The "Summary" HTTP
    header should therefore be reserved for a brief description.

    At some point in the future we may wish to extend HTTP
    to allow clients to request named portions of the document.
    Right now the #ID suffix is interpreted by the client, but there
    is now reason why with a suitable HTTP method (e.g. "EXTRACT")
    the server couldn't extract the contents of the named container
    and send that rather than the entire document.

    Dave Raggett

It would be nice if the document fragment could be specified stuff between two
arbitrary points in a document, rather than relying exclusively on everything
to be named.



From rst@ai.mit.edu  Tue Mar 22 21:19:04 1994 --100
Message-Id: <9403222016.AA02693@volterra>
Date: Tue, 22 Mar 1994 21:19:04 --100
From: rst@ai.mit.edu (Robert S. Thau)
Subject: Future of meta-indices: site indexing proposal and Perl script

   Date: Tue, 22 Mar 1994 19:19:33 --100
   From: Dave Raggett <dsr@hplb.hpl.hp.com>

   HTML+ includes the ABSTRACT element for this. The "Summary" HTTP
   header should therefore be reserved for a brief description.

Neat.  Another twist for the next spin of the script, I guess.

However, (getting back to the original application) some of the files that
people want to have indexed may not have text set off as a formal abstract,
so <ABSTRACT> is not a 100% solution.  I'm thinking of search-engine cover
sheets in particular.  These frequently consist of nothing but an input
<FORM>, with no text that could be considered a proper abstract.  For these
documents at least, I think I'd still need some other way of including
descriptive information which is not necessarily displayed, whether it's

   <meta name="subject" value="...">

or

   <meta name="iafa-description" value="...">

or even

   <!-- INDEXER description "..." -->

Sigh...

   Dave Raggett

rst



From jcma@reagan.ai.mit.edu  Tue Mar 22 21:59:43 1994 --100
Message-Id: <19940322205739.9.JCMA@JEFFERSON.AI.MIT.EDU>
Date: Tue, 22 Mar 1994 21:59:43 --100
From: jcma@reagan.ai.mit.edu (John C. Mallery)
Subject: Future of meta-indices: site indexing proposal and Perl script

    From: rst@ai.mit.edu (Robert S. Thau)

       Date: Tue, 22 Mar 1994 19:19:33 --100
       From: Dave Raggett <dsr@hplb.hpl.hp.com>

       HTML+ includes the ABSTRACT element for this. The "Summary" HTTP
       header should therefore be reserved for a brief description.

    Neat.  Another twist for the next spin of the script, I guess.

    However, (getting back to the original application) some of the files that
    people want to have indexed may not have text set off as a formal abstract,
    so <ABSTRACT> is not a 100% solution.  I'm thinking of search-engine cover
    sheets in particular.  These frequently consist of nothing but an input
    <FORM>, with no text that could be considered a proper abstract.  For these
    documents at least, I think I'd still need some other way of including
    descriptive information which is not necessarily displayed, whether it's

In fact, you want to make assertions about the document using typed links, eg.
"abstract abstracts document".  It would seem the mechanism is available in
the protocols, if not all the implmentations.

The power is the links not hacks to the markup language.



From rst@ai.mit.edu  Tue Mar 22 22:21:20 1994 --100
Message-Id: <9403222118.AA02744@volterra>
Date: Tue, 22 Mar 1994 22:21:20 --100
From: rst@ai.mit.edu (Robert S. Thau)
Subject: Future of meta-indices: site indexing proposal and Perl script

   Date: Tue, 22 Mar 1994 15:57-0500
   From: John C. Mallery <jcma@reagan.ai.mit.edu>

   In fact, you want to make assertions about the document using typed
   links, eg.  "abstract abstracts document".  It would seem the mechanism
   is available in the protocols, if not all the implmentations.

   The power is the links not hacks to the markup language.

The trouble with this idea for my application is that, from a brutally
practical point of view, it's as much blood, sweat, and tears to maintain a
setup like this (links to abstracts, etc., in separate files) as it is to
maintain the IAFA template files manually.  In fact, it's *more* trouble
--- you get two URIs per document to type in (and check for typos), one for
the keywords attribute and one for the description --- and on top of that,
you've suddenly got a slew of itsy-bitsy files to keep track of.  The whole
thing turns into a configuration management nightmare.

The bottom line, as far as I'm concerned, is this: If people can get their
files into an index by adding two lines of <meta> tags, *and that's it*,
there's a chance that they will.  If, instead, they have to create two
small files and a couple of cross-references, that chance is much reduced.

If I wanted a system about which it's easy to build neat theories, I might
do it your way regardless, but that's not my goal here.  I'm looking to
save work (for myself, and eventually, the other people with files on the
main lab server), and I'm prepared for elegance to give way to that where
it has to.  (If I wasn't, I suppose I'd be writing Lisp instead of Perl ;-).

rst




From masinter@parc.xerox.com  Wed Mar 23 02:30:31 1994 --100
Message-Id: <94Mar22.172717pst.2732@golden.parc.xerox.com>
Date: Wed, 23 Mar 1994 02:30:31 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script 

'meta' is one of those words that evaporates when you look at it.
Everything is either 'information' or 'metainformation'; just as we
don't enclose the entire document in a <information ...> tag, we
shouldn't encode metainformation in a <meta ...> tag.

There are lots of *kinds* of information that are currently lumped
together under the umbrella term of "meta-information". Some of it is
related to the content of the document (its current representation,
the title, number of authors, etc.), some to the context of the
production of the document (date of first publication), and some to
the current instance (price of the document for various pricing
strategies.) 

Content-related information might arguably appear in the body of the
document, while contextual information (which might change from
instance to instance or time to time) probably shouldn't.

The fact that we talk about 'meta-information' doesn't mean that it
should appear in a particular place in the protocol labelled as
"meta".




From jcma@reagan.ai.mit.edu  Wed Mar 23 03:10:33 1994 --100
Message-Id: <19940323020856.5.JCMA@JEFFERSON.AI.MIT.EDU>
Date: Wed, 23 Mar 1994 03:10:33 --100
From: jcma@reagan.ai.mit.edu (John C. Mallery)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script 

    From: Larry Masinter <masinter@parc.xerox.com>

    ...

    The fact that we talk about 'meta-information' doesn't mean that it
    should appear in a particular place in the protocol labelled as
    "meta".

In fact, it would be desirable to have a recursive theory and implementation
so that you can talk ABOUT the "meta" at any level.

This is where you go when you take links seriously.



From fielding@simplon.ICS.UCI.EDU  Wed Mar 23 03:18:31 1994 --100
Message-Id: <9403221815.aa17408@paris.ics.uci.edu>
Date: Wed, 23 Mar 1994 03:18:31 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: SOAP Requirements 

>      Hmmm, I seem to remember there being colons in the URN spec; does that
>      mean that URNs can't be transmitted as HTTP object headers, but
>      only as request URIs?

No, it does not.  Colons can occur in header data items provided that only
the real header Name: occurs at the beginning of the line.  I.e.

Date: is a legal header, but
      Date: is just considered a continuation of the previous header.

> ...

> It seems likely that the metainformation for the SOAP should be kept
> external to the document itself.  Assuming use of the HTTP protocol as the
> transfer method (as my prototypes do), I think the HEAD information should
> probably be generated from information external to the document.  Although
> this introduces some problems with respect to keeping all the documention
> information "in one place", I think the difficulties associated with
> hard-coding the information in the document are far greater.
> 
> Given that, it seems that a good location for the SOAPs would be by using
> the META tag of HTML+.  As such, the entire line must conform to RFC822 so
> that the HTTP protcol can transfer it as header field (key: value).

The above two paragraphs are contradictory.  The META element only exists
within HTML+ documents and thus using META for this purpose would necessitate
embedding that information within the document.  Such a solution would also
be inadequate for non-HTML documents (e.g. MPEG movies or GIFs).

What you really want is a metainformation table -- something that can be
loaded by the server (either on a per site or per document basis) and which
contains the list of SOAPs (and/or other metainfo) for each document+version.
Note that version is important because a SOAP should only be applied to a
particular version of a document (not all documents of that name).

> Thus, roughly, for an enumerated SOAP, the HEAD of a document would contain
> 
>   <meta name="x-SOAP#American_Children's_Association!suitable-for-children"#
> 	value="boolean#true">

Yuck!  That would result in a header like:

x-SOAP#American_Children's_Association!suitable-for-children: boolean#true

which is not what an rfc822-type header should look like.

Instead, it should be like

SOAP: American_Children's_Association!suitable-for-children=true,
      American_Children's_Association!reading-level=8

I don't think that you need to (or even should) encode range or typing
information in the header -- that should be defined by the name of the SOAP
as part of some SOAP name registry, i.e.  all SOAPs ending in
"suitable-for-children" have legal values of (true,false).

BTW, it is my opinion that people should not bother with "x-Type" naming
schemes because they provide no useful advantage over just plain "Type" 
(i.e. they are just as likely to conflict with someone else's x-Type as
they are with someone else's Type).  The only thing that x-Names accomplish
is to make it bloody difficult to change all the software that makes use of
them once they become official Names.  However, that is just my opinion and
it is contrary to the "official" protocols.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From fielding@simplon.ICS.UCI.EDU  Wed Mar 23 04:02:39 1994 --100
Message-Id: <9403221900.aa19378@paris.ics.uci.edu>
Date: Wed, 23 Mar 1994 04:02:39 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script 

Larry Masinter writes:

> The fact that we talk about 'meta-information' doesn't mean that it
> should appear in a particular place in the protocol labelled as "meta".

That is correct.  However, there does need to be some mechanism to allow
a document author to provide metainformation within the document.  For
common metainfo such as Title and Abstract, this is provided by specific
HTML+ elements for that purpose.  However, it is quite impossible for the
HTML+ specification to anticipate and define all possible metainfo that
may be desired by all possible authors.  Furthermore, there is no reason
why HTML browsers should be required to parse different element names for
each possible metainfo name.

The solution is to provide a single mechanism whereby authors can define
their own names (possibly following guidelines external to HTML) and the
content associated with those names in such a way that they can be easily
identified by programs which are capable of extracting metainformation
from HTML documents.  That mechanism is the META element.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From fielding@simplon.ICS.UCI.EDU  Wed Mar 23 06:09:23 1994 --100
Message-Id: <9403222107.aa24363@paris.ics.uci.edu>
Date: Wed, 23 Mar 1994 06:09:23 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script 

Robert S. Thau writes:

> One open question here (on which there is perhaps some disagreement between
> your note and Dave Raggett's earlier one) is the intended purpose of the
> <meta ...> tag.  If it's a general-purpose hook for all sorts of
> metainformation (some of which may not be appropriate to be sent out as
> headers for every GET request), one set of tradeoffs are appropriate; if
> it's simply a hook to get things into the headers, then another.

The definition in the HMTL+ DTD reflects the latter purpose in that it
specifically says the server should make a header out of that element.
However, it does seem reasonable to make that an option instead.

How about this:

----------------------------------------------------------------------
<!--
 The META element can be used to embed document metainformation not
 defined by other HTML+ elements for use by servers/clients capable
 of extracting that information.

 Servers should read the document head to generate HTTP headers
 corresponding to any META elements with the HEADER attribute,
 e.g. if the document contains:

     <meta header name="Expires" value="Tue, 04 Dec 1993 21:29:02 GMT">

 The server should include the header:

     Expires: Tue, 04 Dec 1993 21:29:02 GMT

 as part of the HTTP response to a GET or HEAD request for that document.
 When the HEADER attribute is not present, the server should not generate
 an HTTP header for this metainformation; e.g.

     <meta name="IndexType" value="Service">

 would not generate an HTTP header but would still allow clients or
 other tools to make use of that metainformation.

 Other likely names are "Keywords", "Created", "Owner" (a name)
 and "Reply-To" (an email address).  
-->

<!ELEMENT META - O EMPTY>
<!ATTLIST META
        id      ID      #IMPLIED -- to allow meta info         --
        nohead (nohead) #IMPLIED -- don't generate HTTP header --
        name    CDATA   #IMPLIED -- HTTP header e.g. "Expires" --
        value   CDATA   #IMPLIED -- associated value           -->
----------------------------------------------------------------------

Comments? 


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From dsr@hplb.hpl.hp.com  Wed Mar 23 10:37:47 1994 --100
Message-Id: <9403230934.AA24410@dragget.hpl.hp.com>
Date: Wed, 23 Mar 1994 10:37:47 --100
From: dsr@hplb.hpl.hp.com (Dave Raggett)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script

Kevin Altis writes:

> you should be able to extract particular sections or pages of
> any document. So, you could retrieve page 3-4 of a PostScript
> document, paragraph three on page 4 of a Word document, a rectangle
> from 10,15 to 108,112 of a GIF image, etc.

and John C. Mallery writes:

> It would be nice if the document fragment could be specified
> stuff between two arbitrary points in a document, rather than
> relying exclusively on everything to be named.

Some of the HyTime concepts may be useful here. HyTime extends
SGML's reference capability to accommodate those cases in which
no unique identifier exists in the document's name space. Authors
can create a location address element that pairs the object in
question with a unique local identifier. There are three kinds
of addressing:

    a)  by name (using ID attributes)
    b)  by position in a coordinate space (the GIF example)
    c)  by semantic construct (e.g. the 3rd polygon on the left)

A summary of these capabilities can be found in the November '91
issue of Communications of the ACM (Volume 34, number 11).

--
Best wishes,

Dave Raggett

-----------------------------------------------------------------------------
Hewlett Packard Laboratories,           +44 272 228046
Bristol, England                        dsr@hplb.hpl.hp.com



From jzk1111@stud.u-szeged.hu  Wed Mar 23 11:59:06 1994 --100
Message-Id: <9403231056.AA04560@dxmint.cern.ch>
Date: Wed, 23 Mar 1994 11:59:06 --100
From: jzk1111@stud.u-szeged.hu (jzk1111@stud.u-szeged.hu)
Subject: 

Hello everybody!

I'm a Hungarian student, I study programming and other computer sciences. I
will finish my studies in May, and because of this I have to write a thesis
submitted for a degree, whose theme is the introducing of the World Wide Web. I
have collected a lot of information and documents, so I understand its basic
conception and function, but there are few things, which aren't clear, mostly
in the HTTP specification. I'd be very thankful if someone could help me.

Janos Kormendi

JZK1111@HUSZEG11.BITNET
jzk1111@jatevm.bke.hu
jzk1111@alfa.cab.jate.u-szeged.hu

P.S. I'm not a member of this mailing list, so please send e-mails directly to
me.



From Jared_Rhine@hmc.edu  Wed Mar 23 12:11:20 1994 --100
Message-Id: <199403231108.DAA22797@osiris.ac.hmc.edu>
Date: Wed, 23 Mar 1994 12:11:20 --100
From: Jared_Rhine@hmc.edu (Jared_Rhine@hmc.edu)
Subject: Re: SOAP Requirements 

JRhine> It seems likely that the metainformation for the SOAP should be kept
JRhine> external to the document itself.  Assuming use of the HTTP protocol
JRhine> as the transfer method (as my prototypes do), I think the HEAD
JRhine> information should probably be generated from information external
JRhine> to the document.  Although this introduces some problems with
JRhine> respect to keeping all the documention information "in one place", I
JRhine> think the difficulties associated with hard-coding the information
JRhine> in the document are far greater.

JRhine> Given that, it seems that a good location for the SOAPs would be by
JRhine> using the META tag of HTML+.  As such, the entire line must conform
JRhine> to RFC822 so that the HTTP protcol can transfer it as header field
JRhine> (key: value).

Roy> The above two paragraphs are contradictory.  The META element only
Roy> exists within HTML+ documents and thus using META for this purpose
Roy> would necessitate embedding that information within the document.

I may have phrased things in an unclear manner, but I don't believe the
ideas themselves are contradictory.  While I am, of course, forced to agree
that the META tag proper belongs solely to HTML+, I was thinking more in
terms of what that tag does (currently); it provokes a HTTP transaction
header (or at least that's the semantic value in which I'm currently
interested).

The model I'm proposing is to construct the information that would normally
be contained within the META tag via a dynamic lookup on a distributed
database backend (the RLS).  Thus, HEAD information provided in both the
HEAD and GET methods (and others, as appropriate) would not be derived
directly from the document, but instead from an external source.  The
document would have certain items fixed (such as its identifier in the
namespace; basically its URN) via the HTML+ META tags, but other
metainformation would be stored external to the document, as it must be (can
anyone seriously suggest encoding SOAPs into the document itself?)

Roy> Such a solution would also be inadequate for non-HTML documents
Roy> (e.g. MPEG movies or GIFs).

I certainly don't see why that is the case; if the transactions headers (the
moral equivalent of the META tag, in my mind) are constructed from
information external to the object, how in any way would that not apply to
MPEGs or GIFs?

Roy> What you really want is a metainformation table -- something that can
Roy> be loaded by the server (either on a per site or per document basis)
Roy> and which contains the list of SOAPs (and/or other metainfo) for each
Roy> document+version.  Note that version is important because a SOAP should
Roy> only be applied to a particular version of a document (not all
Roy> documents of that name).

I want something slightly more complex than a table; I want a dynamic
database query, possibly constructed from information obtained via
distributed means (not every server will be able to store all the
information for all documents).  There is a variety of other information
besides SOAPs that would need to be implemented, as well. I don't think a
table would scale well; providing the information "on a per site or per
document basis" isn't really what I'm after.  The document publisher should
be the final arbiter of the metainformation (aside from SOAPs; a publisher
can not chose to prevent a SOAP from being attached to the document).  The
mechanism to retrieve that metainformation should incorporate the idea of a
"canonical" source, with TTLs among secondary servers.  As I said, the best
model I've seen so far is DNS.

Note also that the Interpedia project has not (yet) made the assumption that
a SOAP must expire when a document version changes, since the question of
implied SOAPs is still open.  Yes, yes, it seems the logical thing to do,
and many documents will operate that way by default, but I have yet to be
convinced that it applies in all cases.  I won't sacrifice SOAP reliability
to gain a minor advantage, though.

JRhine> Thus, roughly, for an enumerated SOAP, the HEAD of a document would
JRhine> contain
JRhine> 
JRhine> <meta
JRhine> name="x-SOAP#American_Children's_Association!suitable-for-children"#
JRhine> value="boolean#true">

Roy> Yuck!

No kidding; I wasn't particularly happy with it either, which is why I asked
for feedback, which I appreciate.

Roy> That would result in a header like:

Roy> x-SOAP#American_Children's_Association!suitable-for-children: boolean#true

Roy> which is not what an rfc822-type header should look like.

Agreed.  I still think we need the separator, so I'll still keep the
exclamation reserved.  After rereading the RFC at some reasonable hour (only
3 AM tonight instead of last nights' 5 AM :), the colon doesn't need to be
reserved either.

Roy> Instead, it should be like

Roy> SOAP: American_Children's_Association!suitable-for-children=true,
Roy>       American_Children's_Association!reading-level=8

Is there any particular advantage to grouping all the SOAPs on a single
header field instead of using multiple header fields, one per SOAP?  I'd
prefer the latter method unless there is some overiding reason not to use
it.

Roy> I don't think that you need to (or even should) encode range or typing
Roy> information in the header -- that should be defined by the name of the
Roy> SOAP as part of some SOAP name registry, i.e.  all SOAPs ending in
Roy> "suitable-for-children" have legal values of (true,false).

Yeah, I can see arguments either way.  I was trying to get away from having
all new SOAPs have to become some sort of "standard".  Great, this SOAP
issuer wants to issue a SOAP of this type (where the range is defined in
some manner).  They flat out _can't_, because neither the browser nor the
client has the semantic value of that SOAP encoded in their static table.
The ICC could try to approve a list of different types, but no list is ever
complete.  It seems probable that the number of different types would reach
some plateau and then grow only _very_ slowly.  Given this assumption, using
a static encoding of SOAP names could probably work.  I've just never been a
big fan of "registries".

--
Jared Rhine         Jared_Rhine@hmc.edu
wibstr              Harvey Mudd College
                    http://www.hmc.edu/www/people/jared.html

"Come, let us retract the foreskin of misconception and apply the wire
 brush of enlightenment." -- Geoff Miller



From yosi@ubique.co.il  Wed Mar 23 11:17:42 1994 --100
Message-Id: <199403231013.MAA11789@tovlan.ubique.co.il>
Date: Wed, 23 Mar 1994 11:17:42 --100
From: yosi@ubique.co.il (Mass Yosi)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script

Dave Raggett wrote:

> Authors can create a location address element that pairs the object in
> question with a unique local identifier. There are three kinds
> of addressing:

>    a)  by name (using ID attributes)
>    b)  by position in a coordinate space (the GIF example)
>    c)  by semantic construct (e.g. the 3rd polygon on the left)


My suggestion is that the position will be 

    1) A character number N (from the begining of the document) 
    2) An (x,y) pixel inside image number N in the document.

If there is a well defined syntax for this, it can be used in a URL
to point to any location in the document.

    Yosi.




From heaney@gatwick.sgp.slb.com  Wed Mar 23 16:23:30 1994 --100
Message-Id: <9403231517.AA18177@mordred.gatwick.sgp.slb.com>
Date: Wed, 23 Mar 1994 16:23:30 --100
From: heaney@gatwick.sgp.slb.com (heaney@gatwick.sgp.slb.com)
Subject: Access control and the CERN httpd


A problem I have run into that some kind soul may be able to help me with:-

The CERN http (2.16) supports access control configurable by domain and/or 
user name and password.  If a domain mask is specified then there is no 
prompt for a passord - if a list of users is specified then a user name and 
password are required.  Taking an example from the CERN doc:

   authors: john, james
   cern_people: @128.141.*.*

the first line is user based access, the second domain mask based.

I would like to run CGI script which must for various reasons be run setuid - 
hence requiring the browser to provide an username and password.  However, 
the list of users that are to be permitted to use the script is vast.  To 
maintain a list of all of the valid users in the .group file is impractical.  
Much simpler would be in some way to specify all users from a particular domain.

_However_, there does not seem to be a valid way of doing this. Neither 
(*)@134.32.*.* or *@124.32.*.* seems to be valid group declaration.


So, the basic question is, (using the CERN httpd), how do you get the browser 
to supply a username and password without supplying a group definition which 
specifies user names?


Any suggestions?  With thanks,

Steve Heaney

Schlumberger Geco-Prakla
heaney@gatwick.sgp.slb.com




From rst@ai.mit.edu  Wed Mar 23 16:49:44 1994 --100
Message-Id: <9403231546.AA03369@volterra>
Date: Wed, 23 Mar 1994 16:49:44 --100
From: rst@ai.mit.edu (Robert S. Thau)
Subject: Future of meta-indices: site indexing proposal and Perl script 

   Date: Wed, 23 Mar 1994 06:10:24 --100
   From: "Roy T. Fielding" <fielding@simplon.ics.uci.edu>

To summarize, Roy suggests adding a "header" attribute to <META>, so that

	<meta header name="Expires" value="Tue, 04 Dec 1993 21:29:02 GMT">

would cause a server to generate

	Expires: Tue, 04 Dec 1993 21:29:02 GMT

while it silently ignored <meta ...> tags without the header attribute, such as

	<meta name="IndexType" value="Service">

FWIW, that certainly sits fine with me, but it might be possible to
consider more general schemes.  So, for the sake of concreteness, I'll
propose a "purpose" attribute, whose value is a string defining the purpose
(or purposes) of this particular piece of meta-info.  A few things which it
might be reasonable to have in a document are:

   <meta purpose="header" name="Expires" value="Tue, 04 Dec 1993...">
   <meta purpose="index" name="Type" value="service">

and even

   <meta purpose="index,header" name="Keywords"
         value="metaindex, resource discovery, DTD syntax">

(The last one might be cleaner as <meta purpose="header" purpose="index"...>;
I did it as a single CDATA string assumed to contain a comma-separated list
of purposes simply to make it easier to type in manually).

Either Roy's scheme or mine is adequate to my current needs, of course, but
a purpose attribute may cope perhaps a little more gracefully when the
*third* use for explicit metainformation in HTML+ documents comes along...

   ...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
		      (fielding@ics.uci.edu)
       <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>

rst



From Merry.Cushing@ucs.umass.edu  Wed Mar 23 22:20:24 1994 --100
Message-Id: <199403232117.QAA22946@titan.ucs.umass.edu>
Date: Wed, 23 Mar 1994 22:20:24 --100
From: Merry.Cushing@ucs.umass.edu (Merry Cushing)
Subject: Subscribe to listserver...

I would like to subscribe to the listserver about www-talk.
                                  Thanks,
                                       Merry
-- 
Merry Cushing    Academic Computing    Office of Information Technology
A117 Lederle Research Center           FAX:   (413) 545-3203
University of Massachusetts            Phone: (413) 545-1887
Amherst, MA  01003                     Internet: cushing@oit.umass.edu



From lenst@lysator.liu.se  Wed Mar 23 22:28:37 1994 --100
Message-Id: <199403232125.WAA02106@astrid.lysator.liu.se>
Date: Wed, 23 Mar 1994 22:28:37 --100
From: lenst@lysator.liu.se (lenst@lysator.liu.se)
Subject: Future of meta-indices: site indexing proposal and Perl script 


rst@ai.mit.edu (Robert S. Thau) wrote:
>
>   <meta purpose="index,header" name="Keywords"
>         value="metaindex, resource discovery, DTD syntax">
>
>(The last one might be cleaner as <meta purpose="header" purpose="index"...>;
>I did it as a single CDATA string assumed to contain a comma-separated list
>of purposes simply to make it easier to type in manually).

I assume it should still be SGML.  Then neither of the two syntaxes is
any good.  The second isn't even valid SGML.  A better way would be to
declare PURPOSE as NAMES.  Then it can be a list of names.  Attribute
value lists in SGML are written without comma, only white-space
separate the list items.

Lennart Staflin  <lenst@lysator.liu.se>
You are in a twisty little maze of URLs, all alluring.



From rst@ai.mit.edu  Wed Mar 23 22:58:46 1994 --100
Message-Id: <9403232155.AA03722@volterra>
Date: Wed, 23 Mar 1994 22:58:46 --100
From: rst@ai.mit.edu (Robert S. Thau)
Subject: Future of meta-indices: site indexing proposal and Perl script 

   Date: Wed, 23 Mar 1994 22:30:16 --100
   From: lenst@lysator.liu.se


   rst@ai.mit.edu (Robert S. Thau) wrote:
   >
   >   <meta purpose="index,header" name="Keywords"
   >         value="metaindex, resource discovery, DTD syntax">
   >
   >(The last one might be cleaner as <meta purpose="header" purpose="index"...>;
   >I did it as a single CDATA string assumed to contain a comma-separated list
   >of purposes simply to make it easier to type in manually).

   I assume it should still be SGML.

Ah yes, you clipped the part where I said, "not being an SGML adept...".

   Then neither of the two syntaxes is
   any good.  The second isn't even valid SGML.  A better way would be to
   declare PURPOSE as NAMES.  Then it can be a list of names.  Attribute
   value lists in SGML are written without comma, only white-space
   separate the list items.

   Lennart Staflin  <lenst@lysator.liu.se>
   You are in a twisty little maze of URLs, all alluring.

This would be, then, <meta purpose="index header" ...>?  At any rate, I'll
bow out to your expertise on the details, here...

rst



From fielding@simplon.ICS.UCI.EDU  Thu Mar 24 00:29:12 1994 --100
Message-Id: <9403231525.aa18412@paris.ics.uci.edu>
Date: Thu, 24 Mar 1994 00:29:12 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script 

Robert S. Thau writes:

> FWIW, that certainly sits fine with me, but it might be possible to
> consider more general schemes.  So, for the sake of concreteness, I'll
> propose a "purpose" attribute, whose value is a string defining the purpose
> (or purposes) of this particular piece of meta-info.  A few things which it
> might be reasonable to have in a document are:
> 
>    <meta purpose="header" name="Expires" value="Tue, 04 Dec 1993...">
>    <meta purpose="index" name="Type" value="service">
> 
> and even
> 
>    <meta purpose="index,header" name="Keywords"
>          value="metaindex, resource discovery, DTD syntax">

I'd have to disagree here.  There could be hundreds of different
purposes for a given bit of metainfo, most of which the author would
not be aware of.  Having a HEADER option makes sense for reasons of
efficiency; the others would just make it more convenient for a particular
tool.  The META name should be sufficient to define the content-type of
the metainfo and, once that is known, any tool should be able to decide
on its own as to whether or not that metainfo is useful to their purpose.

Once the metainfo is in the document (where any grubby client can get
its hands on it), the author has no control over how it is used, nor
should they care.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>



From rramacha@cisco.com  Thu Mar 24 01:20:44 1994 --100
Message-Id: <9403240018.AA02618@ce-nfs-1.cisco.com>
Date: Thu, 24 Mar 1994 01:20:44 --100
From: rramacha@cisco.com (Ranjini Ramachandran)
Subject: tabs ( or CRs) in form fields? 


Hello,

Has anyone implemented tabbing to fields in forms in X-Mosaic?

I was trying to add functionality to forms so that when a user hits a 
return ( since I didn't know how to deal with tabs) in any of the form 
textwidgets, the insertion bar advances to the next widget, unless, 
ofcourse, it's the last widget, then the form is submitted.
 
What I did was:
 
Edit HTMLwidgets.c:
and added the following to the CBActivateField function
  
if (cnt == count)
        {
                CBSubmitForm(w, client_data, call_data);
        }
 
#ifdef Motif
        (void) XmProcessTraversal(w, XmTRAVERSE_NEXT);
#endif Motif
 
*************************************************************
But for some reason, this won't work....

Any suggestions?

Also, does anyone have any documentation that will discuss X-Mosaic 
architecture ( or shed some light on it) becuase I'm having a hard 
time trying to figure it out.

Thanks for any help

Ranjini



From wei@xcf.Berkeley.EDU  Thu Mar 24 02:57:26 1994 --100
Message-Id: <9403240154.AA06829@xcf.Berkeley.EDU>
Date: Thu, 24 Mar 1994 02:57:26 --100
From: wei@xcf.Berkeley.EDU (Pei Y. Wei)
Subject: Collapsible/expandable list

For HTML+, what do people think about adding an attribute to indicate 
that an element is collapsible (click on a folder icon to toggle 
shrink/expand a list)?

In this example, the sub list "HTML Stuff" starts out expanded,
and the other two sub lists start out collapsed.
---------------------------------------------------------------
<UL FOLD=NO LABEL="W3 Designs">
  <LI><UL FOLD=NO LABEL="HTML Stuff">
      <LI>Stuff a.
      <LI>Stuff b.
      </UL>
  <LI><UL FOLD=YES LABEL="HTTP Stuff">
      <LI>Stuff a.
      <LI>Stuff b.
      </UL>
  <LI><UL FOLD=YES LABEL="More Stuff">
      <LI>Stuff.
      <LI>Stuffy.
      </UL>
</UL>
---------------------------------------------------------------

The FOLD attribute specifies that the list is foldable. And the value 
of that attribute specifies whether or not to fold the intial layout 
of the list.

It should also take another attribute "LABEL" so that one can tell what
the folded list contains. Later on, maybe the <UL> could take the HREF
(or perhaps SRC?) attribute for pointing to the content of the list
(fetch the content of the list only when expanded).

This FOLD attribute would be good definitely for list elements like 
<UL> and <OL>. But it could be useful for things like <XMP> etc as well.

Comments?

-Pei




From garylang@netcom.com  Thu Mar 24 04:07:07 1994 --100
Message-Id: <Pine.3.85.9403231907.A15670-0100000@netcom2>
Date: Thu, 24 Mar 1994 04:07:07 --100
From: garylang@netcom.com (Gary Lang)
Subject: Re: Collapsible/expandable list

Hmm... how does the WAIS interface work without something like this?

-g

Gary Lang 
Email:garylang@netcom.COM





From klute@tommy.informatik.uni-dortmund.de  Thu Mar 24 09:34:40 1994 --100
Message-Id: <9403240831.AA05527@tommy.informatik.uni-dortmund.de>
Date: Thu, 24 Mar 1994 09:34:40 --100
From: klute@tommy.informatik.uni-dortmund.de (Rainer Klute)
Subject: Re: forwarding cache requests 

>1) "forwarders": if you want to reduce e.g. national and international
>   traffic, one could imagine a big national cache, which acts as proxy
>   to international sites that could be used by local proxy or cache
>   servers.

This would require a very tough and accurate accounting mechanism,
because the managers of the "big national cache" have to be able to
bill their clients for the transfer costs, especially the
international ones.

Best regards
Rainer Klute

  Dipl.-Inform.                     IRB  - immer richtig beraten
  Rainer Klute                      EXUG - European X User Group
  Universitdt Dortmund, IRB
D-44221 Dortmund

    <http://www.informatik.uni-dortmund.de/IRB/Klute.html>



From wei@xcf.Berkeley.EDU  Thu Mar 24 09:52:50 1994 --100
Message-Id: <9403240849.AA09020@xcf.Berkeley.EDU>
Date: Thu, 24 Mar 1994 09:52:50 --100
From: wei@xcf.Berkeley.EDU (Pei Y. Wei)
Subject: Re: Collapsible/expandable list

> Hmm... how does the WAIS interface work without something like this?

I'm not sure I follow your meaning. The WWW's WAISGate stuff works 
just fine _without_ the list's presentation being collapsible. 

With it, however, the search output might be easier to use, in that
the output could then be presented with less clutter. That is, the 
"most relevant" search results would be plainly shown, but the 
"less relevant" search leads could be folded, to be further expanded
only if the user wants to follow those links.

-Pei



From wei@ora.com  Thu Mar 24 13:11:57 1994 --100
Message-Id: <199403241208.HAA22340@ruby>
Date: Thu, 24 Mar 1994 13:11:57 --100
From: wei@ora.com (Pei Wei)
Subject: ViolaWWW Release

Another beta release of Viola is now available. Source and binary can
be found in "ftp://ora.com/pub/www/viola". For more information please
see "http://xcf.berkeley.edu/ht/projects/viola/README".

Aside from the numerous bug fixes, its got a working prototype of the
collapsible/expandable <UL>  :-)

-Pei						Pei Y. Wei
						O'Reilly & Associates, Inc



From dsr@hplb.hpl.hp.com  Thu Mar 24 13:57:14 1994 --100
Message-Id: <9403241253.AA28773@dragget.hpl.hp.com>
Date: Thu, 24 Mar 1994 13:57:14 --100
From: dsr@hplb.hpl.hp.com (Dave Raggett)
Subject: Re: Collapsible/expandable list

Pei's suggestion of folded lists is an interesting one:

> The FOLD attribute specifies that the list is foldable. And the value 
> of that attribute specifies whether or not to fold the intial layout 
> of the list.

I think FOLDED would read better, e.g.

<UL FOLDED=NO LABEL="W3 Designs">
  <LI><UL FOLDED=NO LABEL="HTML Stuff">
      <LI>Stuff a.
      <LI>Stuff b.
      </UL>
  <LI><UL FOLDED=YES LABEL="HTTP Stuff">
      <LI>Stuff a.
      <LI>Stuff b.
      </UL>
  <LI><UL FOLDED=YES LABEL="More Stuff">
      <LI>Stuff.
      <LI>Stuffy.
      </UL>
</UL>

Surely this is a sufficiently general mechanism which should apply
to other elements e.g. OL, DL, FORM and DIVn ?

Best wishes,

Dave Raggett



From Jared_Rhine@hmc.edu  Thu Mar 24 14:52:15 1994 --100
Message-Id: <199403241348.FAA08980@osiris.ac.hmc.edu>
Date: Thu, 24 Mar 1994 14:52:15 --100
From: Jared_Rhine@hmc.edu (Jared_Rhine@hmc.edu)
Subject: Re: SOAP Requirements 

[speaking of the HTML+ META tag]

Roy> Right, but it only does that for HTML documents being served by an HTTP
Roy> server.  Outside of that context, it simply doesn't make sense to use
Roy> the HTML syntax of the META construct.

But it makes no sense to define the syntax without reference to the META
syntax if it is intended that SOAPs have the ability to be encoded within a
META.  I could probably come up a syntax exactly more appropriate to the
SOAP model, but unless it is compatible with the HTTP header syntax, I've
lost a desired design specification.

Roy> If the SOAPs are located in some external source (file or database or
Roy> nameserver), then the syntax should be appropriate for that source
Roy> (table entries, database records, linked lists, etc.).  Defining them
Roy> using HTML syntax only confuses the matter.

I agree, but, again, the SOAP naming syntax must not preclude transmission
in either the request or response of an HTTP transaction.  This is the main
reason I was discussing the syntax in terms of the META syntax, because the
META syntax automatically provides HTTP compliance.

Roy> It would be easier for the reader (me) if you just present the output
Roy> (i.e. the actual header) and don't worry about the syntax of how it is
Roy> stored.

Ok, understood; you've made a good point.  Please realize that a great deal
of the things I write were thought up while I wrote the document.  I
apologize for forcing others to listen to my ramblings while I thrash out
some ideas over the course of my letter.  But answering queries like yours
really help me to refine the ideas.  I didn't realize exactly what I was
looking for (a SOAP naming syntax compatible with HTTP request/object header
transmission) until started answering this letter.

JRhine> The model I'm proposing is to construct the information that would
JRhine> normally be contained within the META tag via a dynamic lookup on a
JRhine> distributed database backend (the RLS).

Roy> Such a solution would also be inadequate for non-HTML documents
Roy> (e.g. MPEG movies or GIFs).

JRhine> I certainly don't see why that is the case; if the transactions
JRhine> headers (the moral equivalent of the META tag, in my mind) are
JRhine> constructed from information external to the object, how in any way
JRhine> would that not apply to MPEGs or GIFs?

Roy> There is no equivalent way to embed such information in a GIF without
Roy> changing the GIF specification.

Once again, I _do not_ wish to embed this metainformation in the document,
even within the META element.  The vast majority of headers I wish to
generate will be generated on the fly by the server.  Thus, it does not
matter at all that I can't encode the META element within the GIF syntax.  I
don't want to.

Roy> The only way to provide such information for GIFs (and any other
Roy> non-HTML format document) to the server for use as header information
Roy> is to provide it via external tables (whether those tables be in the
Roy> form of a flat file, database, or DNS lookups).

JRhine> I want something slightly more complex than a table; I want a dynamic
JRhine> database query, possibly constructed from information obtained via
JRhine> distributed means (not every server will be able to store all the
JRhine> information for all documents).

Roy> Well, this has little to do with the rest of this discussion, but I
Roy> think you are trying to do too much here.  If retrieving a document
Roy> (particularly via hypertext) requires multiple DNS-type lookups to
Roy> remote sites, then SOAPs will become a curse of death.

I don't think the goals should be abandoned because current implementations
of distributed information system backends are weak.  IHMO, DNS provides
extremely good service considered the task it is faced with.  Consider, your
already doing a DNS lookup every time you make a connection; how horrible is
the response there.  As with DNS, there should be secondary servers,
providing redundancy of service for large portions of the SOAP space.
Caching will also improve response.  You can cache a lot of SOAPs; they are
generally much smaller than the documents that current Web caching
prototypes are caching.

If performance actually turned out to be abismal (and I refuse to accept
that as a foregone conclusion), then you would simply lower the number of
SOAPs that you are applying to a particular search.  The user is already
prepared to wait a little while; they are performing a search, after all.
And extra second or two to apply all the SOAPs isn't going to be "a curse of
death".

Roy> Even one such lookup (beyond that of finding the original document)
Roy> would be longer than a typical user is willing to wait for a document.

I don't understand the assumption that performance will be horrible.  It is
effectively a DNS lookup; what's the longest you've waited to resolve a
hostname?  Obviously current Web users don't think it excessive; they seem
instead to be increasing their utilization of the system.

Roy> Further, if the document providers did not have control over what SOAPs
Roy> are applied, then an unethical user could kill any document by simply
Roy> applying several SOAPs to it -- the nature of the SOAPs wouldn't matter
Roy> -- just their existance would be enough to kill the server.

What?  SOAPs are applied by the end user; no one but me can select what
SOAPs are applied to my search.  And no server is required to serve SOAPs
from anyone who wants to register a SOAP.  If someone wishes to issue a
SOAP, they are responsible for providing the resources necessary to serve
that SOAP.

Roy> The only practical solution is to allow the document provider to
Roy> control what SOAPs are applied and to store those SOAPs at the same
Roy> location as the document.

In all honesty, you have not convinced me a bit.  Could you cogently review
the reasons you think external, distributed SOAPs are not feasible, and why
those problems don't apply to other distributed object systems?

JRhine> Note also that the Interpedia project has not (yet) made the
JRhine> assumption that a SOAP must expire when a document version changes,
JRhine> since the question of implied SOAPs is still open.  Yes, yes, it
JRhine> seems the logical thing to do, and many documents will operate that
JRhine> way by default, but I have yet to be convinced that it applies in
JRhine> all cases.  I won't sacrifice SOAP reliability to gain a minor
JRhine> advantage, though.

Roy> Really?  I'd think this would be obvious.  E.g., if version 3.1 is
Roy> given the SOAP American_Children's_Association!
Roy> suitable-for-children=true and version 3.2 contains (whether
Roy> accidentally or purposely) a pornographic picture, and the SOAP is
Roy> automatically carried forward, then the American Children's Association
Roy> becomes criminally liable for contributing to the delinquency of
Roy> minors.

Oh, please.  One, as I clearly stated, by far the vast majority of the
documents, such as the situation you describe, would _default_ to expiring a
SOAP upon any change in the source document.  Second, I am not criminally
liable for advocating any type of behavior I wish.

Roy> Absolutely nobody in their right mind gives a real Seal of Approval
Roy> without first inspecting the specific version of the product given
Roy> approval.
 
Ok, how about a document which is published by the same organization which
issues the SOAP?  One mechanism is cluttering up the SOAP space by issuing
an entirely new SOAP for this document and duplicating a lot of information
(unless we come up with a good property inheritance scheme).  Or one could
define that any version 3.x of this document has been approved.  The latter
mechansim is cleaner, more powerful, and scales better.

All I am claiming is that it _might_ be desireable to have inherited SOAPs
(or even SOAP property inheritance).  It's just not my style to assume
certain paradigms are the only way of doing something or that the convential
wisdom is always correct.  I don't design any system which precludes future
expansion and I won't do it with the SOAP specification.  At this early in
the development of any project, brainstorming is essential.

Roy> BTW, I just noticed the use of an apostrophe in that SOAP.  I'd be
Roy> surprised if that is an allowable character in your naming scheme.

Currently, the only specification I've made is that the bang is the path
separator, the equal sign is reserved key/content separation (well,
actually, I just now specified that) and the character set is defined is
defined by RFC 822 (possibly ISO LATIN 1, as allowed by the HTTP spec).
Thus, apostrophe's are allowed, as are spaces, for that matter.  It's not
_my_ naming scheme; it's HTTP's.

JRhine> Is there any particular advantage to grouping all the SOAPs on a
JRhine> single header field instead of using multiple header fields, one per
JRhine> SOAP?

Roy> Nope, I just think it looks better and I wanted to show an example of
Roy> using multiple data items (separated by comma and/or whitespace) in one
Roy> header.

Thanks; I hadn't considered the option before.

Roy> Even with multiple SOAP headers, this is what you should get back from
Roy> an rfc822 parser.

I didn't know there was a RFC 822 specification that multiple headers should
be squished into a single key/content value to be passed to the client.  Is
this what normally happens with Received headers?  None of the parsers I've
ever written have done that, and upon a second look, I fail to find anything
in section 3 of RFC 822.  Could you provide some more detailed references
that would support your assertion?

--
Jared Rhine         Jared_Rhine@hmc.edu
wibstr              Harvey Mudd College
                    http://www.hmc.edu/www/people/jared.html

"To hear many religious people talk, one would think God created the
 torso, head, legs and arms, but the devil slapped on the genitals."
        -- Don Schrader



From paolo@venedig.ai.univie.ac.at  Thu Mar 24 15:16:06 1994 --100
Message-Id: <199403241412.AA17191@moskau.ai.univie.ac.at>
Date: Thu, 24 Mar 1994 15:16:06 --100
From: paolo@venedig.ai.univie.ac.at (paolo petta)
Subject: Re: FOLDED

[original message by Dave Raggett ]
> Pei's suggestion of folded lists is an interesting one:
> ...
> Surely this is a sufficiently general mechanism which should apply
> to other elements e.g. OL, DL, FORM and DIVn ?

Is there any "deeper" similarity/relation between this suggested
FOLDED attribute and the HTML+ EFFECT attribute for <A> tags?

>From htmlplus.doc:
> effect  A string defining how the linked node is shown: "replace",
> "new", "overlay",
> with the default effect of replacing the current document.

although with the later restriction/attenuation:
> The effect attribute is a hint and may be disregarded by browsers. It
> allows you to click on an image and to see a linked movie as an
> overlay at the same position. The browser tries to position the
> overlay at the same origin as the link. In some cases, the linked node
> is a description of the current node. By including effect="new", the
> linked node will appear in a new window so that users can see both
> nodes at the same time. This hint should be used sparingly!

Phrased differently - why should the 'folding' be restricted to
lists and forms? (I get to think of replacement buttons in Guide,
"transclusion",...)

Apologies for the extremely ad-hoc style&contents of this message: I
wanted to mention this while the topic was still a current focus of
discussion. 

Thank you

paolo



From cwilson@ncsa.uiuc.edu  Thu Mar 24 15:53:26 1994 --100
Message-Id: <9403241449.AA02226@void.ncsa.uiuc.edu>
Date: Thu, 24 Mar 1994 15:53:26 --100
From: cwilson@ncsa.uiuc.edu (Chris Wilson)
Subject: Re: Collapsible/expandable list

Dave Raggett sez:
>Pei's suggestion of folded lists is an interesting one:
>...
>I think FOLDED would read better, e.g.

Hmm.  FOLDED doesn't seem to say anything about foldability, though, only if 
the item is currently folded.

>Surely this is a sufficiently general mechanism which should apply
>to other elements e.g. OL, DL, FORM and DIVn ?

This was my initial thought also - wouldn't it make more sense to have an 
enclosing <FOLDED> tag (or something like that), with an ALT attribute for 
the text to use when folded?  (Or TEXT, TITLE, whatever.)  It seems that if 
the leap is going to be made to expandable/compressible sections of HTML, it 
should not be limited to just ULs.

-Chris Wilson
 cwilson@ncsa.uiuc.edu




From bert@let.rug.nl  Thu Mar 24 16:30:38 1994 --100
Message-Id: <9403241527.AA20438@freya.let.rug.nl>
Date: Thu, 24 Mar 1994 16:30:38 --100
From: bert@let.rug.nl (Bert Bos)
Subject: Re: FOLDED

I second Pei's idea of FOLDED elements in HTML. (If only because it
show the added value of electronic documents over paper ones...) Maybe
it can even be generalized, not only to other elements than lists, but
also to other types of `folding'.

Paolo's suggestion that there is overlap with the EFFECT attribute of
A elements is valid, but I don't think they are the same.

First, there is no longer an EFFECT attribute, but the same semantics
could be achieved with the other attributes, if we wanted to. But do
we?

Second, I think the hyperlinks are for larger units of information
than the folded elements. Although you can switch back and forth
between two linked nodes, this is usually a result of a bad design of
the hypertext. Normally you go from node to node, not expecting two
nodes to be merely different views onto the same information.

Folding of elements is much more a layout issue. Most layout is
2 dimensional, but why restrict it to that? I would compare
(un)folding with choosing a different font or resizing the
window: some elements will resize with it while others won't. (There
is an echo of Ted Nelson's `stretch text' here, I think.)

Maybe folding isn't even the only thing we can do to elements.  A few
suggestions (some rather pointless, I admit): assume an attribute
EFFECT=

- "FOLDED": as described by Pei
- "SCROLLING": a text that draws attention by scolling from right to left
- "FLASHING"
- "SECRET": for the solution to a puzzle (press `?' to reveal...)
- "UPSIDEDOWN"
- "WINDOWED": for looking at a very wide text through a scolled viewport
- "STACKED": the subelements are cards in a 3 dimensional pile


Bert
-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|



From timbl@ptpc00.cern.ch  Thu Mar 24 17:37:10 1994 --100
Message-Id: <9403241636.AA14779@ptpc00.cern.ch>
Date: Thu, 24 Mar 1994 17:37:10 --100
From: timbl@ptpc00.cern.ch (Tim Berners-Lee)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script 


> 

> Date: Tue, 22 Mar 1994 17:41:08 --100  from rst@ai.mit.edu

>    How about:
> 

>        <meta name="Summary"
>        value="MIT AI lab events, including seminars, conferences, and  
tours">


This suggestion (on www-talk@info.cern.ch) happens to overlap with
an SGML suggestion on uri@bunyip.com, in a discussion of URC
(Universal Resource Citations, aka Metainformation?).
so I cross-post.

Another possibility is to use

	<meta name="summary">
	MIT AI lab events, including seminars, conferences, and tours
	</meta>

which has the advantage that it can be nested:

	<meta name="author">
	    <meta name="name">Jane Doe</meta>
	    <meta name="email">jd@weird.com</meta>
	    <meta name="urn">/people/1967/us/va/12437234hgj3246h</meta>
	</meta>
	
and is equivalnt to the LISP which was also proposed on
the uri list.  This way of using SGML gets around the necessity to
write a DTD every time a new fieled name crops up somewhere,
but has the disadvantage that you can't check it using an
SGML parser (So what? I hear you say).  I am comparing it here with

	<author>
	    <name>Jane Doe</name>
	    <email>jd@weird.com</memail>
	    <urn>/people/1967/us/va/12437234hgj3246h</urn>
	</author>

Perhaps it would be useful to distinguish between two
semantics:

1.   A noun clause for the object which has properties
	urn=sdfgwkedf, height=1237123, fsize=9.5

2.   A *statement* that the object define by
	urn=sdfhjsdf
     has properites
         height=1237123, fsize=9.5
	 

The URC discussion is only considering point 1, but I wonder whether
in fact the information is in fact more of the form of point 2.

<ramble>
Maybe we need a more mathematical expression

	For the book x such that
		x.isbn = 1231231232
	I assert that {
		x.price = $23;
		x.author= y such that {
			y.name="fred"
		}
	}
	
[A x . isbn(x)=12378097 E edition e . format(e,x) & back(e)=hard &  
price(e)=$12 Where "A" and "E" should be rotated through 180 degrees
of course (-:  ]

Rambling into SGML:

<forall id=x>
   <suchthat>
      <meta idref=x name="name">John Doe</meta>
   </suchthat>
   <assert>
      <meta idef=x name="state">ficticious</meta>
   </assert>
</forall>

This is, of course, ridiculous, but there is a serious point in it,especially
for systems which store meta information as retrieval hints.

</ramble>

timbl



From hoesel@chem.rug.nl  Thu Mar 24 19:08:24 1994 --100
Message-Id: <9403241804.AA01172@Xtreme>
Date: Thu, 24 Mar 1994 19:08:24 --100
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Re: Collapsible/expandable lis

about Pei Wei's suggestion for using FOLD (ED):

I think it is not a good idea, because it lacks general usage.
better is a general way of including any URL inside the document;
more like

the plane was a boeing 747
<src="URL">detailed description</a>
bla bla
and the URL would be include inside the document
just as in <img src="URL">
the alt tag could be defined too; and it even would be a reason
to forget about the <img part alltogether, because the normal
rules would apply and the browser would see that is is an image, but
that is a point that is a bit besides the track.
The real point is that you can use the <src anywhere in your document; not
just in lists.
A browser could always include them, or by means of a switch a la 
'delayed image loading' could just make it clear that there is something
that could be include; but you didn't yet.

great for copyright messages and lots of other stuff.
could be cached the same way images are.

I didn't follow the specs of html+, so it may in fact already be possible.
Pei Wei's example of the folded list would easely be created with the <src
element.

- frans






From hoesel@chem.rug.nl  Thu Mar 24 19:12:50 1994 --100
Message-Id: <9403241809.AA01181@Xtreme>
Date: Thu, 24 Mar 1994 19:12:50 --100
From: hoesel@chem.rug.nl (frans van hoesel)
Subject: Collapsible/expandable list

just to follow up on my own mailing.

perhaps better is to have
<html src="url"> for using elements with possible delayed loading.
that makes sure that the browser can see the difference between
<img src="url">
and can display the delayed element differently.
(otherwise it would have no way of knowing what type of
 element would be returned via the url (img/text) on beforehand)

- frans





From rst@ai.mit.edu  Thu Mar 24 19:33:37 1994 --100
Message-Id: <9403241830.AA04327@volterra>
Date: Thu, 24 Mar 1994 19:33:37 --100
From: rst@ai.mit.edu (Robert S. Thau)
Subject: Future of meta-indices: site indexing proposal and Perl script 

   Date: Thu, 24 Mar 1994 17:38:28 --100
   From: Tim Berners-Lee <timbl@ptpc00.cern.ch>

   This suggestion (on www-talk@info.cern.ch) happens to overlap with
   an SGML suggestion on uri@bunyip.com, in a discussion of URC
   (Universal Resource Citations, aka Metainformation?).
   so I cross-post.

   Another possibility is to use

	   <meta name="summary">
	   MIT AI lab events, including seminars, conferences, and tours
	   </meta>

   which has the advantage that it can be nested:

	   <meta name="author">
	       <meta name="name">Jane Doe</meta>
	       <meta name="email">jd@weird.com</meta>
	       <meta name="urn">/people/1967/us/va/12437234hgj3246h</meta>
	   </meta>

   and is equivalnt to the LISP which was also proposed on
   the uri list.

Unfortunately, over the short term, it also has a disadvantage, in that
documents with this particular form of metainformation coding would
probably be mishandled by plain-jane HTML browsers --- these would ignore
the <meta> and </meta> tags (as they generally ignore any tags which they
aren't specifically prepared for), and present the values of the
metainformation into the document text.  

By contrast, with the <meta name="..." value="..."> scheme which I (and a
few others) have been discussing recently, the browsers don't wind up
displaying the metainformation, since it's *all* buried in tags which they
simply ignore.

(Notice of covert agenda: the reason I'm particularly concerned about this
is that I'm looking for something I can use to drive my autoindexing script
now, meaning that it has to cope well with the existing infrastructure,
including browsers which have never heard of any sort of <meta ...> tag).

Still, if there were a nested structure which the existing browsers would
ignore, I and my indexer could easily live with that.  There's a hint of a
way to get one in the distinction below:

   Perhaps it would be useful to distinguish between two
   semantics:

   1.   A noun clause for the object which has properties
	   urn=sdfgwkedf, height=1237123, fsize=9.5

   2.   A *statement* that the object define by
	   urn=sdfhjsdf
	has properites
	    height=1237123, fsize=9.5


If we use different tags for the two levels, we could have a structure like
this (with apologies in advance for any unintended breach of SGML convention):

	<metaobject name="author">
	    <metastmt name="name" value="Jane Doe">
	    <metastmt name="email" value="jd@weird.com">
	    <metastmt name="urn" value="/people/1967/us/va/12437234hgj3246h">
        </metaobject>

One thing that is lost this way is that you can't put HTML tags in the
metavalues, but it's not clear that's necessarily wise to permit anyway.

Comments?

   timbl

rst



From connolly@hal.com  Thu Mar 24 20:20:18 1994 --100
Message-Id: <9403241917.AA05799@ulua.hal.com>
Date: Thu, 24 Mar 1994 20:20:18 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: LISP for Complex URC Sytax [WAS: Re URC func spec ] 

In message <m0pjpuS-0004MnC@inca.gate.net>, Dirk Herr-Hoyman writes:
>At  8:32 PM 3/23/94 -0500, Simon E Spero wrote:
>>Using a lisp syntax is a great idea (hell, my next home system is going to be
>>a symbolics 3645); however it might be more appropriate to use SGML to 
>>do structured text. If you compare the number of HTML pages out there with
>>the number of completed IAFA templates, it's clear that using RFC822 style
>>layout isn't always a big win.
>
>And with SGML there is at least some way of handling the character set issue.

Would you care to expand on that? I've been studying the expression of
character set issues in SGML for about two years, and I'm almost
completely confused at this point.

As far as I know, the character set issues that SGML deals with are
along the lines of "Ok... I've got this hardcopy of fred's sgml document,
so I'll key it in. Now... when fred writes &#65;, what character is that?
Ahh... I see in the SGML declaration that he's using ISO-646:1983, so
that's an 'A' character."

There is only ONE character set used in an SGML document -- the
document character set. You can play games with NOTATIONS, i.e.  ways
of interpreting the characters that differ from the document character
set. I guess that would do for the purposes of using different languages
for different elements, ala

	<Title notation="Kanji">&#134;&#125;...</Title>
(apologies to folks who actually know how to express Kanji using octets...)

>urc.dtd, why not?

Well, mostly because I think HTML is something of a disaster. My group
is trying to build a product that deals with HTML, and it's nearly
impossible. HTML is, in practice, not defined by any DTD. It's defined
by the NCSA mosaic source code.

But I'll have to admit I have been playing around with something you
could call urc.dtd. It's part of the "whole ball of wax" sort of
Integrated Open Hypermedia formalism I'm trying to figure out...

Some folks define hypertext as "text that's not constrained to be read
linearly." I think the more salient part about hypertext is that the
computer helps you nagivate and read it.

So the object of the game, in my view, is to allow the computer to
help us express/understand the messages we send daily. Things like:

	"The lcs.mit.edu:/pub/contrib directory is mirrored at
	gatekeeper.dec.com:/pub/mirror/mit/contrib"

	"On Feb 27, Fred Flinstone said 'lakjsdlfkj' (and you
	can read the rest of what he said in ...)"

	"I think section 2 of that draft is bogus (here's how
	you can read the draft yourself...)"

The reason that MIME is such a breakthrough for IOH is that there is
now a reliable way for the machine to peek into the body of the
message and understand it. As long as messages were plain text, you
were constrained to heuristic strategies. And if messages are programs
(like TeX and nroff) then there's only one way to get information out
of them -- run them. SGML offers an unprecedented level of structure
and reliability that we can use to enhance communications.




From pmr1716@ggr.co.uk  Thu Mar 24 20:36:07 1994 --100
Message-Id: <Pine.3.89.9403241936.A1314-0100000@ukwbf07>
Date: Thu, 24 Mar 1994 20:36:07 --100
From: pmr1716@ggr.co.uk (Murray-Rust Dr P)
Subject: ANNOUNCE: MAP_MARKER V0.1, Editor for clickable maps


We have just started reading this newsgroup and noticed the discussion:

koblas@netcom.com (David Koblas) wrote:
>> 
>> I recently got around to reading the NCSA "document" on how to setup
>> clickable image maps.  Now the big question is how many people use
>> this method (XPaint & XV) for creation maintinace of your image maps, or
>> what do you use?
>> 
>> What features in a tool would you like to see to make this task easier?

Kevin Hughes * kevinh@eit.com also wrote:

>	I would certainly like to see an X program that allows you to
>create the shapes over an image, showing a transparent overlay of each
>shape to easily show any overlapping areas. I'd like it to be able to
>reload the shapes from a config file and have the points editable, too!
>

We have essentially finished just such a tool and append a pre-release 
notice.  We'd be grateful for beta-testers - please be gentle as this is 
Gudge's first project!

   o /
----X------------------------------------------------------------------------
   o \


				MAP_MARKER V0.1

	MAP_MARKER is a tool for generating clickable image maps for use
with HTML browsers.  It has been written by Gudge Chandramohan,
undergraduate from South Bank University, London, UK as part of an
industrial training year at Glaxo R&D, Greenford.  This is a PRE-RELEASE
version to meet the need for creating image maps. 

	We would be grateful for anyone wishing to beta-test this over the
next week before release.  You will need tcl7.3/tk3.6.  If you wish to use 
colour images you will need them as rppm files and must have the photo
widget as part of your "wish".  We would prefer to mail the files as shar
files or tar.Z.uue in the first instance (ca 170 Kb), but could upload to 
a ftp/incoming if required.  When ready we shall probably deposit the 
code in the tk archive.

MAP_MARKER V0.1 is written in tcl/tk and allows the user to:

	- read an image file in EITHER *.xbm OR *.rppm format

	- optionally read in a NCSA format imagemap file (*.map)

	- create, move, edit, copy and delete the following shapes:
		- circle
		- rectangle
		- polygon

 	- display these in a variety of styles and colour on top of the image

	- add an optional grid

	- annotate objects with comments and a URL.  (The program can force the
		user to annotate all objects and will detect unannotated ones).

	- interactively test the validity of all URL's associated with objects

        - save the session as a NCSA imagemap file, including the display 
		attributes

The first full release will have the additional features:

	- Motif-style bindings

	- Filebrowsers for images, maps and URLs

	- Display of comments as textual annotation on the image

	- automatic conversion of gif files to images.


Gudge Chandramohan , akc22282@ggr.co.uk
Peter Murray-Rust,   pmr1716@ggr.co.uk



------------------------------------------------------------------------------
Peter Murray-Rust                  | "Nothing exists except atoms and empty
pmr1716@ggr.co.uk                  | space; all else is opinion" (Democritos).
Protein Structure Group, Glaxo Group Research, Greenford, MIDDX, UB6 0HE, UK
------------------------------------------------------------------------------




From montulli@stat1.cc.ukans.edu  Thu Mar 24 22:47:39 1994 --100
Message-Id: <9403242143.AA18253@stat1.cc.ukans.edu>
Date: Thu, 24 Mar 1994 22:47:39 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: Collapsible/expandable list

> 
> Pei's suggestion of folded lists is an interesting one:
> 
> > The FOLD attribute specifies that the list is foldable. And the value 
> > of that attribute specifies whether or not to fold the intial layout 
> > of the list.
> 
> I think FOLDED would read better, e.g.
> 
> <UL FOLDED=NO LABEL="W3 Designs">
>   <LI><UL FOLDED=NO LABEL="HTML Stuff">
>       <LI>Stuff a.
>       <LI>Stuff b.
>       </UL>
>   <LI><UL FOLDED=YES LABEL="HTTP Stuff">
>       <LI>Stuff a.
>       <LI>Stuff b.
>       </UL>
>   <LI><UL FOLDED=YES LABEL="More Stuff">
>       <LI>Stuff.
>       <LI>Stuffy.
>       </UL>
> </UL>
> 
If we go with this I would rather see the syntax as:
 
 <UL FOLDED=NO LABEL="W3 Designs">
   <LI><UL FOLDED=NO>HTML Stuff
       <LI>Stuff a.
       <LI>Stuff b.
       </UL>
   <LI><UL FOLDED=YES>HTTP Stuff
       <LI>Stuff a.
       <LI>Stuff b.
       </UL>
   <LI><UL FOLDED=YES>More Stuff
       <LI>Stuff.
       <LI>Stuffy.
       </UL>
 </UL>
This would allow multiline labels, entities and images.

But I would rather see a more general purpose mechanism
that would allow arbitrary URL's to be expanded within
the document.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *      Let's face it, I'm a nerd, why else would I have a sig file?      *
  **************************************************************************



From Lillian.Elam@eng.sun.com  Fri Mar 25 00:26:06 1994 --100
Message-Id: <9403242332.AA01542@bayside.Eng.Sun.COM>
Date: Fri, 25 Mar 1994 00:26:06 --100
From: Lillian.Elam@eng.sun.com (Lile Elam)
Subject: Compton's patent is dead.


Here's some good news. :)  Compton's patent has been rejected upon
re-examination and is now dead.. This will really make a difference 
for things such as Mosaic and is a great win for all.

-lile



------- start of forwarded message (RFC 934) -------
Message-Id: <199403241941.AA04453@world.std.com>
From: srctran@world.std.com (Gregory Aharonian)
To: patents@world.std.com
Subject: Compton's patent rejected during reexamination
Date: Thu, 24 Mar 1994 14:41:58 -0500


    Ding-dong, the wicked patent is dead.  Though I bet Compton's made more
money on the PR generate than any royalties that could have been collected.
Surprisingly, there are more serious software patent infringement cases with
worse claims than the Compton patent, though not getting as much attention.

Greg Aharonian
Internet Patent News Service


==============================================================
    ||||||||||     ||||||||||     Information Express
        ||        ||               a voorhees report
       ||        ||               
      ||        |||||||           718-369-0906 voice                  
     ||        ||                 636-8931  MCI Mail            
    ||        ||                                
||||||||||   ||||||||||     411 First St., Brooklyn, NY 11215       
==============================================================
(c) 1994 Mark Voorhees    

PATENT OFFICE REJECTS COMPTON'S NEWMEDIA PATENT
Controversial CD-ROM filing falls during re-exam

March 24, 1994--The U.S. Patent and Trademark Office has rejected the
controversial Compton's NewMedia patent during a re-examination. 

   The patent covers basic techniques for searching and
retrieving information from CD-ROM data bases. It has generated
wide criticism in the computer and technology fields as a symbol
of the problems with software patents. Some critics say the
techniques were not new at the time of the patent filing. A
minority of critics say patents should not cover software.  

   Most or all of the 41 claims were rejected on the basis that
they were not novel or were obvious. The rejection was delivered
to Encyclopaedia Britannica, one of the patent owners, last
Wednesday. 

   Encyclopaedia Britannica and Tribune Co., the other patent
owner, will have at least once chance to persuade the office that
its rejection is wrong. Afterward, the owners can appeal either
to federal court or to an appellate tribunal within the patent
office.
 
   Examiner Archie Williams cited a popular computer book, The
HyperCard Handbook, and several patents as evidence that
Compton's NewMedia was not the first inventor of the CD-ROM
techniques. 

   Patent Commissioner Bruce Lehman took the rare step of
ordering the re-exam after industry protest over how
the patent would upset the advance of multimedia and interactive
technologies.

|      Mark Voorhees     | voorhees reports
|                        | 411 first street
|    636-8931 MCI Mail   | brooklyn, ny 11215-2507
|                        | 1-718-369-0906 (voice)
|  markvoor@phantom.com  | 1-718-369-3250 (fax)

------- end -------






From wei@xcf.Berkeley.EDU  Fri Mar 25 01:26:48 1994 --100
Message-Id: <9403250024.AA14490@xcf.Berkeley.EDU>
Date: Fri, 25 Mar 1994 01:26:48 --100
From: wei@xcf.Berkeley.EDU (Pei Y. Wei)
Subject: Re: folding elements

Dave Ragget writes:

> Surely this is a sufficiently general mechanism which should apply
> to other elements e.g. OL, DL, FORM and DIVn ?

Oh I agree. The reason for prototyping this on <UL> is just b/c lists
are particularily amendable to be folded.

Then Chris Wilson writes:

> This was my initial thought also - wouldn't it make more sense to have an 
> enclosing <FOLDED> tag (or something like that), with an ALT attribute for 
> the text to use when folded?  (Or TEXT, TITLE, whatever.)  It seems that if 
> the leap is going to be made to expandable/compressible sections of HTML, it 
> should not be limited to just ULs.

That was my initial thought as well, to use a <FOLDER> tag, so that
anything contained in <FOLDER> can be folded away. But early discussion
with someone at ORA (Terry Allen) convinced me that it's better to use 
attributes. The factors were (not in any particular order):

* This is more of a presentation issue, and if this can be hinted with 
  attributes that browsers can easily ignore, just use attributes.

* Implementation wise (in viola anyway), it is easier to use the one 
  <FOLDER> tag, rather than stick the folding logic into every sensibly 
  foldable elements. But using the extra tag inccurs more house keeping
  resouces (violaWWW keeps lots of information about the elements, so
  that the document can be reversed back to HTML... Not an apparently 
  useful feature now, but later...).

* When the stylesheet thing comes along, the folding infomation should be
  describable in the stylesheet. When that happens, the extra tags will
  probably be extraneous.

* Less HTML mark up to write, in using attributes rather than an enclosing
  tag. Could be pretty messy if you're dealing wiht.

* Some other DTD related arguments which ezcapes me now.

Lou Montulli writes:

> If we go with this I would rather see the syntax as:
>
>  <UL FOLDED=NO LABEL="W3 Designs">
>    <LI><UL FOLDED=NO>HTML Stuff
>      <LI>Stuff a.
> [...]
> This would allow multiline labels, entities and images.

Good point... How about the following revision:

 <UL FOLDED=NO><LABEL>W3 Designs</LABEL>
   <LI><UL FOLDED=NO><LABEL>HTML Stuff</LABEL>
     <LI>Stuff a.
	...

Bert Bos writes:
> ...
> Maybe folding isn't even the only thing we can do to elements.  A few
> suggestions (some rather pointless, I admit): assume an attribute
> EFFECT=
>...
> - "STACKED": the subelements are cards in a 3 dimensional pile
     ^^^^^^^
Actually came close to doing this :)


Frans Van hoesel writes:

> I think it is not a good idea, because it lacks general usage.
> better is a general way of including any URL inside the document;
> more like
> 
> the plane was a boeing 747
> <src="URL">detailed description</a>
> bla bla
..
> perhaps better is to have
> <html src="url"> for using elements with possible delayed loading.
> that makes sure that the browser can see the difference between
> <img src="url">
> and can display the delayed element differently.
> (otherwise it would have no way of knowing what type of
> element would be returned via the url (img/text) on beforehand)

This is a valid concern, which is why I mentioned that ``perhaps later
we can put HREF or SRC into <UL>'', etc. But the thing is, for lots of
cases, it's a bit too tedious to do includes, when all you want is
to visually fold some text out of the way.

But I agree that being able to do generalized includes would be very 
useful. FYI, ViolaWWW already has an <INCLUDE> tag (for the client, 
not to be confused with the server interpreted <INC>) for just this 
situation. Although, it'd be even more useful if <INCLUDE> is made 
delayable.

Like, <INCLUDE SRC="a_quote.html"> which automatically gets fetched
and expanded; and the user triggered version:
<INCLUDE SRC="a_quote.html" FOLD=YES LABEL="A quote from Carl Jung">
or I guess:
<INCLUDE SRC="a_quote.html" FOLD=YES><LABEL>A quote from Carl Jung</LABEL>
</INCLUDE>


-Pei



From waterbug@epims1.gsfc.nasa.gov  Fri Mar 25 13:23:35 1994 --100
Message-Id: <9403251219.AA24946@epims1>
Date: Fri, 25 Mar 1994 13:23:35 --100
From: waterbug@epims1.gsfc.nasa.gov (Steve Waterbury)
Subject: Re: Future of meta-indices: site indexing proposal and Perl script


Tim BL writes:

> Another possibility is to use
> 
> 	<meta name="summary">
> 	MIT AI lab events, including seminars, conferences, and tours
> 	</meta>
> 
> which has the advantage that it can be nested:
> 
> 	<meta name="author">
> 	    <meta name="name">Jane Doe</meta>
> 	    <meta name="email">jd@weird.com</meta>
> 	    <meta name="urn">/people/1967/us/va/12437234hgj3246h</meta>
> 	</meta>
> ... [omissions] ...  I am comparing it here with
> 
> 	<author>
> 	    <name>Jane Doe</name>
> 	    <email>jd@weird.com</memail>
> 	    <urn>/people/1967/us/va/12437234hgj3246h</urn>
> 	</author>

Agree with the desire for flexibility, but it might be good to 
have certain frequently used data structures "standardized", such 
as personal information.  I don't know if the Internet Drafts for 
"SGML-based Hierarchical Attribute/Value Encoding (SHAVE)" 
(draft-adie-spci-00.*) and "SGML-based Personal Contact Information 
(SPCI)" (draft-adie-shave-00.*) are still being seriously 
considered (they are due to expire April 1994), but they seemed 
like a good approach.  

In particular, the SPCI document proposed an actual DTD for Personal 
Contact Information.  

Steve Waterbury.
                                           oo _\o
                                            \/\ \
                                              /
____________________________________________ oo _________________
"Sometimes you're the windshield; sometimes you're the bug."
- Knopfler



From jcma@reagan.ai.mit.edu  Fri Mar 25 10:06:09 1994 --100
Message-Id: <19940325090229.9.JCMA@JEFFERSON.AI.MIT.EDU>
Date: Fri, 25 Mar 1994 10:06:09 --100
From: jcma@reagan.ai.mit.edu (John C. Mallery)
Subject: Re: Collapsible/expandable list

Why not just have a scollable embedded window?

This could achieve the same pruposes and provide wider utility.



From robm@ncsa.uiuc.edu  Fri Mar 25 10:02:46 1994 --100
Message-Id: <9403250859.AA20232@void.ncsa.uiuc.edu>
Date: Fri, 25 Mar 1994 10:02:46 --100
From: robm@ncsa.uiuc.edu (Rob McCool)
Subject: ACCESS CONTROL PROBLEM in NCSA httpd



A vulnerability has been identified with NCSA httpd 1.1's access
control. The impact of this is that if you have files which are
protected by httpd's access control via the global ACF access.conf,
the protection can be circumvented and access can be gained to the
files regardless of the client's DNS hostname, host IP address, or
HTTP user name.

Any users of NCSA httpd 1.1 should pick up a patch from
ftp://ftp.ncsa.uiuc.edu/Web/ncsa_httpd/httpd_1.1/httpd-access-patch and
recompile httpd immediately. The distribution binaries and .tar files
have also been updated so that binary users can install a new copy of
the httpd binary and have the patch installed.

Thanks for your patience.

--Rob

--
Rob McCool, robm@ncsa.uiuc.edu
Software Development Group, National Center for Supercomputing Applications
It was working ten minutes ago, I swear...
<A HREF="http://hoohoo.ncsa.uiuc.edu/~robm/sg.html">A must see.</A>



From phillips@cs.ubc.ca  Fri Mar 25 09:57:59 1994 --100
Message-Id: <7876*phillips@cs.ubc.ca>
Date: Fri, 25 Mar 1994 09:57:59 --100
From: phillips@cs.ubc.ca (George Phillips)
Subject: Re: folding elements

Of course, you can do folding with a server hack.  See
http://www.cs.ubc.ca/inmenu/top/- for an example.  You'll
also see that a round-trip for each unfolding is not a lot
of fun.  More to the point, the current proposals can't
do what my hack does because the conditional text (the list
to in-line) and the condition toggle (the triangle-thingy)
are separate.  I'd suggest going to a more general mechanism
by having some sort of tag to set conditions and and another
tag to display or supress stuff based on a condition.

Condition setting can be done by leveraging off FORMs support.
Add an INTERNAL keyword to indicate the new role.  Use a
new container tag that can test values of the NAMEd variables
of a form.  An expandable menu might look like this:

    <FORM INTERNAL><INPUT TYPE=checkbox NAME=menu1></FORM>
    Some Foods
    <WHEN NAME=menu1 VALUE=yes>
      <UL>
        <LI> Grapes
        <LI> Cheetos
      </UL>
    </WHEN>

The INTERNAL status of the form would make the browser update
as soon as something was done to it.  We could adopt the convention
that INPUTs outside of any <FORM></FORM> pair are INTERNAL.  Here's
another example:

    Show me hello in 
    <INPUT TYPE=radio NAME=lang VALUE=en>English
    <INPUT TYPE=radio NAME=lang VALUE=au>Australian
    <INPUT TYPE=radio NAME=lang VALUE=ca>Canadian
    <P>
    <WHEN NAME=lang VALUE=en> 'Morning, old chap. </WHEN>
    <WHEN NAME=lang VALUE=au> G'day, mate. </WHEN>
    <WHEN NAME=lang VALUE=ca> Hello, eh. </WHEN>

Besides expanding lists, you could use this to do delayed image
loading, forms that only show what needs to be filled out and
other useful stuff (single page hangman?).

You still can't exactly reproduce what my menu hack does.  SELECT almost
does it.  Perhaps a version of SELECT that only allows you to step
sequentially through a bunch of options:

    <SEQUENCE NAME=menu1>
      <OPTION VALUE=closed><IMG SRC=closed.gif ALT=")">
      <OPTION VALUE=open><IMG SRC=open.gif ALT="v">
    </SEQUENCE>

SEQUENCE would create a selectable area which would cycle through
the options when selected.  In this case it's really only a cosmetic
advantage over a check box.  It could be useful in conventional
forms.  Whatever.

You might consider this a step onto the slipperly slope of making
HTML into a turing-complete language.  Maybe.  From my late night
viewpoint, it seems worth the risks.



From courtaud@limeil.cea.fr  Fri Mar 25 15:48:41 1994 --100
Message-Id: <9403251328.AA02294@limeil.cea.fr>
Date: Fri, 25 Mar 1994 15:48:41 --100
From: courtaud@limeil.cea.fr (Didier.Courtaud)
Subject: Paper version of an HTML document

I have recently built a large documentation on HTML with numerous links
inside it.

It works very well with the browsers but I am now asked to make a paper version
of this documentation.

So my problem is :

How can I convert this document in a paper document without repeating the
linked sections ?

Did someone had the same problem and has he got a solution ?

(:(

------------------------------------------------------------------------------
Didier  COURTAUD

 Graphical Applications Group Leader
   Commissariat a l'Energie Atomique
    France

       Phone : (33 - 1) 45 95 67 07
         Fax   : (33 - 1) 45 95 95 55
           Email : courtaud@limeil.cea.fr
-----------------------------------------------------------------------------





From CJA@ml0.ucs.edinburgh.ac.uk  Fri Mar 25 14:08:28 1994 --100
Message-Id: <1F77794014E@ml0.ucs.ed.ac.uk>
Date: Fri, 25 Mar 1994 14:08:28 --100
From: CJA@ml0.ucs.edinburgh.ac.uk (Chris Adie)
Subject: Re: Future of meta-indices: site indexing proposal and Perl

Steve Waterbury writes:
> Agree with the desire for flexibility, but it might be good to 
> have certain frequently used data structures "standardized", such 
> as personal information.  I don't know if the Internet Drafts for 
> "SGML-based Hierarchical Attribute/Value Encoding (SHAVE)" 
> (draft-adie-spci-00.*) and "SGML-based Personal Contact Information 
> (SPCI)" (draft-adie-shave-00.*) are still being seriously 
> considered (they are due to expire April 1994), but they seemed 
> like a good approach.  

I've had some good feedback about these docs, and was going to write up
the changes and update the internet drafts.  Just havn't had time yet.
I forgot they were about to expire - thanks for reminding me.

There was some interest in this (and in an alternative called STIF/PCI
by Dave Crocker) at a MIME-types BOF at Houston IETF.  Unfortunately I
was at another conference and couldn't attend.  There was some interest in
forming an IETF working group to take things forward, but nothing seems
to have happened.

Regards,

Chris Adie                                   Phone:  +44 31 650 3363
Edinburgh University Computing Service       Fax:    +44 31 662 4809
University Library, George Square            Email:  C.J.Adie@edinburgh.ac.uk
Edinburgh EH8 9LJ, United Kingdom



From jac15@po.cwru.edu  Fri Mar 25 17:36:52 1994 --100
Message-Id: <9403251633.AA11158@thor.INS.CWRU.Edu>
Date: Fri, 25 Mar 1994 17:36:52 --100
From: jac15@po.cwru.edu ()
Subject: subscribe?


I know, I know, I hate it when people do this too, but: could someone
please send me info on how to get signed up to this mail-list?

Thanks a lot,
yours virtually,
JC





From bajan@bunyip.com  Fri Mar 25 16:40:55 1994 --100
Message-Id: <9403251528.AA00257@mocha.bunyip.com>
Date: Fri, 25 Mar 1994 16:40:55 --100
From: bajan@bunyip.com (Alan Emtage)
Subject: Re: Indexing the List of Lists

Hi Rob,

> Alan Emtage writes regarding Lists of Lists:
> >... I don't believe that manual maintenance of this kind of data
> >is feasible any longer.... the Internet is now too big for this kind of
> >thing. 
> I strongly suggest that a first step in any effort of this kind must be 
> the definition of exactly what we are trying to collect information on 
> because the issues of indexing vs. the generation of a 'table of 
> contents' are very different indeed.  

[...as you can see I'm a little behind on my mail...]

Perhaps some clarification of what I said may show that we're in
agreement. Some people have taken what I wrote to mean that computers
would _catalog_ the data rather I meant that computers should gather and
index the data and at at that point humans can then come in and further
refine it into information (there being a difference between "data" and
"information"). The inital cataloging step will have to be done by humans
for the foreseeable future. I agree that the scattershot approach is
going to give us a lot of noise and little signal.


-- 
-Alan

------------------------------------------------------------------------------
Alan Emtage,				"The Left in Canada is more gauche
Bunyip Information Systems,		 than sinister"
Montreal, CANADA			 -The Economist

bajan@bunyip.com
Voice: +1 (514) 875-8611		Fax: +1 (514) 875-8134




From putz@parc.xerox.com  Fri Mar 25 19:39:25 1994 --100
Message-Id: <94Mar25.103617pst.2445@spoggles.parc.xerox.com>
Date: Fri, 25 Mar 1994 19:39:25 --100
From: putz@parc.xerox.com (Steve Putz)
Subject: Re: Collapsible/expandable list


John C.Mallery <jcma@reagan.ai.mit.edu> wrote:
> Why not just have a scollable embedded window?
> 
> This could achieve the same pruposes and provide wider utility.

I disagree.  scrollable is very different than collapsible/expandable.

-- Steve Putz



From waterbug@epims1.gsfc.nasa.gov  Fri Mar 25 20:28:44 1994 --100
Message-Id: <9403251924.AA25362@epims1>
Date: Fri, 25 Mar 1994 20:28:44 --100
From: waterbug@epims1.gsfc.nasa.gov (Steve Waterbury)
Subject: Re: Collapsible/expandable list


> John C.Mallery <jcma@reagan.ai.mit.edu> wrote:
> > Why not just have a scollable embedded window?
> > 
> > This could achieve the same pruposes and provide wider utility.
> 
> I disagree.  scrollable is very different than collapsible/expandable.
> 
> -- Steve Putz

I second that.

Steve Waterbury.



From omy@San-Jose.ate.slb.com  Fri Mar 25 20:42:11 1994 --100
Message-Id: <9403251936.AA11475@San-Jose.ate.slb.com>
Date: Fri, 25 Mar 1994 20:42:11 --100
From: omy@San-Jose.ate.slb.com (Omy Ronquillo)
Subject: ahost?


	Hi,

	Where can I get an application like xhost that will do it for
	sound? I guess it's called ahost. (??)

	Omy



From vinay@eit.COM  Fri Mar 25 21:43:16 1994 --100
Message-Id: <9403252040.AA10583@eit.COM>
Date: Fri, 25 Mar 1994 21:43:16 --100
From: vinay@eit.COM (Vinay Kumar)
Subject: Re: ahost?

There is something called "AudioFile" server available from

	ftp://crl.dec.com/pub/DEC/AF

--
  Vinay Kumar
vinay@eit.com

-----------------------------
> From www-talk@www0.cern.ch Fri Mar 25 11:46:57 1994
> 
> 
> 	Hi,
> 
> 	Where can I get an application like xhost that will do it for
> 	sound? I guess it's called ahost. (??)
> 
> 	Omy
> 



From connolly@hal.com  Fri Mar 25 22:26:23 1994 --100
Message-Id: <9403252124.AA06313@ulua.hal.com>
Date: Fri, 25 Mar 1994 22:26:23 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: should we standardize HTML/HTMLPLUS? 


Eric van Herwijnen writes:
>I want to start a new work item within ISO/TC 46/SC 4/WG6 which would be
>the standardization of a DTD based on HTML/HTMLPLUS.
>
>Comments please.
>
>Please send email to ERIC@CERNVM.CERN.CH if you are interested in
>participating in the working group.

The HTML specification was last published as:

ftp://ds.internic.net/internet-drafts/draft-ietf-iiir-html-01.txt

Oops! It's been deleted. It expired some time ago. The working document
is available at:

http://info.cern.ch/hypertext/WWW/MarkUp/HTML.html

My notes on the design are available at:

http://www.hal.com/~connolly/html-design.html


In an effort to replace the Internet Draft of HTML, I solicited and
nominated working group memebers and came up with:

# HTML design discussion forum...
html-design:	Dan Connolly -- HTML draft spec author <connolly@hal.com>,\
	Tim Berners-Lee -- WWW Project Lead <timbl@info.cern.ch>,\
	Terry Allen GNN representative <terry@ora.com>,\
	Dave Raggett -- HTML+ draft spec author <dsr@hplb.hpl.hp.com>,\
	Dave Hollander -- SDL designer <dmh@hpfcma.fc.hp.com>,\
	Elliot Kimber -- HyTime advocate <kimber@passage.com>,\
	Peter Flynn -- HTML educator<pflynn@curia.ucc.ie>,\
	Rob McCool -- NCSA representative<robm@ncsa.uiuc.edu>,\
	Erik Naggum -- SGML advocate <erik@naggum.no>,\
	Lou Burnard -- TEI representative <lou@vax.ox.ac.uk>,\
	WWW Talk List -- Archive Mechanism <www-talk@info.cern.ch>


Dan



From chi@nb.rockwell.com  Fri Mar 25 23:26:26 1994 --100
Message-Id: <9403252222.AA12884@atlas.nb.rockwell.com>
Date: Fri, 25 Mar 1994 23:26:26 --100
From: chi@nb.rockwell.com (Keh-Fan Chi)
Subject: Re: ahost?

Hi,

   Try dime.cs.umass.edu
        /pub/rcf/exp/build/alpha/sound/AF/clients   (unpacked already)

    or gatekeeper.dec.com
       /.6/FF_PROGS/TELECOM/AHOST.LZH


   Calvin Chi    chi@nb.rockwell.com



From rik.harris@fulcrum.com.au  Sat Mar 26 06:52:29 1994 --100
Message-Id: <199403260548.PAA24646@brain.vifp.monash.edu.au>
Date: Sat, 26 Mar 1994 06:52:29 --100
From: rik.harris@fulcrum.com.au (rik.harris@fulcrum.com.au)
Subject: Re: Paper version of an HTML document 

courtaud@limeil.cea.fr wrote:
> I have recently built a large documentation on HTML with numerous links
> inside it.
> 
> It works very well with the browsers but I am now asked to make a paper
> version of this documentation.
> 
> So my problem is :
> 
> How can I convert this document in a paper document without repeating the
> linked sections ?
> 
> Did someone had the same problem and has he got a solution ?

I'm also very interested in this question.  I've played with a few
ideas, but it looks like the one I will go with is to use Frame to
generate and maintain the documents with conversion programs to create
HTML, and printing from Frame.  Now this isn't likely to be useful for
people who don't have access to frame, but I believe there are similar
possibilities for other software packages.  Depending on the structure
of your documents, it might be appropriate to have the linked sections
map to a cross reference ("see section 3.2 on page 3"), which could be
done automatically.

It is also a posibility to use the fact that HTML browsers ignore
information that they don't understand, and do things like:

<A HREF="blahdfad" RIKXREF="yes" RIKXREFSECTNAME="Name of a Section">...</A>

If your document is well structured, this should be quite a reasonable
solution.

I'm interested to see other people's ideas and comments.

rik.
--
The Fulcrum Consulting Group                                           o
------------------------------------------------------------------------------
Rik Harris - rik.harris@fulcrum.com.au   +61 3 621-2100 (BH)       /\
12th Floor, 10-16 Queen St. Melbourne VIC 3000.  +61 3 621-2724 (Fax)



From marca@eit.COM  Sat Mar 26 09:34:17 1994 --100
Message-Id: <199403260832.IAA14436@threejane>
Date: Sat, 26 Mar 1994 09:34:17 --100
From: marca@eit.COM (Marc Andreessen)
Subject: heretical suggestion

How about we take the state of the art of HTML plus forms as currently
supported in Mosaic for X 2.2 -- and either fully supported or getting
pretty damn close to being fully supported in a number of other
browsers -- and issue an informational RFC within the next month or
two, including a working & complete DTD.

This would add some greatly-needed coherence -- "meat", if you will --
to the WWW standards evolution process.  And just think, we could
actually register to get a real text/html MIME type finally.  While
having something that matches existing, widely-used implementations.

Cheers,
Marc



From bajan@bunyip.com  Sat Mar 26 11:02:19 1994 --100
Message-Id: <9403260959.AA06954@mocha.bunyip.com>
Date: Sat, 26 Mar 1994 11:02:19 --100
From: bajan@bunyip.com (Alan Emtage)
Subject: Re: heretical suggestion

Hi Marc,

> How about we take the state of the art of HTML plus forms as currently
> supported in Mosaic for X 2.2 -- and either fully supported or getting
> pretty damn close to being fully supported in a number of other
> browsers -- and issue an informational RFC within the next month or
> two, including a working & complete DTD.

I think that's a great idea. You can either send it to the RFC editor
directly or you can run it through the IETF under a group like IIIR.
Either way since you're documenting current practice this would not go
through the standards process so no changes would be needed (other than
possible suggestions for readability etc etc).



-- 
-Alan

------------------------------------------------------------------------------
Alan Emtage,				"The Left in Canada is more gauche
Bunyip Information Systems,		 than sinister"
Montreal, CANADA			 -The Economist

bajan@bunyip.com
Voice: +1 (514) 875-8611		Fax: +1 (514) 875-8134




From jcma@reagan.ai.mit.edu  Sat Mar 26 15:25:57 1994 --100
Message-Id: <19940326142418.3.JCMA@JEFFERSON.AI.MIT.EDU>
Date: Sat, 26 Mar 1994 15:25:57 --100
From: jcma@reagan.ai.mit.edu (John C. Mallery)
Subject: A Truly Heretical Suggestion

Any chance of cleaning up the way form processing works before casting it in
stone?

	* Although I have CLOS multimethods in my Common Lisp HTTP server, it
	seems like HTTP post is overloaded when one must read random headers
	in order to decide what to do with a form.

	* The vocabulary of input types could also use enriching and some
	hierarchical typing along the lines of CLIM presentation types. (Dunno
	if Dave Raggett has got around to looking at this question. He has the
	relevant pointers.See the referenced URL.)

	* Incremental local redisplay as function of user choices would be
	highly desirable, but requires a framework for local input-relative
	computations, i.e. selection of one choice changes the set of choices
	offered.  These are essentially constraints between queries.

	* Addition of the abstraction of a query with a set of values.  I hate
	reassembling choices (values) associated with one query from
	n-different ``pseudo queries'' into one actual query.

	* Addition of some abstraction on the processing side for people who
	don't have presentation-based automatic form-processing
	infrastructure, i.e, get the values of the form and run my response
	function on them. (Maybe I don't understand what others do here, but
	my impression is that it is every man/woman scripting for
	his/herself.).

If form processing were cleaned up, I expect considerable work could be
offloaded to the client.

For standard input types, the client would be responsible for returning
syntactically correct values, freeing the server from having to handle
type/syntax errors for conforming clients.

At the same time, typed presentations and input types can allow a richer
variety of gestures for acquiring input from the user, including pointing and
presentation translation from one type to another. This also opens the
possibility of tracking what input types are on the user's display.

The important point of this UI technology is that it allows the user to
directly manipulate program data (appropriately translated) and this maintains
a tight coupling between the user model and the program model.

These ideas date from a doctoral thesis by Gene Cicarrelli at MIT in the early
1980s.  Many concepts were implemented in the Symbolics Dynamic Window System
in 1987.  These were then reimplemented (with better engineering and
simplification) in CLIM the portable Common Lisp Interface Manager, c. 1990.
To date, we know of no other window system managers which have adopted an
interaction model of this sophistication.

Before flaming, best to check out an application running a decent
presentation-based interface, and I'm sure all would agree that it would be a
really big plus if W3 could exploit at least some of the key concepts.



From jcma@reagan.ai.mit.edu  Sat Mar 26 16:04:29 1994 --100
Message-Id: <19940326150245.7.JCMA@JEFFERSON.AI.MIT.EDU>
Date: Sat, 26 Mar 1994 16:04:29 --100
From: jcma@reagan.ai.mit.edu (John C. Mallery)
Subject: A Truly Heretical Suggestion w/ URL


[losing listserve loses the X-URL header, so here it is:
X-URL: ftp://cambridge.apple.com/pub/clim/papers/clim.ps.Z]

Any chance of cleaning up the way form processing works before casting it in
stone?

	* Although I have CLOS multimethods in my Common Lisp HTTP server, it
	seems like HTTP post is overloaded when one must read random headers
	in order to decide what to do with a form.

	* The vocabulary of input types could also use enriching and some
	hierarchical typing along the lines of CLIM presentation types. (Dunno
	if Dave Raggett has got around to looking at this question. He has the
	relevant pointers.See the referenced URL.)

	* Incremental local redisplay as function of user choices would be
	highly desirable, but requires a framework for local input-relative
	computations, i.e. selection of one choice changes the set of choices
	offered.  These are essentially constraints between queries.

	* Addition of the abstraction of a query with a set of values.  I hate
	reassembling choices (values) associated with one query from
	n-different ``pseudo queries'' into one actual query.

	* Addition of some abstraction on the processing side for people who
	don't have presentation-based automatic form-processing
	infrastructure, i.e, get the values of the form and run my response
	function on them. (Maybe I don't understand what others do here, but
	my impression is that it is every man/woman scripting for
	his/herself.).

If form processing were cleaned up, I expect considerable work could be
offloaded to the client.

For standard input types, the client would be responsible for returning
syntactically correct values, freeing the server from having to handle
type/syntax errors for conforming clients.

At the same time, typed presentations and input types can allow a richer
variety of gestures for acquiring input from the user, including pointing and
presentation translation from one type to another. This also opens the
possibility of tracking what input types are on the user's display.

The important point of this UI technology is that it allows the user to
directly manipulate program data (appropriately translated) and this maintains
a tight coupling between the user model and the program model.

These ideas date from a doctoral thesis by Gene Cicarrelli at MIT in the early
1980s.  Many concepts were implemented in the Symbolics Dynamic Window System
in 1987.  These were then reimplemented (with better engineering and
simplification) in CLIM the portable Common Lisp Interface Manager, c. 1990.
To date, we know of no other window system managers which have adopted an
interaction model of this sophistication.

Before flaming, best to check out an application running a decent
presentation-based interface, and I'm sure all would agree that it would be a
really big plus if W3 could exploit at least some of the key concepts.



From montulli@stat1.cc.ukans.edu  Sun Mar 27 00:02:03 1994 --100
Message-Id: <9403262257.AA18357@stat1.cc.ukans.edu>
Date: Sun, 27 Mar 1994 00:02:03 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: heretical suggestion

> 
> How about we take the state of the art of HTML plus forms as currently
> supported in Mosaic for X 2.2 -- and either fully supported or getting
> pretty damn close to being fully supported in a number of other
> browsers -- and issue an informational RFC within the next month or
> two, including a working & complete DTD.
> 
> This would add some greatly-needed coherence -- "meat", if you will --
> to the WWW standards evolution process.  And just think, we could
> actually register to get a real text/html MIME type finally.  While
> having something that matches existing, widely-used implementations.
> 
Are you kidding?  And break with the long established WWW tradition of
undocumented features.  If we had documentation we wouldn't need 
WWW Wizards, and then what would they do? :)

On a more serious (and somewhat unrelated) note:  
I would really like to get forms clients to send the 
value of the "submit" button selected to the server.  This
will enable multiple "submit" buttons to performs different functions
on the same data.  Maybe a separate attribute is needed to specify
whether or not to send the submit value so that existing forms
are not broken.  There was alot of discussion about this when
Xmosaic forms first came out but I don't think anything happened
with it.  

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *      Let's face it, I'm a nerd, why else would I have a sig file?      *
  **************************************************************************



From montulli@stat1.cc.ukans.edu  Sun Mar 27 00:06:00 1994 --100
Message-Id: <9403262258.AA47561@stat1.cc.ukans.edu>
Date: Sun, 27 Mar 1994 00:06:00 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: A Truly Heretical Suggestion

> 
> Before flaming, best to check out an application running a decent
> presentation-based interface, and I'm sure all would agree that it would be a
> really big plus if W3 could exploit at least some of the key concepts.
> 
What application would that be?

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *      Let's face it, I'm a nerd, why else would I have a sig file?      *
  **************************************************************************



From duns@vsdeol.cern.ch  Mon Mar 28 10:44:07 1994 --100
Message-Id: <9403280827.AA05578@dxmint.cern.ch>
Date: Mon, 28 Mar 1994 10:44:07 --100
From: duns@vsdeol.cern.ch (MARK DONSZELMANN)
Subject: bug fixes for WWW216 VMS

VMS users of WWW library, linemode and Daemon

All reported bugs were fixed in a new version of the WWW software for VMS:

www216-1betavms.tar_Z on info.cern.ch

or get it from

http://delonline.cern.ch/disk$user/duns/doc/vms/distribution.html

See here also for VMS specific DOC.

I will be on a holiday for 2 weeks, but do send your bug reports.

cheers 
Mark Donszelmann (CERN)




From Paul.Wain@brunel.ac.uk  Mon Mar 28 10:38:38 1994 --100
Message-Id: <24850.9403280801@thor.brunel.ac.uk>
Date: Mon, 28 Mar 1994 10:38:38 --100
From: Paul.Wain@brunel.ac.uk (Paul )
Subject: Re: heretical suggestion

@ On a more serious (and somewhat unrelated) note:  
@ I would really like to get forms clients to send the 
@ value of the "submit" button selected to the server.  This
@ will enable multiple "submit" buttons to performs different functions
@ on the same data.  Maybe a separate attribute is needed to specify
@ whether or not to send the submit value so that existing forms
@ are not broken.  There was alot of discussion about this when
@ Xmosaic forms first came out but I don't think anything happened
@ with it.  

Erm, yes, What did happen to this? I vaguely remember seeing some forms
that had this feature... but I may be mistaken, since Im not awake this
morning :) 

Anyway what Im trying to say is a big YES we need this feature. Multiple
submits would be a very powerful tool to have. 

Paul
.-------------------------------------------------------------------------.
|       Paul S. Wain, (X.500 Project Engineer and WWW/HTTP chappie),      |
|-------------------------------------------------------------------------+
| Computer Centre, Brunel University, Uxbridge, Middx., UB8 3PH, ENGLAND. |
|   VOICE: +44 895 274000 extn 2391       EMAIL: Paul.Wain@brunel.ac.uk   |
|               http://http1.brunel.ac.uk:8080/~ccsrpsw/                  |
`-------------------------------------------------------------------------'



From wei@xcf.Berkeley.EDU  Mon Mar 28 10:47:42 1994 --100
Message-Id: <9403270610.AA29987@xcf.Berkeley.EDU>
Date: Mon, 28 Mar 1994 10:47:42 --100
From: wei@xcf.Berkeley.EDU (Pei Y. Wei)
Subject: viola-talk@ora.com

Hi everybody,

A new mailing list "viola-talk@ora.com" has been set up for the 
general discussion and news of the viola language/toolkit and the
ViolaWWW application. Anything specific to viola, or related to 
ViolaWWW but too viola specific to go to www-talk@info.cern.ch, 
is appropriate subject for this list.

To subscribe to the list, email to "listproc@ora.com" with the
following message in the body of the letter:

	subscribe viola-talk Your_Name of Your_Institution


-Pei					Pei Y. Wei
					R&D, Digital Media Group
					O'Reilly & Associates, Inc



From kevinh@eit.COM  Mon Mar 28 10:52:21 1994 --100
Message-Id: <9403270421.AA22316@eit.COM>
Date: Mon, 28 Mar 1994 10:52:21 --100
From: kevinh@eit.COM (Kevin 'Kev' Hughes)
Subject: New (beta) Web Guide available for testing


	"Entering the World-Wide Web: A Guide to Cyberspace" version 6.0
is available in PostScript form and FrameMaker 4.0 read-only versions at:

	ftp://ftp.eit.com/pub/web.guide/guide.60

	This version was done rather quickly for a couple of Webmastery
and hypermedia design seminars I gave recently at Stanford, and I'd
like to hear feedback on this version from the Web wizards and newbies
out there - does it need more meat? Does it give the right information
to new users?
	The next version, 6.1, will be updated and expanded even more
(taking into account all the activity and statistics that have been
gathered on the Web over the past two weeks), and will be translated
into a well-linked, eye-catching HTML package as well as the standard
formats.

	-- Kevin

--
Kevin Hughes * kevinh@eit.com
Enterprise Integration Technologies Webmaster (http://www.eit.com/)
Hypermedia Industrial Designer * Duty now for the future!



From marca@eit.COM  Mon Mar 28 10:56:53 1994 --100
Message-Id: <199403270244.CAA16561@threejane>
Date: Mon, 28 Mar 1994 10:56:53 --100
From: marca@eit.COM (Marc Andreessen)
Subject: A Truly Heretical Suggestion

John C. Mallery writes:
> Any chance of cleaning up the way form processing works before
> casting it in stone?

I suggest taking John's suggestions into close account when we design
the next generation of forms, but I think we should freeze where we're
at now, now, because it's a workable and stable point of convergence
for a large number of currently active clients and servers.

What we've done already is rudimentary in some senses, but it's more
powerful than anything else on the net, it's been proven to work in
the real world for real applications, and it's currently getting a lot
of use.

Cheers,
Marc


> 	* Although I have CLOS multimethods in my Common Lisp HTTP server, it
> 	seems like HTTP post is overloaded when one must read random headers
> 	in order to decide what to do with a form.
> 
> 	* The vocabulary of input types could also use enriching and some
> 	hierarchical typing along the lines of CLIM presentation types. (Dunno
> 	if Dave Raggett has got around to looking at this question. He has the
> 	relevant pointers.See the referenced URL.)
> 
> 	* Incremental local redisplay as function of user choices would be
> 	highly desirable, but requires a framework for local input-relative
> 	computations, i.e. selection of one choice changes the set of choices
> 	offered.  These are essentially constraints between queries.
> 
> 	* Addition of the abstraction of a query with a set of values.  I hate
> 	reassembling choices (values) associated with one query from
> 	n-different ``pseudo queries'' into one actual query.
> 
> 	* Addition of some abstraction on the processing side for people who
> 	don't have presentation-based automatic form-processing
> 	infrastructure, i.e, get the values of the form and run my response
> 	function on them. (Maybe I don't understand what others do here, but
> 	my impression is that it is every man/woman scripting for
> 	his/herself.).
> 
> If form processing were cleaned up, I expect considerable work could be
> offloaded to the client.
> 
> For standard input types, the client would be responsible for returning
> syntactically correct values, freeing the server from having to handle
> type/syntax errors for conforming clients.
> 
> At the same time, typed presentations and input types can allow a richer
> variety of gestures for acquiring input from the user, including pointing and
> presentation translation from one type to another. This also opens the
> possibility of tracking what input types are on the user's display.
> 
> The important point of this UI technology is that it allows the user to
> directly manipulate program data (appropriately translated) and this maintains
> a tight coupling between the user model and the program model.
> 
> These ideas date from a doctoral thesis by Gene Cicarrelli at MIT in the early
> 1980s.  Many concepts were implemented in the Symbolics Dynamic Window System
> in 1987.  These were then reimplemented (with better engineering and
> simplification) in CLIM the portable Common Lisp Interface Manager, c. 1990.
> To date, we know of no other window system managers which have adopted an
> interaction model of this sophistication.
> 
> Before flaming, best to check out an application running a decent
> presentation-based interface, and I'm sure all would agree that it would be a
> really big plus if W3 could exploit at least some of the key concepts.




From dsr@hplb.hpl.hp.com  Mon Mar 28 14:28:00 1994 --100
Message-Id: <9403281225.AA05817@dragget.hpl.hp.com>
Date: Mon, 28 Mar 1994 14:28:00 --100
From: dsr@hplb.hpl.hp.com (Dave Raggett)
Subject: Re: heretical suggestion

> On a more serious (and somewhat unrelated) note:  
> I would really like to get forms clients to send the 
> value of the "submit" button selected to the server.  This
> will enable multiple "submit" buttons to performs different functions
> on the same data.  Maybe a separate attribute is needed to specify
> whether or not to send the submit value so that existing forms
> are not broken.  There was alot of discussion about this when
> Xmosaic forms first came out but I don't think anything happened
> with it.

This is now part of the HTML+ spec, and will soon appear in Mosaic.

Dave Raggett



From connolly@hal.com  Mon Mar 28 17:25:39 1994 --100
Message-Id: <9403281523.AA06764@ulua.hal.com>
Date: Mon, 28 Mar 1994 17:25:39 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: heretical suggestion 

In message <199403260832.IAA14436@threejane>, Marc Andreessen writes:
>How about we take the state of the art of HTML plus forms as currently
>supported in Mosaic for X 2.2 -- and either fully supported or getting
>pretty damn close to being fully supported in a number of other
>browsers -- and issue an informational RFC within the next month or
>two, including a working & complete DTD.

Seconded.

I've had the same idea in mind ever since I got the notice that the
draft spec was expiring.

My strategy will be to start with the DTD from the draft spec,
add forms, make lists nestable, and make the DTD generally match
current practice, and then:

	ASSEMBLE A TEST SUITE

of documents demonstrating the various features and exploring boundary
cases an wierdnesses to be sure we've got what we want.

I've been collecting information on
	* HTML "how to" documents
	* Tools that produce/munge HTML
	* Browsers/tools that consume HTML
in an effort to catalogue the features in use.

But I haven't spent the time to get much further than that.

If you've got an HTML file that you're not sure is compliant or
not, mail it to me and I'll put it in the suite.

Dan



From dsr@hplb.hpl.hp.com  Mon Mar 28 17:09:28 1994 --100
Message-Id: <9403281505.AA05947@dragget.hpl.hp.com>
Date: Mon, 28 Mar 1994 17:09:28 --100
From: dsr@hplb.hpl.hp.com (Dave Raggett)
Subject: Re: A Truly Heretical Suggestion

Marc,

> How about we take the state of the art of HTML plus forms as currently
> supported in Mosaic for X 2.2 -- and either fully supported or getting
> pretty damn close to being fully supported in a number of other
> browsers -- and issue an informational RFC within the next month or
> two, including a working & complete DTD.

I have a stripped down DTD which may be suitable for this, and am
discussing this with CERN. Our goal is to smooth the path to the
richer features in HTML+. Incidentally, my HTML+ browser will be
on show at WWW'94 and will be publically released soon after. You
can get an early glimpse of how it looks at:

        ftp://15.256.100.100/pub/htmplus.gif

John C. Mallery writes:
> Any chance of cleaning up the way form processing works before
> casting it in stone?

There are several new field types in the works: scribble on image,
file transfer (Lou Montulli's suggestion) and a record-oriented database
entry widget. The submit button has been cleaned up and should be
in the next release of Mosaic. I am also working on a clean script API
for client-side handling of dynamic dependencies which a lot of
people have been asking for. The aim here is to avoid tying WWW into
a particular scripting language forever more by taking advantage
of URLs and HTTP format negotiation to give us this independence.
--
Best wishes,

Dave Raggett



From dsr@hplb.hpl.hp.com  Mon Mar 28 18:47:09 1994 --100
Message-Id: <9403281643.AA06099@dragget.hpl.hp.com>
Date: Mon, 28 Mar 1994 18:47:09 --100
From: dsr@hplb.hpl.hp.com (Dave Raggett)
Subject: Re: A Truly Heretical Suggestion

>>        ftp://15.256.100.100/pub/htmplus.gif

> Oooh, cool!  Is this one of those new expanded-internet addresses? ;-)

Finger trouble I'm afraid :-) I should have written:

        ftp://15.254.100.100/pub/htmplus.gif

Dave Raggett



From letovsky-stan@CS.YALE.EDU  Mon Mar 28 19:03:33 1994 --100
Message-Id: <199403281700.AA19730@RA.DEPT.CS.YALE.EDU>
Date: Mon, 28 Mar 1994 19:03:33 --100
From: letovsky-stan@CS.YALE.EDU (Stan Letovsky)
Subject: Re: A Truly Heretical Suggestion 

Subject: Re: A Truly Heretical Suggestion
From:    Dave Raggett <dsr@hplb.hpl.hp.com>
Date:    Mon, 28 Mar 94 17:07:50
To:      Multiple recipients of list <www-talk@www0.cern.ch>
---------

>Marc,
>
>> How about we take the state of the art of HTML plus forms as currently
>> supported in Mosaic for X 2.2 -- and either fully supported or getting
>> pretty damn close to being fully supported in a number of other
>> browsers -- and issue an informational RFC within the next month or
>> two, including a working & complete DTD.
..
As long as fill-out form wish-lists are being discussed --
although I suspect that any RFC will freeze the current
design and leave extensions to HTML+ -- my three favorites
are:

*row-extensible tabular arrays of input widgets for entry
and editing of tabular data -- we have a prototype that
allows any other fill-out widget to be used as a column

*a <pop> meta-directive that causes the previous document
to be popped from the client's document stack -- important
for preventing interactive multi-form dialogs from causing
progressive increases in stack depth

*arbitrary hashed widget attributes, settable by HTML commands,
which can be included in POST messages. Allows for structured
representations of interaction state.

I like the idea of putting names on submit button messages too.

-Stan



From dduchier@csi.uottawa.ca  Mon Mar 28 18:06:35 1994 --100
Message-Id: <9403281603.AA07048@csi0.csi.uottawa.ca>
Date: Mon, 28 Mar 1994 18:06:35 --100
From: dduchier@csi.uottawa.ca (Denys Duchier)
Subject: Re: A Truly Heretical Suggestion

Dave Raggett writes:
 >richer features in HTML+. Incidentally, my HTML+ browser will be
 >on show at WWW'94 and will be publically released soon after. You
 >can get an early glimpse of how it looks at:
 >
 >        ftp://15.256.100.100/pub/htmplus.gif

Actually the correct URL is:

	ftp://15.254.100.100/pub/htmlplus.gif

--Denys
Dr. Denys Duchier			Dept of Computer Science
email:	dduchier@csi.uottawa.ca		University of Ottawa
tel:	(613) 564-5427			Ottawa, Ont.
fax:	(613) 564-9486			K1N 6N5 Canada



From kevinh  Mon Mar 28 09:28:59 1994 PST
Message-Id: <9403281728.AA03482@eit.COM>
Date: Mon, 28 Mar 94 09:28:59 PST
From: kevinh (Kevin 'Kev' Hughes)
Subject: httpd security patch


> A vulnerability has been identified with NCSA httpd 1.1's access
> control. The impact of this is that if you have files which are
> protected by httpd's access control via the global ACF access.conf,
> the protection can be circumvented and access can be gained to the
> files regardless of the client's DNS hostname, host IP address, or
> HTTP user name.

	Just so everyone knows, EIT's server and the CommerceNet server
have been patched.

	-- Kev



From montulli@stat1.cc.ukans.edu  Mon Mar 28 22:26:08 1994 --100
Message-Id: <9403282022.AA37637@stat1.cc.ukans.edu>
Date: Mon, 28 Mar 1994 22:26:08 --100
From: montulli@stat1.cc.ukans.edu (Lou Montulli)
Subject: Re: heretical suggestion

> 
> > On a more serious (and somewhat unrelated) note:  
> > I would really like to get forms clients to send the 
> > value of the "submit" button selected to the server.  This
> > will enable multiple "submit" buttons to performs different functions
> > on the same data.  Maybe a separate attribute is needed to specify
> > whether or not to send the submit value so that existing forms
> > are not broken.  There was alot of discussion about this when
> > Xmosaic forms first came out but I don't think anything happened
> > with it.
> 
> This is now part of the HTML+ spec, and will soon appear in Mosaic.
> 
I didn't see mention of this in 
ftp://15.254.100.100/pub/draft-raggett-www-html-00.txt
How is it specified.  Is there anonther attribute to the submit type
or is the value of the submit button always sent?  Will this break
existing implementations?  How *EXACTLY* is it being implemented in
Xmosaic. (It's not always the same as the spec)  Once I get the 
answers to these burning questions I can get it implemented in
2.3 for release soon.

:lou
-- 
  **************************************************************************
  *           T H E   U N I V E R S I T Y   O F   K A N S A S              *
  *         Lou  MONTULLI @ Ukanaix.cc.ukans.edu                           *
  *                         Kuhub.cc.ukans.edu      ACS Computing Services *
  *     913/864-0436        Ukanvax.bitnet             Lawrence, KS 66044  *
  *      Let's face it, I'm a nerd, why else would I have a sig file?      *
  **************************************************************************



From alb@cupido.inesc.pt  Tue Mar 29 09:58:08 1994 --100
Message-Id: <9403290854.AA29750@cupido.inesc.pt>
Date: Tue, 29 Mar 1994 09:58:08 --100
From: alb@cupido.inesc.pt (Alberto Silva)
Subject: Re: Documentation of the WWW Library

> 
> On Fri, 25 Feb 1994, Alberto Silva wrote:
> > As a start point I think it's important to understand the standard WWW
> > library.  I would like to know if there is some documentatio  that explain 
> > the use of these library ?  Even documentation with some extracts of
> > code...
> 
> Hi Alberto,
> 	I am just wondering what type of documentation if any that you
> were able to obtain.  Please e-mail me.
> 
> 	Thanks,
> 		Garrett.
> 

Hello,

The unique and relevant documentation that I found about the Standard
WWW library was at

   http://info.cern.ch/hypertext/WWW/Library/Implementation/Overview.html
   with title: "W3 Library Internals Overview"

However at the moment I have some questions about the WWW library' "Standard".  Namely what are the differences between the NCSA and the CERN ones ?

Does anybody really knows what are they?

Best wishes,

-- Alberto Silva



---------------------------------------------------------------
Alberto M. Rodrigues da Silva
INESC - R. Alves Redol, 9-4o Sala 436 - 1000 Lisboa - Portugal
Tel	+(351) 1 3100 000 (std)   +(351) 1 3100 305 (direct line) 
Fax	+(351) 1 52 58 43         e-mail:    alb@inesc.pt 
---------------------------------------------------------------



From marca@eit.COM  Tue Mar 29 11:32:07 1994 --100
Message-Id: <199403290928.JAA07802@threejane>
Date: Tue, 29 Mar 1994 11:32:07 --100
From: marca@eit.COM (Marc Andreessen)
Subject: Re: A Truly Heretical Suggestion

Dave Raggett writes:
> > How about we take the state of the art of HTML plus forms as currently
> > supported in Mosaic for X 2.2 -- and either fully supported or getting
> > pretty damn close to being fully supported in a number of other
> > browsers -- and issue an informational RFC within the next month or
> > two, including a working & complete DTD.
> 
> I have a stripped down DTD which may be suitable for this, and am
> discussing this with CERN. Our goal is to smooth the path to the
> richer features in HTML+. Incidentally, my HTML+ browser will be
> on show at WWW'94 and will be publically released soon after. You
> can get an early glimpse of how it looks at:
> 
>         ftp://15.256.100.100/pub/htmplus.gif
> 
> John C. Mallery writes:
> > Any chance of cleaning up the way form processing works before
> > casting it in stone?
> 
> There are several new field types in the works: scribble on image,
> file transfer (Lou Montulli's suggestion) and a record-oriented database
> entry widget. 

I strongly urge that those features be deferred for the purposes of
the first RFC.

> The submit button has been cleaned up and should be in the next
> release of Mosaic.

That may be minor enough that it'd squeeze in.

> I am also working on a clean script API for client-side handling of
> dynamic dependencies which a lot of people have been asking for. 

That's definitely post-first-RFC :-).

> The aim here is to avoid tying WWW into a particular scripting
> language forever more by taking advantage of URLs and HTTP format
> negotiation to give us this independence.

Sounds fascinating...

Marc



From duns@vxdeop.cern.ch  Tue Mar 29 16:34:38 1994 --100
Message-Id: <9403291429.AA03917@dxmint.cern.ch>
Date: Tue, 29 Mar 1994 16:34:38 --100
From: duns@vxdeop.cern.ch (MARK DONSZELMANN)
Subject: new version of 2.16-1 for WWW on VMS

The monday morning version of www216-1betavms was wrong. This version should
be better.  It was posted at 16:30 Cern Time.

cheers Mark Donszelmann (CERN)




From joe@MIT.EDU  Wed Mar 30 06:05:49 1994 --100
Message-Id: <9403300359.AA08151@theodore-sturgeon.MIT.EDU>
Date: Wed, 30 Mar 1994 06:05:49 --100
From: joe@MIT.EDU (Joseph Wang)
Subject: ANNOUNCING TkWWW-0.11 prerelease 2


I've uploaded tkWWW prelrelease 2 to info.cern.ch and it should
migrate over to /pub/dev over the next few days.  The big change is
that I incorporated libwww 2.15 into the release.  Since causes
everything to be different, I didn't generate patch files.

Let me know if there are any installation problems, and people can get
it to work.





From joe@MIT.EDU  Wed Mar 30 06:05:49 1994 --100
Message-Id: <9403300359.AA08151@theodore-sturgeon.MIT.EDU>
Date: Wed, 30 Mar 1994 06:05:49 --100
From: joe@MIT.EDU (Joseph Wang)
Subject: ANNOUNCING TkWWW-0.11 prerelease 2


I've uploaded tkWWW prelrelease 2 to info.cern.ch and it should
migrate over to /pub/dev over the next few days.  The big change is
that I incorporated libwww 2.15 into the release.  Since causes
everything to be different, I didn't generate patch files.

Let me know if there are any installation problems, and people can get
it to work.





From ee01th@surrey.ac.uk  Wed Mar 30 12:11:32 1994 --100
Message-Id: <9403301105.aa14038@ainur.ee.surrey.ac.uk>
Date: Wed, 30 Mar 1994 12:11:32 --100
From: ee01th@surrey.ac.uk (Piglet)
Subject: Why the Web needs to change

Being fairly new to this discussion group, I really don't know
how new the ideas I am presenting here are.  Comments welcome!
The following can also be seen on:
http://www.ee.surrey.ac.uk/Personal/TimothyH/webchange.html
-----
How the Web must change.

Consider first the library

Here's a little story...(by the way, this is completely
fictional - as if you couldn't guess anyway!) 

One day, Timothy reads an article in his weekly magazine, Lego
Engineering, about many Civil Engineering disasters resulting
from the misuse of Lego in their construction. At the end of the
article, in it's references, it mentions a book The Tacoma
Narrows Bridge: Lego caused it's failure. 

Now, Timothy, being interested in this topic, decides to see if his
local library has a copy. He pops down, and asks the Librarian.
She apologises profusely as they do not have a copy, but she is
willing to obtain a copy through inter-library loans. A week or so
later, Timothy has the book in his hands, and reads it through.
He then returns it to his local library, and it is returned to the
library it was borrowed from. 

Shortly after this, Timothy mentions this article to his friend
Duncan, who borrows the magazine to find out more. He, too,
wants to borrow the book mentioned and goes to the same
Library. Again, the Librarian obligingly gets the book through
inter-library loan. Duncan reads and returns the book, just like
Timothy before him. 

At work, Duncan mentions the article to his colleague, Bevis.
Bevis borrows the magazine, and he to wants the book. He goes
off to the library, and asks the Librarian. The Librarian, being
wise (as librarians are) had noticed that this book was becoming
quite popular, so had ordered a copy for the library. Bevis,
therefore, borrows the library's own copy, which is much faster
as he doesn't have to wait for it to arrive. In time, he reads, and
returns the book. 

Let's now look at the World Wide Web

With the current design of the Web, there is a problem
concerning traffic (as the people at NCSA will tell you!). 

At present, a document is referenced by its protocol and location.
In order to reduce network traffic, some method of storing the
documents locally must be found. 

It is easy for a web administrator to bring down a copy of a
document (which I shall call DocA) and store it locally. However,
if documents that DocA are also to be stored locally, then the
references in DocA must be changed to point to the local copies
rather than the original ones. It also means that any document
that points to DocA will actually point to the original version,
rather than the local copy, which rather defeats the object. One
other problem with this is if the original copy of DocA is updated,
this will not be reflected locally. 

We therefore propose a change:

Each document should have a unique identifier (as indeed it
already does) which would be generated by the site creating the
document, much in the same way that publishers generate
ISBNs. 

When a client requests a document, rather than going to the
source of the document (the publisher), it asks the local server 
(the library) if it has a copy. If it does not, the local server fetches
it from the original site and sends it to the client for display. If
that document is requested frequently, the local server makes a
copy of it, and when asked for it again, simply asks the document
source if the copy it already has is up to date and only retrieves it
again from the source if necessary. 

This whole procedure could be done hierarchically, for example
based on internet domains. So, the surrey.ac.uk server doesn't
have the document it asks some generic ac.uk super-server if it
has it, which in turn passes the request on to the uk super-server,
which if necessary downloads the document from source. As
before, the time-stamping checks sould be done hierarchically
too. 

The advantage of a hierarchic scheme is that there could be some
documents that lots of people in the UK want to read, but these
people are all from different sites (e.g. documentation on how to
set up a server) 

If the protocol is set up correctly, the super-servers themselves
need not keep copies of the most looked at documents. If a server
in the hierarchy already has a copy, it simply asks for a
datestamp to check its copy is up to date. If a super-server
receives such a request, it simply passes it on to the next level up,
not bothering to keep a copy for itself. 

Also, if a document is not requested for a long time, the relevent
server in the hierarchy deletes its copy (unless, of course, it is the
original source of the document!) 

The important point we are trying to make is that the reference
which uniquely identifies the document should be just that--a
document identifier, rather than where an up to date copy of the
document can be found. 

In fact (as a complete afterthought) those pages that the client
caches during a session could be checked against this unique
reference, so if the user doesn't use the Back option or the 
Window History that Mosaic provides, it still calls up the
cached version rather than reloading it from the local server. 

Comments and questions to either of the people below are
welcomed 


T.Hunt@ee.surrey.ac.uk (http:www.ee.surrey.ac.uk/People/T.Hunt.html)
D.White@ee.surrey.ac.uk (http:www.ee.surrey.ac.uk/People/D.White.html)
30th March 1994



From marca@eit.COM  Wed Mar 30 12:22:07 1994 --100
Message-Id: <199403301018.KAA11229@threejane>
Date: Wed, 30 Mar 1994 12:22:07 --100
From: marca@eit.COM (Marc Andreessen)
Subject: Why the Web needs to change

The IETF URI group has been working for decades on defining such a
beast -- the URN (Uniform Resource Name).  It's currently expected by
the year 2009.

Cheers,
Marc



From Werner.Icking@gmd.de  Wed Mar 30 13:08:18 1994 --100
Message-Id: <9403301105.AA02268@sunick.gmd.de>
Date: Wed, 30 Mar 1994 13:08:18 --100
From: Werner.Icking@gmd.de (Werner Icking)
Subject: HTML / HTML+ -- need for hyphenation

German language uses a lot of composita e.g. the manager of a 
file system may be called "Dateisystemmanager"; if this is a
program it may be named "Dateisystemmanagerprogram" ...

In this situation German www-documents may look very ugly unless
a "still hyphen" would be introduced into HTML and it's browsers.

Another possibility could be to add an automatic hyphenation which
is available e.g from TeX.

Werner

PS: Is this a mailing list? if yes, how can I subscribe.



From bert@let.rug.nl  Wed Mar 30 13:17:44 1994 --100
Message-Id: <9403301114.AA18362@freya.let.rug.nl>
Date: Wed, 30 Mar 1994 13:17:44 --100
From: bert@let.rug.nl (Bert Bos)
Subject: Re: Why the Web needs to change

 |The IETF URI group has been working for decades on defining such a
 |beast -- the URN (Uniform Resource Name).  It's currently expected by
 |the year 2009.
 |
 |Cheers,
 |Marc

Marc is a bit pessimistic, I think. Maybe we won't have `real' URNs in
this decade, but for most document the URL *is* the URN. And indeed
the current caches use URLs in that way.

-- 
                     _________________________________
                    / _   Bert Bos <bert@let.rug.nl>  |
           ()       |/ \  Alfa-informatica,           |
            \       |\_/  Rijksuniversiteit Groningen |
             \_____/|     Postbus 716                 |
                    |     9700 AS GRONINGEN           |
                    |     Nederland                   |
                    \_________________________________|



From fielding@simplon.ICS.UCI.EDU  Wed Mar 30 18:24:41 1994 --100
Message-Id: <9403300815.aa17472@paris.ics.uci.edu>
Date: Wed, 30 Mar 1994 18:24:41 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: Why the Web needs to change 

Piglet writes:

> Being fairly new to this discussion group, I really don't know
> how new the ideas I am presenting here are.  Comments welcome!

[nice story deleted]

So, to make a long story short, you want hierarchical, caching proxy
servers that use location-independent URNs.  Join the crowd ;-)
Actually, I think you will find that URNs are only useful when the
document is a fixed standard, such as an Internet RFC, or externally-
published book, but that is an even longer story.

I suggest looking at the hypertext archive of this mailing list
at <http://gummo.stanford.edu/html/hypermail/archives.html>
and reading all the threads on caching, forwarding cache requests,
and URNs.  This is going to take a while, but it's the only way to
find out what's already been discussed.

Cheers,


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>

[Please note that our WWW server and all mail connections to UC Irvine
 will be down this weekend (April 2, 01:00 GMT through April 4, 16:00 GMT)
 due to a local power shutdown]



From connolly@hal.com  Thu Mar 31 01:03:03 1994 --100
Message-Id: <9403302259.AA14125@ulua.hal.com>
Date: Thu, 31 Mar 1994 01:03:03 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: URL decisions in Seattle, & changes 


In message <9403301700.AA00480@ptpc00.cern.ch>, Tim Berners-Lee writes:

[Lots of stuff that, in my view, contradicts current practice. Details
below...]

My perspective on this is (1) a commercial implementor trying to build
a product can claim to be "conforming to the URL spec" and (2) a
formal systems purist trying to be sure we've got a well-defined
specification.

The URL specification is getting hopelessly watered down. Everybody
wants to just take their favorite addressing scheme, slap a few
letters and a colon on the front, and call it a URL.

For example, the current working requirements document

<http://www.acl.lanl.gov/URI/archive/uri-archive.messages/1063.html>

is almost completely devoid of content.

If I specify that a URL is "any sequence of letters, followed by a :
and the characters 'FRED'.  Access is defined by choosing a number
between 90 and 2000. If the result is greater than the year, access is
successful.", then I have satisfied all the URL requirements:

	5.1 Locators are transient.

	The probability with which a given Internet resource locator
	leads to successful access decreases over time.

Check.

	5.2 Locators have global scope.

	The name space of resource locators includes the entire world.
	The outcome of a client access attempt using an Internet
	locator depends in no way, modulo resource availability, on
	the geographical or Internet location of the client.

Check.

	5.3 Locators are parsable.

	Internet locators can be broken down into complete constituent
	parts sufficient for interpreters (software or human) to attempt
	access if desired.

Check. All you have to do is see "xyz:FRED" and that tells you enough
to attempt access.

	5.4 Locators can be readily distinguished from naming and
	descriptive identifiers that may occupy the same name space.

Check. If it says "xyz:FRED," it's a locator. Otherwise, it's not.

	5.5 Machines can readily identify locators as such.

Check. The regexp /[a-zA-Z]+:FRED/ will do it.

	5.6 Locators are "transport-friendly".

	Internet locators can be transmitted from user to user (e.g,
	via e-mail) across Internet standard communications protocols
	without loss or corruption of information.

Check.

	5.7 Locators are human transcribable.

Check.

	5.8 An Internet locator consists of a service and an opaque
	parameter package.

Check. The parameter package is "FRED". The service is named by the
letters.

	5.9 The set of services is extensible.

Check. Just substitute different letters. The draft says nothing about
the features available from these services.

Yes, I'm being somewhat pedantic, and I can see that the approach is
to bite off a little of the problem (Internet Information
Architecture) at a time. But the result of the current "we don't know
how it's gonna work yet, so we won't make any constraints" attitude is
a lot of hot air, if you ask me.

In contrast to the above requirements, why don't we look at the
already deployed applications, and then extrapolate to the future.
Let's look at specific scenarios and be sure we've satisfied their
requirements. For example:

	* The Campus-Wide Information Service scenario. From this
	we realized that allowing aggregation of information spread
	over various administrative domains -- not to mention physical
	machines -- is a requirement.

	* Corporate promotion/support scenario. These folks want
	snazzy images and stuff. They want their online collateral
	material to enhance their image.

	* The Software distribution and support scenario, i.e.
	"Here's the 1.2 release of the quartz package. For more
	info, see <http://ftp.host.com/quartz/info.html>". We need support
	for mirroring archives.

	* Online Documentation. Support for man pages is a requirement.
	The various hacks involved in putting Info trees online shows
	some needed features.

	* The FAQ distribution scenario: From this we learn about
	expiration dates as a mechanism for versioning. There's a
	lot more to learn/do here.

	* Serving newsgroup archives through WAIS. Here we learn
	that while fulltext search and relavence feedback are
	great tools, we'd like to do SQLish things (e.g. select article
	where author="fred" and date>"Jan 1 1994") too.

	* Online technical reports and Journals. Postscript support
	is a requirement. But plain-text abstracts are too. And
	searching a database of abstracts with hypertext pointers to
	postscript is nearly optimal.
	
	* Collaborating on WWW specifications. This is pretty painful.
	It shows a need for something like the NCSA annotation server.

Before we decide on an addressing standard, we need to establish a
little more context. To me, it's not clear that we need a universal
naming _syntax_. There are lots of uses for a global namespace, but
the syntax isn't important.

I think the basic starting point in all this is a model of
communication: take information, express it as a sequence of bits,
send the bits to someboy else, and extract the information again.

But you can't do that without conventions about what the bits
represent. MIME provides a namespace of content types to fill this
need (except for compression and encryption... Hmmm). Then, once you
know how to interpret the bits, you can (1) present the bits in some
physical form, and/or (2) get at the navigational components like
addresses, names, citations, and structure.

The idea that everything has to be expressible as ASCII text is so
backwards! Surely the widespread deployment of applications like word
processors and spreadsheets shows that the computer should be employed
as a tool to construct information products. And I'm not talking about
emacs html-mode. I'm talking about direct manipulation interfaces.

Let's get off URL syntax and onto features like drag-n-drop and
paste-link between internet information resources.

Anyway... back to the matter at hand...

>The gopher string wants to use the "?" and "/" characters (which
>in WWW have convention syntax role) within a string without any
>special role.  This was upheld, so slashes and ? may be put into
>a URL without being encoded.  The characters are still in the
>reserved set, but in Gopher, / and %2F have the same effect.

So we're ruling out the possibility of having collections of HTML
files with relative links served through gopher. Oh well... the only
material difference between gopher and HTTP 0.9 is the TCP port number
anyway.

This completely destroys the concept of a comprehensive URI syntax.
The grammar must include all the special parsing features of each
scheme, and it must be explicitly ammended with each new scheme.

In future schemes, will '/' and '%2F' mean the same thing or different
things? I gather that the answer is "it depends." This rules out the
idea of having one algorithm for reducing a URI to canonical form.  So
the question of whether

	x-my-scheme:abc/def
and
	x-my-scheme:abc%2fdef

can occupy the same slot in a cache isn't specified. Bummer.

It also means that one can't build "dumb clients" that count on proxy
gateways to deal with all the various protocols. Suppose I'm viewing

	x-new-scheme://host.com/database/cover.html

and it contains the <ISINDEX> tag. How do I construct a query for
"abc def"? I can't unless I know the rules for x-new-scheme.

> I'd also like Dan Connolly's input on the
>grammar -- in which places is it ambiguous in its current form, Dan?

This is an improvement... but there are still problems. (user and
password can contain @, so login is ambiguous) I'm going to have to
convert it to lex/yacc to find all the problems. Why not use that
format permanently in stead of this "BNF-like" informalism?

Dan



From dupuy@smarts.com  Thu Mar 31 03:59:55 1994 --100
Message-Id: <9403310157.AA28621@brainy.smarts.com>
Date: Thu, 31 Mar 1994 03:59:55 --100
From: dupuy@smarts.com (Alexander Dupuy)
Subject: Re: URL decisions in Seattle, & changes

Dan Connolly wrote:

[many things which I agree with, but which are probably considered closed
issues now that everything has been resolved "in the flesh" at Seattle]

In particular, I feel the following sums up the problem with the decisions:

> This completely destroys the concept of a comprehensive URI syntax.
> The grammar must include all the special parsing features of each
> scheme, and it must be explicitly amended with each new scheme.

However, there are a few points which don't seem to have been completely
closed out:

> In future schemes, will '/' and '%2F' mean the same thing or different
> things? I gather that the answer is "it depends." This rules out the
> idea of having one algorithm for reducing a URI to canonical form.  So
> the question of whether
> 
> 	x-my-scheme:abc/def
> and
> 	x-my-scheme:abc%2fdef
> 
> can occupy the same slot in a cache isn't specified. Bummer.

I thought that one of the requirements for URLs was that it be possible to
compare them for equality.  It seems to me that a single algorithm for
canonicalization, regardless of scheme, is essential for providing this
functionality.

If everyone wants to roll over for the gopher requirement of using '/' and
'%2F' interchangeably, then that implies that other schemes may not interpret
them differently.  In particular, if a scheme like http wants to distinguish
between hierarchical and non-hierarchical uses of /, it will have to use some
scheme-specific quoting mechanism (do I hear %5C?) to mark non-hierarchical
uses of /.

I guess this makes life fractionally more difficult for Macintosh WWW servers
that want to use relative URLs, but anybody who can't hack it should just run
Gopher instead, where they won't ever need to quote their /s.

@alex



From bloch@tempo.Stanford.EDU  Thu Mar 31 06:15:27 1994 --100
Message-Id: <9403310413.AA19952@tempo.Stanford.EDU>
Date: Thu, 31 Mar 1994 06:15:27 --100
From: bloch@tempo.Stanford.EDU (Eric Bloch)
Subject: Online Dates for WWW Servers?

                          PLEASE PLEASE REPLY!!

Hi All,

In the name of documenting the expansion of the WorldWideWeb, I'm trying to 
collect the on-line dates of all WWW (http) servers.  Please help if you can!  

If you know the date that your (or any) WWW server came on line
could you please send me the date.  I'm particularly interested in the dates 
of servers that have been around for a long time.

If the server is no longer available, I would like to get the date it ceased 
operation, too.

Please help me!  Information on as many servers as you know about would
be great!  A reply with the following would be great:

Server Home Page URL:
On-line Date: (Year, Month, and Day if known) 
(server shutdown Date if applicable)

Much Thanks!!
-Eric Bloch

bloch@leland.stanford.edu
-------------------------------------------------------------------------------
Home:                              School:
811 Ashbury Street, #2             Sociology Department, Building 120, Room 039
San Francisco, CA 94117            Stanford University
415-731-0589                       Stanford, CA 94305
                                   415-723-1692



From masinter@parc.xerox.com  Thu Mar 31 07:34:18 1994 --100
Message-Id: <94Mar30.213053pst.2732@golden.parc.xerox.com>
Date: Thu, 31 Mar 1994 07:34:18 --100
From: masinter@parc.xerox.com (Larry Masinter)
Subject: Re: URL decisions in Seattle, & changes

>> In future schemes, will '/' and '%2F' mean the same thing or different
>> things? I gather that the answer is "it depends." This rules out the
>> idea of having one algorithm for reducing a URI to canonical form.  So
>> the question of whether

Well, in fact, the 'canonical' form for any URL must necessarily be
protocol specific. This is true for the default port (e.g., that
http://host:80/ is the same as http://host/ but gopher: has 70 as a
default port, etc.) that the same host might have multiple DNS names,
or that some FTP servers allow case insensitive file names, any number
of actual equivalences, symbolic links, etc.

In the grand scheme of things, if you treat "/" and "%2F" as
different, then at most you'll treat a few things as 'different' that
are really the 'same', but in fact, this will be an insignificant
amount compared to the other kinds of duplications.

It *IS* important for each protocol-specific type to specify whether
"/" and "%2F" are the same or different. For FTP and HTTP, they are
different; for Gopher, they are the same.
 





From connolly@hal.com  Thu Mar 31 18:03:25 1994 --100
Message-Id: <9403311600.AA14222@ulua.hal.com>
Date: Thu, 31 Mar 1994 18:03:25 --100
From: connolly@hal.com (Daniel W. Connolly)
Subject: Re: URL decisions in Seattle, & changes 

In message <94Mar30.213053pst.2732@golden.parc.xerox.com>, Larry Masinter write
s:
>>> In future schemes, will '/' and '%2F' mean the same thing or different
>>> things? I gather that the answer is "it depends." This rules out the
>>> idea of having one algorithm for reducing a URI to canonical form.  So
>>> the question of whether
>
>Well, in fact, the 'canonical' form for any URL must necessarily be
>protocol specific.

I still disagree. It is possible to specify a canonical form for URLs
independent of scheme. The quoting scheme described by Tim and myself
(and implemented in HTParse.c and tested in my test suite...) does
just this.

> This is true for the default port (e.g., that
>http://host:80/ is the same as http://host/ but gopher: has 70 as a
>default port, etc.)

Given the definition of equality I proposed, http://host:80/ is
different from http://host/. The fact that they resolve to the same
thing is not part of the URL spec.

> that the same host might have multiple DNS names,
>or that some FTP servers allow case insensitive file names, any number
>of actual equivalences, symbolic links, etc.

None of these things should be part of the URL spec. But things
that are used in practice today, i.e. the significance of ?, /,
and %xx, should be.

>In the grand scheme of things, if you treat "/" and "%2F" as
>different, then at most you'll treat a few things as 'different' that
>are really the 'same', but in fact, this will be an insignificant
>amount compared to the other kinds of duplications.

In the grand scheme of things, the question is whether there's any
common structure to the "parameter package" of a URL. It sounds like
the decision is that there is not, even though this contradicts current
practice.

So the grammar for URLs is just:

	URL : IALPHA ':' CHARS
		;

with terminals:
	IALPHA =~ /[a-zA-z][a-zA-Z0-9-_]*/;
	CHARS =~ /[^ <>]*/;

I'm interested to know if the most widely deployed URL implementations
(www, Mosaix, ...) are going to change to conform to this.

Dan



From ee01th@surrey.ac.uk  Thu Mar 31 19:39:09 1994 --100
Message-Id: <9403311836.aa10042@ainur.ee.surrey.ac.uk>
Date: Thu, 31 Mar 1994 19:39:09 --100
From: ee01th@surrey.ac.uk (Piglet)
Subject: Re: Why the Web needs to change

This is an outline of and comments on replies I have
recieved...

****

> From: Marc Andreessen <marca@eit.com>
> Message-Id: <199403301018.KAA11229@threejane>
> Date: Wed, 30 Mar 1994 10:18:02 GMT
>
> The IETF URI group has been working for decades on defining such a
> beast -- the URN (Uniform Resource Name).  It's currently expected by
> the year 2009.

****

> From: "Peter Lister, Cranfield Computer Centre" <P.Lister@cran.ac.uk>
> Message-Id: <9403301036.AA02573@xdm039.ccc.cranfield.ac.uk>
> Date: Wed, 30 Mar 94 11:35:59 BST
>
> You just described URNs and local caching. URNs are not by any means unique to 
> WWW in any case and are discussed separately by uri@bunyip.com; don't 
> underestimate the scale of the issues.

How do I subscribe to uri@bunyipcom ??
>
> Local caching is happening now - there is a UK cache at unix.hensa.ac.uk.

It may well be, but what's the point unless my client know about it?
My argument is that my client ALWAYS BUT ALWAYS asks its `local'
server for the document, and that server responds with an uptodate
copy, be it locally cached, `area'ly cached, or from the source.

****

> From: "Roy T. Fielding" <fielding@simplon.ics.uci.edu>
> Message-Id:  <9403300815.aa17472@paris.ics.uci.edu>
> Date: Wed, 30 Mar 1994 08:15:49 -0800
>
> So, to make a long story short, you want hierarchical, caching proxy
> servers that use location-independent URNs.  Join the crowd ;-)
> Actually, I think you will find that URNs are only useful when the
> document is a fixed standard, such as an Internet RFC, or externally-
> published book, but that is an even longer story.

I disagree.  That's the whole point of checking to see if the
document is uptodate.  In fact, I am quite willing for the URL to be
the URN, just that the way the client gets the document be different.

>
> I suggest looking at the hypertext archive of this mailing list
> at <http://gummo.stanford.edu/html/hypermail/archives.html>
> and reading all the threads on caching, forwarding cache requests,
> and URNs.  This is going to take a while, but it's the only way to
> find out what's already been discussed.

I've looked here, and couldn't find much... maybe I'm looking for the
wrong keywords... any suggestions?

Timothy Hunt

<a href="http://www.ee.surrey.ac.uk/People/T.Hunt.html">About
Timothy</a>
<a href="http://www.ee.surrey.ac.uk/">About his department</a>



From ee01th@surrey.ac.uk  Thu Mar 31 19:48:47 1994 --100
Message-Id: <9403311845.aa10318@ainur.ee.surrey.ac.uk>
Date: Thu, 31 Mar 1994 19:48:47 --100
From: ee01th@surrey.ac.uk (Piglet)
Subject: html+ - remove <p> and </p>

first, in html we have the <p> flag to be an inter-paragraph marker
then, in html+ we have <p>...</p> to be a paragraph container.

Yet, we have arguments saying don't allow people to put <p>'s in
willy-nilly, because it may look good on their client, but really
poor on someone elses.   The argument is that the client should
decide how a document should be formatted depending on the logical
style that portion of text is in.

Why then, seeing as paragraph breaks are the most common formatting
feature, can't we have a simple blank line as paragraph break, just
like in LaTeX?
When transforming documents from some format to HTML it would be a
lot easier, and would enforce slightly more readability of the source
text.

after all, with a <p> flag, you could have the whole document without
a line break, and with a <p> legally coming in the middle of a line.

comments please?

<a href="http://www.ee.surrey.ac.uk/People/T.Hunt.html">About
Timothy</a>
<a href="http://www.ee.surrey.ac.uk/">About his department</a>




From ee01th@surrey.ac.uk  Thu Mar 31 19:53:31 1994 --100
Message-Id: <9403311848.aa10403@ainur.ee.surrey.ac.uk>
Date: Thu, 31 Mar 1994 19:53:31 --100
From: ee01th@surrey.ac.uk (Piglet)
Subject: printing postscript from xmosaic and macmosaic

When printing an html document in postscript, macmosaic seems to get
the A4 paper size right when it converts it, but xmosaic seems to
think it's paper is (approx) 10'x6' which isn't even the same aspect
ratio as A4.  Has anybody else had the same problem? and do they have
a fix for it?

<a href="http://www.ee.surrey.ac.uk/People/T.Hunt.html">About
Timothy</a>
<a href="http://www.ee.surrey.ac.uk/">About his department</a>




From timbl@ptpc00.cern.ch  Thu Mar 31 21:49:21 1994 --100
Message-Id: <9403311945.AA02575@ptpc00.cern.ch>
Date: Thu, 31 Mar 1994 21:49:21 --100
From: timbl@ptpc00.cern.ch (Tim Berners-Lee)
Subject: URIs in WWW

A couple of weeks before the IETF, I released an internet draft
which explained the WWW architecture from URIs. This is for 

infomation of the internet community.  It contains some
overlap with the URL document, but has a different basic
order of the concepts, and includes the hierahical bits,
partials, fragment identifiers, etc.  It has in fact been modified
quite a lot to fit in as much as possible with the feelings
of the URI working group of the IETF.

Unfortunately, it bounced twice due to changes to the
filename at the top of the first page, and so has only
just been released (touch wood). Expect it to come out at
your favorite site in a few days, but for now you can find it at

ftp://info.cern.ch/pub/www/doc/draft-bernerslee-www-uri-00.{txt,ps}

I hope that it helps provide where the WWW people have been coming from,
even though it is a bit terse and spec like.  It is also of course on
the web in hypertext - linked from

http://info.cern.ch/hypertext/WWW/Addressing/Addressing.html

along with everything else.  I may make one set of modifications
before its next release which should be as an informational RFC,
so I welcome any comments about its accuracy as a WWW document
or about the WWW design.

Tim Berners-Lee
CERN





From fielding@simplon.ICS.UCI.EDU  Thu Mar 31 22:06:24 1994 --100
Message-Id: <9403311201.aa16745@paris.ics.uci.edu>
Date: Thu, 31 Mar 1994 22:06:24 --100
From: fielding@simplon.ICS.UCI.EDU (Roy T. Fielding)
Subject: Re: URL decisions in Seattle, & changes 

Dan writes:

> Given the definition of equality I proposed, http://host:80/ is
> different from http://host/. The fact that they resolve to the same
> thing is not part of the URL spec.

In that case, the definition of equality must be changed.  The URLs
http://host:80/  and  http://host/  are equivalent in every sense of
the word except string comparison, and string comparison is not what
we are looking for when we compare URLs.


...Roy Fielding   ICS Grad Student, University of California, Irvine  USA
                   (fielding@ics.uci.edu)
    <A HREF="http://www.ics.uci.edu/dir/grad/Software/fielding">About Roy</A>

[Please note that our WWW server and all mail connections to UC Irvine
 will be down this weekend (April 2, 01:00 GMT through April 4, 16:00 GMT)
 due to a local power shutdown]



From Jared_Rhine@hmc.edu  Thu Mar 31 23:15:24 1994 --100
Message-Id: <199403312113.NAA17573@osiris.ac.hmc.edu>
Date: Thu, 31 Mar 1994 23:15:24 --100
From: Jared_Rhine@hmc.edu (Jared_Rhine@hmc.edu)
Subject: URL equality

[discussing how to define URL equality via the URL syntax]

Larry> the same host might have multiple DNS names, or that some FTP servers
Larry> allow case insensitive file names, any number of actual equivalences,
Larry> symbolic links, etc.

Dan> None of these things should be part of the URL spec. [...]  Given the
Dan> definition of equality I proposed, http://host:80/ is different from
Dan> http://host/. The fact that they resolve to the same thing is not part
Dan> of the URL spec.

I must agree that we should avoid burdening the syntax with any conception
of equality beyond string equality.  In the long run, if one wished to
checked for equality, one should check the associated URNs/URSNs.

If two URLs point to the same resource, all that signifies is there are two
ways to access the resource (two _locations_, if you will).  URLs should be
concerned (almost) exclusively with access, not naming.

--
Jared Rhine         Jared_Rhine@hmc.edu
wibstr              Harvey Mudd College
                    http://www.hmc.edu/www/people/jared.html

"To hear many religious people talk, one would think God created the
 torso, head, legs and arms, but the devil slapped on the genitals."
        -- Don Schrader



